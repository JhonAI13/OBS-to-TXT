

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\indice - Copia.md
### Última modificação: 2025-05-28 21:13:26


**Módulo Adicional: Curso demonstrativo**
(5 capítulos)

*   ## 01 Introdução
    (1 lição, Opcional)
    *   O que esperar deste curso (Teoria, Opcional)
*   ## 02 Análise Exploratória de Dados
    (4 lições, Opcional)
    *   Introdução à Análise Exploratória de Dados (Teoria, Opcional)
    *   Análise de uma Variável (Teoria, Opcional)
    *   Análise de duas Variáveis (Teoria, Opcional)
    *   Conclusões (Teoria, Opcional)
*   ## 03 Narrativa de Dados
    (8 lições, Opcional)
    *   Introdução a narrativa (Teoria, Opcional)
    *   Extraindo dados com SQL (Teoria, Opcional)
    *   Filtrando resultados em SQL (Teoria, Opcional)
    *   Juntando tabelas em SQL (Teoria, Opcional)
    *   Primeiros passos com o Tableau (Teoria, Opcional)
    *   Transformando dados em recursos visuais (Teoria, Opcional)
    *   Construindo o nosso painel no Tableau (Teoria, Opcional)
    *   Conclusões (Teoria, Opcional)
*   ## 04 Prevendo o Futuro
    (6 lições, Opcional)
    *   Introdução (Teoria, Opcional)
    *   Análise Exploratória de Dados (Teoria, Opcional)
    *   Geração de hipóteses (Teoria, Opcional)
    *   Correlação (Teoria, Opcional)
    *   Introdução à Regressão Linear (Teoria, Opcional)
    *   Conclusões (Teoria, Opcional)
*   ## 05 Mudando o Futuro
    (3 lições, Opcional)
    *   Introdução (Teoria, Opcional)
    *   Introdução ao Aprendizado de Máquina (Teoria, Opcional)
    *   Avaliando a qualidade de seu modelo (Teoria, Opcional)

---

**Onboarding: É bom ver você na TripleTen!**
(14 aulas)

*   Olá! (Teoria)
*   Agile: sprints e projetos (Teoria)
*   Como ajudamos você a aprender (Teoria)
*   Prepare-se para estudar (Teoria)
*   Introdução ao letramento digital (Teoria)
*   Entender os sistemas operacionais (Teoria)
*   Arquivos e navegação (Teoria)
*   Instalação e atualização de aplicativos (Teoria)
*   Colaboração em nuvem: Google Drive (Teoria)
*   Capturas de tela e gravações de tela (Teoria)
*   Ferramentas de videoconferência: Zoom e Google Meet (Teoria)
*   Quiz: letramento digital (Teoria)
*   Carta para o seu futuro eu (Teoria)
*   Feedback (Teoria)

---

**Módulo Adicional: Excel: Trabalhando com Dados**
(3 capítulos restantes)

*   ## 01 Módulo 1. Análise de Dados
    (8 lições, Opcional)
    *   Introdução (Teoria, Opcional)
    *   Mil linhas e um relatório (Teoria, Opcional)
    *   Lição Bônus (Teoria, Opcional)
    *   SUMIF: calcular uma soma com base em critérios (Teoria, Opcional)
    *   Relatório avançado (Teoria, Opcional)
    *   Como otimizar fórmulas do Excel (Teoria, Opcional)
    *   Fórmulas e funções numéricas básicas (Teoria, Opcional)
    *   Tabelas dinâmicas (Teoria, Opcional)
*   ## 02 Módulo 2. Trabalhando com Planilhas
    (6 lições, Opcional)
    *   Introdução (Teoria, Opcional)
    *   Processamento de dados, parte 1 (Teoria, Opcional)
    *   Processamento de dados, parte 2 (Teoria, Opcional)
    *   Funções lógicas (Teoria, Opcional)
    *   VLOOKUP (Teoria, Opcional)
    *   Importar dados de outras tabelas (Teoria, Opcional)
*   ## 03 Módulo 3. Visualização de dados
    (7 lições, Opcional)
    *   Introdução (Teoria, Opcional)
    *   Formatação de tabelas (Teoria, Opcional)
    *   Tipos de gráficos (Teoria, Opcional)
    *   Configurações de gráficos (Teoria, Opcional)
    *   Personalização de gráficos (Teoria, Opcional)
    *   Avaliação Final (Teoria, Opcional)
    *   Conclusão (Teoria, Opcional)

---

**Python básico**
(9 capítulos)

*   ## 01 Conheça a sua nova profissão
    (4 lições)
    *   Conheça a sua nova profissão (1 tarefa)
    *   Módulo 1 – Introdução à análise de dados (Teoria)
    *   Introdução à sandbox e ao Jupyter Notebook (Teoria)
    *   Experimente o notebook Jupyter (1 tarefa, Opcional)
*   ## 02 Variáveis, Tipos de Dados, e Operações Aritméticas
    (10 lições)
    *   Introdução (Teoria)
    *   Por que Python? (Teoria)
    *   Variáveis (Teoria)
    *   Impressão (Teoria)
    *   Operações aritméticas (Teoria)
    *   Tipos de dados (Teoria)
    *   Conversão de tipos de dados (Teoria)
    *   Mensagens de erro (Teoria)
    *   Atividade prática do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 03 Strings
    (9 lições)
    *   Introdução (Teoria)
    *   Propriedades básicas das strings (Teoria)
    *   Continuação das propriedades básicas das strings (Teoria)
    *   Índices (Teoria)
    *   Fatias (Teoria)
    *   Strings formatadas (Teoria)
    *   Métodos de strings (Teoria)
    *   Atividade prática do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 04 Listas
    (9 lições)
    *   Introdução (Teoria)
    *   Propriedades básicas das listas (Teoria)
    *   Índices e fatias (Teoria)
    *   Modificação de listas (Teoria)
    *   Ordenação de listas (Teoria)
    *   Listas aninhadas (Teoria)
    *   Processamento de strings (Teoria)
    *   Atividade prática do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 05 Ciclos
    (7 lições)
    *   Introdução (Teoria)
    *   Operadores de atribuição composta e funções integradas (Teoria)
    *   Ciclos For (Teoria)
    *   Percorrer listas aninhadas (Teoria)
    *   Ciclos While (Teoria)
    *   Atividade prática do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 06 Instruções Condicionais
    (9 lições)
    *   Introdução (Teoria)
    *   Expressões lógicas e operadores de comparação (Teoria)
    *   Instruções if e else (Teoria)
    *   Criação de várias ramificações com else e elif (Teoria)
    *   Modificações em tabelas (Teoria)
    *   Filtragem de tabelas (Teoria)
    *   Operadores lógicos e filtros (Teoria)
    *   Atividade prática do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 07 Uma Olhadinha Rápida no Jupyter Notebook
    (2 lições)
    *   Introdução ao Jupyter Notebook (Teoria)
    *   Seu primeiro caderno de teste (1 tarefa, Opcional)
*   ## 08 Projeto Final
    (2 lições)
    *   Seu primeiro projeto (Teoria)
    *   Sprint 1 - Projeto (Projeto, 1 tarefa)
*   ## 09 Conclusão
    (2 lições)
    *   Feedback do Sprint 1 (Teoria)
    *   Conclusão (Teoria)

---

**Sprint 2: Continuação de Python básico**
(7 capítulos)

*   ## 01 Introdução
    (1 lição)
    *   Introdução (Teoria)
*   ## 02 Dicionários
    (6 lições)
    *   Introdução aos dicionários (Teoria)
    *   Propriedades de dicionários (Teoria)
    *   Como extrair valores de dicionários (Teoria)
    *   Adição e remoção de itens (Teoria)
    *   Atividade prática do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 03 Funções
    (7 lições)
    *   Introdução (Teoria)
    *   Sintaxe das funções (Teoria)
    *   Uso de funções para simplificar códigos (Teoria)
    *   Funções de filtragem (Teoria)
    *   Variáveis globais e locais (Teoria)
    *   Atividade prática do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 04 A biblioteca Pandas
    (9 lições)
    *   Introdução (Teoria)
    *   DataFrames na Biblioteca Pandas (Teoria)
    *   Importação e uso de pacotes (Teoria)
    *   Pandas para Excel e arquivos CSV (Teoria)
    *   Indexação do DataFrame (Teoria)
    *   Filtragem com indexação lógica (Teoria)
    *   Vamos contar e somar (Teoria)
    *   Atividade prática do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 05 Introdução ao pré-processamento de dados e análise inicial
    (9 lições)
    *   Introdução (Teoria)
    *   Problemas com dados: garbage in, garbage out (Teoria)
    *   Renomear colunas (Teoria)
    *   Processamento de valores ausentes (Teoria)
    *   Processamento de valores duplicados (Teoria)
    *   Agrupamento de dados (Teoria)
    *   Ordenamento de dados (Teoria)
    *   Quiz do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 06 Projeto Final
    (2 lições)
    *   Seu segundo projeto (Teoria)
    *   Sprint 2 - Projeto (Projeto, 1 tarefa)
*   ## 07 Conclusão
    (2 lições)
    *   Feedback do Sprint 2 (Teoria)
    *   Conclusão (Teoria)

---

**Sprint 3: Manipulação de dados**
(11 capítulos)

*   ## 01 Introdução
    (1 lição)
    *   Introdução (Teoria)
*   ## 02 Leitura e visualização de dados
    (7 lições)
    *   Introdução (Teoria)
    *   Como resolver problemas com arquivos CSV (Teoria)
    *   Como ler arquivos do Excel (Teoria)
    *   Olhando para os nossos dados (Teoria)
    *   Descrições numéricas e describe() (3 tarefas)
    *   Quiz do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 03 Tratamento de valores ausentes e duplicados
    (8 lições)
    *   Introdução (Teoria)
    *   Contagem de valores ausentes (Teoria)
    *   Filtragem de DataFrames com NaNs (2 tarefas)
    *   Preenchimento de valores categóricos ausentes (Teoria)
    *   Preenchimento de valores quantitativos ausentes (Teoria)
    *   Como lidar com duplicados (Teoria)
    *   Quiz do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 04 Filtragem de dados
    (8 lições)
    *   Introdução (Teoria)
    *   Índices em DataFrames e Series (Teoria)
    *   Filtragem personalizada usando query() (Teoria)
    *   Uso de estruturas de dados externas para filtrar DataFrames (Teoria)
    *   Filtragem com base em várias condições (Teoria)
    *   Substituição de valores com where() (Teoria)
    *   Quiz do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 05 Estudo de caso
    (1 lição)
    *   Estudo de caso (11 tarefas)
*   ## 06 Visualização de dados
    (12 lições)
    *   Introdução (Teoria)
    *   Visualização de dados: visão geral (Teoria)
    *   O que podemos aprender com visualizações ruins (Teoria)
    *   Como criar gráficos em Python com matplotlib (1 tarefa)
    *   Gráficos de dispersão (1 tarefa)
    *   Correlação (Teoria)
    *   Matrizes de dispersão (1 tarefa)
    *   Gráficos de linha (Teoria)
    *   Gráficos de barras (1 tarefa)
    *   Histogramas (2 tarefas)
    *   Quiz do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 07 Tipos de dados
    (7 lições)
    *   Introdução (Teoria)
    *   Conjunto de dados à primeira vista (Teoria)
    *   Como trabalhar com tipos de dados numéricos e de string (3 tarefas)
    *   Como trabalhar com datas e horas (Teoria)
    *   Como trabalhar com atributos datetime e fusos horários (3 tarefas)
    *   Quiz do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 08 Engenharia de características
    (6 lições)
    *   Introdução (Teoria)
    *   Criação de novas colunas com base em valores de outras colunas (1 tarefa)
    *   Criação de colunas categóricas com apply() (3 tarefas)
    *   Criação de categorias com funções de tratamento de linhas (2 tarefas)
    *   Quiz do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 09 Transformações de dados
    (7 lições)
    *   Introdução (Teoria)
    *   Processamento de dados agrupados com agg() (1 tarefa)
    *   Tabelas dinâmicas (1 tarefa)
    *   Combinação de DataFrames com concat() (2 tarefas)
    *   Combinação de DataFrames com merge() (2 tarefas)
    *   Quiz do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 10 Projeto
    (2 lições)
    *   Conheça o segundo projeto (Teoria)
    *   Sprint 3 - Projeto (Projeto, 1 tarefa)
*   ## 11 Conclusão
    (2 lições)
    *   Feedback do Sprint 3 (Teoria)
    *   Conclusão (Teoria)

---

**Sprint 4: Análise Estatística de Dados**
(6 capítulos)

*   ## 01 Introdução
    (1 lição)
    *   Introdução (Teoria)
*   ## 02 Estatística descritiva
    (10 lições)
    *   Introdução (Teoria)
    *   Variáveis contínuas e discretas (Teoria)
    *   Histogramas de frequência (Teoria)
    *   Histogramas de densidade (Teoria)
    *   Medidas de localização e variabilidade (Teoria)
    *   Variância (Teoria)
    *   Desvio padrão e distribuição normal (Teoria)
    *   Dados assimétricos e valores atípicos (Teoria)
    *   Quiz do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 03 Teoria da probabilidade
    (10 lições)
    *   Introdução (Teoria)
    *   Como prever o futuro? (Teoria)
    *   Eventos independentes e multiplicação de probabilidades (Teoria)
    *   Fundamentos da combinatória e mais problemas de probabilidades (Teoria)
    *   A ideia de distribuição (Teoria)
    *   Como trabalhar com uma distribuição binomial (Teoria)
    *   Como trabalhar com uma distribuição normal (Teoria)
    *   Combinação de distribuições binomial e normal (Teoria)
    *   Quiz do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 04 Testes de hipóteses
    (9 lições)
    *   Introdução (Teoria)
    *   Amostras aleatórias e médias amostrais (Teoria)
    *   Formulação de hipóteses bicaudais (Teoria)
    *   Testes de hipóteses em Python. Valores-p (Teoria)
    *   Formulação de hipóteses unicaudais (Teoria)
    *   Hipótese sobre a igualdade das médias de duas populações (Teoria)
    *   Hipótese sobre a igualdade das médias de amostras pareadas (Teoria)
    *   Quiz do capítulo (Teoria)
    *   Conclusão (Teoria)
*   ## 05 Projeto
    (1 lição)
    *   Sprint 4 - Projeto (Projeto, 1 tarefa)
*   ## 06 Conclusão
    (2 lições)
    *   Feedback do Sprint 4 (Teoria)
    *   Conclusão (Teoria)

---

**Sprint 5: Ferramentas de Desenvolvimento de Software**
(8 capítulos)

*   ## 01 Introdução
    (1 lição)
    *   Introdução (Teoria)
*   ## 02 Introdução à linha de comando
    (18 lições)
    *   Introdução (Teoria)
    *   Como abrir o Terminal no MacOS (Teoria)
    *   Como abrir o Terminal no Windows (Teoria)
    *   Como abrir o Terminal em um SO baseado em Linux (Teoria)
    *   Como executar seu primeiro comando (Teoria)
    *   Navegação no sistema de arquivos (Teoria)
    *   Dicas e truques (Teoria)
    *   Como trabalhar com arquivos e diretórios: parte 1 (Teoria)
    *   Como trabalhar com arquivos e diretórios: parte 2 (Teoria)
    *   Curingas (Teoria)
    *   Como visualizar e editar arquivos: parte 1 (Teoria)
    *   Como visualizar e editar arquivos: parte 2 (Teoria)
    *   Comandos de busca (Teoria)
    *   Alias (Teoria)
    *   Variáveis de ambiente (Teoria)
    *   Pipes (Teoria)
    *   Histórico (Teoria)
    *   Conclusão (Teoria)
*   ## 03 Ambiente de programação
    (8 lições)
    *   Introdução (Teoria)
    *   Ambientes de programação (Teoria)
    *   Instalação do Python por meio da distribuição Anaconda (Windows) (Teoria)
    *   Instalação do Python por meio da distribuição Anaconda (Mac) (Teoria)
    *   Como executar código Python na linha de comando (Teoria)
    *   Como escrever e executar código no Jupyter Notebook (Teoria)
    *   Como instalar e usar módulos e pacotes (Teoria)
    *   Conclusão (Teoria)
*   ## 04 Git e GitHub
    (7 lições)
    *   Introdução (Teoria)
    *   Controle de versão (Teoria)
    *   Git (Teoria)
    *   GitHub (Teoria)
    *   Repositórios (Teoria)
    *   Confirmar alterações (Teoria)
    *   Conclusão (Teoria)
*   ## 05 Python intermediário
    (11 lições)
    *   Introdução (Teoria)
    *   Adição de flexibilidade a scripts Python (Teoria)
    *   Funções (Teoria)
    *   Tratamento de erros (Teoria)
    *   Módulos e estrutura do projeto (Teoria)
    *   Programação orientada a objetos (5 tarefas)
    *   Entrada e saída de arquivos (File I/O) (3 tarefas)
    *   API Web (3 tarefas)
    *   Aplicativos web (Teoria)
    *   Qualidade do código (Teoria)
    *   Conclusão (Teoria)
*   ## 06 Ambiente de programação individual
    (10 lições)
    *   Introdução (Teoria)
    *   O que é um IDE? (Teoria)
    *   Instalação do VS Code (Teoria)
    *   Trabalho no VS Code (Teoria)
    *   Clonar um repositório remoto Git (Teoria)
    *   Controle de origem no VS Code (Teoria)
    *   Jupyter Notebook no VS Code (Teoria)
    *   Linting e formatação de código (Teoria)
    *   JupyterLab (Teoria)
    *   Conclusão (Teoria)
*   ## 07 Projeto
    (1 lição)
    *   Sprint 5 - Projeto (Projeto)
*   ## 08 Conclusão
    (2 lições)
    *   Feedback do Sprint 5 (Teoria)
    *   Conclusão (Teoria)

---

**Sprint 6: Projeto Integrado 1**
(5 capítulos)

*   ## 01 Conclusões fundamentals do primeiro módulo
    (1 lição)
    *   Principais conclusões do primeiro módulo (Teoria)
*   ## 02 Uso de documentação
    (2 lições)
    *   Como trabalhar com documentação (Teoria)
    *   Como ler erros (Teoria)
*   ## 03 Python em mais detalhes
    (9 lições)
    *   Introdução (Teoria)
    *   Dicionários: uso de ciclos em dicionários (Teoria)
    *   Dicionários: estruturas de dados aninhadas com dicionários (Teoria)
    *   Dicionários: processamento de listas de dicionários (Teoria)
    *   Funções: parâmetros e valores padrão (Teoria)
    *   Funções: retorno de valores (Teoria)
    *   Pandas: o objeto Series (Teoria)
    *   Pandas: estatística descritiva (Teoria)
    *   Conclusão (Teoria)
*   ## 04 Projeto integrado
    (2 lições)
    *   Sprint 6 - Projeto (Projeto, 1 tarefa)
    *   Feedback do Sprint 6 (Teoria)
*   ## 05 Agora você é um Aspirante a Analista de Dados!
    (1 lição)
    *   Agora você é um Aspirante a Analista de Dados! (Teoria)

---

**Módulo Adicional: Sandbox**
(2 capítulos restantes)

*   ## 01 Seu espaço pessoal de trabalho
    (3 lições, Opcional)
    *   O que é uma sandbox? (Teoria, Opcional)
    *   IDE (1 tarefa, Opcional)
    *   Seu próprio notebook Jupyter (1 tarefa, Opcional)
*   ## 02 Materiais adicionais deste sprint de Python Básico
    (6 lições, Opcional)
    *   Operações com strings (Teoria, Opcional)
    *   A função range() (Teoria, Opcional)
    *   Contadores (Teoria, Opcional)
    *   Expressões Lógicas Compostas (3 tarefas, Opcional)
    *   Listas de Dicionários (2 tarefas, Opcional)
    *   Combinando listas e funções (Teoria, Opcional)

---

**Sprint 7: Coleta e Armazenamento de Dados (SQL)**
(8 capítulos)

*   ## 01 Introdução à Coleção e Armazenamento de Dados (SQL)
    (2 lições)
    *   Introdução (Teoria)
    *   O Processo da Análise de Dados (Teoria)
*   ## 02 Extraindo Dados de Recursos Online
    (12 lições)
    *   Introdução (Teoria)
    *   O que é a Mineração de Dados na Web (Web Mining)? (Teoria)
    *   Algumas Coisas que um Analista Deve Saber sobre a Internet. (Teoria)
    *   Protocolos de Transferência (Teoria)
    *   Introdução a HTML (Teoria)
    *   Ferramentas de desenvolvedor (Teoria)
    *   Sua Primeira Requisição GET (2 tarefas)
    *   Expressões Regulares (3 tarefas)
    *   Análise sintática de HTML (4 tarefas)
    *   API (2 tarefas)
    *   JSON (2 tarefas)
    *   Conclusão (Teoria)
*   ## 03 SQL como uma Ferramenta para Trabalhar com Dados
    (8 lições)
    *   Introdução (Teoria)
    *   Bancos de Dados e Tabelas (Teoria)
    *   Tabelas (Teoria)
    *   Sua primeira instrução SQL (4 tarefas)
    *   Fatias de Dados em SQL (3 tarefas)
    *   Funções de agregação (7 tarefas)
    *   Convertendo Tipos de Dados (4 tarefas)
    *   Conclusão (Teoria)
*   ## 04 Funcionalidades Avançadas de SQL para Análise
    (9 lições)
    *   Introdução (Teoria)
    *   Agrupando Dados (4 tarefas)
    *   Ordenando Dados (3 tarefas)
    *   Processando os Dados dentro de um Agrupamento (3 tarefas)
    *   Operadores e Funções para Trabalhar com Datas (4 tarefas)
    *   Subconsultas (4 tarefas)
    *   Funções de Janela (2 tarefas)
    *   Uma análise mais detalhada das funções de janela (2 tarefas)
    *   Conclusão (Teoria)
*   ## 05 Relacionamentos Entre Tabelas
    (14 lições)
    *   Introdução (Teoria)
    *   Tipos de Relacionamentos entre Tabelas (Teoria)
    *   Diagramas ER (Teoria)
    *   Prazer em Conhecê-las, Tabelas! (Teoria)
    *   Tipos de Usuários de Bancos de Dados (Teoria)
    *   Procurando por Valores Vazios (4 tarefas)
    *   Procurando por Dados na Tabela (2 tarefas)
    *   JUNÇÃO. JUNÇÃO INTERNA. (5 tarefas)
    *   Junção externa. LEFT JOIN (3 tarefas)
    *   Junção externa. RIGHT JOIN (2 tarefas)
    *   Juntando múltiplas tabelas (3 tarefas)
    *   Combinação de agregações e junções (5 tarefas)
    *   Juntando dados de instruções (5 tarefas)
    *   Conclusão (Teoria)
*   ## 06 Soft Skills
    (5 lições)
    *   Introdução (Teoria)
    *   Contexto da Tarefa (Teoria)
    *   Elaboração da Tarefa (Teoria)
    *   Revisão Adicional (Teoria)
    *   Conclusão (Teoria)
*   ## 07 Projeto do Curso
    (5 lições)
    *   Resultados do Curso (Teoria)
    *   Descrição do Projeto (Teoria)
    *   Analisando dados (1 tarefa)
    *   Trabalhando com Bancos de Dados (6 tarefas)
    *   Sprint 7 - Projeto (Projeto, 1 tarefa)
*   ## 08 Conclusão
    (2 lições)
    *   Feedback do Sprint 7 (Teoria)
    *   Você é um Campeão de SQL! (Teoria)

---

*(Repetição do Módulo Adicional: Curso demonstrativo - omitido para brevidade, pois é idêntico ao primeiro listado)*

---

*(Repetição do Onboarding - omitido para brevidade, pois é idêntico ao primeiro listado)*

---

**Sprint 8: Análise de Negócio**
(9 capítulos)

*   ## 01 Introdução à Análise de Negócios
    (1 lição)
    *   Introdução à Análise de Negócios (Teoria)
*   ## 02 Métricas e Funis
    (9 lições)
    *   Introdução (Teoria)
    *   Receita, Renda e Retorno do Investimento (Teoria)
    *   Conversão (Teoria)
    *   Funis (Teoria)
    *   O Funil de Marketing: Impressões, Cliques, CTR e CR (2 tarefas)
    *   O Funil de Marketing: Agrupamento por Semanas e Meses (2 tarefas)
    *   Funis de Produtos Simples (2 tarefas)
    *   Funis de Produtos com Sequência de Eventos (3 tarefas)
    *   Conclusão (Teoria)
*   ## 03 Análise de Coorte
    (11 lições)
    *   Introdução (Teoria)
    *   Análise de Coorte (Teoria)
    *   Análise de Coorte em Python (4 tarefas)
    *   Avaliando Mudanças em Valores Absolutos por Mês (Teoria)
    *   Avaliando Mudanças em Valores Relativos pelo Tempo de Vida (4 tarefas)
    *   Visualizando a Análise de Coorte (1 tarefa)
    *   Taxa de Retenção e Índice de Cancelamento (Teoria)
    *   Calculando Taxas de Retenção em Python (3 tarefas)
    *   Calculando índices de cancelamento em Python (4 tarefas)
    *   Coortes Comportamentais (6 tarefas)
    *   Conclusão (Teoria)
*   ## 04 Unit Economics (Economia Unitária)
    (6 lições)
    *   Introdução (Teoria)
    *   Economia Unitária por Venda (Teoria)
    *   Economia Unitária por Venda: Construindo um Modelo (3 tarefas)
    *   Economia Unitária por Cliente: LTV e CAC (Teoria)
    *   Economia Unitária por Cliente para uma Loja Online (5 tarefas)
    *   Conclusão (Teoria)
*   ## 05 Métricas de Usuário
    (9 lições)
    *   Introdução (Teoria)
    *   Avaliando a Atividade do Usuário (2 tarefas)
    *   Sessões do Usuário (3 tarefas)
    *   Frameworks de Métricas (Teoria)
    *   Detecção de Anomalias (Teoria)
    *   Y.Metrica (Teoria)
    *   Y.Metrica API (Teoria)
    *   Trabalhando com Dados Brutos (5 tarefas)
    *   Conclusão (Teoria)
*   ## 06 Soft Skills
    (1 lição)
    *   O Que Você Precisa Descobrir Sobre uma Empresa Durante sua Primeira Semana (Teoria)
*   ## 07 Licâo Bônus: Otimizando Dados em pandas
    (1 lição)
    *   Otimizando Dados em pandas (Teoria)
*   ## 08 Projeto do Curso
    (1 lição)
    *   Sprint 8 - Projeto (Projeto, 1 tarefa)
*   ## 09 Conclusão
    (2 lições)
    *   Feedback do Sprint 8 (Teoria)
    *   Conclusão (Teoria)

---

**Sprint 9: Tomando Decisões de Negócios Baseadas em Dados**
(9 capítulos)

*   ## 01 Introdução a Tomando Decisões de Negócios Baseados em Dados
    (1 lição)
    *   Introdução a "Tomando Decisões de Negócios Baseados em Dados". (Teoria)
*   ## 02 Introdução aos Testes de Hipóteses em Negócios
    (9 lições)
    *   Introdução (Teoria)
    *   O Que um Negócio Precisa? (Teoria)
    *   Principais Métricas. Decomposição (Teoria)
    *   Identificando Métricas Importantes (Teoria)
    *   Diários de Experimento (Teoria)
    *   Fazer ou Não Fazer Experimentos? Eis a Questão. (Teoria)
    *   Como Gerar Ideias (Teoria)
    *   Formulando uma Boa Hipótese (Teoria)
    *   Conclusão (Teoria)
*   ## 03 Escolhendo um Método Experimental
    (7 lições)
    *   Introdução (Teoria)
    *   Métodos Experimentais (Teoria)
    *   Métodos Qualitativos de Testes de Hipóteses (Teoria)
    *   Métodos quantitativos de testes de hipóteses: teste A/B (Teoria)
    *   Prós e contras do teste A/B (Teoria)
    *   Um exemplo de resultados de teste A/B (2 tarefas)
    *   Conclusão (Teoria)
*   ## 04 Priorizando Hipóteses
    (7 lições)
    *   Introdução (Teoria)
    *   Como e por que priorizamos hipóteses (3 tarefas)
    *   O parâmetro de Alcance (Teoria)
    *   O parâmetro de Impacto (Teoria)
    *   O parâmetro de Esforço (Teoria)
    *   O parâmetro de confiança (Teoria)
    *   Conclusão (Teoria)
*   ## 05 Como se preparar para um Teste A/B
    (7 lições)
    *   Introdução (Teoria)
    *   Testes A/A (Teoria)
    *   Erros do Tipo I e II em Testes de Hipóteses. Poder e Significância (Teoria)
    *   Comparações Múltiplas: Testes A/A e A/B (Teoria)
    *   Calculando o Tamanho da Amostra e a Duração do Teste (Teoria)
    *   Análise Gráfica de Métricas e Definindo a Indústria (Teoria)
    *   Conclusão (Teoria)
*   ## 06 Analisando os Resultados de um Teste A/B
    (10 lições)
    *   Introdução (Teoria)
    *   Testando a Hipótese de que as Proporções são Iguais (1 tarefa)
    *   Testes de Normalidade. O Teste Shapiro–Wilk (1 tarefa)
    *   O Teste Não-Paramétrico Wilcoxon-Mann-Whitney (1 tarefa)
    *   Estabilidade de Métricas Cumulativas (6 tarefas)
    *   Analisando Valores Atípicos e Altas: Valores Extremos (7 tarefas)
    *   Análise Passo a Passo de um Teste A/B (5 tarefas)
    *   As Tão Esperadas Conclusões do Teste A/B (Teoria)
    *   Erros Comuns na Análise do Teste A/B (Teoria)
    *   Conclusão (Teoria)
*   ## 07 Soft Skills
    (2 lições)
    *   Fatos, Emoções e Avaliações (Teoria)
    *   Expressar Claramente sua Posição (Teoria)
*   ## 08 Projeto do Curso
    (1 lição)
    *   Sprint 9 - Projeto (Projeto, 1 tarefa)
*   ## 09 Conclusão
    (3 lições)
    *   Feedback do Sprint 9 (Teoria)
    *   Conclusão (Teoria)
    *   Preparação de Carreira 60% (Teoria)

---

**Sprint 10: Como Relatar uma História Usando Dados**
(6 capítulos)

*   ## 01 Introdução a Como Relatar uma História Usando Dados
    (2 lições)
    *   Introdução a Como Relatar uma História Usando Dados (Teoria)
    *   O Processo da Análise de Dados (Teoria)
*   ## 02 Preparando Apresentações
    (12 lições)
    *   Introdução (Teoria)
    *   Como Falar Sobre a Sua Pesquisa (Teoria)
    *   Para Quem Contar Sua História (Teoria)
    *   Sazonalidade e Fatores Externos (Teoria)
    *   Vamos Comprar as Passagens de Avião (Teoria)
    *   Valores Absolutos e Relativos (3 tarefas)
    *   Paradoxo de Simpson (Teoria)
    *   Quando os Gráficos são os Inimigos (Teoria)
    *   Diferentes Gráficos para Diferentes Tipos de Informação (1 tarefa)
    *   Noções Básicas de Fazer Apresentações (Teoria)
    *   Relatórios no Jupyter Notebook (Teoria)
    *   Conclusão (Teoria)
*   ## 03 A biblioteca Seaborn
    (9 lições)
    *   Introdução (Teoria)
    *   Por que apenas o Matplotlib não é o suficiente (2 tarefas)
    *   O Método jointplot() (3 tarefas)
    *   Paletas de cores (1 tarefa)
    *   Estilos de Gráfico (1 tarefa)
    *   Dados Categóricos (2 tarefas)
    *   Visualizando Distribuições (2 tarefas)
    *   Gráficos Especiais em Seaborn (4 tarefas)
    *   Conclusão (Teoria)
*   ## 04 A biblioteca Plotly
    (7 lições)
    *   Introdução (Teoria)
    *   Gráficos Interativos (Teoria)
    *   Instalando o Python e o Jupyter Notebook (Teoria)
    *   Gráficos Básicos da Biblioteca plotly (Teoria)
    *   Gráficos de Pizza (Teoria)
    *   Gráficos de funis de vendas (Teoria)
    *   Conclusão (Teoria)
*   ## 05 Projeto do Curso
    (1 lição)
    *   Sprint 10 - Projeto (Projeto, 1 tarefa)
*   ## 06 Conclusão
    (3 lições)
    *   Feedback do Sprint 10 (Teoria)
    *   Conclusão (Teoria)
    *   Você é um Mestre em Narrativa de Dados! (Teoria)

---

**Sprint 11: Projeto Integrado 2**
(2 capítulos)

*   ## 01 Análise Baseada em Eventos
    (6 lições)
    *   Introdução (Teoria)
    *   A Essência de Análise Baseada em Eventos (Teoria)
    *   Como Eventos São Rastreados (Teoria)
    *   Métodos de Análise Baseada em Eventos (Teoria)
    *   O Momento "Aha!" (Teoria)
    *   Conclusão (Teoria)
*   ## 02 Materiais para revisão
    (3 lições)
    *   Sprint 11 - Projeto (Projeto, 1 tarefa)
    *   Feedback do Sprint 11 (Teoria)
    *   Preparação de Carreira 80% (Teoria)

---

**Sprint 12: Automação**
(6 capítulos)

*   ## 01 Introduções à Automação
    (1 lição)
    *   Introdução (Teoria)
*   ## 02 Pipelines de Dados e Para Que Servem
    (4 lições)
    *   Introdução (Teoria)
    *   Automatizando Dashboards com Dados Pipelines (Teoria)
    *   Criando um Script de Pipeline (Teoria, Opcional)
    *   Conclusão (Teoria)
*   ## 03 Desenhando e Criando Dashboards com Dash
    (10 lições)
    *   Introdução (Teoria)
    *   Dashboards (4 tarefas)
    *   Coletando Requisitos ao Construir um Dashboard (Teoria)
    *   Criando Gráficos Básicos no Dash (8 tarefas)
    *   Noções Básicas de Trabalho com Controles (1 tarefa, Opcional)
    *   Controles Básicos no Dash (1 tarefa, Opcional)
    *   Controles e Interatividade (4 tarefas)
    *   Elementos do Dashboard (5 tarefas)
    *   Desenvolvimento do Dashboard e o Básico da Composição (7 tarefas)
    *   Conclusão (Teoria)
*   ## 04 Tableau
    (12 lições)
    *   Introdução (Teoria)
    *   Começando com Tableau Public (Teoria)
    *   O Básico do Tableau (Teoria)
    *   Preparando Dados (Teoria)
    *   Tabelas e Cálculos Simples (Teoria)
    *   Filtros (Teoria)
    *   Publicando Dashboards (Teoria)
    *   Gráficos Simples (Teoria)
    *   Gráficos Lineares e Gráficos de Área Empilhada (Teoria)
    *   Gráficos Especiais (Teoria)
    *   Construindo um Dashboard (Teoria)
    *   Conclusão (Teoria)
*   ## 05 Projeto do Curso
    (4 lições)
    *   Projeto do Curso (Teoria)
    *   Parte 1. Elaboração de Requisitos Técnicos (Teoria)
    *   Parte 2. Construindo o dashboard (Teoria)
    *   Sprint 12 - Projeto (Projeto)
*   ## 06 Conclusão
    (3 lições)
    *   Feedback do Sprint 12 (Teoria)
    *   Conclusão (Teoria)
    *   Agora você é um Gênio dos Dashboards (Teoria)

---

**Módulo Adicional: Suplementos de Automação**
(5 capítulos restantes)

*   ## 01 Introdução a Suplementos de Automação
    (1 lição, Opcional)
    *   Introdução a Suplementos de Automação (Teoria, Opcional)
*   ## 02 O Básico de Como Executar Scripts
    (11 lições, Opcional)
    *   Introdução (Teoria, Opcional)
    *   Linhas de Comando Básicas (Teoria, Opcional)
    *   Acessando a Linha de Comando da sua Máquina Local (Teoria, Opcional)
    *   Interface de Comando Básica (Teoria, Opcional)
    *   Tarefa Independente 1: Definindo uma conta AWS (Teoria, Opcional)
    *   Tarefa Independente 2: Instalando Python em uma Máquina Local (Teoria, Opcional)
    *   Iniciando um Script a partir da Linha de Comando (3 tarefas, Opcional)
    *   Programação de Scripts (Teoria, Opcional)
    *   Checklist de Depuração de Problemas cron (Teoria, Opcional)
    *   Tarefa independente 3: Iniciando um Script a Partir das Linhas de Comando no AWS (Teoria, Opcional)
    *   Conclusão (Teoria, Opcional)
*   ## 03 Mais sobre Pipelines de Dados
    (8 lições, Opcional)
    *   Introdução (Teoria, Opcional)
    *   Tarefa Independente 1: Implantando Servidor de Banco de Dados (Teoria, Opcional)
    *   Tarefa independente 2: Trabalhando com Backups de Banco de Dados (Teoria, Opcional)
    *   Tarefa Independente 3: Fazendo Backup de um Banco de Dados (Teoria, Opcional)
    *   Agregando Вados e Сriando Tabelas em Bancos de Dados (Teoria, Opcional)
    *   Tabelas Verticais e Horizontais (Teoria, Opcional)
    *   Trabalhando com o Postgres SGBD do script Python (Teoria, Opcional)
    *   Conclusão (Teoria, Opcional)
*   ## 04 Mais sobre Dash
    (4 lições, Opcional)
    *   Introdução (Teoria, Opcional)
    *   Tarefa Independente 1: Iniciando Dashboard em uma Máquina Local (Teoria, Opcional)
    *   Tarefa Independente 2: Iniciando Dashboard em uma Máquina Virtual (Teoria, Opcional)
    *   Conclusão (Teoria, Opcional)
*   ## 05 Mais sobre Tableau
    (1 lição, Opcional)
    *   Preparando Dados de um Banco de Dados Implementados em uma Máquina Virtual (Teoria, Opcional)
*   ## 06 Conclusão
    (1 lição, Opcional)
    *   Conclusão (Teoria, Opcional)

---

**Sprint 13: Previsões e Predições**
(6 capítulos)

*   ## 01 Introdução a Previsões e Predições
    (1 lição)
    *   Introdução a Previsões e Predições (Teoria)
*   ## 02 Tarefas de Negócios Envolvendo Aprendizado de Máquina
    (12 lições)
    *   Introdução (Teoria)
    *   O que é Aprendizado? (Teoria)
    *   Apresentação à Previsão e Aprendizado de Máquina (Teoria)
    *   Aprendizado Supervisionado (Teoria)
    *   Aprendizado não supervisionado (Teoria)
    *   Treinando um Modelo em Python: sklearn (3 tarefas)
    *   Treinamento, Validação, e Teste de Dados (Teoria)
    *   Subajuste e Superajuste (Teoria)
    *   Dividir e Validar (2 tarefas)
    *   O Pipeline do Aprendizado de Máquina (2 tarefas)
    *   Por que o Aprendizado de Maquina Não é Universal? (Teoria)
    *   Conclusão (Teoria)
*   ## 03 Algoritmos de Aprendizado de Máquina
    (18 lições)
    *   Introdução (Teoria)
    *   Funções de regressão linear e erro (2 tarefas)
    *   Gradiente Descendente (Teoria)
    *   Pré-processamento: Características de Dimensionamento (1 tarefa)
    *   Regularização (Teoria)
    *   Implementação de Modelos Lineares (2 tarefas)
    *   Métricas de Regressão (1 tarefa)
    *   Regressão logística (2 tarefas)
    *   Métricas de Classificação: Trabalhando com Legendas (1 tarefa)
    *   Métricas de Classificação: Estudando Probabilidades (1 tarefa)
    *   Limiares e Balanço de Classe (1 tarefa)
    *   Árvores de decisão (1 tarefa)
    *   Conjuntos de Arvores de Decisão: Floresta Aleatória e Gradient Boosting (1 tarefa)
    *   Algoritmos de Aprendizado Não Supervisionado: Agrupamento (Teoria)
    *   O Que Tudo Isso Tem a Ver com Distância? (Teoria)
    *   K-Means e Agrupamento Aglomerativo Hierárquico (1 tarefa)
    *   Métricas para Problemas de Aprendizado Não Supervisionado (1 tarefa)
    *   Conclusão (Teoria)
*   ## 04 Resolvendo Tarefas Relacionadas a Aprendizado de Máquina
    (11 lições)
    *   Introdução (Teoria)
    *   Instruções da Tarefa (Teoria)
    *   AED: Analisando a Qualidade das Características (2 tarefas)
    *   AED: Formulando Hipóteses (1 tarefa)
    *   Pré-processamento de dados (3 tarefas)
    *   Aleatoriedade e Divisão de Tempo (Teoria)
    *   Selecionando Métricas (Teoria)
    *   Selecionando o Modelo de Aprendizado de Máquina (Teoria)
    *   Treinando Modelos e Selecionando o Melhor (2 tarefas)
    *   A Importância das Características (1 tarefa)
    *   Conclusão (Teoria)
*   ## 05 Projeto do Curso
    (1 lição)
    *   Sprint 13 - Projeto (Projeto, 1 tarefa)
*   ## 06 Conclusão
    (4 lições)
    *   Feedback do Sprint 13 (Teoria)
    *   Conclusão (Teoria)
    *   Guru de Decisões de Negócios (Teoria)
    *   Questionário sobre situação empregatícia (Teoria)

---

**Sprint 14: Projeto Final**
(2 capítulos restantes)

*   ## 01 Bootcamp
    (4 lições)
    *   Projeto Final: Bootcamp (Teoria)
    *   Papéis e Regras do Jogo (Teoria)
    *   Comunicação (Teoria)
    *   O Projeto (Teoria)
*   ## 02 Revisão e Avaliação
    (5 lições)
    *   O Ponto do Sistema e Critérios de Avaliação (Teoria)
    *   Soft Skills (Teoria)
    *   Como Decompor uma Tarefa (Teoria)
    *   A Quem Fazer Perguntas (Teoria)
    *   Conselho Geral para Completar os Projetos (Teoria)
*   ## 03 Tarefas Opcionais
    (1 lição)
    *   Dashboard (Teoria)
*   ## 04 Entregando Projetos
    (Faltam 4 lições)
    *   Projeto Final. Decomposição (Projeto, 1 tarefa)
    *   Projeto de Teste A/B (Projeto, 1 tarefa)
    *   Projeto SQL (Projeto, 1 tarefa)
    *   Projeto Final. Entregando o Projeto (Projeto, 1 tarefa)
*   ## 05 Conclusão
    (3 lições)
    *   Você conseguiu — parabéns! (Teoria)
    *   Curso de Preparação de Carreira. (Teoria)
    *   Feedback (Teoria)

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-05-50-264Z.md
### Última modificação: 2025-05-28 18:05:50

# Home

Análise de Dados

83%

![](https://practicum-content.s3.amazonaws.com/resources/Final_Project_2_1700743144.svg)

### Sprint 14

## Projeto Final

### Capítulo 4

## Entregando Projetos

Projeto Final. Decomposição

[Avançar](/trainer/data-analyst/lesson/feaa4b83-c847-44c3-9187-c1a4ec45e577/)

## Conteúdo

1.  ![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/what_to_expect_1679489901_1681843066.svg)
    
    Módulo Adicional
    
    ### Curso demonstrativo
    
    5 capítulos
    
2.  ![](https://practicum-content.s3.amazonaws.com/resources/what_to_expect_1_1700743172.svg)
    
    Onboarding
    
    ### É bom ver você na TripleTen!
    
    14 aulas
    
3.  ![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1._Advanced_Spreadsheets_1676362144_1681842191.svg)
    
    Módulo Adicional
    
    ### Excel: Trabalhando com Dados
    
    3 capítulos restantes
    
4.  ![](https://practicum-content.s3.amazonaws.com/resources/5._Python_Fundamentals_1700742947.svg)
    
    Sprint 1
    
    ### Python básico
    
    9 capítulos
    
5.  ![](https://practicum-content.s3.amazonaws.com/resources/5._Python_Fundamentals_1700743842.svg)
    
    Sprint 2
    
    ### Continuação de Python básico
    
    7 capítulos
    
6.  ![](https://practicum-content.s3.amazonaws.com/resources/data_prep_1700742967.svg)
    
    Sprint 3
    
    ### Manipulação de dados
    
    11 capítulos
    
7.  ![](https://practicum-content.s3.amazonaws.com/resources/1._Advanced_Spreadsheets_1700742977.svg)
    
    Sprint 4
    
    ### Análise Estatística de Dados
    
    6 capítulos
    
8.  ![](https://practicum-content.s3.amazonaws.com/resources/6._Software_Development_Tools_1700742989.svg)
    
    Sprint 5
    
    ### Ferramentas de Desenvolvimento de Software
    
    8 capítulos
    
9.  ![](https://practicum-content.s3.amazonaws.com/resources/Final_Project_2_1700742997.svg)
    
    Sprint 6
    
    ### Projeto Integrado 1
    
    5 capítulos
    
10.  ![](https://practicum-content.s3.amazonaws.com/resources/sandbox_1679490946_1681843095_1682758313_1685704627_1687875109.svg)
    
    Módulo Adicional
    
    ### Sandbox
    
    2 capítulos restantes
    
11.  ![](https://practicum-content.s3.amazonaws.com/resources/2._SQL_Databases_1700743030.svg)
    
    Sprint 7
    
    ### Coleta e Armazenamento de Dados (SQL)
    
    8 capítulos
    
12.  ![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/10._Integrated_Project_2_1681844030.svg)
    
    Módulo Adicional
    
    ### Introdução ao HTML, CSS
    
    3 capítulos
    
13.  ![](https://practicum-content.s3.amazonaws.com/resources/9._Business_Analytics_1_1700743059.svg)
    
    Sprint 8
    
    ### Análise de Negócio
    
    9 capítulos
    
14.  ![](https://practicum-content.s3.amazonaws.com/resources/B_Testing_1700743072.svg)
    
    Sprint 9
    
    ### Tomando Decisões de Negócios Baseadas em Dados
    
    9 capítulos
    
15.  ![](https://practicum-content.s3.amazonaws.com/resources/4._Storytelling_with_Data_1_1700743084.svg)
    
    Sprint 10
    
    ### Como Relatar uma História Usando Dados
    
    6 capítulos
    
16.  ![](https://practicum-content.s3.amazonaws.com/resources/Final_Project_2_1700743102.svg)
    
    Sprint 11
    
    ### Projeto Integrado 2
    
    2 capítulos
    
17.  ![](https://practicum-content.s3.amazonaws.com/resources/11.__Intro_to_Machine_Learning_1700743112.svg)
    
    Sprint 12
    
    ### Automação
    
    6 capítulos
    
18.  ![](https://practicum-content.s3.amazonaws.com/resources/11.__Intro_to_Machine_Learning_1700743126.svg)
    
    Módulo Adicional
    
    ### Suplementos de Automação
    
    5 capítulos restantes
    
19.  ![](https://practicum-content.s3.amazonaws.com/resources/8.Data_Visualization_with_Python_1700743135.svg)
    
    Sprint 13
    
    ### Previsões e Predições
    
    6 capítulos
    
20.  ![](https://practicum-content.s3.amazonaws.com/resources/Final_Project_2_1700743144.svg)
    
    Sprint 14
    
    ### Projeto Final
    
    2 capítulos restantes
    

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-09-10-752Z.md
### Última modificação: 2025-05-28 18:09:11

# O que esperar deste curso - TripleTen

Capítulo 1/5

Introdução

Opcional

# O que esperar deste curso

Este curso é exclusivo porque vamos instruir você sobre a análise dos quatro conjuntos de dados utilizando **SQL**, **Python** e **Tableau**.

-   O Python é a principal linguagem de programação para análise de dados e aprendizado de máquina.
-   O SQL é a ferramenta usada para extrair dados dos bancos de dados onde estão armazenados.
-   O Tableau é uma ferramenta poderosa para compilar visualizações interativas para apresentar os resultados de nossas descobertas.

Essas ferramentas são amplamente utilizadas por profissionais na área de dados. Mas não se preocupe! Mesmo que seja a primeira vez que você esteja sendo apresentado a uma linguagem de programação ou ferramenta de visualização, você conseguirá tirar tudo isso de letra.

Contaremos com algo chamado **abstração**. Então, o que é abstração?

Da mesma forma que podemos aprender a dirigir um carro sem entender como ele foi construído ou funciona, também podemos aprender a usar o **Python**, por exemplo.

Isso ocorre porque tanto Python quanto dirigir um carro dependem de um conjunto de regras e conceitos que podem ser aprendidos e aplicados sem a necessidade de conhecer todos os detalhes de como eles funcionam.

Ao aprender a dirigir um carro, não precisamos entender os mínimos detalhes de como o motor funciona, de como a transmissão de energia opera ou de como os freios funcionam. Em vez disso, aprendemos um conjunto de habilidades, como dar partida no carro, dirigir, acelerar, frear e seguir as leis de trânsito. Ao praticar essas regras repetidamente, desenvolvemos gradualmente uma memória muscular e também reflexos mentais necessários para dirigir um carro com segurança e eficiência.

![](https://practicum-content.s3.amazonaws.com/resources/27PT_1688455247.png)

Da mesma maneira, ao aprender **Python** (ou **SQL**), não precisamos saber os detalhes de baixo nível de como a linguagem de programação é implementada, como o sistema operacional do computador interage com ela ou como o funciona o hardware do computador. Em vez disso, aprendemos um conjunto de habilidades, como usar **variáveis** e **estruturas de dados**, como controlar o fluxo de execução de um programa e como usar **funções** e **bibliotecas**. Ao praticar essas habilidades repetidamente, podemos desenvolver gradualmente uma intuição e também as habilidades necessárias para escrever um código **Python** que resolva problemas reais.

Certamente, assim como ao dirigir um carro, uma compreensão mais profunda sobre os conceitos que estão por trás disso pode ser benéfica em determinadas situações. Por exemplo, saber como funcionam os freios de um carro pode ajudar a evitar acidentes usando ele de maneira adequada e oportuna, ou saber como o **Python** interage com a memória do computador pode ajudar ele para escrever um código mais eficiente. No entanto, esses detalhes não são estritamente necessários para aprender a usar o **Python** de forma eficaz, pelo menos não para concluir as tarefas deste curso.

Em resumo, nossa abordagem para este curso será aprender **Python**, **SQL** e **Tableau** com um alto nível de abstração e usar eles de maneira semelhante a como dirigiríamos um carro, usar um eletrodoméstico, ou nossos celulares, etc. Nesse curso, aprenderemos um conjunto básico de regras e conceitos que podem ser aplicados sem entender cada detalhe de como eles funcionam. No entanto, esse conhecimento será suficiente para:

1.  Usar o Python para ajudar uma startup a descobrir por que seus clientes param de usar um serviço.
2.  Usar o SQL para obter dados sobre a eficácia das vacinas contra a Covid e ilustrar os resultados no Tableau.
3.  Utilizar o aprendizado de máquina para ajudar Carlos, um vendedor de sorvete local, a planejar melhor suas vendas de sorvete.
4.  Crie um modelo de aprendizado de máquina que ajudará uma empresa a aprimorar sua taxa de retenção de clientes.

Animado? Vamos começar!

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_1_1688451149.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-12-56-332Z.md
### Última modificação: 2025-05-28 18:12:56

# Introdução à Análise Exploratória de Dados - TripleTen

Capítulo 2/5

Análise Exploratória de Dados

Opcional

# Introdução à Análise Exploratória de Dados

## Introdução à EDA

Sua jornada começará aprendendo Python e resolvendo desafios reais de negócios.

Pense no Python como uma caixa de ferramentas por enquanto. Assim como um carpinteiro usa ferramentas de carpintaria para construir móveis e um encanador usa um conjunto diferente de ferramentas para consertar tubulações antigas, um profissional de dados pode usar Python para realizar suas tarefas de análise de dados.

Vamos começar!

Jonathas

Vamos começar!

Então, imagine que você iniciou sua empresa de consultoria "one-man" e conseguiu seu primeiro emprego em uma startup de comércio eletrônico que experimentou um crescimento significativo após a pandemia de COVID-19 de 2020. Esta empresa é especializada na construção de sites, principalmente para lojas online. As medidas de quarentena tiveram dois efeitos principais que contribuíram para esse crescimento:

1.  As empresas que tiveram que fechar suas lojas físicas precisaram vender produtos e serviços online para evitar a falência.
2.  Comprar online tornou-se o novo normal. Muitas pessoas que nunca haviam comprado coisas online e começaram a fazer durante a pandemia.

O modelo de negócios da empresa para a qual você trabalha é semelhante ao da Shopify, com dois principais interessados:

-   Empresas que vendem produtos e precisam de uma loja online. As empresas podem configurar sua loja online em questão de minutos, ter integrações de pagamento e envio, tudo por uma pequena taxa.
-   Consumidores que fazem compras online.

A empresa contratou seus serviços porque notou um aumento no **cancelamento** entre as lojas que usaram o site que ajudamos a criar. Em particular, há uma necessidade de entender quem está cancelando e por quê. Idealmente, eles gostariam de configurar alertas para negócios com probabilidade de cancelamento.

**Cancelamento** é um problema comum entre as empresas. Refere-se à frequência com que os clientes (no nosso caso, empresas que precisam de uma loja online) abandonam ou deixam de utilizar o serviço de uma empresa. Pode ter um impacto significativo nos resultados financeiros de uma empresa.

![](https://practicum-content.s3.amazonaws.com/resources/1.2.2PT_1688474075.png)

Como esta é a primeira vez que eles trabalham com você, eles concordaram em compartilhar um conjunto de dados limitado com você para exploração. Esse conjunto de dados contém informações sobre as compras de um usuário, contatos de suporte, método de pagamento e se o usuário cancelou ou não a compra.

Lembre-se de que seu foco para essa análise deve ser as empresas que estão pagando a startup para ter uma loja virtual para vender seus produtos.

### Reflita sobre suas perguntas

A capacidade de enquadrar um problema de negócios como um conjunto de perguntas é uma das habilidades mais valiosas de um profissional de dados.

Esse curso será muito prático, então vamos mergulhar direto no Python e começar a explorar os dados. Em seguida, apresentaremos algumas perguntas para responder.

Aqui estão alguns conceitos de coisas que usaremos, para que você possa se familiarizar.

### Saiba mais!

-   **Biblioteca**: uma coleção de ferramentas que implementam uma funcionalidade específica. Cada biblioteca é um kit de ferramentas disponível em Python para abordar problemas comuns.
-   **Variável**: um local nomeado na memória que armazena um valor ou uma referência a um valor. Pense em uma variável como uma “caixa” onde você pode armazenar coisas que precisará usar mais tarde.
-   **Função**: um bloco de código que executa uma tarefa específica e pode ser reutilizado ao longo do programa. Um exemplo seria a necessidade de extrair um código postal de um endereço. Se precisarmos fazer isso para muitos endereços, precisamos de uma função que faça isso repetidamente para nós para cada endereço que dermos.
-   **Método**: uma operação que pode ser aplicada a um objeto. Por exemplo, se quisermos modificar algo, empregaremos um método

Vamos ver como as bibliotecas funcionam na prática e começar com as **bibliotecas** do Python. Uma biblioteca é uma coleção de ferramentas que já estão disponíveis em Python. Nosso objetivo é obter acesso a essas bibliotecas para que possamos usar suas funcionalidades para nossas necessidades.

Abaixo mostraremos um código que importa uma biblioteca:

```
import pandas as pd
```

Importamos a biblioteca `pandas` e damos a ela um alias usando `as`. Um alias é um nome alternativo que podemos usar para qualquer biblioteca. Geralmente encurtamos o nome para melhorar a conveniência. Com o apelido que demos, podemos nos referir aos `pandas` como `pd`.

Em seguida, vamos trabalhar com variáveis. Abaixo, você verá como criar uma variável chamada `file_path` e armazenar a localização de um arquivo (ou caminho para um arquivo) nela.

```
file_path = "<https://raw.githubusercontent.com/JJTorresDS/ds-data-sources/main/churn_dataset.csv>"
```

A localização de um arquivo, neste caso, é determinada por um link, que é simplesmente um endereço do arquivo na internet.

Agora, vamos passar para as funções. Queremos ler o endereço do arquivo que armazenamos na variável `file_path`. Esse arquivo contém dados que analisaremos nessa lição. Para fazer isso, precisamos de uma função chamada `read_csv` que está disponível na biblioteca do `pandas`. Você se lembra quando decidimos nos referir aos `pandas` como `pd`? Aqui será bem útil! Portanto, para acessar uma função dos pandas, precisamos começar com o nome da biblioteca `pd`, digite `.`, depois o nome da função e, por último, o parêntese. Portanto, `pd.read_csv()` é a função que vamos usar. Aqui está como:

```
df = pd.read_csv(file_path)
```

Essa linha de código armazena o conteúdo de um arquivo (que é um arquivo **csv**) em outra variável que chamamos de `df`. Por que `df`? Porque é a abreviação de **DataFrame**. Um DataFrame na biblioteca pandas é apenas um nome para uma tabela, semelhante às planilhas do Excel.

Ok, vamos passar para os métodos agora. Estes são como funções, mas são aplicados a objetos. Qual é o objeto em nosso caso? Por exemplo, pode ser um arquivo de dados que acabamos de ler. E se quisermos olhar para ele? Existe um método legal que nos permite ver as primeiras linhas de arquivos com dados. É chamado de método `head()`. Vamos ver como funciona, aplicando na variável `df` onde nossos dados estão armazenados!

```
print(df.head())
```

O que está acontecendo aqui? Queríamos aplicar o método `head`, mas também há algo mais aqui! Algo sobre a impressão! Não se preocupe! O comando `print()` apenas informa ao Python que desejamos mostrar ou imprimir algo na tela. No nosso caso, este é o resultado do método `head()` aplicado. A que ele retorna? Bem, como esperado, as primeiras linhas. Aqui está o que exatamente:

```
        user_id  age payment_method  support_contacts_last_30_days  \
0        0   40    credit_card                              3   
1        1   37    credit_card                              0   
2        2   43    credit_card                              2   
3        3   41    credit_card                              0   
4        4   38    credit_card                              1   

   items_sold_last_30_days  churned  
0                      108        0  
1                      138        0  
2                      166        0  
3                      100        0  
4                       35        0
```

O que vemos? As linhas de nossos dados e as colunas. À esquerda estão os índices das linhas: `0`, `1`, `2` … até 4. As colunas são `'user_id'`, `'age'` e outras.

Legal! Vamos agora ver todo esse código combinado. Damos a você a chance de executá-lo e ver o resultado.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd \# importe a biblioteca com o alias pd

  

\# defina a variável file\_path com o caminho para nossos dados csv

file\_path \= "https://raw.githubusercontent.com/JJTorresDS/ds-data-sources/main/churn\_dataset.csv"

df \= pd.read\_csv(file\_path) \# leia os dados csv para o df DataFrame

  

print(df.head()) \# imprima as primeiras 5 linhas do df DataFrame

Mostrar a soluçãoExecutar

Ótimo trabalho!

Tenho certeza que isso é completamente novo para você. Apenas lembre-se de usar a abstração. Nosso objetivo neste curso introdutório não é para aprender Python, mas aprender quais tarefas podem ser realizadas se soubermos usar essa linguagem de programação.

Algumas coisas a serem observadas:

-   **Aliases**: importamos **pandas** (ou seja, nossa biblioteca) e demos a ele o alias `pd` (abreviação de pandas). Esta é uma convenção em pandas e é utilizada para digitar menos código quando queremos "extrair uma ferramenta" desta caixa de ferramentas. Então, como utilizamos as ferramentas dessa caixa de ferramentas? Em vez de escrever `pandas` o tempo todo, apenas escrevemos `pd`.
-   Notação **Métodos** e **ponto** (`.`): Depois de importar nossa biblioteca `pd`, agora podemos usar eles para acessar todo o conteúdo da biblioteca. Criamos um objeto **DataFrame** chamado `df.`. Com isso podemos dizer ao Python que usaremos uma de suas ferramentas digitando um ponto `.` após o nome da variável, seguido do nome da ferramenta que queremos, por exemplo `head` (essas "ferramentas" são chamadas de **métodos** e usaremos esse nome a partir de agora).
-   Por último, mas não menos importante, temos o que é conhecido como **funções** e o uso de parênteses (por exemplo, `print()`). O Python tem funções que estão disponíveis prontas para uso (ou seja, você não precisa importar uma biblioteca), e a maioria das funções recebe uma entrada e retorna algo em troca. Em nosso exemplo, a função **print** pega os dados que precisa imprimir e retorna em um resultado (uma visualização desses dados).

Vamos dar uma olhada rápida na imagem a seguir. Esta imagem é um instantâneo dos dados que acabamos de importar.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_4_1688451884.png)

Parece que nossos dados contêm seis colunas diferentes, com diferentes **tipos** de dados (por exemplo, números, categorias). Não vamos nos aprofundar nos tipos de dados, mas vale ressaltar que em Python e em muitas outras linguagens de programação, o tipo dos dados nos permite realizar certas operações com eles (por exemplo, subtrair e calcular médias com tipos numéricos).

Parece que um dos dados em nosso conjunto de dados é **categórico** (`payment_method`), enquanto os outros são numéricos (como `items_sold_last_30_days`). Isso é fácil de entender. Se faz sentido somar valores diferentes, provavelmente é numérico. Se estiver apenas descrevendo algo sobre os dados, provavelmente é categórico.

![](https://practicum-content.s3.amazonaws.com/resources/PT2.3.1.1_1688457044.png)

Não seria ótimo se pudéssemos obter rapidamente um resumo dos dados em nosso conjunto de dados?

Bem, acontece que nosso **DataFrame** tem alguns métodos (ou seja, ferramentas) próprios, e um deles nos permite imprimir rapidamente algumas estatísticas descritivas:

Execute o bloco de código abaixo para imprimir algumas estatísticas descritivas das variáveis numéricas.

Observe como no primeiro exemplo, usamos a notação de ponto (`.`) para "dizer" ao Python que usaremos o método `describe()` do dataframe `df`.

CódigoPYTHON

9

1

print(df.describe())

Mostrar a soluçãoExecutar

Vamos detalhar as informações mostradas por este método:

-   `count`: o número de valores não-N/A (ou seja, em branco ou ausentes) em nossos dados
-   `mean`: é a média aritmética, também chamada de "média".
-   `std`: este é o desvio padrão de cada conjunto de dados. O desvio padrão é útil para entender a variabilidade em nossos dados (quanto maior o "std", maior a variabilidade).
-   `25%`, `50%` e `75%`: representam os valores do 1º, 2º e 3º quartis. O 2º quartil ou 50º percentil também é conhecido como **mediana**.
-   `min`, `max`: são apenas os valores mínimo e máximo de cada variável. Isso nos dá uma ideia da "faixa" de valores dentro de cada dado numérico.

Você provavelmente pode entender facilmente as variáveis de contagem, média, mínimo e máximo, mas pode estar pensando: o que é um quartil?

Não se preocupe com isso agora, a ideia é que com apenas uma linha de código possamos obter muitas informações sobre nossos dados. Legal, né? Fora isso, você notou alguma coisa?

Por padrão, o método `describe()` mostra apenas dados numéricos, ou pelo menos os dados que ele interpreta/pensa que são numéricos (ou seja, `user_id` claramente não é um número, mas um tipo chamado string, ou seja, um texto). Vamos verificar: faz sentido adicionar, digamos, 10 ao seu user\_id? Isso apenas faria de você um usuário diferente!

🔥 Quer pegar o jeito das estatísticas? Temos um curso sobre isso no [programa completo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/ptbr-syllabus/DA_PT_Syllabus_V5.pdf)!

**Observação**: Não há problema se você se sentir perdido com a quantidade de informações que acabou de processar. Nossos bootcamps de dados possuem capítulos completos dedicados a ensinar estatísticas para que você se sinta confortável usando essas ferramentas.

Por enquanto, concentre-se apenas em sua melhor amiga, a **abstração**, e no fato de que obter uma quantidade semelhante de informações em ferramentas como o Excel exigiria muito mais trabalho e a instalação de complementos.

Dar uma primeira olhada em um conjunto de dados desperta a curiosidade e o pensamento criativo de um profissional de dados. É aqui que podemos começar a gerar hipóteses para as questões de negócios em questão.

Pergunta

**Qual dessas perguntas você acha que seria útil responder no contexto do nosso problema?**

Escolha quantas quiser

Os clientes mais jovens são mais propensos a cancelar uma compra devido à falta de fidelidade? (ou seja, eles preferem uma loja online com preços mais baratos).

Sim! Seria definitivamente útil saber disso.

Mais vendas nos últimos 30 dias se correlacionam com uma taxa de retenção mais alta (ou seja, menos cancelamentos)?

Isso também seria super útil para o seu cliente saber.

O cancelamento (ou seja, clientes saindo de uma empresa) está associado a contatos de suporte mais frequentes? As pessoas que entram em contato com o suporte com mais frequência estão enfrentando problemas com mais frequência.

Sim! Descobrir se as pessoas que entram em contato com o suporte com mais frequência estão enfrentando problemas com mais frequência definitivamente seria algo que vale a pena saber!

Trabalho maravilhoso!

Responder a essas perguntas seria um bom ponto de partida. Nosso cliente pode agir assim que souber as respostas.

Você fez muito trabalho desde a primeira olhada nos dados. Elogie-se, descanse um pouco e prepare-se para mergulhar no que é frequentemente chamado de **Análise de Dados Exploratórios (EDA, do inglês Exploratory Data Analysis)**, que é o pão com manteiga do trabalho da maioria dos profissionais de dados.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-19-03-340Z.md
### Última modificação: 2025-05-28 18:19:04

# Análise de uma Variável - TripleTen

Capítulo 2/5

Análise Exploratória de Dados

Opcional

# Análise de uma Variável

A análise de uma variável, ou análise univariada, é o processo de análise de uma (uni) variável (variada) de cada vez. Nós podemos observar as distribuições de cada variável em nosso conjunto de dados, entendendo melhor os dados e gerar perguntas ainda mais úteis para responder.

Quanto mais perguntas úteis você for capaz de gerar, melhores respostas você provavelmente receberá.

### Saiba mais!

-   Em estatística, uma distribuição é uma forma de descrever um padrão de dados. Uma distribuição muito comum usada em estatística é a distribuição normal, também conhecida como distribuição Gaussiana.

Vamos dar outra olhada nas primeiras observações do nosso conjunto de dados. Poderíamos fazer isso por você, mas a ideia é ajudá-lo a construir “memória muscular” e perder o medo de usar Python. Portanto, use o método adequado para imprimir as cinco primeiras linhas do nosso DataFrame `df`, assim como você viu na lição anterior.

Se precisar de ajuda, não hesite em clicar na **Dica**.

CódigoPYTHON

9

1

2

print(df.head())

  

Dica

Mostrar a soluçãoValidar

🔥 Quer aprender outros métodos para exibir seus dados? Confira o que você vai aprender em nosso [programa completo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/ptbr-syllabus/DA_PT_Syllabus_V5.pdf)!

### Distribuição de Idade

O que foi feito até agora está ótimo, mas uma habilidade essencial de um profissional de dados é a capacidade de usar gráficos para explorar e descrever os dados. Afinal, uma imagem vale mais que mil palavras, certo?

Então, vamos começar a gerar gráficos!

Execute o bloco de código a seguir para ver como é fácil gerar um gráfico da distribuição `age`.

CódigoPYTHON

9

1

df\["age"\].hist() #trace o histograma

Mostrar a soluçãoExecutar

Então, o que acabou de acontecer? Ao importar uma biblioteca de construção, demos recursos adicionais aos nossos pandas DataFrame. Não é legal?!

### Saiba mais!

-   Um **histograma** é uma forma de visualizar a distribuição de uma variável numérica. Ele exibe o número de observações dentro de um determinado **intervalo**, ou **barra**, de valores no eixo X, com a altura da barra representando a frequência das observações que caem dentro dessa barra.

A primeira observação mostra um pico por volta dos 40 anos, com outro pico por volta dos 60 anos. O eixo X representa a idade do cliente, enquanto o eixo Y representa o número de clientes que se enquadram nessas seções de idade.

Isso significa que o gráfico não parece ser **normalmente distribuído**. Na verdade, esse tipo de distribuição é chamado de **bimodal**, o que significa basicamente que tem dois picos.

### Saiba mais!

-   Uma **distribuição normal** é um tipo de distribuição que possui uma forma característica do sino, com a maioria das observações agrupadas em torno da média. Muitos fenômenos naturais, como a altura humana e as pontuações de QI, seguem uma distribuição normal.

Então, aqui está como o gráfico foi criado: A string `"age"` dentro dos colchetes é uma forma de dizer aos pandas que queremos ver o conteúdo de uma variável específica. Em seguida, usamos o ponto (`.`) para indicar que vamos usar o método `hist()` (lembre-se de que podemos pensar em métodos como ferramentas que podemos usar) para construir um histograma. Então, os pandas não são superpoderosos?

![](https://practicum-content.s3.amazonaws.com/resources/1.2.3.1PT_1688474373.png)

Você está começando a ver os benefícios do Python?

-   Temos uma ampla família de métodos/ferramentas integrados (e novas caixas de ferramentas/bibliotecas estão sendo lançadas constantemente).
-   Podemos fazer muito com apenas uma linha de código.
-   Nosso trabalho até agora está fácil de ler, pois é basicamente um relatório com código incorporado. Você já teve que olhar a planilha de outra pessoa? Essa planilha tinha várias abas com várias fórmulas? Você já quase perdeu a sanidade tentando descobrir o que estava acontecendo? Isso não acontecerá com a nossa forma atual de trabalhar.

Em um cenário real, você não usaria nossa plataforma para fazer a análise, mas alguma outra estrutura. Uma ferramenta muito popular para fazer análises exploratórias é o **Jupyter Notebook**. Assim como em nossa plataforma, você pode explorar os dados do caderno (notebook) e adicionar seus pensamentos/achados na forma de texto.

Não seria ótimo se também pudéssemos adicionar títulos e rótulos de eixo ao nosso gráfico para tornar ele mais legível?

Estaremos usando a biblioteca `pandas` para fazer isso.

Vamos ver como é fácil adicionar um título e nomes em ambos os eixos.

CódigoPYTHON

9

1

2

3

4

5

plt \= df\["age"\].hist() \# traçe o histograma

  

plt.set\_title("Distribuição de idade") \# adiciona um título

plt.set\_xlabel("Idade") \# adiciona rótulo ao eixo X

plt.set\_ylabel("Contagens") \# adiciona rótulo ao eixo Y

Mostrar a soluçãoExecutar

Agora temos um histograma bonito e informativo! Hora de fazer isso sozinho.

**Explorando** **Vendas**

Os profissionais de dados são hábeis em “reciclar” códigos. Copie o código que gerou o primeiro **histograma de idade** e faça a distribuição de `items_sold_last_30_days`.

CódigoPYTHON

9

1

df\["items\_sold\_last\_30\_days"\].hist()

Dica

Mostrar a soluçãoValidar

**Tornando o gráfico mais legível.**

Vamos agora adicionar rótulos de eixo e um título ao nosso gráfico.

Use como título: **“Número de clientes por número de itens comprados”**

O eixo X deve ser **“Itens comprados nos últimos 30 dias”**

O eixo Y deve ser **“Clientes”**

CódigoPYTHON

9

1

2

3

4

5

plt \= df\["items\_sold\_last\_30\_days"\].hist()

  

plt.set\_title("Número de clientes por número de itens comprados")

plt.set\_xlabel("Itens comprados nos últimos 30 dias")

plt.set\_ylabel("Clientes")

Dica

Mostrar a soluçãoValidar

**Explorando os contatos de suporte**

Vamos fazer a mesma análise para `support_contacts_last_30_days`. Use os seguintes rótulos:

Título: Número de contatos de suporte por número de clientes

eixo X: Contatos de suporte

eixo Y: Clientes

CódigoPYTHON

9

1

2

3

4

5

6

plt \= df\["support\_contacts\_last\_30\_days"\].hist()\# Escreva seu código aqui

  

plt.set\_title("Número de contatos de suporte por número de clientes")

plt.set\_xlabel("Contatos de suporte")

plt.set\_ylabel("Clientes")

  

Dica

Mostrar a soluçãoValidar

### Métodos de Pagamento

E as variáveis categóricas? Também é fácil traçar? Sim, isso aí!

Vamos pensar em como faríamos isso:

1.  Selecione a variável `payment_method` no DataFrame.
2.  Conte o número de ocorrências para cada `payment_method`.
3.  Crie um gráfico de barras mostrando o método mais popular.

Execute o seguinte bloco de código para executar as etapas 1 e 2.

CódigoPYTHON

9

1

print(df\["payment\_method"\].value\_counts())

Mostrar a soluçãoExecutar

Dissemos aos pandas qual variável queríamos traçar (`df["payment_method"]`) e, em seguida, usamos o método `value_counts()` para contar o número de vezes que cada método de pagamento foi usado. Espero que você esteja se familiarizando com a notação `dot`, no caso, o uso do ponto para acessar métodos. Agora você deve entender que só precisamos saber o nome de um método para usá-lo. Se você não tiver certeza do nome do método, Google e/ou ChatGPT podem ser seus melhores amigos.

Então, como poderíamos traçar isso? Alguma ideia?

Poderíamos adicionar outro ponto (`.`) para chamar um novo método de traçar?

Vamos tentar executar o seguinte bloco de código.

CódigoPYTHON

9

1

2

plt \= df\["payment\_method"\].value\_counts().plot(kind\="bar")

plt.set\_title("Forma de pagamento")

Mostrar a soluçãoExecutar

E é assim que você faz um gráfico de barras! Dê uma olhada na linha de código que faz a mágica acontecer:

```
df["payment_method"].value_counts().plot(kind="bar");
```

-   Existem duas chamadas de método aninhadas uma com a outra
-   Primeiro `value_counts` é chamado e depois o gráfico é chamado para seus resultados
-   E o tipo de gráfico que queremos é passado dentro do método plot

Um **gráfico de barras** é uma forma de visualizar dados categóricos usando barras retangulares. A altura de cada barra representa a contagem ou frequência da categoria.

Outro benefício dos pandas é que você pode aninhar métodos, como fizemos no bloco de código acima. Essa propriedade de aninhamento nos permitiu gerar um gráfico muito útil com apenas uma única linha de código.

### Saiba mais!

-   O aninhamento de métodos em Python refere-se ao ato de chamar um método dentro dos parâmetros de outro método, ou, neste caso, um método a ser trabalhado no retorno de outro método. No exemplo, o gráfico foi feito sobre os resultados do método `value_counts`. Isso é possível porque os métodos retornam aos objetos, que podem ser usados como entrada para outro método. Os métodos de aninhamento podem tornar o código mais conciso e legível, permitindo que vários métodos sejam chamados em uma única linha de código.

Para fechar esse capítulo, você poderia repetir o gráfico usando a variável `churned`? Trace um gráfico de barras para a variável e adicione o seguinte:

Título: Status Cancelado

eixo X: Cancelado (0: Não Cancelado, 1: Cancelado)

eixo Y: Clientes

CódigoPYTHON

9

1

2

3

4

5

plt \= df\["churned"\].value\_counts().plot(kind\='bar')\# Escreva seu código aqui

  

plt.set\_title("Status Cancelado")

plt.set\_xlabel("Cancelado (0: Não Cancelado, 1: Cancelado)")

plt.set\_ylabel("Clientes")

Dica

Mostrar a soluçãoValidar

Ótimo trabalho! Também seria útil entender a índice médio de cancelamento de nosso conjunto de dados. Como você deve ter adivinhado, isso é bastante simples de calcular usando outro método pandas.

A seguinte linha de código faz uso de 2 métodos:

-   `mean()`: calcula a índice médio de cancelamento do conjunto de dados.
-   `round()`: arredonda a média para 2 casas decimais.

O resultado dessas operações aninhadas é armazenado em uma variável chamada `churn_rate`.

CódigoPYTHON

9

1

2

churn\_rate \= df\["churned"\].mean().round(2)

print(churn\_rate)

Mostrar a soluçãoExecutar

E se quiser imprimir esse resultado em uma linha, como faríamos?

Vamos usar nossa função de impressão mais uma vez!

CódigoPYTHON

9

1

2

churn\_rate \= df\["churned"\].mean().round(2)

print("Índice Médio de Cancelamento:", churn\_rate)

Mostrar a soluçãoExecutar

E é muito fácil!

Vamos adicionar um pouco mais de complexidade e ver o que a análise bivariada pode fazer! No entanto, isso é para a próxima lição. Por enquanto, podemos respirar!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-19-10-424Z.md
### Última modificação: 2025-05-28 18:19:10

# Análise de duas Variáveis - TripleTen

Capítulo 2/5

Análise Exploratória de Dados

Opcional

# Análise de duas Variáveis

Ótimo trabalho! Você conseguiu explorar a distribuição de cada variável. Como lembrete, estamos realmente interessados nos dados `churned` (de cancelamento) e tentando descobrir como outros pontos de dados podem ajudar a explicá-los.

A análise bivariada, ou a análise de duas variáveis, envolve o exame de duas variáveis por vez e sua relação entre si. Uma variável, chamada variável independente, é livre para variar, enquanto a outra variável, chamada variável dependente, muda em resposta a mudanças na variável independente. Agora, estaremos usando essa técnica.

A variável `churned` será nossa variável dependente, já que podemos supor que seu valor depende de outras variáveis no conjunto de dados (por exemplo, pessoas que fazem mais chamadas para a linha de suporte podem ter maior probabilidade de cancelar). Vamos nos referir às outras variáveis como independentes.

### Saiba mais!

-   **Variável dependente**: Uma variável cujo valor depende das outras variáveis no conjunto de dados. É a variável que está sendo estudada ou medida, como é afetada pelas variáveis independentes. Nesse caso, `churned` é a variável dependente.
-   **Variável independente**: Uma variável que não depende de outras variáveis no conjunto de dados. É a variável que é manipulada ou controlada para estudar o efeito na variável dependente. Nesse caso, todas as variáveis, exceto `churned`, são variáveis independentes.

Não seria ótimo explorar como as variáveis independentes interagem com a nossa variável dependente `churned`? E podemos ver em gráficos, para ficar mais fácil de entender!

Vamos examinar como nossas variáveis impactam a variável `churned` e tentar tirar conclusões a partir disso.

### Gráficos de barras bivariados

Lembre-se, na última lição, aprendemos como traçar gráficos de barras para análise univariada. Também podemos usar gráficos de barras para realizar análises bivariadas.

Vamos agora traçar a variável `churned`, usando o `payment_method` e `support_contacts_last_30_days` como variáveis independentes.

Vamos começar com o `payment_method`.

Fazer esse tipo de gráfico requer algumas etapas adicionais:

-   Etapa 1: Agrupe os dados por nossa variável independente e calcule o índice médio de cancelamento. Armazene a saída em uma variável e imprima o resultado. Para isso, utilizamos o método `groupby` do nosso DataFrame, juntamente com o `mean()`:

Dá só uma olhada.

CódigoPYTHON

9

1

2

3

\# agrupe a variável churned para o método de pagamento e, em seguida, calcule a média

df\_grp \= df.groupby("payment\_method")\["churned"\].mean()

print(df\_grp)

Mostrar a soluçãoExecutar

Como podemos ver, existem duas opções para o `'payment_method'`: via `'bank_transfer'` e via `'credit_card'`. Olhando apenas para os números, parece que os clientes que escolheram `'bank_transfer'` como método de pagamento cancelaram com mais frequência.

-   Etapa 2: você pode usar o método `.plot()` para desenhar um gráfico de barras passando o valor `bar` para o argumento `kind=`: `df_grp.plot(kind="bar")`

Vamos construir o gráfico de barras e obter uma visualização do que estávamos falando. Afinal, fica mais fácil de entender quando vemos da forma de um gráfico.

CódigoPYTHON

9

1

2

3

4

5

\# agrupe a variável churned para o método de pagamento e, em seguida, calcule a média

df\_grp \= df.groupby("payment\_method")\["churned"\].mean()

  

\# construa o gráfico de barras com o título

df\_grp.plot(kind\="bar", title \="Taxa de Cancelamento por Método de Pagamento");

Mostrar a soluçãoExecutar

Bem, parece que podemos confirmar que os clientes que pagam por `bank_transfer` possuem um índice de cancelamento maior em média.

Para entender melhor os trechos de código que escrevemos, o método `groupby()` agrupa os dados por uma determinada variável. Em seguida, selecionamos a variável que queríamos agrupar (ou seja, a variável `churned`) e procedemos ao cálculo índice médio de cancelamento por grupo. Ele simplesmente junta todos os clientes cancelados para cada um dos métodos de pagamento e calcula a média deles.

Finalmente, usamos **aninhamento** (nesting) e adicionamos o método `plot()` ao DataFrame agrupado para mostrar o gráfico.

Vamos avançar, e agora vamos agrupar pela segunda variável, `support_contacts_last_30_days`.

Execute o código abaixo para traçar o gráfico da rotatividade em relação ao número de vezes que um cliente entrou em contato com o suporte.

CódigoPYTHON

9

1

2

3

4

5

\# agrupe o cancelamento para os contatos de suporte e calcule a média

df\_grp2 \= df.groupby("support\_contacts\_last\_30\_days")\["churned"\].mean()

  

\# construa o gráfico de barras com o título

df\_grp2.plot(kind\="bar", title \="Índice de Cancelamento vs. Contatos de Suporte");

Mostrar a soluçãoExecutar

Em média, os clientes possuem maior probabilidade de cancelar quando tiveram mais de 4 contatos de suporte em 30 dias (com base no índice de cancelamento mostrado no gráfico acima). No entanto, esta é apenas uma observação e não uma instrução causal.

E com este exemplo, você pode apreciar totalmente o poder de um gráfico! Imagine ler uma tabela como a que tínhamos antes, tentar entendê-la e tirar conclusões. Seria muito mais difícil, não é? Com um gráfico, é bastante óbvio!

Ao agregar dados, é importante considerar o volume dentro de cada categoria que foi agregada. Quanto maior o volume, mais confiável é a relação observada.

Tudo bem, vamos verificar quantas ligações foram feitas pelos clientes que nos deixaram. Para fazer isso, basta trocar mean() por count() e pronto! Teremos as informações do nosso volume agregado.

CódigoPYTHON

9

1

2

3

4

5

\# conte o número de inscrições canceladas por contato de suporte

df\_grp2 \= df.groupby("support\_contacts\_last\_30\_days")\["churned"\].count()

  

\# construa o gráfico de barras com o título

df\_grp2.plot(kind\="bar", title \="Índice de Cancelamento vs. Contatos de Suporte");

Mostrar a soluçãoExecutar

O gráfico de barras acima mostra, que a maioria dos clientes no conjunto de dados teve menos de 5 contatos de suporte nos últimos 30 dias.

Esse conhecimento adicional pode ser útil na criação dos alertas de que nosso cliente precisa.

### 🔥

Ansioso para aprender novas formas de representação de dados? No [programa completo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/ptbr-syllabus/DA_PT_Syllabus_V5.pdf), você aprenderá como obter informações das caixas de bigodes!

## Reflexões e recomendações

Depois de concluir nossa **Análise Exploratória de Dados**, é hora de resumir nossas descobertas e tentar responder às perguntas de interesse de nossos clientes:

-   Quem está cancelando?
-   Por que?

Conseguimos identificar os seguintes padrões:

-   Clientes que pagam por transferência bancária possuem um índice de cancelamento maior do que aqueles que usam cartão de crédito.
-   Parece que os clientes que entram em contato com o suporte mais de quatro vezes em um período de 30 dias têm, em média, maior probabilidade de cancelar a inscrição.
-   A maioria dos clientes teve quatro ou menos chamadas de suporte de contato.

Esse padrão nos ajuda a entender quais são os cuidados especiais que precisamos ter para evitar que os clientes cancelem.

Com base neles, podemos criar uma regra simples:

-   Se um cliente entrou em contato com o suporte mais de quatro vezes nos últimos 30 dias e paga por transferência bancária, um alerta deve ser acionado.

![](https://practicum-content.s3.amazonaws.com/resources/1.2.3.2PT_1688474645.png)

Você poderia argumentar que esta não é uma regra perfeita, mas serve como um bom ponto de partida. É impressionante como a partir de uma análise simples, podemos tirar conclusões concretas e até fornecer aos nossos clientes ações específicas que eles podem tomar.

Mas antes de fazer isso, a pergunta que devemos fazer é: esse alerta é realmente útil?

Pode parecer que no começo, mas podemos testar nossa regra contra os dados que temos para responder a essa pergunta com certeza. Mas primeiro, vamos construir nosso sistema de alerta.

Como podemos fazer isso? Queremos saber se o cliente teve mais de 4 contatos de apoio e se paga por transferência bancária. Então, vamos começar criando vetores `boolean`. `Booleans` são variáveis que podem ser tanto `True`(Verdadeiro) quanto `False`(Falso), tipo o **Sim** e o **Não** para a resposta da nossa pergunta.

Dessa forma, o cliente terá dois sinalizadores atribuídos a ele. Se ambos forem **SIM**, então bingo! Temos um candidato que está prestes a realizar um cancelamento.

## Saiba mais!

-   **Booleano**: Um tipo de dado que pode ser `True` ou `False`.
-   **Vetor (Array)**: Uma coleção de valores, que podem ser de diferentes tipos, armazenados sob um único nome de variável.
-   **Vetor booleano (Boolean Array)**: Um vetor composto por valores booleanos. É uma forma de representar uma série de valores Verdadeiro/Falso.

Execute o código abaixo para ver a aparência de um vetor booleano.

Vamos criar dois vetores booleanos. `more_4_contacts` indicará se o cliente contatou o suporte mais de 4 vezes ou não, e `bank_transfer` indicará se o cliente paga por transferência bancária ou não.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

\# calcule se o cliente entrou em contato com o suporte mais de 4 vezes ou não

more\_4\_contacts \= df\["support\_contacts\_last\_30\_days"\] \> 4

  

\# calcule se o cliente paga por transferência bancária ou não

bank\_transfer \= df\["payment\_method"\] \== "bank\_transfer"

  

\# imprima o vetor more\_4\_contacts

print(more\_4\_contacts)

  

\# imprima o vetor bank\_transfer

print(bank\_transfer)

Mostrar a soluçãoExecutar

Usaremos esses dois vetores para construir nosso alerta. Para simplificar nosso alerta, vamos importar uma nova biblioteca chamada `numpy`. Essa biblioteca nos permite realizar operações de vetores.

Não se preocupe se isso for novo; nossos bootcamps de dados possui um curso inteiro dedicado a aprender `numpy`, `pandas` e outras bibliotecas mencionadas nesse curso.

Escreveremos uma linha de código que seja equivalente a dizer: Se um cliente entrou em contato com o suporte mais de 4 vezes nos últimos 30 dias e pagou por meio de transferência bancária, ele corre o risco de estar em situação de cancelamento.

A função que nos ajudará chama-se `logical_and`. Seu funcionamento é muito simples:

Ele compara dois vetores, elemento por elemento, e retorna um vetor com valores `True` onde ambos os vetores possuem valores `True` no mesmo índice e valores `False` caso contrário. Portanto, a função compara `more_4_contacts` e `bank_transfer` e retorna um vetor com valores `True` onde ambas os vetores possuem valores `True` no mesmo índice. Isso indica que o cliente entrou em contato com o suporte mais de 4 vezes nos últimos 30 dias e pagou por transferência bancária.

Vamos ver nossos resultados na prática.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

import numpy as np \# importe a biblioteca numpy

  

\# calcule a variável churn\_risk

\# a função logic\_and retorna true if

\# encontra verdadeiro em ambas os vetores, caso contrário

\# retorna false

churn\_risk \= np.logical\_and(more\_4\_contacts, bank\_transfer)

  

\# imprima a lista

print(churn\_risk)

Mostrar a soluçãoExecutar

Os clientes que marcaram `True` para ambas as variáveis serão rotulados como `True` em nosso `churn_risk`. Por outro lado, aqueles que usaram um método de pagamento diferente de transferência bancária ou tiveram menos de 4 contatos com suporte nos últimos 30 dias serão rotulados como `False`.

### 🔥

Temos mais conhecimentos sobre essas bibliotecas e muito mais em nosso curso completo. [Entrar agora](https://tripleten.com/es-mex/data-scientist/webinar/)!

### Testando nosso alerta baseado em regras

Agora nós temos:

1.  Nossa lista de clientes com todos os clientes que cancelaram até o momento;
2.  Nossa lista de alerta, com todos os clientes que identificamos como risco de cancelamento.

Isso significa que podemos testar nosso sistema de alerta comparando essas variáveis!

O código abaixo compara as instâncias em que nossa variável `churn_risk` e a variável `churned` correspondem. Em outras palavras, ele determina quantas vezes teríamos previsto corretamente as perdas de clientes se tivéssemos executado esse alerta para os clientes desse conjunto de dados.

Usaremos a função `crosstab` da biblioteca pandas para comparar as duas variáveis e gerar uma tabela informativa. Essa função fará o trabalho pesado para nós e analisaremos os resultados posteriormente.

CódigoPYTHON

9

1

2

3

4

5

\# gera uma tabela com os dados de cancelados

tab \= pd.crosstab(df\["churned"\], churn\_risk)

  

\# imprima a tabela

print(tab)

Mostrar a soluçãoExecutar

Então, o que esta tabela nos diz?

Das 1.025 vezes que o alerta `churn_risk` tivesse sido acionado (a soma da coluna True), teríamos acertado 170 vezes. Os 855 alertas restantes teriam sido "alarmes falsos" (clientes que não cancelaram).

Além disso, 546 clientes cancelaram, mas nenhum alerta foi gerado.

Isso significa que fornecemos algum valor ao cliente, mas ainda há um longo caminho a percorrer!

Mas, por enquanto, isso é suficiente! Nossa análise simples provou nosso caso e produziu bons resultados que podemos implementar rapidamente em nossos negócios. Isso, sem dúvida, fará a diferença. Afinal, 170 clientes não é um número pequeno. E temos um bom lugar para melhorar. Faremos isso mais adiante nesse curso!

Você pegou o jeito desse processo e está pronto para seguir em frente.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-19-17-444Z.md
### Última modificação: 2025-05-28 18:19:17

# Conclusões - TripleTen

Capítulo 2/5

Análise Exploratória de Dados

Opcional

# Conclusões

Nosso sistema de alerta baseado em regras é melhor do que nada, e a empresa startup ficou satisfeita com seu trabalho. No entanto, como você pode imaginar, usar uma abordagem baseada em regras possui muitas limitações. Poderíamos gastar muito tempo tentando descobrir novas regras, e esse processo se tornaria muito difícil de executar se estivéssemos usando um conjunto de dados com muitas variáveis (o que geralmente é o caso de cenários da vida real).

Existe uma maneira de automatizar o processo de identificação de regras para esse tipo de problema? No capítulo final deste curso introdutório, você terá uma ideia de como esse problema pode ser resolvido usando o aprendizado de máquina.

Uau, você conseguiu tanto! Nesse capítulo, você:

-   Explorou algumas das bibliotecas Python mais populares para análise de dados (`pandas`, `matplotlib`, `numpy`).
-   Realizou análise exploratória de dados em um conjunto de dados.
-   Conseguiu obter percepções e fez recomendações.

Você deveria estar orgulhoso de si mesmo!

Como reiteramos ao longo deste capítulo, nosso curso tem como objetivo dar a você uma visão rápida do que você aprenderá em nossos bootcamps de dados. Nossos bootcamps fornecem uma explicação completa sobre Python, estatísticas e análise exploratória de dados.

### 🔥

Visite o link a seguir se quiser saber mais [sobre o programa](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/ptbr-syllabus/DA_PT_Syllabus_V5.pdf)

### 🔥

A propósito, você poderá participar de nossos webinários gratuitos para se informar mais sobre sua futura carreira no campo de dados. Se inscreva [aqui.](https://practicum.com/es-mex/data-scientist/webinar/)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-21-10-129Z.md
### Última modificação: 2025-05-28 18:21:10

# Introdução a narrativa - TripleTen

Capítulo 3/5

Narrativa de Dados

Opcional

# Introdução a narrativa

A narrativa é a arte de transmitir uma história ou uma série de eventos a um público através de uso de palavras, recursos visuais e outras técnicas criativas.

Em sua essência, a narrativa é sobre a criação de uma conexão entre o narrador e o público, bem como entre o público e a própria história. Compartilhando uma história, o narrador convida o ouvinte ou espectador para se juntar a ele em uma jornada de imaginação e exploração e se tornar envolvido emocionalmente com os personagens e eventos da narrativa.

Como um profissional de dados, você quer que seu público esteja envolvido quando você está apresentando os resultados de uma análise, já que seu feedback é de extrema importância.

Criar uma narrativa em torno de suas visualizações e percepções é fundamental para alcançar isso.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_9_1688453635.png)

Aqui estão 3 dicas que te ajudarão a criar uma história convincente:

-   Ela deve começar com um contraponto, um conflito para engajar o seu público.
-   O desenvolvimento da história é onde você mostra suas percepções por uma série de recursos visuais bonitos e envolventes. Esses recursos visuais devem estar conectados entre si por uma narrativa.
-   O final da história assume a forma de suas recomendações baseadas nas percepções mostradas durante a apresentação.

Ser capaz de contar uma boa e convincente história às vezes pode ser mais eficaz do que apresentar fatos concretos em uma tabela ou até mesmo em literatura científica.

Para este capítulo, trabalharemos na seguinte história:

Apesar de abundância de provas da eficiência das vacinas em geral, ainda há grupos de indivíduos que não acreditam que a vacinação seja necessária. Esta discussão se tornou mais polarizada após a criação das vacinas contra a COVID.

Vamos ver como podemos apresentar a nossa história usando as dicas acima:

-   Conflito: A vacinação contra a COVID é eficaz? (Usar uma questão aberta polarizante como o título geralmente desperta o interesse do público).
-   Desenvolvimento: As perguntas que você buscará responder com sua análise exploratória (isso representará cerca de 80% do seu projeto).
-   O final: A parte divertida sobre o trabalho de um profissional de dados é que nós realmente não sabemos como a história termina até que processemos e resumamos nossas percepções.

Esperamos que você esteja animado e ansioso para ver como ESTA história se desenrola!

Para contar uma história convincente sobre a eficiência da vacinação contra a COVID, você vai usar SQL para extrair dados relevantes de um **conjunto de dados** (DB, abreviação para "database") e o Tableau para visualizá-los. Você poderá compartilhar essa história com sua família e amigos e até mesmo em redes maiores, como o LinkedIn.

**O que é SQL?**

**SQL** (Structured Query Language, ou Linguagem de Consulta Estruturada) é uma linguagem de programação usada para gerenciar bancos de dados relacionais. Ela provê uma maneira padronizada de acessar e manipular dados armazenados nos sistemas de gerenciamento de bancos de dados relacionais (SGBDs). SQL pode ser usada para recuperar, inserir, atualizar e remover dados de bancos de dados, bem como criar e modificar esquemas de bancos de dados.

Os profissionais da área de dados, como analistas, cientistas de dados e, administradores de bancos de dados, usam SQL amplamente para consultar e analisar dados. A SQL permite que eles extraiam informações específicas de grandes conjuntos de dados e façam cálculos e agregações complexas. Também podemos usar SQL para limpar, transformar e normalizar dados, bem como criar relatórios e dashboards para visualizá-los. Esta é uma ferramenta essencial nas profissões relacionadas a dados, e a proficiência em SQL é uma habilidade muito procurada no mercado de trabalho.

**Introdução ao Tableau**

**Tableau** é uma ferramenta de visualização de dados que permite que os usuários criem dashboards interativos e visualmente atraentes, relatórios e gráficos com base em várias fontes de dados. Ele é usado pelos profissionais de dados, como analistas, cientistas de dados e, analistas de negócios, para transformar dados em percepções acionáveis.

Tableau permite aos usuários se conectarem e juntarem dados de várias fontes, incluindo planilhas, bancos de dados e serviços em nuvem. Ele oferece uma interface drag-and-drop que facilita a criação de recursos visuais interativos e exploração de dados em tempo real. Tableau permite que os usuários criem dashboards que podem ser compartilhados em toda a empresa e fornece recursos avançados, como combinação de dados, previsão e modelagem estatística.

Profissionais de dados podem usar o Tableau para analisar e interpretar grandes conjuntos de dados, identificar tendências e padrões e comunicar suas percepções para as partes interessadas. As capacidades poderosas de visualização que o Tableau tem ajudam os profissionais de dados a apresentar dados complexos de uma forma facilmente compreensível e permitem que os tomadores de decisão tomem decisões informadas com base nas percepções baseadas em dados. O Tableau é amplamente usado em várias indústrias, incluindo finanças, saúde, marketing e setores governamentais, e a proficiência nele é uma habilidade altamente desejada para os profissionais da área de dados.

![](https://practicum-content.s3.amazonaws.com/resources/1.3.1PT_1688486547.png)

**Saiba mais?**

-   Um conjunto de dados é uma coleção estruturada de dados que é armazenada e organizada de uma forma que permite recuperar e modificar os dados de forma eficiente.
-   Um banco de dados relacional é um tipo de banco de dados que organiza os dados em uma ou várias tabelas com uma chave unívoca para cada linha. As tabelas estão relacionadas entre si através dessas chaves, o que permite fazer consultas complexas e manipular dados em todas as tabelas.

### 🔥

Ambos os nossos bootcamps de dados incluem cursos sobre SQL, e o percurso de Analista de Dados contém um curso dedicado ao Tableau. [Venha estudar tudo isso conosco](https://practicum.com/es-mex/data-scientist/webinar/)!

### Refletindo sobre as questões de pesquisa

Agora vamos pensar nas nossas questões de interesse: a vacinação contra a COVID-19 é eficiente?

Esta é uma questão muito complexa, e nós, como profissionais da área de dados, precisamos dividir uma questão complexa em perguntas menores, para que possamos tentar responder a elas usando a análise exploratória de dados. Responder a essas questões secundárias deve nos levar à resposta à nossa questão principal de interesse.

Vamos pensar nas questões mais simples e específicas sobre este tópico:

-   As vacinas contra a COVID reduzem o número das mortes pela COVID-19?
-   As vacinas reduzem o número de novos casos da COVID-19?
-   Elas fornecem a imunidade contra a COVID-19?

Ótimo! Agora temos perguntas um pouco mais específicas, mas ainda precisamos descobrir quais dados podemos usar para responder a elas.

Pergunta

Você pode pensar em algumas outras perguntas a que também seria útil responder?

A eficiência das vacinas não depende da idade?

A severidade dos sintomas após a vacinação é a mesma que sem vacinas? (ou seja, podemos analisar o número de hospitalizações por causa da Covid etc)

Seu entendimento sobre o material é impressionante!

Você conversa com um colega mais experiente, e ele sugere que você se concentre em alguns países da América Latina e mantenha a análise no nível dos países. Ele indica alguns conjuntos de dados coletados pela Organização Mundial da Saúde (OMS).

Estes conjuntos de dados estão armazenados em um banco de dados.

Este curso introdutório não abordará bancos de dados com profundidade, mas é importante saber que as empresas geralmente armazenam dados "tabulares" (ou seja, dados em tabelas) em bancos de dados. Como um profissional da área de dados, você vai frequentemente trabalhar com bancos de dados para extrair dados para seus projetos.

### Seu futuro resultado

No final deste capítulo, você não apenas encontrará as respostas às questões da sua pesquisa, mas também construirá um dashboard no Tableau para ilustrar suas descobertas. Além disso, te ajudaremos a publicar seu dashboard, para outras pessoas poderem ver os resultados do seu extenso trabalho!

Ele ficará mais ou menos assim:

![](https://practicum-content.s3.amazonaws.com/resources/image_1688692519.png) _[tableau](https://public.tableau.com/app/profile/jonas.torres/viz/Pract-CovidAnalysis/Dashboard1)_

Então vamos manter os nossos olhos no prêmio e continuar a jornada em direção à criação de uma história convincente e de um dashboard que comunique de forma eficaz as nossas descobertas sobre a vacinação contra a COVID. Vamos avançar em direção a este objetivo!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-21-13-089Z.md
### Última modificação: 2025-05-28 18:21:13

# Extraindo dados com SQL - TripleTen

Capítulo 3/5

Narrativa de Dados

Opcional

# Extraindo dados com SQL

Extrair dados é o primeiro passo que precisamos dar quando iniciamos um projeto de análise de dados. Como já discutimos, dados são, normalmente, armazenados em bancos de dados. Então, como os profissionais da área de dados interagem com bancos de dados?

Você precisa de dois elementos:

-   Uma conexão com o banco de dados do qual você deseja extrair os dados
-   A habilidade de escrever consultas utilizando a Linguagem de Consulta Estruturada (do inglês, Structured Query Language), ou SQL. Você pode pensar em uma consulta como uma pergunta feita a um banco de dados usando SQL.

**Saiba mais!**

-   Uma consulta é uma questão ou solicitação de informações de um banco de dados feita usando a Linguagem de Consulta Estruturada (SQL). Consultas podem ser usadas para extrair, filtrar e ordenar dados de um banco de dados para responder a questões específicas ou realizar análises.

Atualmente, bancos de dados são frequentemente armazenados em provedores de armazenamento em nuvem, como Azure, AWS ou GCP. Para consultar dados, você normalmente usa uma interface fornecida por uma empresa específica.

Neste capítulo, você fará suas consultas no ambiente do TripleTen, que reflete cenários do mundo real. Seu objetivo é se familiarizar com a SQL e se abstrair da tecnologia subjacente e infraestrutura atrás do mecanismo virtual de banco de dados. Este exercício é como aprender a dirigir um carro. Depois de aprender os princípios básicos, você pode alternar entre várias marcas e modelos de carros sem quaisquer problemas. O mesmo vale para o uso de diferentes provedores de armazenamento em nuvem para executar a SQL.

Neste capítulo, você vai trabalhar com duas tabelas do banco de dados:

-   `vaccination_data`: contém três variáveis/colunas:
    -   `COUNTRY`: nome do país.
    -   `ISO3`: código ISO de três caracteres para o país.
    -   `FIRST_VACCINE_DATE`: a data em que a primeira vacina foi aplicada em um determinado país.
-   `who_covid_global_data`:
    -   `Date_reported`: a data em que o resto dos valores na tabela foi registrado.
    -   `Country_code`: código do país.
    -   `COUNTRY`: nome do país.
    -   `WHO_region`: região monitorada pela Organização Mundial da Saúde (de agora em diante – WHO).
    -   `New_cases, Cumulative_cases`: novos casos da Covid e casos cumulativos.
    -   `New_deaths, Cumulative_deaths`: novas mortes e mortes cumulativas de pacientes com a Covid.

Vamos continuar. O problema da conexão foi resolvido fornecendo à plataforma uma conexão com o banco de dados especificado. Agora compartilhamos com você duas consultas simples para te ajudar a começar.

Se executarmos o bloco de código abaixo:

```
SELECT 
  * 
FROM 
  vaccination_data 
LIMIT 
  5;
```

Ele retornará 5 linhas com todos os campos da tabela `vaccination_data`:

```
                    COUNTRY ISO3 FIRST_VACCINE_DATE
0     Afghanistan  AFG         2021-02-22
1         Albania  ALB         2021-01-13
2         Algeria  DZA         2021-01-30
3  American Samoa  ASM         2020-12-21
4         Andorra  AND         2021-01-20
```

Este comando SQL é usado para recuperar dados da tabela `vaccination_data`.

A instrução `SELECT` especifica as colunas que queremos recuperar da tabela. Neste caso, usamos `*` para selecionar todas as colunas.

A instrução `FROM` especifica a tabela da qual queremos recuperar dados. Neste caso, queremos recuperar dados da tabela `vaccination_data`.

A instrução `LIMIT` é usada para limitar o número de linhas retornadas pela consulta. Neste caso, ela limita o resultado às 5 primeiras linhas. Então, este comando SQL recupera todas as colunas da tabela `vaccination_data` e retorna as 5 primeiras linhas.

Agora, como podemos escrever uma consulta semelhante à anterior, mas que retorne 5 linhas com todos os campos da tabela `who_covid_global_data` em vez da tabela `vaccination_data`? Pense em como você completaria a consulta abaixo:

```
SELECT 
  * 
FROM 
  # Escreva seu código aqui
LIMIT 
  5;
```

Para comprovar que sua intuição está correta, aqui está a solução:

```
SELECT 
  * 
FROM 
  who_covid_global_data 
LIMIT 
  5;
```

E o resultado:

```
      Date_reported Country_code      Country  New_cases  \
0    2020-01-03           AF  Afghanistan      0
1    2020-01-04           AF  Afghanistan      0
2    2020-01-05           AF  Afghanistan      0
3    2020-01-06           AF  Afghanistan      0
4    2020-01-07           AF  Afghanistan      0

   Cumulative_cases  New_deaths  Cumulative_deaths
0                 0           0                  0
1                 0           0                  0
2                 0           0                  0
3                 0           0                  0
4                 0           0                  0
```

Os comandos acima dizem ao banco de dados que você quer selecionar todas as variáveis de uma tabela específica (por exemplo, `who_covid_global_data`) e exibir apenas as 5 primeiras linhas (`LIMIT 5`).

Vamos dar uma olhada mais de perto nas duas tabelas resultantes das nossas consultas.

`vaccination_data`:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_18_1687512615.png)

`who_covid_global_data`:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_19_1687512629.png)

Pergunta

Temos dados suficientes para tentar responder às nossas três questões da lição anterior?

-   As vacinas COVID reduzem o número das mortes pela COVID-19?
-   As vacinas reduzem o número de novos casos da COVID-19?
-   Elas fornecem a imunidade contra a COVID-19?

Sim

Não

Seu entendimento sobre o material é impressionante!

Parece que temos dados suficientes para avançar para os próximos passos. Vamos começar filtrando nossos resultados para que possamos chegar mais perto de uma resposta concreta às nossas perguntas.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-21-16-042Z.md
### Última modificação: 2025-05-28 18:21:16

# Filtrando resultados em SQL - TripleTen

Capítulo 3/5

Narrativa de Dados

Opcional

# Filtrando resultados em SQL

### Filtrando o conjunto de dados da Covid com base em países específicos

Agora sabemos como extrair dados do conjunto de dados usando `SELECT`, mas e se precisarmos escolher apenas os dados que atendam aos critérios da nossa pesquisa? Imagine uma tabela que contém milhões de linhas. Faria sentido baixar todas elas ou apenas aquelas que precisamos para responder às questões da nossa pesquisa?

Vamos nos concentrar em um seleto grupo dos países da América Latina que você precisa analisar. Os nossos dados precisam ser filtrados para incluírem apenas aqueles países: México, Chile, Equador, Brasil, Peru e Argentina.

Nós também demonstraremos como analisar dados diariamente fornecendo snapshots diários dos dados disponíveis. Selecionamos uma data que queremos examinar, a qual é mostrada abaixo.

Observe que a etapa de filtragem é realizada dentro da instrução `WHERE` usando a cláusula `IN`.

A seguinte consulta SQL diz ao banco de dados para retornar apenas as linhas onde `Country` está dentro de um conjunto específico de valores.

```
SELECT 
  * 
FROM 
  who_covid_global_data 
WHERE 
  Country in (
    "Mexico", "Chile", "Ecuador", "Brazil", 
    "Peru", "Argentina"
  ) 
  and Date_reported = "2021-11-07"
```

E este é o resultado que obtemos:

Date\_reported

Country\_code

Country

New\_cases

Cumulative\_cases

New\_deaths

Cumulative\_deaths

2021-11-07

AR

Argentina

568

5367868

3

116887

2021-11-07

BR

Brazil

13321

21862458

389

609060

2021-11-07

CL

Chile

2796

1706622

27

37841

2021-11-07

EC

Ecuador

116

518975

4

32988

2021-11-07

MX

Mexico

3231

3872539

150

296850

2021-11-07

PE

Peru

1062

2205486

23

200373

Uau, há milhares de linhas dedicadas a apenas esses países!

Vamos analisar o que acabamos de fazer:

-   `SELECT`: Esta palavra-chave informa ao banco de dados que você vai selecionar algumas colunas. Após `SELECT`, você pode digitar `*` para selecionar todas as colunas ou escolher explicitamente um conjunto de colunas (veremos em breve como fazer isso).
-   `FROM`: Esta palavra-chave diz ao banco de dados qual tabela usar. Ela tem que ser seguida pelo nome da tabela fornecida, no exemplo acima é `who_covid_global_data`.
-   `WHERE`: Esta palavra-chave diz ao banco de dados que nós vamos aplicar uma determinada condição para filtrar os dados. Ela é seguida pela condição que queremos aplicar.
-   A condição no nosso exemplo `Country in ("Mexico", "Chile", "Ecuador", "Brazil", "Peru", "Argentina")` está especificando o nome de uma variável (`Country`) que deve ser comparada a uma lista de países. Isso é feito usando a palavra-chave `in` para indicar uma lista em que a pesquisa deve ser feita e a própria lista em forma de strings entre parênteses.
-   A condição `and Date_reported = "2021-11-07"` está especificando que queremos filtrar os dados para que eles incluam apenas as linhas em que a coluna `Date_reported` seja igual a `"2021-11-07"`. Isso é útil quando queremos selecionar os dados para um dia ou um intervalo de tempo específico.

Há apenas algumas maneiras de passar regras para filtrar dados por meio de uma consulta SQL. Não se preocupe se isso te parece assustador; com a prática, você vai entender melhor como isso funciona.

Agora vamos pensar em como escrever o código que mudará sua análise para o Canadá e os Estados Unidos. Para isso, você precisará selecionar todos os dados da nossa tabela `who_covid_global_data` em que os países são `"Canada"` e `"United States of America"` e a data reportada é `"2021-11-07"`.

Para comprovar que sua intuição está correta, tente completar as lacunas abaixo por conta própria antes de compará-las com nossa solução. Aqui estão as lacunas que você precisa preencher:

```
SELECT 
  # reflita sobre como você preencheria as lacunas
FROM 
  # escreva da qual tabela
WHERE 
  # escreva seus filtros
```

Aqui está a solução:

```
SELECT 
  * 
FROM 
  who_covid_global_data 
WHERE 
  Country in ("Canada", "United States of America") 
  and Date_reported = "2021-11-07"
```

e o resultado:

Date\_reported

Country\_code

Country

New\_cases

Cumulative\_cases

New\_deaths

Cumulative\_deaths

2021-11-07

CA

Canada

2533

1727683

17

28847

2021-11-07

US

United States of America

89858

46149896

1614

752136

Se você quiser verificar qualquer outro país, é tão simples como alterar seus filtros. Quando o nosso sistema de análise estiver pronto e funcionando, você será capaz de verificar tudo em que possa estar interessado!

### Selecionando variáveis

Para selecionar um conjunto específico de variáveis de uma tabela SQL, você pode substituir o caractere `*` por uma lista das colunas que você quer incluir separadas por vírgulas após a instrução `SELECT`. Por exemplo, no bloco de código abaixo, estamos selecionando um subconjunto de colunas da tabela `who_covid_global_data`, em que o país é um dos seis países da América Latina:

```
SELECT 
  Date_reported, 
  Country, 
  New_cases, 
  Cumulative_cases 
FROM 
  who_covid_global_data 
WHERE 
  Country in (
    "Mexico", "Chile", "Ecuador", "Brazil", 
    "Peru", "Argentina"
  ) 
  and Date_reported = "2021-11-07"
```

E aqui está o resultado:

Date\_reported

Country

New\_cases

Cumulative\_cases

2021-11-07

Argentina

568

5367868

2021-11-07

Brazil

13321

21862458

2021-11-07

Chile

2796

1706622

2021-11-07

Ecuador

116

518975

2021-11-07

Mexico

3231

3872539

2021-11-07

Peru

1062

2205486

Esta consulta vai retornar uma tabela que inclui apenas as colunas `Date_reported`, `Country`, `New_cases` e `Cumulative_cases`, filtrando os resultados de modo que eles incluam apenas os seis países listados na cláusula `WHERE` e a informação de `Date_reported = "2021-11-07"`.

Agora podemos coletar dados de uma tabela, mas para obter informações valiosas, precisaremos cruzar os dados de ambas as tabelas. Vamos aprender como fazer isso!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-21-55-203Z.md
### Última modificação: 2025-05-28 18:21:55

# Juntando tabelas em SQL - TripleTen

Capítulo 3/5

Narrativa de Dados

Opcional

# Juntando tabelas em SQL

Nesta etapa, as coisas ficam um pouco mais complicadas. Para completar a tarefa atual, precisaremos juntar ambas as tabelas.

**Saiba mais!**

-   Uma junção em SQL permite combinar dados de duas ou mais tabelas com base em uma coluna relacionada entre elas.
-   A sintaxe de junção de tabelas envolve a palavra-chave `JOIN` seguida pelo nome da segunda tabela.
-   A palavra-chave `ON` é usada para especificar a coluna/as colunas compartilhadas entre as duas tabelas.
-   Há diferentes tipos de junções em SQL, como `INNER JOIN`, `LEFT JOIN`, `RIGHT JOIN` e `FULL OUTER JOIN`, cada uma delas tem seu comportamento ligeiramente diferente entre si.

Vamos dar uma olhada em como funciona uma instrução JOIN. Juntaremos nossas duas tabelas:

```
SELECT 
  c.Date_reported, 
  c.Country, 
  c.New_cases, 
  c.Cumulative_cases, 
  c.New_deaths, 
  c.Cumulative_deaths, 
  v.FIRST_VACCINE_DATE 
FROM 
  who_covid_global_data AS c 
  LEFT JOIN vaccination_data AS v ON c.Country = v.COUNTRY 
WHERE 
  c.Country IN(
    "Mexico", "Chile", "Ecuador", "Brazil", 
    "Peru", "Argentina"
  ) 
  and c.Date_reported = "2021-11-07"
```

E este é o resultado:

Date\_reported

Country

New\_cases

Cumulative\_cases

New\_deaths

Cumulative\_deaths

FIRST\_VACCINE\_DATE

2021-11-07

Argentina

568

5367868

3

116887

2020.12.29

2021-11-07

Brazil

13321

21862458

389

609060

2021-01-17

2021-11-07

Chile

2796

1706622

27

37841

2020-12-21

2021-11-07

Ecuador

116

518975

4

32988

2021-01-20

2021-11-07

Mexico

3231

3872539

150

296850

2020-12-24

2021-11-07

Peru

1062

2205486

23

200373

2021-02-09

Bem, vamos passar pelos novos elementos do código:

-   Aliases de tabelas: Esses são caracteres adicionados ao final do nome de cada tabela, por exemplo, `who_covid_global_data as c`. Os aliases reduzem a quantidade de código e de digitação necessários, o que, por sua vez, reduz a probabilidade de erros de digitação. Por exemplo, `c.Date_reported` é mais curto e menos propenso a erros do que `who_covid_global_data.Date_reported`. Quando juntamos duas ou mais tabelas, é uma boa prática usar a sintaxe `table_name.variable_name` para evitar bugs. Por exemplo, uma junção de duas tabelas que contêm colunas com o mesmo nome pode confundir o interpretador SQL e causar um erro.
-   A instrução `LEFT JOIN` instrui o banco de dados para juntar a tabela `vaccination_data` (seu alias é `v`) à tabela `who_covid_global_data` (já referenciada pela palavra-chave `FROM`). Você aprenderá mais sobre os tipos de JOIN disponíveis no futuro. Isso é alcançado usando a cláusula `ON` e combinando `c.Country` (a variável country da tabela com o alias `c`) com `v.COUNTRY` (a variável country da tabela`v`).

Uau! Fizemos tanta coisa com apenas algumas linhas de código. Poderíamos fazer as mesmas operações com tabelas que contêm milhões de linhas. Esta é uma das muitas vantagens de aprender SQL.

Abaixo estão as seis primeiras linhas da tabela que acabamos de criar. A coluna `FIRST_VACCINE_DATE` vem da tabela `vaccination_data`, enquanto todas as outras colunas vêm da tabela `who_covid_global_data`.

Date\_reported

Country

New\_cases

Cumulative\_cases

New\_deaths

Cumulative\_deaths

FIRST\_VACCINE\_DATE

2021-11-07

Argentina

568

5367868

3

116887

2020.12.29

2021-11-07

Brazil

13321

21862458

389

609060

2021-01-17

2021-11-07

Chile

2796

1706622

27

37841

2020-12-21

2021-11-07

Ecuador

116

518975

4

32988

2021-01-20

2021-11-07

Mexico

3231

3872539

150

296850

2020-12-24

2021-11-07

Peru

1062

2205486

23

200373

2021-02-09

Agora que temos os dados necessários para começar a responder a nossas perguntas, podemos avançar e fazer uma análise exploratória no Tableau. Salvamos as nossas tabelas SQL em arquivos CSV. Podemos usar a biblioteca Pandas, que já apresentamos no primeiro capítulo, para realizar algumas transformações em nossos dados.

Como você é novato nisso, nós vamos nos oferecer para fazer isso para você. O código que escrevemos será mostrado mais abaixo. Não se preocupe se você achar que está muito avançado, estamos apenas dando um exemplo do que você aprenderá a fazer em nossos bootcamps de dados.

Pergunta

Quando podemos juntar duas tabelas?

Quando quisermos.

Quando as duas tabelas têm pelo menos uma coluna comum.

Excelente!

### 🔥

Quer entender e fazer consultas complexas em SQL? [Junte-se ao nosso bootcamp](https://practicum.com/es-mex/data-scientist/webinar/)!

Como mencionamos anteriormente, agora vamos mudar para Python, para preparar os nossos dados para uma análise futura no Tableau. Abaixo está o código que fará isso para nós. Não se preocupe se você não entender cada linha; nós incluímos comentários para esclarecer todos os passos.

O nosso objetivo é criar duas novas colunas. A primeira coluna exibirá uma contagem regressiva de dias até que a primeira vacina seja aplicada. Depois que a primeira vacina for aplicada, a coluna vai exibir a contagem progressiva de dias. A segunda coluna vai indicar se a vacinação começou ou não usando um valor "yes" (sim) ou "no" (não).

Vamos dar uma olhada em como podemos cumprir esta missão. Usaremos a biblioteca pandas para facilitar nosso trabalho:

```
import pandas as pd  # importe a biblioteca pandas
from datetime import datetime as dt  # importe a biblioteca datetime

# passo 1: Leia os dados que você acabou de baixar
df_eda = pd.read_csv(
    "https://raw.githubusercontent.com/JJTorresDS/ds-data-sources/main/covid_data_raw.csv"
)

# passo 2: Crie uma variável que armazena os dias desde a aplicação da primeira vacina
# passo 2.1: Para isso, precisamos converter as nossas colunas de date para datetime, 
# para que possamos calcular com facilidade a diferença entre elas
df_eda["Date_reported"] = pd.to_datetime(
    df_eda["Date_reported"], infer_datetime_format=True
)
df_eda["FIRST_VACCINE_DATE"] = pd.to_datetime(
    df_eda["FIRST_VACCINE_DATE"], infer_datetime_format=True
)

# passo 2.2: Crie uma variável que contém os dias desde a aplicação da primeira vacina
# calculando a diferença entre o dia atual e o dia em que a primeira vacina
# foi aplicada
df_eda["days_since_vaccine"] = (
    df_eda["Date_reported"] - df_eda["FIRST_VACCINE_DATE"]
).dt.days

# passo 3: Crie um rótulo para indicar se, em uma data especifica,
# a vacinação já comecou
df_eda["vaccination_started"] = df_eda["days_since_vaccine"].apply(
    lambda x: "Yes" if x >= 0 else "No"
)

# imprima as 5 primeiras linhas do nosso DataFrame
print(df_eda.head())
```

Para obter esse resultado:

```
Date_reported Country_code  ... days_since_vaccine vaccination_started
0    2020-01-03           AR  ...               -361                  No
1    2020-01-04           AR  ...               -360                  No
2    2020-01-05           AR  ...               -359                  No
3    2020-01-06           AR  ...               -358                  No
4    2020-01-07           AR  ...               -357                  No

[5 rows x 12 columns]
```

Podemos realizar essas transformações usando SQL, mas isso exigiria o uso de funções adicionais de SQL adicionando um nível desnecessário de complexidade.

Além disso, é uma prática comum usar uma combinação de SQL e Python para transformar dados para análises futuras. Esta sequência de transformações é normalmente conhecida como um **pipeline de dados**.

### 🔥

Você pode aprender mais sobre SQL e pipelines de dados inscrevendo-se nos bootcamps de dados da TripleTen. [Junte-se a nós agora](https://practicum.com/es-mex/data-scientist/webinar/)!

Então, o que conseguimos alcançar usando essas transformações?

Adicionamos duas variáveis.

-   `days_since_vaccine` registra o número de dias desde a aplicação da primeira vacina: números negativos indicam os dias que faltam para a primeira aplicação da vacina, e números positivos indicam o número de dias que se passaram desde a aplicação da primeira vacina contra a covid-19. Por exemplo, -2 indica 2 dias antes da primeira aplicação da primeira dose da vacina, e 2 (ou +2) indica que 2 dias se passaram desde que a primeira vacina contra a covid-19 foi aplicada.
-   `vaccination_started` é uma variável rotulada como True para os dias que vêm após a data da primeira aplicação da vacina.

E como fizemos isso? Utilizando as bibliotecas, conversões e tipos de dados disponíveis para nós! Por exemplo, a biblioteca datetime, que pode ser nova para você, oferece uma grande flexibilidade com trabalho de dados.

E nós já estamos prontos para prosseguir com o Tableau!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-21-56-523Z.md
### Última modificação: 2025-05-28 18:21:56

# Primeiros passos com o Tableau - TripleTen

Capítulo 3/5

Narrativa de Dados

Opcional

# Primeiros passos com o Tableau

O que é o Tableau? Essa é uma das ferramentas mais populares de visualização de dados que nos permite criar apresentações convincentes em questão de minutos. Embora ela seja orientada para analistas de dados, aqueles que querem se tornar cientistas de dados também podem beneficiar de aprender os fundamentos do Tableau.

Vamos completar as seguintes lições passo a passo. Por favor, siga os passos que especificamos ao longo das lições.

O que você precisa para começar? Bem, vamos começar baixando o Tableau!

### Baixando o Tableau

Para instalar o Tableau Public, siga os passos abaixo:

1.  Acesse este link: [https://public.tableau.com/en-us/s/download](https://public.tableau.com/en-us/s/download).
2.  Insira seu endereço de e-mail, baixe o instalador e execute-o.

![](https://practicum-content.s3.amazonaws.com/resources/image_4_1687513288.png)

3.  Depois de instalá-lo, inicie o Tableau. O sistema iniciará automaticamente com a interface de gerenciamento de projetos.

![](https://practicum-content.s3.amazonaws.com/resources/image_4_1688493869.png)

### Importando conjuntos de dados

Antes de começarmos a nossa análise, seria útil saber a população total dos países em que nos vamos concentrar. Estes dados nos permitem calcular métricas úteis, como a porcentagem da população que contraiu a Covid.

No trabalho real de um profissional de dados, você teria que obter o conjunto de dados da Internet ou de outra fonte ou talvez até mesmo comprá-lo.

Para os propósitos deste curso, vamos cobrir essa necessidade. Vamos baixar os conjuntos de dados:

[covid\_data.csv](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_1_Sprint/covid_data.csv)

[population\_by\_country.csv](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_1_Sprint/population_by_country.csv)

O primeiro arquivo contém os dados obtidos após as nossas últimas transformações feitas na lição anterior. O segundo arquivo contém a população dos países que nós vamos analisar.

Por favor, baixe ambos os arquivos e armazene-os em uma pasta local. Escolhemos o nome **2-datastorytelling**.

A pasta deve conter todos os arquivos CSV que você vai usar.

Para importar os arquivos para o Tableau Public, siga estes passos:

1.  Abra o Tableau Public.
2.  Selecione a opção **Text file** que te permite importar arquivos CSV.
3.  Selecione o arquivo que você quer importar (vamos começar com **covid\_data.csv**) e clique em **Open** (Abrir).

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_20_1687513461.png)

4.  Agora você deve ver que o arquivo foi importado.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_21_1687513483.png)

5.  Clique no botão **Add** (Adicionar) e selecione **Text file** (Arquivo de texto) novamente.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_22_1687513504.png)

6.  Agora importe o arquivo **population\_by\_country.csv**:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_10_1688453725.png)

7.  Você deve ver este conjunto de dados no menu **files**

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_24_1687513547.png)

O Tableau é uma ferramenta poderosa de visualização de dados que pode ajudar analistas e cientistas de dados a criar visualizações convincentes em apenas alguns minutos. Nós aprendemos como baixar e instalar o Tableau, bem como importar conjuntos de dados. Agora vamos começar a construir o dashboard da nossa análise!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-21-57-869Z.md
### Última modificação: 2025-05-28 18:21:58

# Transformando dados em recursos visuais - TripleTen

Capítulo 3/5

Narrativa de Dados

Opcional

# Transformando dados em recursos visuais

Nesta lição, mostraremos como criar recursos visuais que se tornarão um ótimo painel no Tableau. Abordaremos como juntar conjuntos de dados, visualizar dados e criar alguns gráficos fantásticos. E não se preocupe, preparamos muitas capturas de tela que te vão ajudar ao longo do caminho. Vamos começar!

### Juntando conjuntos de dados no Tableau

Assim como em SQL, muitas vezes é necessário unir conjuntos de dados no Tableau para combinar informações de várias fontes. Reunindo vários conjuntos de dados, você obtém uma imagem mais completa de seus dados. Desta forma, você pode chegar a algumas conclusões que pode não ver analisando cada conjunto de dados separadamente.

Agora vamos juntar os nossos dois conjuntos de dados.

Comece arrastando **population\_by\_country.csv** (população por país) para a tela "Need more data?" (o que significa "Precisa de mais dados?"). Você deve ver algo parecido com a seguinte tela.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_25_1687513585.png)

O programa pedirá que você selecione campos correspondentes. Você vai notar que neste caso, as chaves comuns para juntar ambos os conjuntos de dados são **Country** e **Country (population by country.csv**) Vá em frente e selecione-as na tela de relacionamento:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_26_1687513602.png)

Assim que você selecionar os campos apropriados, o relacionamento será estabelecido.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_27_1687513619.png)

### Vamos visualizar!

Agora que podemos acessar os dados, vamos começar a visualizá-los.

**Mortes vs Tamanho da população**

Isso nos pode ajudar a responder à pergunta: As vacinas COVID reduzem o número das mortes por Covid-19?

Um mapa pode nos ajudar a responder rapidamente a esta pergunta. Entretanto, antes de avançarmos, precisamos determinar a taxa de mortalidade marginal/total para cada país.

Para fazer isso, podemos somar todas as mortes e dividir o resultado pela população de um determinado país. Já que a taxa resultante é muito pequena, seria bom convertê-la em uma ordem de magnitude apropriada.

Neste caso, vamos multiplicar esta taxa de mortalidade por um milhão para obter **Deaths per Million** (Mortes por milhão) para cada país.

Siga estes passos para criar o mapa:

-   Clique na aba **Sheet 1 (Planilha 1)** na parte inferior da tela.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_28_1687513640.png)

-   Isso levará você a uma tela em branco:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_29_1687513658.png)

-   Arraste o campo **Country (population by country.csv) (País (população por país.csv))** para a seção **Marks (Marcas)** .

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_30_1687513700.png)

-   Você vai notar que o Tableau cria automaticamente um mapa logo que ele reconhece os nomes dos países.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_31_1687513723.png)

-   Arraste a variável **Population (População)** para a seção **Marks (Marcas)**

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_32_1687513744.png)

-   Clique nos três pontos e selecione **size** (tamanho).

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_33_1687513772.png)

-   Agora os pontos devem ser dimensionados proporcionalmente a suas populações respectivas.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_34_1687513808.png)

-   Arraste a variável **New deaths (Novas mortes)** para a seção **Marks (Marcas)**

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_35_1687513825.png)

-   Para modificar a métrica, clique em "New Deaths" duas vezes e substitua a fórmula atual pela seguinte: `1000000*SUM([New deaths])/MAX([Population])`. Esta fórmula divide o número de mortes em um país por sua população. Nós usamos a função **SUM** para **New Deaths** porque queremos somar todas as novas mortes. Usamos a função **MAX** para **Population** porque precisamos de apenas um valor que representa toda a população. ![](https://practicum-content.s3.amazonaws.com/resources/Untitled_36_1687513849.png)
-   Agora clique nos 3 pontos para **New Deaths** e selecione **Color** (Cor)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_37_1687513876.png)

-   O gradiente de cor agora está visível.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_38_1687513895.png)

-   Nós clicaremos no menu suspenso para editar as cores.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_39_1687513913.png)

-   E selecionaremos a paleta laranja no menu (clique em **OK** depois de fazer isso)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_40_1687513931.png)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_41_1687513944.png)

-   Simplesmente edite o título para ler **Deaths Per Million**.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_42_1687513962.png)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_43_1687513975.png)

E voilà, podemos ver que o tamanho da população na verdade não é um fator, já que o Peru tem o maior índice de **Deaths per Million** sendo um dos menores países em termos de população.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_44_1687513993.png)

Um gráfico simples pode nos ajudar a responder de forma eficiente as perguntas necessárias para construir nossa narrativa.

**Novos casos antes da vacinação/após ela**

Você se lembra de como usamos pandas para criar as variáveis 'days\_since\_vaccine' e 'vaccination\_started'? Agora nós usaremos essas variáveis para chegar mais perto de responder às nossas perguntas.

Veja como podemos fazer isso no Tableau:

-   Clique em "New Worksheet" (Nova Planilha)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_45_1687514015.png)

-   Arraste **New Deaths** para a seção **Rows** (Linhas) e **Days Since Vaccine** (Dias desde a vacinação) para a seção **Columns** (Colunas). Em seguida, converta a variável **Days Since Vaccine** para **Dimension** (Dimensão).

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_46_1687514033.png)

-   Arraste **Vaccination Started (Vacinação começou)** para a seção **Marks (Marcas)** e selecione **Color**.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_47_1687514054.png)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_48_1687514073.png)

-   Arraste **Country** para a seção **Filter** e selecione todos os valores.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_49_1687514093.png)

-   Selecione **Show Filter** (Exibir filtro) no menu suspenso.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_50_1687514111.png)

-   Seu gráfico deve se parecer com o seguinte:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_51_1687514136.png)

-   Vamos mudar a marca de uma linha para uma barra.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_52_1687514161.png)

-   E então reduzir o tamanho para o mínimo, clicando em Size (tamanho).

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_53_1687514177.png)

-   Seu gráfico deve ficar como este.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_54_1687514197.png)

-   As cores também são importantes. Para melhorar a visualização, vamos alterar a cor da variável **Vaccination Started** para cinzenta quando seu valor for **No** e para verde quando for **Yes**. Para fazer isso, siga estes passos:
-   Clique no menu suspenso do rótulo **Vaccination Started**.
-   Clique em **Edit Colors**.
-   Selecione as cores correspondentes.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_55_1687514214.png)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_56_1687514228.png)

-   Para continuar, clique no menu suspenso **Country** e selecione o filtro **Single Value (list)** (Valor único (lista)). Em seguida, clique em **Customize** (Personalizar) e se certifique so que a opção **Show All Values** (Mostrar todos os valores) está desmarcada.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_57_1687514248.png)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_58_1687514264.png)

-   Por fim, vamos fazer algumas alterações e dar nomes apropriados às nossas abas. Clique duas vezes nas abas dos dois gráficos que você criou para editar seus nomes tornando-os mais apropriados. Chame a aba do **Map** **Deaths per million map** (Mortes por milhão) e o gráfico de barras **New cases pre/post vaccination** (Novos casos pré/pós vacinação).

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_59_1687514284.png)

A versão final do gráfico de vacinação deve ser parecida ao gráfico abaixo para a **Argentina**. Ele mostra que o número de casos aumento após a vacinação.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_60_1687514300.png)

Mas o que tudo isso significa? Descobriremos isso junto com você nas próximas lições!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-22-03-205Z.md
### Última modificação: 2025-05-28 18:22:03

# Construindo o nosso painel no Tableau - TripleTen

Capítulo 3/5

Narrativa de Dados

Opcional

# Construindo o nosso painel no Tableau

Para continuar criando o nosso painel, precisamos construir mais dois gráficos para visualizar a resposta para a seguinte questão: **Qual é a taxa de mortalidade antes e após a vacinação?**

Para ilustrar a relação entre a taxa de mortalidade e a vacinação, usaremos um gráfico parecido com o anterior. Vamos começar!

-   Clique com o botão direito do mouse na aba **New cases pre/post vaccination** e selecione **Duplicate**.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_61_1687514346.png)

-   Clique duas vezes no nome do novo gráfico e renomeie-o para **Cumulative death rate** (Taxa de mortalidade cumulativa).
    
-   Clique com o botão direito do mouse no campo **SUM(new\_deaths)** exibido na linha **Rows** e **remova-o**.
    

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_63_1687514470.png)

-   E agora clique duas vezes no espaço em branco ao lado de **Rows** para começar escrevendo uma fórmula.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_64_1687514487.png)

-   Para calcular a taxa entre mortes cumulativas (Cumulative Deaths) e casos cumulativos (Cumulative Cases) para cada linha, use a fórmula: **`sum([Cumulative deaths])/sum([Cumulative cases])`**.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_65_1687514507.png)

-   Altere **Marks** para **Shape** (forma)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_66_1687514524.png)

-   Seu gráfico deve se parecer com o gráfico abaixo:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_67_1687514570.png)

Cada ponto no gráfico representa uma observação.

A tendência decrescente da linha é um sinal positivo, já que ela indica uma diminuição na taxa de mortalidade. Isso pode ser devido a um aumento no número de pessoas vacinadas ao longo do tempo, o que as torna mais resistentes aos sintomas da Covid e, portanto, reduz a taxa de mortalidade.

Se filtrarmos por países específicos, descobriremos padrões mais interessantes. Por exemplo, o Brasil, muito criticado por suas políticas de lockdown relaxadas, ainda bem e graças a uma expressiva parcela consciente, nunca ultrapassou uma taxa de mortalidade de 7%, apesar de ter passado situações bem difíceis.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_68_1687514592.png)

A Argentina, o país com o lockdown mais longo (que durou 234 dias), teve uma taxa de mortalidade maior do que o Brasil.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_69_1687514611.png)

Nós criaremos um gráfico final que exibe países, populações e as datas das primeiras vacinas aplicadas nesses países. Siga estes passos:

-   Crie um novo gráfico

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_70_1687514631.png)

-   Renomeie-o para **Country Facts** (fatos sobre países)
-   Arraste **Population** para **Columns**.
-   Arraste **Country** para **Rows**.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_71_1687514649.png)

-   Arraste **First Vaccine Date** para **Marks**, selecione **Label** (rótulo) e então clique duas vezes para abrir o menu suspenso e selecione **Exact Date** (data exata).

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_72_1687514672.png)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_73_1687514684.png)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_74_1687514700.png)

-   Clique duas vezes (ou com o botão direito) em **Country** e escolha **Sort** (Classificar). Ordene as barras por **First Vaccine Date** usando as opções como mostrado abaixo.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_75_1687514716.png)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_76_1687514739.png)

-   Aqui é como o gráfico deve parecer.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_77_1687514760.png)

Então, o que podemos dizer sobre as vacinas e a Covid-19? Vamos reunir toda essa informação e tentar analisá-la!

## Um painel vale mais que mil palavras

Como etapa final, colocaremos todos os nossos gráficos em um painel (dashboard) que poderemos usar para contar uma história para nossos amigos e familiares. Podemos adicionar mais alguns gráficos para ter uma melhor compreensão do impacto das vacinas nas mortes pela Covid.

Para completar esta tarefa final, siga estes passos:

-   Clique no botão **Create New Dashboard (Novo Painel)**. Este botão está localizado no meio da captura de tela abaixo.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_78_1687514779.png)

-   Arraste os gráficos para a tela em branco e organize-os para que seu resultado corresponda à captura de tela abaixo. Remova todos os rótulos.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_79_1687514800.png)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_80_1687514816.png)

1.  Se seus gráficos parecem desordenados, você pode usar a opção **Automatic** do menu Size no lado esquerdo da sua tela.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_81_1687514853.png)

-   Remova todos os rótulos e filtros, exceto **Death per Million** e **Vaccination Started**. Para fazer isso, basta clicar em **X**.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_82_1687514872.png)

-   Para organizar os rótulos restantes de acordo com a imagem, clique na seta do **menu suspenso com mais opções** em cada rótulo e selecione **Floating** (flutuante).

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_83_1687514893.png)

-   Seu painel deve ficar como a imagem abaixo.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_84_1687514911.png)

-   Para acessar o painel **Map**, clique nele no menu. Depois que o painel for carregado, clique no ícone **filter**. Isso te permitira filtrar todos os gráficos clicando nos países no mapa. É bem legal!

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_85_1687514928.png)

Ótimas notícias, seu painel está pronto! Agora é hora de começar a analisar seus dados e encontrar as respostas para suas perguntas. Você está quase lá! Entre de cabeça e comece a explorar diferentes países no seu painel. Agora é só aproveitar!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-22-04-537Z.md
### Última modificação: 2025-05-28 18:22:04

# Conclusões - TripleTen

Capítulo 3/5

Narrativa de Dados

Opcional

# Conclusões

Agora que temos um dashboard claro que exibe as tendências da COVID-19, dados da população e vacinação, podemos começar a responder às perguntas que listamos no início deste sprint.

Vamos nos lembrar delas:

-   As vacinas contra a COVID reduzem o número das mortes pela COVID-19?
-   As vacinas reduzem o número de novos casos da COVID-19?
-   Elas fornecem a imunidade contra a COVID-19?

Vamos responder a cada uma delas separadamente.

**As vacinas COVID reduzem o número das mortes pela COVID-19?**

Depois de analisarmos o gráfico **Cumulative death rate** (Taxa de mortalidade cumulativa) que criamos, é evidente que o número das mortes relacionadas à COVID-19 diminuiu após um pico inicial, com algumas variações nos números. No entanto, em um primeiro momento, parece não haver uma tendência clara de queda no número de mortes após o início da vacinação.

Esta observação pode levar ao questionamento sobre a eficácia das vacinas COVID-19 na redução do número das mortes pela vírus, mas ainda sem levar em consideração uma ampla análise de contexto. São necessárias análises adicionais para determinar se essa tendência se deve a algum fator ligado às vacinas ou se há outros fatores em jogo.

No geral, estes dados do gráfico **Cumulative death rate** (Taxa de mortalidade cumulativa) não nos dão, a princípio, uma resposta clara à pergunta se as vacinas reduzem o número das mortes relacionadas à COVID-19.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_86_1687514971.png)

**As vacinas reduzem o número de novos casos da COVID-19? Elas fornecem a imunidade contra a COVID-19?**

Quando analisamos o gráfico **New cases pre/post vaccination**, podemos ver que há um aumento significativo no número de novos casos da COVID-19 após o início das campanhas de vacinação. Esta observação, considerando o contexto que as regras de lockdown vão sendo afrouxadas ao longo das campanhas, gera preocupações sobre a eficiência das vacinas, neste contexto, na redução da propagação do vírus. No entanto, após o pico de novos casos, há uma diminuição substancial do número de novos casos, sugerindo que a campanha de vacinação é eficaz na redução da propagação do vírus.

É importante observar que enquanto esta diminuição de novos casos pode ser relacionada à campanha de vacinação, não podemos associá-la definitivamente apenas à vacina em si. Pode haver outros fatores que contribuíram para a diminuição do número de novos casos. Por exemplo, mudanças nas políticas governamentais, campanhas de saúde pública e comportamentos individuais também podem desempenhar seu papel na redução da propagação do vírus.

Portanto, precisamos de mais dados para entender o impacto total da campanha de vacinação na redução do número de novos casos da COVID-19. A análise posterior é necessária para determinar a eficiência das vacinas na redução do número de novos casos e para identificar outros fatores que podem contribuir para a diminuição de novos casos. Fazer isso nos permite entender melhor o impacto das campanhas de vacinação e desenvolver estratégias eficientes para controlar a propagação do vírus.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_87_1687514990.png)

**Conclusão**

Como diz a frase famosa de Platão, "_**Só sei que nada sei**_". A nossa análise provou ser inconclusiva. Entretanto, é importante observar que você provavelmente nunca teve dados tão claros sobre a pandemia da COVID-19 nos países que acabamos de examinar.

Além disso, tornamos os dados visualmente atraentes e fáceis de entender. Embora o nosso dashboard possa não responder a todas as perguntas difíceis, podemos sem dúvidas aprender muita coisa com base nele. Você deve estar orgulhoso do que conseguimos fazer porque nós certamente estamos!

Antes de passarmos para o próximo capítulo, vá para **File** e clique na opção **Save to Tableau Public As…** (Salvar no Tableau Public como...). Isso carregará seu trabalho e o adicionará a seu portfólio pessoal!

Poste um link para o seu painel no LinkedIn para mostrar seu trabalho e marque um amigo ou um contato, para que ele ou ela saiba sobre o seu novo conjunto de habilidades e o curso gratuito. Use o hashtag #tripleten\_free para podermos celebrar seu sucesso!

Poste um link para o seu painel no LinkedIn para mostrar seu trabalho e marque um amigo ou um contato, para que ele ou ela saiba sobre o seu novo conjunto de habilidades e o curso gratuito. Use o hashtag #tripleten\_free para podermos celebrar seu sucesso!

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_88_1687515014.png)

### 🔥

Se você quiser discutir o programa completo com um dos nossos consultores de carreira, solicite uma chamada

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-22-05-884Z.md
### Última modificação: 2025-05-28 18:22:06

# Introdução - TripleTen

Capítulo 4/5

Prevendo o Futuro

Opcional

# Introdução

Você aprendeu muita coisa durante os últimos dois capítulos e agora é hora de colocar as coisas em prática!

Neste capítulo nós veremos:

-   Como fazer uma análise exploratória rápida de dados.
-   Como podemos transformar um problema em um conjunto de hipóteses.
-   Como fazer análise de correlação para testar hipóteses.
-   Treinar um modelo de regressão que possa fazer previsões de dados.

Você decidiu usar suas habilidades para fazer algo de bom em sua vizinhança. Você ficou sabendo que a sorveteria onde seus avós costumavam levá-lo está com dificuldades e pode fechar as portas.

Você conversou com o proprietário, Carlos, para entender quais problemas ele está enfrentando, e parece que o principal problema é o desperdício de mercadorias.

A loja de Carlos está localizada em Cartagena, na Colômbia, e atualmente funciona apenas nos finais de semana (sábado e domingo).

Ele faz deliciosos sorvetes artesanais, então os produtos precisam ser preparados com 2 a 3 dias de antecedência e não duram muito.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_89_1687518075.png)

Atualmente, ele calcula o número de sorvetes que precisa fazer a cada fim de semana, calculando a média das vendas dos fins de semana dos três meses anteriores. Ele coleta os dados de vendas com muita atenção e mantém o controle do número de vendas por dia.

Ele identificou dois problemas principais:

-   Tem dias que ele faz sorvete em excesso e acaba jogando fora muitos produtos que não vendem.
-   Em alguns dias, ele faz muito pouco e seus sorvetes se esgotam mais cedo, perdendo a chance de vender mais.

Carlos compartilhou alguns dados de 2022 com vocês. Vamos começar a trabalhar com os dados que ele compartilhou.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-22-10-127Z.md
### Última modificação: 2025-05-28 18:22:10

# Análise Exploratória de Dados - TripleTen

Capítulo 4/5

Prevendo o Futuro

Opcional

# Análise Exploratória de Dados

Vamos dar uma olhada nos dados recebidos de Carlos.

No trecho de código abaixo, você verá que, primeiro, nós importamos a biblioteca `pandas`.

Você deve estar familiarizado com esta biblioteca desde o primeiro capítulo de nosso curso, mas fique à vontade para revisitá-la se necessário. Observe que usamos a função `read_csv()` para ler o conjunto de dados de Carlos.

Para visualizar os dados, precisamos imprimir as primeiras linhas. Você já aprendeu um método para fazer isso, então use-o agora e imprima o resultado na tela.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

file\_path \= "https://raw.githubusercontent.com/dunmacleod/datasets/main/ice\_cream\_sales.csv"

  

df \= pd.read\_csv(file\_path)

  

print(df.head())

Dica

Mostrar a soluçãoValidar

O conjunto de dados fornecido por Carlos parece bastante simples, contendo apenas duas variáveis: o número de casquinhas vendidas em uma determinada data.

Vamos visualizar o conjunto de dados para começar a explorá-lo. Você se lembra de como é fácil visualizar um conjunto de dados usando a biblioteca `pandas`?

Para criar um gráfico de barras em Python usando a biblioteca `pandas`, você pode usar o método `plot(kind="bar")`. Aqui, `bar` é um argumento passado para o parâmetro `kind` do método `plot`, instruindo o programa a usar `bar`para criar um gráfico. Iremos nos referir a esses tipos de entradas como **argumentos** a partir de agora.

Além de `kind="bar"`, estaremos usando dois outros argumentos para este método:

1.  Uma lista de valores a serem traçados no eixo x, a serem passados para o parâmetro `x`
2.  Uma lista de valores a serem traçados no eixo y, a serem passados para o parâmetro `y`

Carlos nos forneceu as datas (a variável `date`) e a variável `# Ice-cream cones sold`(Casquinhas vendidas) para vendas. Vamos usar `date` como a lista para o eixo x e `# Ice-cream cones sold` como a lista para o eixo y.

Com esta rápida introdução, podemos visualizar um gráfico de barras do número de casquinhas vendidas por dia.

Vamos desenhar o gráfico e examinar a saída de um gráfico de barras.

CódigoPYTHON

9

1

2

plt \= df.plot(kind\="bar", x\="date", y\="# Ice-cream cones sold")

plt.set\_title("Casquinhas vendidas por dia")

Mostrar a soluçãoExecutar

Você percebeu algo de errado com nosso gráfico? Temos descontinuidades nas datas em nosso eixo X. Isso porque a loja do Carlos só abre aos finais de semana, então não temos dados para os dias úteis da semana.

E, após examinar o gráfico mais de perto, parece que há apenas cinco meses de dados disponíveis.

Pergunta

Você identificou alguma outra tendência ou padrão?

As vendas parecem aumentar durante os meses mais quentes.

As vendas parecem aumentar nos meses mais frios.

Trabalho maravilhoso!

Observa-se que as vendas são sazonais, com maiores volumes de vendas nas estações mais quentes, como primavera e verão. Para confirmar essa suposição, analisar as vendas por mês em vez de por dia pode ser mais útil.

O método `dt.month` em pandas nos permite extrair o mês de uma data.

Mas, para usar esse método para visualizar as vendas por mês, precisamos primeiro alterar nosso tipo de variável de data, porque `dt.month` funciona apenas com objetos `DateTime`. Podemos fazer isso facilmente usando a seguinte linha de código:

```
df["date"] = pd.to_datetime(df["date"])
```

Vamos ver como fica a comparação antes e depois da alteração:

CódigoPYTHON

9

1

2

3

print("Antes: ", df\["date"\].dtypes) \# dtypes nos dará o tipo da coluna

df\["date"\] \= pd.to\_datetime(df\["date"\]) \# e aqui faremos nossa mágica de conversão

print("Depois: ", df\["date"\].dtypes)

Mostrar a soluçãoExecutar

Ótimo! Agora temos as datas como um objeto DateTime e podemos transformar nossos dados usando dt.month. Vamos fazer!

Vamos aplicar `dt.month` à nossa coluna de `date` e desenhar nosso gráfico de barras novamente.

CódigoPYTHON

9

1

2

3

4

5

6

df\["date"\] \= pd.to\_datetime(df\["date"\]) \# aqui faremos nossa mágica de conversão

  

df\["date"\] \= df\["date"\].dt.month \# aplicamos dt.month à nossa coluna de data

  

plt \= df.plot(kind\="bar", x\="date", y\="# Ice-cream cones sold")

plt.set\_title("Casquinhas vendidas por dia")

Mostrar a soluçãoExecutar

Parece que apenas o mês está sendo exibido para a data, mas não é isso que queremos. Em lugar disso, queremos agregar todas as casquinhas vendidas por mês e exibir apenas uma barra por mês. Dessa forma, podemos facilmente comparar os diferentes meses.

Portanto, esse primeiro passo foi muito bom! Agora, o que precisamos fazer é agrupar todos esses dados por `date`, somando cada valor da variável `# Ice-cream cones sold`. Já vimos isso antes quando usamos `groupby`!

Vamos usar `groupby` para agrupar nossa coluna de `date`, adicionar todos as `# Ice-cream cones sold` e desenhar nosso gráfico de barras mais uma vez.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

df\["date"\] \= pd.to\_datetime(df\["date"\]) \# aqui faremos nossa mágica de conversão

  

df\["date"\] \= df\["date"\].dt.month \# aplicamos dt.month à nossa coluna de data

  

\# agrupamos nossas datas, adicionando as casquinhas vendidas

dfgrp \= df.groupby("date")\["# Ice-cream cones sold"\].sum()

  

\# trace nosso novo DataFrame agrupado

plt \= dfgrp.plot(kind\="bar", x\="date", y\="# Ice-cream cones sold")

plt.set\_title("Casquinhas vendidas por dia")

Mostrar a soluçãoExecutar

Parece que nossa hipótese está correta, pois a Colômbia está localizada na América do Sul e possui as seguintes estações:

-   Outono (21 de março a 20 de junho)
-   Inverno (21 de junho a 20 de setembro)
-   Primavera (21 de setembro a 20 de dezembro)
-   Verão (21 de dezembro a 20 de março)

Mas suponho que você já sabia disso, não é? É lógico: quanto mais quente o clima, mais sorvete a gente compra!

Mas me diga uma coisa, não é bom ter um gráfico bonito para apoiar nosso conhecimento?

Ótimo trabalho até agora! Na próxima lição, exploraremos sua análise com mais detalhes. Estou ansioso para ver!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-22-11-443Z.md
### Última modificação: 2025-05-28 18:22:11

# Geração de hipóteses - TripleTen

Capítulo 4/5

Prevendo o Futuro

Opcional

# Geração de hipóteses

Como profissional de dados, você frequentemente encontrará problemas e perguntas mal elaboradas ou muito difíceis de responder. Nesses casos, uma solução comum é desenvolver um conjunto de hipóteses sobre o problema mais amplo que você está tentando resolver e testá-las usando dados.

Uma hipótese é uma afirmação que pode ser verdadeira ou falsa. Por exemplo, "Está chovendo lá fora" é uma hipótese, enquanto "Que horas são?" não é uma hipótese.

Aqui está o último gráfico que desenhamos em nossa lição anterior:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_90_1687518498.png)

Dê uma olhada e pense em todos os possíveis motivos que podem impulsionar as vendas de sorvete.

Pergunta

Selecione as hipóteses que você acredita serem plausíveis e que façam sentido testar.

Escolha quantas quiser

As pessoas compram mais sorvete no fim de semana do Natal.

As vendas de sorvete não são sazonais.

As vendas de sorvete não são influenciadas pelo clima.

As pessoas tendem a comprar mais sorvete em seus aniversários.

Excelente!

Bom trabalho! Felizmente, você pode ver como esse processo pode continuar indefinidamente.

As hipóteses não devem ser apenas plausíveis, mas também úteis para ajudar a responder às suas questões de pesquisa.

Concluímos que o clima tem um impacto significativo nas vendas de sorvete. Embora essa hipótese esteja relacionada às estações do ano, ela é mais específica e, portanto, mais prática de testar.

Infelizmente, o conjunto de dados não inclui a temperatura de cada dia.

Então quais são suas opções?

É possível que os dados disponíveis não sejam suficientes para responder às suas perguntas de pesquisa. Nesses casos, você precisará tomar decisões sobre como proceder.

Pergunta

Considere o problema em questão. Que ações você tomará para lidar com isso?

Vou cancelar o projeto porque os dados são difíceis ou caros de obter.

Comprarei os dados de uma fonte, como uma estação meteorológica.

Vou contratar uma equipe de desenvolvedores para coletar dados da web.

Vou procurar no Google uma página da web com dados meteorológicos históricos e fazer o download dos dados em um arquivo `CSV`.

Você conseguiu!

Forneceremos a temperatura para Cartagena, Colômbia. Salvamos as temperaturas médias diárias em um arquivo `CSV` para você, juntamente com alguns dados fictícios.

No trecho de código abaixo, você verá uma variável `file_path` que fornece o caminho para os dados de temperatura de Cartagena. Complete o código abaixo: se tudo for feito corretamente, você poderá ver as primeiras 5 linhas dos dados de temperatura.

CódigoPYTHON

9

1

2

3

4

5

6

import pandas as pd

  

file\_path \= "https://raw.githubusercontent.com/JJTorresDS/ds-data-sources/main/temperature.csv"

  

df2 \= pd.read\_csv(file\_path)

print(df2.head())

Dica

Mostrar a soluçãoValidar

Agora você tem dois DataFrames: um contendo os dados de temperatura e o outro contendo os dados de vendas de sorvete. Antes de juntá-los, precisamos alterar a variável `date` do DataFrame de temperatura para o tipo `DateTime`, assim como fizemos com nosso conjunto de dados original.

Demonstraremos como juntar dataframes com Pandas. No entanto, é importante observar que outra habilidade fundamental na análise de dados é a capacidade de pesquisar e ler a documentação. Por exemplo, se você pesquisar "juntar dataframes com Pandas" no Google, provavelmente encontrará o código que estamos prestes a mostrar.

Ser capaz de pesquisar é fundamental, pois existem inúmeras bibliotecas em Python, cada uma com muitas funções. Seria impossível memorizar todos eles.

Para limpar os tipos de variáveis e ingressar nos DataFrames, execute o código abaixo.

CódigoPYTHON

9

1

2

3

4

5

6

7

\# aqui aplicamos a mágica de conversão novamente

df2\["date"\] \= pd.to\_datetime(df2\["date"\])

  

\# e aqui unimos os dois DataFrames nas colunas "data"

df\_merged \= pd.merge(df, df2, left\_on\="date", right\_on\="date")

  

print(df\_merged.head()) \# e temos uma visão disso

Mostrar a soluçãoExecutar

O método `merge` em pandas pode ser usado para combinar dois DataFrames. Os parâmetros `left_on` e `right_on` especificam quais variáveis de cada conjunto de dados devem ser usadas para unir.

Tudo parece como esperado, então vamos continuar!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-22-12-780Z.md
### Última modificação: 2025-05-28 18:22:13

# Correlação - TripleTen

Capítulo 4/5

Prevendo o Futuro

Opcional

# Correlação

Agora que temos nossas duas variáveis, nós precisamos descobrir como elas estão relacionadas. Elas se movem juntas? (ou seja, quando uma sobe, a outra também sobe?)

O conceito de **correlação** é usado para descrever esse relacionamento.

**Saiba mais!**

-   Correlação é uma medida estatística que descreve a força e a direção da relação entre duas variáveis. Ela indica se as variáveis tendem a se mover na mesma direção (**correlação positiva**), se movem em direções opostas (**correlação negativa**) ou não possuem nenhuma relação (correlação zero). O coeficiente de correlação, normalmente denotado por `r`, que é um valor numérico entre `-1` e `1` que representa o grau de correlação entre as variáveis. Um valor de `-1` indica uma correlação negativa perfeita, `0` indica nenhuma correlação e `1` indica uma correlação positiva perfeita.

![](https://practicum-content.s3.amazonaws.com/resources/1.4.4.1PT_1688480075.png)

Está bem... isso pode soar meio complicado, mas a boa notícia é que o Python tem um método integrado para calcular o coeficiente de correlação entre duas variáveis.

Mas há algumas ressalvas a essa técnica:

-   A correlação só pode ser calculada entre variáveis numéricas.
-   O coeficiente de correlação só pode ser calculado para variáveis linearmente relacionadas.

**Saiba mais!**

-   Variáveis linearmente relacionadas são variáveis que possuem uma relação que pode ser descrita por uma linha reta.
-   Em um gráfico de dispersão de variáveis linearmente relacionadas, os pontos formarão uma linha aproximadamente reta.

Para entender visualmente o que queremos dizer com **linearmente relacionado**, vamos dar uma olhada no gráfico a seguir.

CódigoPYTHON

9

1

2

3

4

5

6

plt \= df\_merged.plot(

kind\="scatter", x\="Temperature (Celsius)", y\="# Ice-cream cones sold"

)

plt.set\_xlabel("Temperatura (Celsius)")

plt.set\_ylabel("# de Casquinhas vendidas")

plt.set\_title("Vendas de Casquinha vs Temperatura")

Mostrar a soluçãoExecutar

O gráfico mostrado acima é chamado de **gráfico de dispersão**. É o gráfico apropriado para usar ao analisar como a distribuição de duas variáveis numéricas se relacionam entre si.

À medida que as temperaturas sobem, aumenta também o número médio de casquinhas de sorvete vendidas. Os pontos no gráfico parecem estar espalhados em torno de uma linha, indicando uma **relação linear** entre essas duas variáveis.

Parece que as condições foram atendidas para calcular nosso **coeficiente de correlação**.

Como na maioria dos cálculos que vimos neste curso, executar uma análise de correlação em Python requer apenas uma linha de código.

O método `corr` também faz parte do Pandas. Ele permite calcular a correlação de duas ou mais variáveis numéricas.

O código abaixo executa o método e imprime seu resultado:

CódigoPYTHON

9

1

print(df\_merged\[\["Temperature (Celsius)","# Ice-cream cones sold"\]\].corr())

Mostrar a soluçãoExecutar

O resultado é uma tabela bidirecional. O coeficiente de correlação entre `Temperature (Celsius)` e `# Ice-cream cones sold` é muito próximo de 1 (~,95). Isso indica uma relação positiva muito forte entre as duas variáveis, quando uma aumenta, a outra também. Os outros valores são `1` porque qualquer coisa está perfeitamente correlacionada consigo mesma.

E agora? Aprendemos que a temperatura parece ser o principal fator por trás das vendas de sorvete.

É hora de usar esse conhecimento para ajudar Carlos a estimar suas vendas diárias.

E se pudéssemos criar uma função que calculasse as vendas de sorvete com base na temperatura? Por exemplo, algo como `# of Ice cream cones sold = Temperature (Celsius) * X`, onde esse `X` vai definir como os valores mudam, em relação à temperatura.

É aqui que a **regressão linear** entra em ação.

Mas essa ação vai ficar para nossa próxima aula, pois a gente tem um cafezinho pra tomar agora.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-22-20-537Z.md
### Última modificação: 2025-05-28 18:22:21

# Introdução à Regressão Linear - TripleTen

Capítulo 4/5

Prevendo o Futuro

Opcional

# Introdução à Regressão Linear

Então, o que é regressão linear?

**Saiba mais**

-   A regressão linear é um método estatístico para modelar a relação entre uma variável dependente e uma ou mais variáveis independentes. Envolve encontrar a linha de melhor ajuste que resume a relação entre as variáveis. Esta linha pode ser usada para prever o valor da variável dependente para qualquer valor dado da(s) variável(is) independente(s).

Em outras palavras, a regressão linear nos ajuda a entender como uma mudança em uma variável (a temperatura externa) afeta a(s) outra(s) variável(is) (no nosso caso, sorvete é necessário). A regressão linear assume que existe uma relação linear entre a(s) variável(is) independente(s) e a variável dependente. A qualidade do ajuste é frequentemente avaliada usando o coeficiente de determinação (**R-quadrado**), que mede a proporção da **variância** na variável dependente que pode ser explicada pela(s) variável(is) independente(s).

![](https://practicum-content.s3.amazonaws.com/resources/1.4.5.1PT_1688481499.png)

É, de fato, é muita informação para absorver. Vamos nos concentrar nesta frase:

-   _Em outras palavras, a regressão linear nos ajuda a entender como uma mudança em uma variável afeta a(s) outra(s) variável(is)_

Agora, vamos nos afastar da teoria e colocar a mão na massa.

Como você deve ter adivinhado, o Python já possui um pacote que pode ajudar a você a executar um modelo de regressão linear.

Pergunta

Você consegue descobrir o que a seguinte linha de código faz?

```
from sklearn.linear_model import LinearRegression
```

Realiza uma análise de correlação.

Importa `LinearRegression` da biblioteca `sklearn.linear_model`.

Você conseguiu!

Ótimo! Agora, como de costume, o próximo passo é descobrir quais entradas são necessárias para executar o módulo `LinearRegression`.

Precisamos identificar a variável dependente, que chamaremos de **y**, e a variável independente, que chamaremos de variável **X**. Esses nomes são convenções em estatísticas. Embora possa ser difícil adotar no início, eles simplificam nosso trabalho a longo prazo, garantindo que todos os profissionais da área falem a mesma língua.

Para executar o módulo `LinearRegression`, você precisa armazenar a variável dependente (`y`) e a variável independente (`X`) como um vetor 2D.

**Saiba mais!**

-   Em Python, um vetor 2D é um vetor de vetores. À primeira vista, isso pode parecer confuso. Essencialmente, é uma coleção de vetores, onde cada vetor pode ter diferentes dimensões e tamanhos. Geralmente ele é usado para armazenar dados em formato tabular, onde linhas e colunas representam diferentes entidades e atributos, respectivamente. Para criar um vetor 2D em Python, você pode usar uma lista de listas.

Para obter um vetor 2D, precisamos usar um par de colchetes. Por exemplo, nosso `X` é a temperatura e aqui temos um vetor 2D para ela, assim teremos uma lista de listas:

`X = df_merged[["Temperature (Celsius)"]]`

Vamos armazenar os valores de `Temperature (Celsius)` e `# Ice-cream cones sold` em suas variáveis correspondentes. No pré-código abaixo, já armazenamos os valores de `Temperature (Celsius)` na variável `X`. Seu objetivo é armazenar os valores dos `# Ice-cream cones sold` na variável `y`. Por fim, imprima os 10 primeiros valores de `y` para verificar se tudo funcionou conforme o esperado.

CódigoPYTHON

9

1

2

3

4

x \= df\_merged\[\["Temperature (Celsius)"\]\]

y \= df\_merged\[\["# Ice-cream cones sold"\]\]

  

print(y\[:10\]) \# isso imprimirá as primeiras 10 linhas de y

Dica

Mostrar a soluçãoValidar

O módulo `LinearRegression` tem um método `fit` que tenta ajustar uma linha através dos pontos dispersos. Já importamos o módulo, e agora vamos utilizar para ajustar uma linha.

Execute o código abaixo:

CódigoPYTHON

9

1

2

3

4

5

6

\# importe nosso módulo LinearRegression

from sklearn.linear\_model import LinearRegression

  

reg \= LinearRegression().fit(x, y) \# função que ajusta um modelo aos dados

  

print(reg.score(x, y)) \# imprima a pontuação adequada

Mostrar a soluçãoExecutar

Usamos a função `fit()` para ajustar um modelo aos dados. É aqui que o modelo aprende os padrões de como `X` explica `y`.

O resultado que obtivemos é chamado de **pontuação**, que nos dá uma ideia de quão bem o modelo explica a variabilidade (ou movimento) da variável `y` com base na variável `X`. Quanto mais próxima à pontuação estiver de 1, melhor o modelo.

Essa pontuação é o que descrevemos anteriormente como **R-quadrado** em nossa definição de regressão linear.

A variável `reg` agora entende como essas duas variáveis estão relacionadas e armazena essas informações na forma de uma equação, que é exatamente o que estamos procurando!

O código a seguir imprime os componentes da equação: o intercepto e o coeficiente que multiplica `X`, onde `X` representa a temperatura em graus Celsius. Tanto a interceptação quanto o coeficiente são necessários para entender como `y` se parecerá para um determinado `X`.

CódigoPYTHON

9

1

2

3

4

5

6

7

\# importe nosso módulo LinearRegression

from sklearn.linear\_model import LinearRegression

  

reg \= LinearRegression().fit(x, y) \# função que ajusta um modelo aos dados

  

print(reg.intercept\_) \# imprimiremos a interceptação do modelo ajustado

print(reg.coef\_) \# e imprimiremos o coeficiente do modelo ajustado

Mostrar a soluçãoExecutar

**Saiba mais**

-   O intercepto é o valor que a variável dependente deve assumir quando a variável independente for igual a 0.
-   O coeficiente é o valor que capta a relação entre as variáveis independentes e dependentes. Ele nos informa o quanto se espera que a variável dependente mude para cada aumento de uma unidade na variável independente.

No nosso caso específico, isso significa:

-   A interceptação é o valor que `y` (`# Casquinhas vendidas`) deve assumir quando nossa variável `X` (`temperatura (Celsius)`) for igual a 0.
-   O coeficiente é o valor que captura a relação entre `X` e `y`.

Com base nesses valores, se tivéssemos que escrever a equação para Carlos, seria algo como:

1.  `y = X * coeficiente + intercepto`
2.  `y = X * (0.667) + (- 0.168)`
3.  `y = X * (0.667) - 0.168`
4.  `# Casquinhas vendidas = Temperatura (Celsius) * (0.667) - 0.168`

Se Deus quiser, você perceberá como isso é mágico. A variável `reg` já possui essa equação (a partir de agora denominada **modelo**) pronta para uso. Você pode realmente testar o desempenho dela. Vamos fazer!

Para executar esse **modelo** e ver quanto sorvete Carlos precisa para uma determinada temperatura, usaremos o método `predict`. Vamos dar uma olhada no código abaixo e executar ele.

CódigoPYTHON

9

1

2

y\_pred \= reg.predict(df\_merged\[\["Temperature (Celsius)"\]\])

print(y\_pred\[:10\]) \# imprime os 10 primeiros valores de y

Mostrar a soluçãoExecutar

O que acabou de acontecer? Pedimos ao nosso modelo para prever a quantidade de sorvete necessária em diferentes valores de temperatura. O modelo usou a equação que derivamos acima e gerou os números que traçamos. Ainda parece mágica? Espero que não mais!

Usaremos o gráfico de dispersão visto anteriormente para tentar **encaixar** a linha imaginária que descrevemos usando as previsões de nosso modelo.

Vimos antes como fazer gráficos e linhas de dispersão. Agora vamos colocar eles juntos, primeiro desenhando nosso gráfico de dispersão e, em seguida, adicionando a linha com nossos valores `y` previstos a ele.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

y\_pred \= reg.predict(df\_merged\[\["Temperature (Celsius)"\]\])

  

\# irá desenhar nosso gráfico de dispersão em laranja

plt \= df\_merged.plot(

kind\="scatter",

x\="Temperature (Celsius)",

y\="# Ice-cream cones sold",

color\="orange",

)

  

\# defina os rótulos para os eixos x e y e o título para o gráfico

plt.set\_xlabel("Temperatura (Celsius)")

plt.set\_ylabel("# de Casquinhas vendidas")

plt.set\_title("Ice cream sales vs Temperature")

  

plt.plot(x, y\_pred) \# trace uma linha com nossas previsões

Mostrar a soluçãoExecutar

Visualmente, podemos ver que o modelo se ajusta muito bem a uma linha, pois a maioria dos valores está centrada em torno dela.

Este é o melhor modelo linear? Não, não é!

Podemos melhorar nosso modelo? Sim, nós podemos. No entanto, deve ser bom o suficiente por enquanto.

Como profissional de dados, você sempre pode melhorar suas soluções e conclusões, mas também precisa estar atento à velocidade. Carlos está perdendo dinheiro todo fim de semana, e seu modelo é muito melhor do que a estimativa média anterior.

O código a seguir desenha um gráfico que nos permite comparar o desempenho da **média** em todo o conjunto de dados com a saída do nosso modelo.

Embora possamos calcular uma média móvel de 3 meses usando Python, para simplificar, usaremos apenas a média de todo o conjunto de dados, que abrange 5 meses de vendas. Usar `y.mean()` nos dará esse valor.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

y\_pred \= reg.predict(df\_merged\[\["Temperature (Celsius)"\]\])

  

plt \= df\_merged.plot(

kind\="scatter", x\="Temperature (Celsius)", y\="# Ice-cream cones sold",

color\="orange"

)

  

plt.set\_xlabel("Temperatura (Celsius)")

plt.set\_ylabel("# de Casquinhas vendidas")

plt.set\_title("Vendas de Casquinha vs Temperatura")

  

plt.plot(x, y\_pred) \# trace uma linha com nossas previsões

  

y\_mean \= y.mean().values \# calcule nosso valor médio

  

plt.plot(x, y\_mean.repeat(len(y))) \# trace uma linha com a nossa média

Mostrar a soluçãoExecutar

Intuitivamente, quanto mais próximos os pontos (ou valores reais) estiverem da linha azul (que representa o modelo que criamos), melhor será o desempenho do modelo. A linha laranja no gráfico acima ilustra a estimativa média que Carlos usou todos os dias anteriormente, independentemente do clima. Podemos observar que a estimativa média é confiável apenas quando a temperatura é de aproximadamente 22 graus Celsius.

Existem métodos formais para quantificar a qualidade do ajuste de um modelo de regressão linear, bem como métodos formais para treinar tais modelos. No entanto, esses tópicos não são abordados nesse curso introdutório. Oferecemos cursos dedicados a esses tópicos em nossos bootcamps de dados.

Hora de mostrar ao Carlos o que temos!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-22-40-346Z.md
### Última modificação: 2025-05-28 18:22:40

# Introdução à Regressão Linear - TripleTen

Capítulo 4/5

Prevendo o Futuro

Opcional

# Introdução à Regressão Linear

Então, o que é regressão linear?

**Saiba mais**

-   A regressão linear é um método estatístico para modelar a relação entre uma variável dependente e uma ou mais variáveis independentes. Envolve encontrar a linha de melhor ajuste que resume a relação entre as variáveis. Esta linha pode ser usada para prever o valor da variável dependente para qualquer valor dado da(s) variável(is) independente(s).

Em outras palavras, a regressão linear nos ajuda a entender como uma mudança em uma variável (a temperatura externa) afeta a(s) outra(s) variável(is) (no nosso caso, sorvete é necessário). A regressão linear assume que existe uma relação linear entre a(s) variável(is) independente(s) e a variável dependente. A qualidade do ajuste é frequentemente avaliada usando o coeficiente de determinação (**R-quadrado**), que mede a proporção da **variância** na variável dependente que pode ser explicada pela(s) variável(is) independente(s).

![](https://practicum-content.s3.amazonaws.com/resources/1.4.5.1PT_1688481499.png)

É, de fato, é muita informação para absorver. Vamos nos concentrar nesta frase:

-   _Em outras palavras, a regressão linear nos ajuda a entender como uma mudança em uma variável afeta a(s) outra(s) variável(is)_

Agora, vamos nos afastar da teoria e colocar a mão na massa.

Como você deve ter adivinhado, o Python já possui um pacote que pode ajudar a você a executar um modelo de regressão linear.

Pergunta

Você consegue descobrir o que a seguinte linha de código faz?

```
from sklearn.linear_model import LinearRegression
```

Realiza uma análise de correlação.

Importa `LinearRegression` da biblioteca `sklearn.linear_model`.

Fantástico!

Ótimo! Agora, como de costume, o próximo passo é descobrir quais entradas são necessárias para executar o módulo `LinearRegression`.

Precisamos identificar a variável dependente, que chamaremos de **y**, e a variável independente, que chamaremos de variável **X**. Esses nomes são convenções em estatísticas. Embora possa ser difícil adotar no início, eles simplificam nosso trabalho a longo prazo, garantindo que todos os profissionais da área falem a mesma língua.

Para executar o módulo `LinearRegression`, você precisa armazenar a variável dependente (`y`) e a variável independente (`X`) como um vetor 2D.

**Saiba mais!**

-   Em Python, um vetor 2D é um vetor de vetores. À primeira vista, isso pode parecer confuso. Essencialmente, é uma coleção de vetores, onde cada vetor pode ter diferentes dimensões e tamanhos. Geralmente ele é usado para armazenar dados em formato tabular, onde linhas e colunas representam diferentes entidades e atributos, respectivamente. Para criar um vetor 2D em Python, você pode usar uma lista de listas.

Para obter um vetor 2D, precisamos usar um par de colchetes. Por exemplo, nosso `X` é a temperatura e aqui temos um vetor 2D para ela, assim teremos uma lista de listas:

`X = df_merged[["Temperature (Celsius)"]]`

Vamos armazenar os valores de `Temperature (Celsius)` e `# Ice-cream cones sold` em suas variáveis correspondentes. No pré-código abaixo, já armazenamos os valores de `Temperature (Celsius)` na variável `X`. Seu objetivo é armazenar os valores dos `# Ice-cream cones sold` na variável `y`. Por fim, imprima os 10 primeiros valores de `y` para verificar se tudo funcionou conforme o esperado.

CódigoPYTHON

9

1

2

3

4

x \= df\_merged\[\["Temperature (Celsius)"\]\]

y \= df\_merged\[\["# Ice-cream cones sold"\]\]

  

print(y\[:10\]) \# isso imprimirá as primeiras 10 linhas de y

Dica

Mostrar a soluçãoValidar

O módulo `LinearRegression` tem um método `fit` que tenta ajustar uma linha através dos pontos dispersos. Já importamos o módulo, e agora vamos utilizar para ajustar uma linha.

Execute o código abaixo:

CódigoPYTHON

9

1

2

3

4

5

6

\# importe nosso módulo LinearRegression

from sklearn.linear\_model import LinearRegression

  

reg \= LinearRegression().fit(x, y) \# função que ajusta um modelo aos dados

  

print(reg.score(x, y)) \# imprima a pontuação adequada

Mostrar a soluçãoExecutar

Usamos a função `fit()` para ajustar um modelo aos dados. É aqui que o modelo aprende os padrões de como `X` explica `y`.

O resultado que obtivemos é chamado de **pontuação**, que nos dá uma ideia de quão bem o modelo explica a variabilidade (ou movimento) da variável `y` com base na variável `X`. Quanto mais próxima à pontuação estiver de 1, melhor o modelo.

Essa pontuação é o que descrevemos anteriormente como **R-quadrado** em nossa definição de regressão linear.

A variável `reg` agora entende como essas duas variáveis estão relacionadas e armazena essas informações na forma de uma equação, que é exatamente o que estamos procurando!

O código a seguir imprime os componentes da equação: o intercepto e o coeficiente que multiplica `X`, onde `X` representa a temperatura em graus Celsius. Tanto a interceptação quanto o coeficiente são necessários para entender como `y` se parecerá para um determinado `X`.

CódigoPYTHON

9

1

2

3

4

5

6

7

\# importe nosso módulo LinearRegression

from sklearn.linear\_model import LinearRegression

  

reg \= LinearRegression().fit(x, y) \# função que ajusta um modelo aos dados

  

print(reg.intercept\_) \# imprimiremos a interceptação do modelo ajustado

print(reg.coef\_) \# e imprimiremos o coeficiente do modelo ajustado

Mostrar a soluçãoExecutar

**Saiba mais**

-   O intercepto é o valor que a variável dependente deve assumir quando a variável independente for igual a 0.
-   O coeficiente é o valor que capta a relação entre as variáveis independentes e dependentes. Ele nos informa o quanto se espera que a variável dependente mude para cada aumento de uma unidade na variável independente.

No nosso caso específico, isso significa:

-   A interceptação é o valor que `y` (`# Casquinhas vendidas`) deve assumir quando nossa variável `X` (`temperatura (Celsius)`) for igual a 0.
-   O coeficiente é o valor que captura a relação entre `X` e `y`.

Com base nesses valores, se tivéssemos que escrever a equação para Carlos, seria algo como:

1.  `y = X * coeficiente + intercepto`
2.  `y = X * (0.667) + (- 0.168)`
3.  `y = X * (0.667) - 0.168`
4.  `# Casquinhas vendidas = Temperatura (Celsius) * (0.667) - 0.168`

Se Deus quiser, você perceberá como isso é mágico. A variável `reg` já possui essa equação (a partir de agora denominada **modelo**) pronta para uso. Você pode realmente testar o desempenho dela. Vamos fazer!

Para executar esse **modelo** e ver quanto sorvete Carlos precisa para uma determinada temperatura, usaremos o método `predict`. Vamos dar uma olhada no código abaixo e executar ele.

CódigoPYTHON

9

1

2

y\_pred \= reg.predict(df\_merged\[\["Temperature (Celsius)"\]\])

print(y\_pred\[:10\]) \# imprime os 10 primeiros valores de y

Mostrar a soluçãoExecutar

O que acabou de acontecer? Pedimos ao nosso modelo para prever a quantidade de sorvete necessária em diferentes valores de temperatura. O modelo usou a equação que derivamos acima e gerou os números que traçamos. Ainda parece mágica? Espero que não mais!

Usaremos o gráfico de dispersão visto anteriormente para tentar **encaixar** a linha imaginária que descrevemos usando as previsões de nosso modelo.

Vimos antes como fazer gráficos e linhas de dispersão. Agora vamos colocar eles juntos, primeiro desenhando nosso gráfico de dispersão e, em seguida, adicionando a linha com nossos valores `y` previstos a ele.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

y\_pred \= reg.predict(df\_merged\[\["Temperature (Celsius)"\]\])

  

\# irá desenhar nosso gráfico de dispersão em laranja

plt \= df\_merged.plot(

kind\="scatter",

x\="Temperature (Celsius)",

y\="# Ice-cream cones sold",

color\="orange",

)

  

\# defina os rótulos para os eixos x e y e o título para o gráfico

plt.set\_xlabel("Temperatura (Celsius)")

plt.set\_ylabel("# de Casquinhas vendidas")

plt.set\_title("Ice cream sales vs Temperature")

  

plt.plot(x, y\_pred) \# trace uma linha com nossas previsões

Mostrar a soluçãoExecutar

Visualmente, podemos ver que o modelo se ajusta muito bem a uma linha, pois a maioria dos valores está centrada em torno dela.

Este é o melhor modelo linear? Não, não é!

Podemos melhorar nosso modelo? Sim, nós podemos. No entanto, deve ser bom o suficiente por enquanto.

Como profissional de dados, você sempre pode melhorar suas soluções e conclusões, mas também precisa estar atento à velocidade. Carlos está perdendo dinheiro todo fim de semana, e seu modelo é muito melhor do que a estimativa média anterior.

O código a seguir desenha um gráfico que nos permite comparar o desempenho da **média** em todo o conjunto de dados com a saída do nosso modelo.

Embora possamos calcular uma média móvel de 3 meses usando Python, para simplificar, usaremos apenas a média de todo o conjunto de dados, que abrange 5 meses de vendas. Usar `y.mean()` nos dará esse valor.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

y\_pred \= reg.predict(df\_merged\[\["Temperature (Celsius)"\]\])

  

plt \= df\_merged.plot(

kind\="scatter", x\="Temperature (Celsius)", y\="# Ice-cream cones sold",

color\="orange"

)

  

plt.set\_xlabel("Temperatura (Celsius)")

plt.set\_ylabel("# de Casquinhas vendidas")

plt.set\_title("Vendas de Casquinha vs Temperatura")

  

plt.plot(x, y\_pred) \# trace uma linha com nossas previsões

  

y\_mean \= y.mean().values \# calcule nosso valor médio

  

plt.plot(x, y\_mean.repeat(len(y))) \# trace uma linha com a nossa média

Mostrar a soluçãoExecutar

Intuitivamente, quanto mais próximos os pontos (ou valores reais) estiverem da linha azul (que representa o modelo que criamos), melhor será o desempenho do modelo. A linha laranja no gráfico acima ilustra a estimativa média que Carlos usou todos os dias anteriormente, independentemente do clima. Podemos observar que a estimativa média é confiável apenas quando a temperatura é de aproximadamente 22 graus Celsius.

Existem métodos formais para quantificar a qualidade do ajuste de um modelo de regressão linear, bem como métodos formais para treinar tais modelos. No entanto, esses tópicos não são abordados nesse curso introdutório. Oferecemos cursos dedicados a esses tópicos em nossos bootcamps de dados.

Hora de mostrar ao Carlos o que temos!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-22-41-613Z.md
### Última modificação: 2025-05-28 18:22:42

# Conclusões - TripleTen

Capítulo 4/5

Prevendo o Futuro

Opcional

# Conclusões

Então, e agora? O que vem depois?

Você precisa ser prático. Basta enviar uma mensagem de texto para o Carlos com a equação que você descobriu e aconselhar ele a verificar a previsão do tempo. Então, ele pode inserir a temperatura prevista para estimar quantos sorvetes venderá por dia!

Por exemplo, se Carlos souber que a temperatura amanhã será de 30 graus Celsius, ele precisará fazer cerca de 21 sorvetes, possivelmente mais dois, para que você e eu tenhamos um por conta da casa!

Com certeza ele ficará feliz! E você terá um lugar para onde ir quando a vontade de sorvete bater!

![](https://practicum-content.s3.amazonaws.com/resources/41_1687527390.png)

Ótimo trabalho! Você aprendeu muito!

Vamos recapitular algumas das coisas que você fez:

-   Você realizou algumas análises exploratórias rápidas.
-   Você transformou um problema em um conjunto de hipóteses.
-   Você realizou uma análise de correlação para testar sua hipótese mais promissora.
-   Você treinou um modelo de regressão que pode prever as vendas de sorvete com base na temperatura, e teve um desempenho excelente!
-   Você ajudou seu vizinho, Carlos, a reduzir seus custos futuros!

Descanse um pouco e então vamos começar o sprint final desse curso de introdução.

### 🔥

Você está interessado em saber mais sobre análise de dados e como podemos te ajudar a entrar na área da tecnologia? Temos um [webinário dedicado a isso](https://tripleten.com/es-mex/data-analyst/webinar/). Fique à vontade para se inscrever.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-22-42-917Z.md
### Última modificação: 2025-05-28 18:22:43

# Introdução - TripleTen

Capítulo 5/5

Mudando o Futuro

Opcional

# Introdução

Bem-vindo ao capítulo final deste curso!

Você se lembra da startup que você ajudou no primeiro capítulo deste curso? Eles ficaram realmente impressionados com o seu trabalho e querem que você dê uma olhada no mecanismo de alerta deles e ver se você pode melhorá-lo.

Você já é muito mais experiente (bem, talvez apenas um pouco mais) e confiante o suficiente para resolver este problema usando aprendizado de máquina.

Vamos ver como podemos fazer isso! O nosso objetivo é tornar o mecanismo de alertas mais preciso.

![](https://practicum-content.s3.amazonaws.com/resources/1.7.2PT_1688715978.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-22-49-808Z.md
### Última modificação: 2025-05-28 18:22:50

# Introdução ao Aprendizado de Máquina - TripleTen

Capítulo 5/5

Mudando o Futuro

Opcional

# Introdução ao Aprendizado de Máquina

Então, o que é aprendizado de máquina?

**Saiba mais!**

-   Aprendizado de máquina é uma ferramenta poderosa da ciência da computação que envolve o desenvolvimento de algoritmos e modelos que podem aprender e melhorar seu desempenho ao longo do tempo com base em entradas de dados. Em outras palavras, em vez de programar explicitamente um conjunto de regras ou instruções que um computador precisa seguir, podemos desenvolver modelos de aprendizado de máquina que conseguem identificar automaticamente padrões e tendências em conjuntos de dados. Esses padrões são então usados para fazer previsões ou tomar decisões. Isso pode ser alcançado por meio de algoritmos matemáticos complexos que podem analisar grandes quantidades de dados, identificar padrões e fazer predições com base nesses padrões. Aprendizado de máquina é um campo amplo em constante evolução com muitas áreas de aplicação, incluindo reconhecimento de imagens e fala, processamento de linguagem natural, detecção de fraude e muito mais.

![](https://practicum-content.s3.amazonaws.com/resources/1.7.2.2PT_1688471306.png)

Na nossa tarefa, em vez de programar explicitamente a lógica do sistema de alertas (como fizemos no primeiro capítulo), podemos dar os dados a um modelo de aprendizado de máquina e deixá-lo **aprender** as regras e os padrões atrás do **cancelamento**.

Aprendizado de máquina é um tópico muito amplo. Portanto, pedimos que você use a abstração que te trouxe aqui.

É importante observar que há várias famílias de algoritmos de aprendizado de máquina. Aquele que usaremos para lidar com este problema pertence à família dos algoritmos de **Classificação** que têm como objetivo aprender a categorizar dados. Neste projeto, vamos treinar um modelo de aprendizado de máquina para predizer se os clientes estão propensos a ir embora.

Usaremos um modelo **baseado em árvore** (você verá por que ele é chamado assim). Esses modelos são interpretáveis, o que significa que nós podemos entender as variáveis que o modelo está usando para detectar potenciais clientes migrados.

## Treinando o nosso primeiro modelo

Importe os dados e imprima as primeiras linhas para refrescar a memória sobre como é o conjunto de dados.

Você precisará:

1.  Ler o conjunto de dados usando a função `read_csv` de pandas (`pd`)
2.  Imprimir as primeiras linhas do conjunto de dados usando o método `head()`

CódigoPYTHON

9

1

2

3

4

import pandas as pd

file\_path \= "https://raw.githubusercontent.com/JJTorresDS/ds-data-sources/main/churn\_dataset.csv"

df \= pd.read\_csv(file\_path)

print(df.head())

Dica

Mostrar a soluçãoValidar

Agora nós vamos dividir o conjunto de dados em variáveis `X` e `y`, como fizemos treinando o modelo de regressão. `X` é todas as colunas no nosso conjunto de dados, exceto a coluna `'churn'`. Basicamente, essa é toda a informação que temos para fazer uma predição precisa. `y` é o que estamos tentando predizer. Já que precisamos prever os cancelamentos, esses são os valores da coluna `'churn'`.

Uma explicação formal seria a seguinte:

-   `X`: Lembre que a letra maiúscula X é convencionalmente utilizada para denotar uma **matriz**, o que é uma estrutura do tipo tabela. X conterá nossas variáveis independentes também chamadas de variáveis **preditoras**. Ao contrário do nosso problema de regressão, que consiste em apenas uma variável **independente**, o conjunto de dados `X` é multivariável, o que significa que ele contém muitas variáveis.
-   `y`: a letra minúscula **y** também é convencional e denota um vetor unidimensional. Esta variável contém a nossa variável **dependente** ou **de resposta**, ou seja, a variável de interesse que o nosso modelo precisa classificar e/ou predizer.

Armazene a resposta na variável `y`. Para fazer isso, basta selecionar a coluna `'churned'` da variável `df`: escreva `df` e então o nome da coluna (`'churned'`) entre colchetes de abertura e fechamento.

Imprima as cinco primeira observações de `y` para se certificar do que tudo funcionou conforme esperado.

CódigoPYTHON

9

1

2

3

X \= df\[\["age","support\_contacts\_last\_30\_days","items\_sold\_last\_30\_days"\]\]

y \= df\['churned'\]

print(y.head())

Dica

Mostrar a soluçãoValidar

Modelos de aprendizado de máquina em Python só aceitam matrizes, que são estruturas do tipo tabela que contêm apenas números.

Por exemplo, a variável `payment_method` é uma categoria que precisa ser transformada. Já que esta variável pode aceitar apenas dois valores, criaremos uma nova variável chamada `pmt_credit_card` (a abreviação para "payment method is credit card" – o método de pagamento é um cartão de crédito). Esta variável terá um valor de 1 se o método de pagamento do cliente for um cartão de crédito, e 0 se for algum outro.

Para criar uma nova variável e imprimir as 10 primeiras observações, execute o código abaixo. Não se preocupe se não entende o que o código está fazendo; nós vamos decompô-lo nos próximos parágrafos.

CódigoPYTHON

9

1

2

X\["pmt\_credit\_card"\] \= (df\["payment\_method"\] \== "credit\_card") \* 1

print(X\["pmt\_credit\_card"\].head())

Mostrar a soluçãoExecutar

Vamos decompor a linha anterior de código:

-   A comparação `df["payment_method"] == "credit_card"` retornará `True` se o método de pagamento for um cartão de crédito e `False` em outros casos.
-   Multiplicamos o resultado por 1 para converter `True` em 1 e `False` em 0.

Há um processo formal de treinamento de modelos de aprendizado de máquina, mas iremos evitar alguns daqueles passos para manter as coisas simples. O nosso objetivo neste curso é te relatar sobre o poder de Python na análise de dados, em vez de fornecer um curso abrangente sobre o aprendizado de máquina.

O código abaixo importa o modelo que vamos treinar, que é um modelo `DecisionTreeClassifier`. Esperamos que você ache esta sintaxe familiar.

```
from sklearn.tree import DecisionTreeClassifier
```

Vamos usá-la!

E então, nós avançamos para o seguinte:

Vamos usar o `DecisionTreeClassifier` como um modelo, então criaremos uma instância dele para então usarmos. Semelhante ao modelo de regressão do capítulo anterior, `DecisionTreeClassifier` é uma solução universal que funciona para vários casos.

O modelo já foi inicializado para você. O nosso objetivo é treiná-lo usando o método `fit` (o mesmo que usamos para treinar o modelo de regressão), passando a ele `x` e `y` separados por uma vírgula. Por favor, faça isso.

CódigoPYTHON

9

1

2

clf \= DecisionTreeClassifier(max\_depth\=3)

clf \= clf.fit(x, y)

Dica

Mostrar a soluçãoValidar

Você pode treinar um modelo de aprendizado de máquina para identificar os potenciais clientes migrados em apenas três linhas de código. No entanto, este processo não exibe nada na tela.

E podemos '"ver" o que exatamente o modelo entendeu dos dados que usamos para treiná-lo? Podemos!

Como mencionamos no início do capítulo, este tipo de modelo é interpretável, então nós podemos tentar entender como ele toma suas decisões.

O gráfico abaixo visualiza as regras que o modelo de aprendizado de máquina considera para determinar se um cliente pretende sair ou não.

**Observação**: Não iremos entrar em mais detalhes sobre como visualizar isso. Se você quer aprender mais sobre o aprendizado de máquina, este tópico é abordado com maior profundidade na parte de ambos os bootcamps de dados dedicada à Ciência de Dados, em que você aprenderá como criar gráficos como aquele que estamos prestes a ver.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_92_1687522484.png)

De acordo com a saída do modelo, parece que a idade do cliente e o número de contatos com o suporte realizados por ele dentro dos últimos 30 dias são as variáveis mais importantes na predição do cancelamento. Lemos este gráfico de cima para baixo. Se a idade for menor que 36,5, movemos para a esquerda e verificamos se o número de contatos com o suporte nos últimos 30 dias é menor ou igual a 4,5. A folha azul representa os nossos clientes migrados.

Então, de acordo com o modelo, os clientes que provavelmente irão embora podem ser identificados pelas **folhas** da árvore.

-   Têm menos de 36,5 (~37) anos
-   E entraram em contato com a equipe de suporte mais de 4 vezes nos 30 últimos dias.

**Observação**: Formalmente, para treinar modelos de aprendizado de máquina, realizamos treinamento e testes. Entretanto, treinamento de um modelo é uma tarefa complexa com algumas pecularidades a que devemos prestar atenção. Nesta introdução, não iremos dar aqueles passos para tornar o processo mais simples para você. Nós ensinamos os estudantes dos nossos bootcamps de dados sobre o processo formal de treinamento e testes de modelos de aprendizado de máquina e convidamos você para experimentá-los.

Enquanto você está pensando nisso, vamos avançar para as coisas ainda mais interessantes!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-22-51-131Z.md
### Última modificação: 2025-05-28 18:22:51

# Avaliando a qualidade de seu modelo - TripleTen

Capítulo 5/5

Mudando o Futuro

Opcional

# Avaliando a qualidade de seu modelo

Então, a próxima pergunta é se este modelo é bom. O código abaixo fornece uma pontuação de **precisão** que é calculada como a percentagem das vezes em que a previsão do modelo correspondeu ao valor real.

Primeiro, precisamos armazenar as previsões do modelo em uma variável. Vamos fazer isso.

Execute o código abaixo para fazer previsões e imprimir a acurácia de seu modelo.

CódigoPYTHON

9

1

2

3

4

from sklearn.metrics import accuracy\_score \# importando a função para a acurácia

  

y\_pred \= clf.predict(X) \# pedindo para o modelo fazer previsões

print(accuracy\_score(y, y\_pred)) \# calculando a precisão

Mostrar a soluçãoExecutar

0.8914 significa a precisão de 89,14%. Em termos simples, isso significa que o nosso modelo está correto em 90% dos casos. Nada mal!

Pergunta

Antes de seguirmos em frente, você pode analisar o que fez o trecho de código acima? Selecione todas as opções que se aplicam.

Escolha quantas quiser

A função `accuracy_score` foi importada do pacote `sklearn.metrics`.

Imprimiu o resultado da função `accuracy_score`.

Ela pediu para alienígenas melhorarem o desempenho do modelo.

Muito bem!

Agora vamos fazer uma previsão. Provavelmente, você percebeu que os passos são muito parecidos àqueles que você tomou no capítulo anterior para treinar um modelo de regressão. Essa é mais uma vantagem de usarmos Python e a biblioteca `scikit-learn` no aprendizado de máquina.

Para fazer previsões no conjunto de dados todo e imprimir as 10 primeiras previsões, execute o bloco de código abaixo.

CódigoPYTHON

9

1

2

y\_pred \= clf.predict(X) \# Prevê os valores de y para X

print(y\_pred\[:10\])

Mostrar a soluçãoExecutar

Assim... não é o suficiente.

Precisamos determinar se o resultado do modelo é melhor do que a nossa abordagem baseada em regras que usamos no primeiro capítulo.

Lembrando: nossas regras também podem levar em conta o número de chamadas. Inicialmente, definimos o limite para 4 chamadas, mas decidimos usar isso em combinação com o método de pagamento em vez da idade.

Quando fizemos um teste retroativo de nossas regras comparando os resultados com os dados, descobrimos que:

-   Os nossos alertas identificaram apenas 170 clientes que realmente optaram pelo cancelamento.
-   Houve 855 alertas gerados para os clientes que não optaram pelo cancelamento, o que resultou em 855 positivos falsos.

**Saiba mais**

-   Verdadeiro Positivo: quando o modelo identifica corretamente um caso positivo
-   Verdadeiro Negativo: quando o modelo identifica incorretamente um caso negativo como positivo
-   Falso positivo: quando o modelo identifica corretamente um caso negativo
-   Falso Negativo: quando o modelo identifica incorretamente um caso positivo como negativo

![](https://practicum-content.s3.amazonaws.com/resources/1.7.3.1PT_1688473508.png)

A função `pd.crosstab` pega dois vetores de valores como entrada e retorna uma tabela de dupla entrada.

**Saiba mais!**

-   Um vetor é uma coleção de itens de dados, todos do mesmo tipo, organizados em uma ordem sequencial.

Agora vamos examinar os resultados do teste de retroativo do nosso modelo de aprendizado de máquina.

CódigoPYTHON

9

1

2

3

y\_pred \= clf.predict(X) \# Prevê os valores de y para X

  

print(pd.crosstab(y, y\_pred)) \# Imprime a tabela com as nossas previsões

Mostrar a soluçãoExecutar

O resultado acima fornece informações sobre o desempenho de um modelo de previsão de cancelamento.

-   Em 4081 casos, os clientes realmente não cancelaram, e o modelo previu corretamente que eles não cancelariam. Esses são falsos positivos.
-   Em 103 casos, os clientes realmente não cancelaram, mas o modelo previu que eles iam. Esses são positivos falsos.
-   Em 429 casos, os clientes realmente cancelaram, mas o modelo previu o contrário. Esses são falsos negativos.
-   E, 287 casos, os clientes cancelaram, e o modelo previu isso corretamente. Esses são verdadeiros positivos.

Então, as regras geradas pelo modelo de aprendizado de máquina nos permitem:

-   Identificar os 287 clientes que cancelaram, o que é 117 a mais do que o nosso primeiro sistema de alerta identificou.
-   Ele gera apenas 103 positivos falsos, o que é apenas 12% dos positivos falsos gerados pelo sistema de alerta.

Vamos pausar rapidinho para considerar as consequências de positivos falsos e falsos negativos no mundo real.

-   Positivos falsos: Se usarmos o nosso sistema (baseado em regras ou aprendizado de máquina) para marcar os clientes que podem ir embora e fazer ofertas a eles, para não os deixar fazer isso e "mudar o futuro", positivos falsos podem resultar em gastos da empresa (brindes, meses gratuitos, preços mais baixos etc) com os clientes que inicialmente não planejavam sair.
-   Falsos negativos: Por outro lado, não identificar os clientes que planejam ir embora pode impactar diretamente a base dos clientes de uma empresa e sua receita.

O modelo de aprendizado de máquina tem um desempenho muito melhor do que o nosso sistema baseado em regras em ambas as áreas.

Outra vantagem é que o modelo pode tratar muito mais variáveis, o que o torna uma solução escalável. Isso está em contraste com a nossa abordagem baseada em regras que não pode ser dimensionada adequadamente.

Em geral, saímos bem! Vamos avançar para a última parte do nosso curso e tirar conclusões.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-02-384Z.md
### Última modificação: 2025-05-28 18:26:03

# Olá! - TripleTen

Capítulo

É bom ver você na TripleTen!

# Olá!

Daniel

Oi! Eu sou Daniel

É um prazer conhecer você!

Estamos felizes em ter você no Bootcamp de Análise de Dados!

Você deve estar na expectativa para começar a fazer testes, mas antes disso, que tal descobrir algumas coisas sobre como o programa funciona? Além disso, vamos conhecer (ou relembrar) alguns fundamentos de letramento digital.

Maravilha! O que preciso saber em primeiro lugar?

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-03-684Z.md
### Última modificação: 2025-05-28 18:26:04

# Agile: sprints e projetos - TripleTen

Capítulo

É bom ver você na TripleTen!

# Agile: sprints e projetos

No bootcamp, utilizamos a metodologia Agile para gerenciar sua experiência de aprendizagem. Para começar, queremos apresentar o Agile e como ele vai moldar sua jornada.

**Metodologia Agile** é uma forma de gerenciar projetos dividindo-os em partes menores e mais gerenciáveis. Isso permite a melhoria e adaptação contínuas por meio de feedback constante e progresso iterativo.

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/onboarding_data_pt/0.2.a_PT.png)

### 💡

Quer saber mais sobre o framework Agile? Sugerimos a leitura [deste material](https://learn.microsoft.com/pt-br/devops/plan/what-is-agile).

### Como usamos o Agile no bootcamp

O Bootcamp de Análise de Dados segue a metodologia Agile e inclui 18 sprints. Cada sprint foi criado para fornecer uma experiência de aprendizagem intensiva e focada que se baseia no sprint anterior.

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/image_6_1747916906.png)

Durante cada sprint, você vai:

1.  **Estudar a teoria**: entender os conceitos básicos e métodos relevantes para a análise de dados.
2.  **Realizar tarefas**: participar de tarefas interativas em nossa plataforma para aplicar o que você aprendeu.
3.  **Trabalhar no projeto**: trabalhar com um projeto que simule uma tarefa de desenvolvimento web do mundo real, para permitir que você pratique e aprimore suas habilidades.

Aqui está um mapa dos projetos nos quais você vai trabalhar ao longo do programa:

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/image_7_1747916911.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-05-010Z.md
### Última modificação: 2025-05-28 18:26:05

# Como ajudamos você a aprender - TripleTen

Capítulo

É bom ver você na TripleTen!

# Como ajudamos você a aprender

Os bootcamps da TripleTen são pensados para proporcionar a melhor experiência de aprendizagem, tornando seus estudos agradáveis e preparando você para o trabalho real na área. Como conseguimos isso?

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://youtube.com/embed/8aq1sBZJcOM"></iframe>

Ver transcrição

### 🎬

Certo, vamos ver o que torna a aprendizagem na TripleTen única

### Em primeiro lugar, espere apenas profissionais experientes orientando você

Aprender a programar é mais fácil com o apoio dos nossos tutores, profissionais experientes que já estiveram onde você está. Nossos tutores são seus guias no mundo da tecnologia. Eles realizam webinars e sessões de perguntas e respostas semanais e até se encontram com você pessoalmente para ajudar a entender melhor o material do curso. Eles vão compartilhar suas experiências de trabalho e explicar como resolveram problemas reais para fazer a ponte entre a teoria e a prática.

### Cada projeto vai ajudar a construir seu portfólio

Nossos projetos são práticos e relevantes para o setor, preparando você para tarefas do mundo real. Revisores, que são profissionais experientes do setor, vão verificar seus projetos e fornecer feedback para ajudar você a melhorar. Chamamos isso de _abordagem de construção de portfólio_ porque ensinamos análise de dados e desenvolvimento usando conjuntos de dados reais e casos de negócios reais. À medida que você avança no bootcamp, você vai construir seu portfólio com esses projetos.

### E, por último: sua aprendizagem será baseada na ciência

Você não vai estudar teoria com aulas pré-gravadas e entediantes. Nossa plataforma educacional combina teoria e prática com um livro interativo e um simulador de código, acessíveis a partir do seu notebook. Realizamos pesquisas extensivas sobre métodos de aprendizagem eficazes para criar uma experiência de aprendizagem envolvente.

Como você acabou de ver, são necessários muitos componentes para criar um ótimo ambiente de aprendizagem. O que está achando até agora?

Ótimo! O que mais?

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-25-183Z.md
### Última modificação: 2025-05-28 18:26:25

# Prepare-se para estudar - TripleTen

Capítulo

É bom ver você na TripleTen!

# Prepare-se para estudar

Esta lição é sobre preparar você para o sucesso. Pense nela como seu kit básico, repleto com todos os itens essenciais de que você precisa para ter sucesso neste curso. Vamos abordar como manter a motivação, gerenciar seu tempo de maneira eficaz e aproveitar o sistema de suporte à sua disposição. Vamos começar!

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://youtube.com/embed/zgxivxISTR4"></iframe>

Esperamos que as estratégias que você ouviu no vídeo ajudem você a manter o foco, a motivação e a organização ao longo de todo o bootcamp! Salve este checklist para ajudar você a gerenciar seu tempo sem preocupações:

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/0.4.1PT_1_1747917527.png)

💡 Para ajudar a gerenciar seus intervalos de estudo e pausas de forma eficaz, considere usar o aplicativo Pomodoro Timer no seu celular ou um aplicativo web como [este aqui](https://pomodor.app/timer)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-26-500Z.md
### Última modificação: 2025-05-28 18:26:26

# Introdução ao letramento digital - TripleTen

Capítulo

É bom ver você na TripleTen!

# Introdução ao letramento digital

Para se tornar um analista de dados é necessário dominar não apenas ferramentas profissionais, mas também ser um especialista em tudo relacionado à TI. As próximas 6 lições vão levar apenas algumas poucas horas e vão apresentar os fundamentos de letramento digital.

O que você vai aprender:

-   O que são sistemas operacionais e quais são os mais populares;
-   Como organizar e usar arquivos no computador;
-   Como instalar e atualizar aplicativos;
-   Como fazer capturas de tela e screencasts;
-   Como aproveitar ao máximo alguns aplicativos populares que você vai usar no trabalho: navegador, Google Drive, Zoom, Google Meet e Discord.

### 💡

Mesmo que o conteúdo não seja novidade para você, recomendamos revisar as lições para reforçar seu conhecimento. Além disso, você vai encontrar algumas dicas e truques que podem ser úteis no seu futuro trabalho!

Vamos começar!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-27-821Z.md
### Última modificação: 2025-05-28 18:26:28

# Entender os sistemas operacionais - TripleTen

Capítulo

É bom ver você na TripleTen!

# Entender os sistemas operacionais

Imagine que você acabou de lançar seu primeiro grande projeto como cientista de dados. Você vai trabalhar com várias ferramentas de software, para analisar conjuntos de dados, criar visualizações e trabalhar com aprendizado automático. Mas aqui está o problema: o sistema operacional (SO) do seu computador pode afetar significativamente a facilidade com que você pode usar essas ferramentas e concluir seu trabalho com eficiência.

Esta lição foi elaborada para ajudar você a compreender os diferentes sistemas operacionais disponíveis atualmente e como cada um deles pode afetar seu trabalho.

### 📎

_RESUMO_

Um sistema operacional (SO) é um programa de software que gerencia recursos de hardware e software de computador e fornece serviços comuns para programas de computador. **Em termos mais simples, um sistema operacional é como o maestro de uma orquestra. Ele gerencia e coordena as atividades dos programas e dos recursos do computador, como memória, CPU, dispositivos de entrada/saída e armazenamento.**

Windows, macOS, Linux e ChromeOS são quatro dos sistemas operacionais mais populares em uso atualmente. Ao compreender as diferenças entre esses sistemas operacionais, os usuários podem tomar decisões informadas ao escolher qual deles usar de acordo com suas necessidades.

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/09_cover_1747917946.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/09_slide_16_1747917951.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/09_slide_17_1747917956.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/09_slide_18_1747917977.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/09_slide_19_1747917980.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/09_slide_20_1747917984.png)

### Sistemas operacionais mais populares atualmente

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/onboarding_data_pt/0.5.a_PT.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-29-730Z.md
### Última modificação: 2025-05-28 18:26:30

# Arquivos e navegação - TripleTen

Capítulo

É bom ver você na TripleTen!

# Arquivos e navegação

Como analista de dados, você irá usar grandes conjuntos de dados e trabalhar com muitos arquivos, que deverão estar organizados para manter a produtividade e evitar confusão. Navegadores de Internet são sua principal ferramenta, portanto, você vai usá-los bastante para obter dados, fazer pesquisas e compartilhar seu trabalho online.

Nesta lição, vamos abordar os conceitos básicos de como trabalhar com informações em seu computador e navegador e vamos dar dicas sobre como organizá-las de maneira eficaz!

# Introdução ao sistema de arquivos

Trabalhar com computadores envolve lidar com vários **arquivos**: objetos em um computador que armazenam dados. Existem vários tipos de arquivo em um computador: alguns são de texto, outros armazenam imagens ou vídeos e alguns até correspondem à infraestrutura crítica do sistema operacional. Esses arquivos são armazenados no **sistema de arquivos** do seu computador, que é estruturado como uma hierarquia de **pastas** (ou **diretórios**) aninhados.

Existem alguns diretórios especiais que você precisa conhecer.

-   A **raiz do sistema de arquivos** é o diretório em que todos os outros são armazenados.
-   O **diretório de usuário** (ou **diretório home**) é onde os arquivos específicos desse usuário são armazenados. Mesmo que seja a única pessoa usando o computador, você ainda assim vai ter um diretório de usuário contendo arquivos e configurações pessoais.

Quer aprender a trabalhar com arquivos no seu sistema operacional? Clique na seta e revise as instruções para garantir que você sabe todos os detalhes.

Como trabalhar com arquivos no Windows

### Introdução

No Windows, você pode interagir com o sistema de arquivos usando o aplicativo `**Explorador de Arquivos**`. Você pode abri-lo facilmente ao pressionar a tecla Windows para abrir o menu Iniciar e, em seguida, digitar o nome do programa.

A raiz do sistema de arquivos do Windows é a **unidade C**. Entre outras coisas, a unidade C contém um diretório chamado `Usuários`. Nesse diretório, você encontra o seu **diretório de usuário** (ou **diretório home**).

A maioria dos arquivos com os quais você interage regularmente é armazenada no seu diretório de usuário. Por exemplo, é aqui que você encontra os diretórios `Documentos` e `Downloads`. No vídeo abaixo, mostramos como acessar seu diretório de usuário por meio do Explorador de Arquivos.

Confira como funciona:

<iframe class="base-markdown-iframe__iframe" id="player-Ngs5Gyajous" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Windows File Explorer 1/2" width="640" height="360" src="https://www.youtube.com/embed/Ngs5Gyajous?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Ff4ddcd3c-7d6e-4d07-a4a5-af7623fcc897%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

### Como baixar arquivos

Uma tarefa comum é baixar arquivos de um link em um site. Quando você clica em um link para baixar um recurso, o download é feito por padrão na pasta `Downloads`, que fica no seu diretório de usuário. Você viu como acessar esse diretório no screencast anterior. Para acessar `Downloads`, basta clicar duas vezes no ícone.

![Uma captura de tela de um diretório de usuário no Windows. O diretório Downloads está destacado](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.6.a_PT.png)

### Como manipular arquivos e diretórios

Ter todos os seus arquivos baixados em um único diretório é conveniente, mas não é bom para armazenamento de longo prazo. Em geral, se você quiser guardar algum arquivo, é melhor movê-lo para outro local no sistema de arquivos. Para fazer isso, você precisa entender os conceitos básicos de como trabalhar com o sistema de arquivos.

O screencast a seguir mostra como realizar estas tarefas básicas do sistema de arquivos:

-   Criar arquivos e diretórios;
-   Mover arquivos entre diretórios;
-   Copiar arquivos;
-   Excluir arquivos.

<iframe class="base-markdown-iframe__iframe" id="player-3HiC-IS-Ats" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Windows File Explorer 2" width="640" height="360" src="https://www.youtube.com/embed/3HiC-IS-Ats?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=2&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Ff4ddcd3c-7d6e-4d07-a4a5-af7623fcc897%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

### Como compactar e descompactar arquivos

Alguns arquivos ocupam muito pouco espaço. Isso inclui a maioria dos arquivos que só armazenam texto. Já outros arquivos, como os de vídeo, são muito maiores.

Costuma ser necessário **compactar** `arquivos grandes`para compartilhá-los com outras pessoas. A compactação é um processo no qual otimizamos como os dados de um arquivo são armazenados para reduzir o seu tamanho. Um tipo comum de compactação é o **arquivo zip**, que tem a extensão `.zip`.

O vídeo abaixo mostra como trabalhar com arquivos zip. Você vai ver o seguinte:

-   Como compactar um arquivo ou diretório em um arquivo zip;
-   Como **extrair** os arquivos originais de um arquivo compactado.

<iframe class="base-markdown-iframe__iframe" id="player-PP6L_Ric9A4" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Handling zip archives on Windows" width="640" height="360" src="https://www.youtube.com/embed/PP6L_Ric9A4?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=3&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Ff4ddcd3c-7d6e-4d07-a4a5-af7623fcc897%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Como trabalhar com arquivos no macOS

### Introdução

O Finder é o gerenciador de arquivos padrão do macOS. Ele fornece uma maneira prática de acessar e gerenciar arquivos, pastas, aplicativos e outros recursos armazenados em um computador Mac.

Você pode abrir o Finder de várias maneiras:

1.  Clique no ícone do Finder na barra Dock.
2.  Clique na mesa e depois selecione qualquer pasta no menu "Ir" na barra de menus superior.
3.  Use o atalho de teclado "**Command + Espaço**" para abrir a busca no Spotlight, depois digite "Finder" e selecione-o nos resultados.

Confira como funciona:

<iframe class="base-markdown-iframe__iframe" id="player-VQK-OjUGpPE" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="macOS Finder" width="640" height="360" src="https://www.youtube.com/embed/VQK-OjUGpPE?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=4&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Ff4ddcd3c-7d6e-4d07-a4a5-af7623fcc897%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

### Seu diretório de usuário

A maioria dos arquivos com os quais você interage regularmente é armazenada no seu **diretório de usuário** (ou **diretório home**). Por exemplo, é aqui que você encontra os diretórios `Documentos` e `Downloads`. No próximo vídeo, mostramos como acessar esse diretório usando o Finder.

Confira como funciona:

<iframe class="base-markdown-iframe__iframe" id="player-C3UqVF1dujU" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="MacOS Home Directory" width="640" height="360" src="https://www.youtube.com/embed/C3UqVF1dujU?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=5&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Ff4ddcd3c-7d6e-4d07-a4a5-af7623fcc897%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

### Como baixar arquivos

Uma tarefa comum é baixar arquivos de um link em um site. Quando você clica em um link para baixar um recurso, o download é feito por padrão na sua pasta `Downloads`. Você pode acessar esse diretório pela barra lateral do aplicativo Finder.

![Uma captura de tela do aplicativo Finder. Uma seta laranja aponta para a pasta Downloads na barra lateral.](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.6.b_PT.png)

### Como manipular arquivos e diretórios

Ter todos os seus arquivos baixados em um único diretório é conveniente, mas não é bom para armazenamento de longo prazo. Em geral, se você quiser guardar algum arquivo, é melhor movê-lo para outro local no sistema de arquivos. Para fazer isso, você precisa entender os conceitos básicos de como trabalhar com o sistema de arquivos.

O screencast a seguir mostra como realizar estas tarefas básicas do sistema de arquivos:

-   Criar arquivos e diretórios;
-   Mover arquivos entre diretórios;
-   Copiar arquivos;
-   Excluir arquivos.

Confira como funciona:

<iframe class="base-markdown-iframe__iframe" id="player-73BelLXSJQc" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="MacOS Files and Folders" width="640" height="360" src="https://www.youtube.com/embed/73BelLXSJQc?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=6&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Ff4ddcd3c-7d6e-4d07-a4a5-af7623fcc897%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

### Como compactar e descompactar arquivos

Alguns arquivos ocupam muito pouco espaço. Isso inclui a maioria dos arquivos que só armazenam texto. Já outros arquivos, como os de vídeo, são muito maiores.

Costuma ser necessário **compactar** `arquivos grandes`para compartilhá-los com outras pessoas. A compactação é um processo no qual otimizamos como os dados de um arquivo são armazenados para reduzir o seu tamanho. Um tipo comum de compressão é o **arquivo zip**, que tem a extensão `.zip`.

O vídeo abaixo mostra como trabalhar com arquivos zip. Você vai ver o seguinte:

-   Como compactar um arquivo ou diretório em um arquivo zip;
-   Como **extrair** os arquivos originais de um arquivo compactado.

Confira como funciona:

<iframe class="base-markdown-iframe__iframe" id="player-Ytfly3QexmA" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Working with Zip Archives on MacOS" width="640" height="360" src="https://www.youtube.com/embed/Ytfly3QexmA?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=7&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Ff4ddcd3c-7d6e-4d07-a4a5-af7623fcc897%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Como trabalhar com arquivos no Linux

Conforme já discutimos na lição anterior, existem diversas distribuições do Linux disponíveis.

Por isso, uma abordagem completa sobre como interagir com arquivos usando o Linux está além do escopo desta lição. Em vez disso, para aprender a interagir com o sistema de arquivos em determinada versão do Linux, consulte a documentação online da distribuição e do ambiente de desktop que você está utilizando.

Agora que você já sabe o básico sobre como gerenciar arquivos offline, vamos nos aprofundar no trabalho com informações online.

Com certeza!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-31-046Z.md
### Última modificação: 2025-05-28 18:26:31

# Instalação e atualização de aplicativos - TripleTen

Capítulo

É bom ver você na TripleTen!

# Instalação e atualização de aplicativos

Trabalhando como analista de dados, você vai precisar das ferramentas mais recentes para analisar conjuntos de dados, criar visualizações e colaborar com sua equipe.

Isso significa que você vai precisar instalar vários aplicativos, como um editor de código, software de controle de versão e ferramentas de comunicação para manter tudo funcionando sem problemas.

Nesta lição, vamos guiar você pelos passos essenciais para procurar, instalar e atualizar aplicativos no seu computador. Vamos começar a configurar seu workspace!

## 1\. Procurar por aplicativos

Antes de poder instalar um aplicativo no seu computador, você precisa encontrar o **pacote de instalação**. Esse pacote contém todos os arquivos de que seu computador precisa para instalar e executar o aplicativo.

Também é preciso garantir que a fonte do pacote é confiável e segura. Por isso, recomendamos encontrar o site oficial dos criadores do software. Geralmente há uma página "Download" com links para os pacotes de instalação em diferentes sistemas operacionais, incluindo Windows, macOS e Linux.

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_refactored_demo_V7/ES/Untitled.png)

Na captura de tela acima, temos a página de download do site de um editor de código-fonte. Como você pode ver, os pacotes de instalação para diferentes sistemas operacionais têm links de download separados. É importante escolher aquele que corresponde ao sistema operacional que você está usando. Aprendemos como encontrar a versão do sistema operacional na lição anterior.

## 2\. Instalar um aplicativo

Você provavelmente já adivinhou essa, mas o fluxo de instalação varia conforme o sistema operacional que você está usando. Por exemplo, o pacote de instalação para Windows geralmente é baixado nos formatos `.exe` ou `.msi`. Já em um computador macOS, os formatos do pacote de instalação são `.dmg` ou `.pkg`. Clique na seta para saber mais sobre seu sistema operacional.

Windows

O pacote de instalação para Windows geralmente é baixado nos formatos `.exe` ou `.msi`. Quando o pacote estiver no seu computador, clique duas vezes nele para abrir o instalador.

Nesse momento, talvez o Windows exiba um aviso de segurança para verificar o fornecedor de software. Já que você tem certeza de que o pacote vem de uma fonte confiável, clique em "Executar" para prosseguir.

O instalador costuma exibir várias mensagens antes de iniciar o processo de instalação:

1.  **Aceitar o contrato de licença:** antes de instalar um aplicativo, você precisa concordar com uma série de termos e condições dos criadores do software.
    
    ![ALT: O instalador do VSCode exibindo o contrato de licença. A opção "Eu aceito o contrato" está selecionada.
    LEGENDA: Aceite o contrato e clique em **Próximo**.](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.7.b_PT.png)
    
2.  **Selecionar um destino para a instalação:** por padrão, a maioria dos softwares faz a instalação na pasta Arquivos de Programas ou na pasta Programas na unidade do sistema, mas você pode especificar um local diferente, se desejar.
    
    ![ALT: O instalador do VSCode mostrando a página "Selecionar local de destino". O local padrão está destacado.
    LEGENDA: Aceite o local padrão e clique em **Próximo**.](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.7.c_PT.png)
    
3.  **Selecionar tarefas adicionais:** alguns aplicativos têm opções adicionais para você considerar, como criar uma pasta no menu Iniciar ou adicionar um ícone na área de trabalho para acesso rápido.
    
    ![ALT: A instalação do VSCode mostrando a página "Selecionar tarefas adicionais". As quatro caixas de seleção sob "Outros" estão marcadas.
    CAPTION: Marque as caixas apropriadas. Em seguida, clique em **Próximo**.](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.7.d_PT.png)
    

Após essas etapas, o instalador copia os arquivos do aplicativo para o seu computador.

macOS

Em um computador macOS, os formatos dos pacotes de instalação são `.dmg` ou `.pkg`.

Com um arquivo `.dmg`, você só precisa arrastá-lo da pasta Downloads para a pasta Aplicativos. Assim que você move o pacote para a pasta Aplicativos, o sistema instantaneamente o copia como um aplicativo e ele fica pronto para uso.

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.7.e_PT.png)

Se você baixou o pacote no formato `.pkg`, clique duas vezes nele para abrir o instalador. Assim como no Windows, o instalador solicita que você faça pelo menos duas coisas: aceitar um contrato de licença e escolher o destino do aplicativo. Depois disso, você clica em "Instalar", e o instalador copia os arquivos do aplicativo para o sistema.

## 3\. Atualizar aplicativos

É importante manter os aplicativos atualizados para garantir seu desempenho e estabilidade. Essas atualizações costumam incluir correções de bugs e de ameaças de segurança. Elas também introduzem novos recursos que melhoram a experiência do usuário.

A maioria dos aplicativos busca atualizações automaticamente quando há conexão com a Internet, e você vai receber uma notificação quando uma estiver disponível. Como alternativa, você pode fazer uma busca manual no aplicativo. Essa opção costuma ficar nos menus "Ajuda" ou "Sobre".

## 🚀 Prática

Baixe e instale a ferramenta de videoconferência Zoom no seu computador.

Para instalar o Zoom no seu computador, siga as seguintes etapas:

1.  Vá para [https://zoom.us/download](https://zoom.us/download) e clique no botão "Download" em "Zoom Workplace".
2.  Assim que o download for concluído, execute o arquivo do instalador.
3.  Siga as instruções na tela para terminar o processo de instalação.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-32-362Z.md
### Última modificação: 2025-05-28 18:26:32

# Colaboração em nuvem: Google Drive - TripleTen

Capítulo

É bom ver você na TripleTen!

# Colaboração em nuvem: Google Drive

No seu trabalho como analista de dados, você vai descobrir que a colaboração é fundamental. Muitas vezes você vai ter que compartilhar recursos e trabalhar simultaneamente em documentos com colegas que podem estar espalhados pelo mundo. O Google Drive se destaca como uma ferramenta essencial para esse ambiente colaborativo, e nesta lição, vamos apresentar os fundamentos do uso do Google Drive para colaboração na nuvem.

### 1\. Compartilhar arquivos

O Google Drive permite compartilhar facilmente arquivos com qualquer pessoa seguindo estas etapas:

1.  Abra "Meu Drive" e selecione os arquivos que você quer compartilhar. ![Group 653.png](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.8.a_PT.png)
2.  Clique no botão "Compartilhar" na lista de opções abaixo da barra de pesquisa ou clique com o botão direito e escolha "Compartilhar".
3.  Uma janela pop-up vai aparecer para você digitar o endereço de e-mail ou a conta Google da pessoa com quem deseja compartilhar o arquivo. Você também pode copiar o link compartilhável clicando no botão "Copiar link". ![Group 655.png](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.8.b_PT.png)
4.  Nesse pop-up, você também pode especificar o nível de controle que os outros terão sobre o documento. Você pode conceder a habilidade de ver, comentar ou editar. Por exemplo, com o acesso de "leitor", a pessoa só pode ver o documento, não tendo permissão para deixar comentários nem editar ou alterar diretamente o documento.
5.  Clique em "Concluído" para finalizar o compartilhamento.

### 2\. Exportar arquivos

Se alguém compartilhou um arquivo do Google Drive com você ou se você quiser exportar um arquivo do seu próprio drive, siga estes passos:

1.  Selecione o arquivo que deseja exportar.
2.  Vá até o menu de arquivos, clique com o botão direito no arquivo e escolha "Baixar". Selecione o formato de arquivo de sua preferência. Se não souber qual escolher, o formato PDF costuma ser uma boa opção. ![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/onboarding_data_pt/0.8.c_PT.png)
3.  Assim que você escolher um formato, o arquivo será baixado no seu computador.

### 3\. Fazer upload arquivos

O Google Drive permite fazer o upload de arquivos para que você possa acessá-los de qualquer lugar no qual você tenha acesso à sua conta do Google. Para fazer upload de arquivos no seu drive:

1.  Abra o Google Drive e clique no botão "Novo". ![image 39.png](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.8.d_PT.png)
2.  Escolha "Upload de arquivo" ou "Upload de pasta". Uma pasta é um grupo de arquivos. ![image 40.png](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.8.e_PT.png)
3.  Selecione os arquivos ou a pasta que deseja fazer o upload e clique em "Abrir".
4.  Os arquivos serão enviados para o drive.

### Planilhas, Documentos e Apresentações Google

O Google oferece uma série de ferramentas poderosas de produtividade e colaboração, incluindo Planilhas, Documentos e Apresentações Google. Elas são similares ao Microsoft Excel, Word e PowerPoint. Então, se você já conhece esses recursos, vai se dar bem com as ferramentas do Google.

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/onboarding_data_pt/0.8.f_PT.png)

Para saber mais, recomendamos assistir a este [vídeo do Google](https://www.youtube.com/watch?v=25CtYkqamIA&t=3s) (em inglês) sobre como usar o Google Drive.

Pergunta

## 🚀 Prática

Siga estas etapas para gerenciar o compartilhamento de arquivos no Google Drive.

1.  **Faça o upload de um arquivo**: escolha um arquivo no seu computador e faça o upload no Google Drive.
2.  **Defina o acesso de visualização**: compartilhe o arquivo definindo seu acesso como "Qualquer pessoa com o link pode ver". Clique com o botão direito no arquivo no Google Drive, selecione "Compartilhar", depois em "Acesso geral", escolha "Qualquer pessoa com o link" e defina a função como "Leitor".
3.  **Copie este link**. Copie o link e cole-o no campo abaixo.

Enviar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-34-411Z.md
### Última modificação: 2025-05-28 18:26:34

# Capturas de tela e gravações de tela - TripleTen

Capítulo

É bom ver você na TripleTen!

# Capturas de tela e gravações de tela

Imagine que você não pode participar de uma sessão de coaprendizagem no bootcamp e já é tarde da noite. Você precisa mostrar um problema que está enfrentando ou explicar um problema complexo. Em vez de escrever uma mensagem longa, é muito mais eficaz preparar uma gravação ou uma captura de tela para compartilhar.

Nesta lição, você vai aprender a fazer capturas e gravações de tela profissionais no Windows e no macOS, além de descobrir como usar um programa como o Loom, que ajuda a comunicar suas necessidades de forma eficaz.

<iframe class="base-markdown-iframe__iframe" id="player-97cfzVpYuak" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Capturas de tela e gravações de tela" width="640" height="360" src="https://www.youtube.com/embed/97cfzVpYuak?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F7b377c54-bfde-4344-b2e2-89c0a1bde37a%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Ver transcrição

### 🎬

Você está tendo alguns problemas de software. Talvez seja uma mensagem de erro ou talvez algo simplesmente não esteja funcionando conforme o esperado. Você decide entrar em contato com alguém para pedir ajuda. Mas você deve tentar explicar o problema o mais claramente possível, certo? Bom, antes de começar a escrever todos os detalhes, não se esqueça do velho ditado: _uma imagem vale mais que mil palavras._

Ao enviar imagens, você pode usar seu telefone para tirar uma foto da tela. No entanto, isso é pouco profissional, e esse tipo de imagem costuma ser difícil de ver claramente. Portanto, é sempre melhor fornecer uma **captura de tela** ou uma **gravação de tela**.

## Captura de tela vs. Gravação de tela

### Captura de tela

Uma captura de tela é uma única imagem de tudo o que está sendo exibido na tela. Isso é útil para capturar informações estáticas, como uma mensagem de erro, configurações específicas ou um único passo em um processo. As capturas de tela são rápidas e fáceis de tirar, fornecendo uma imagem clara da tela em determinado momento.

### Gravação de tela

Uma gravação de tela é uma captura de vídeo da sua tela ao longo de um período. Isso é útil para demonstrar uma sequência de ações ou processos, mostrar como navegar em um software ou capturar um problema à medida que ele se desenvolve. As gravações de tela fornecem uma visão dinâmica e abrangente da atividade da sua tela, facilitando a compreensão de problemas ou de fluxos de trabalho complexos.

Na prática, capturas de tela são ótimas para registrar momentos específicos, enquanto gravações de tela são ideais para ilustrar uma série de ações ou problemas em andamento.

## Outras opções

Se for necessário fazer várias gravações de tela para explicar adequadamente um problema, é uma boa ideia gravar a tela e fazer comentários ao mesmo tempo. Isso significa que você pode fornecer uma narração ou usar sua webcam para adicionar um toque pessoal, explicando o que está acontecendo na tela enquanto avança. Existem vários programas gratuitos de gravação de tela multiplataforma que você pode experimentar.

O processo de fazer uma captura ou gravação de tela é diferente no Windows e no macOS, então sugerimos que você revise os passos conforme o sistema operacional que estiver usando:

Windows

### Como criar capturas de tela no Windows

Para criar uma captura de uma parte específica da tela, você pode usar a Ferramenta de Captura nativa. Para abri-la, clique no botão **Iniciar**, digite **Ferramenta de Captura** na caixa de pesquisa e a selecione nos resultados. Depois que a Ferramenta de Captura estiver aberta, clique em **Novo** e selecione a área da tela que você quer capturar. Feito isso, você pode salvar a captura de tela como um arquivo de imagem ou copiar e colar em outro programa.

![ALT: "A Ferramenta de Captura do Windows.”](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.9.a_PT.png)

Também existem vários atalhos de teclado que permitem tirar capturas de tela de diferentes partes da tela:

-   **Windows + Shift + S** abre a Ferramenta de Captura (descrita acima).
-   A tecla **Print Screen** captura uma imagem da tela inteira e a carrega na área de transferência. Você pode então colar essa captura de tela em outro programa, como no Discord.
-   **Windows + Print Screen** tira uma captura da tela inteira e a salva no computador. Normalmente, essas capturas são salvas na pasta `Imagens`.
-   **Alt + Print Screen** tira uma captura de tela da janela ativa e a carrega na área de transferência.

Em alguns teclados, talvez seja necessário pressionar também a tecla **Fn** ao usar a tecla **Print Screen**.

### Como criar gravações de tela no Windows

Para criar gravações de tela sem áudio de uma parte da tela, você pode usar a Ferramenta de Captura que apresentamos na seção anterior. Basta selecionar o botão com o ícone de gravação (1), clicar em "Novo" (2) e selecionar a parte da tela que você quer gravar.

![ALT: "Ferramenta de Captura do Windows configurada para fazer uma gravação de tela".](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.9.b_PT.png)

Você também pode usar o atalho de teclado **Windows + G**. Isso abre o "Xbox Game Bar", que permite fazer capturas e gravações de tela com ou sem som. Não se preocupe, você não precisa de um Xbox para usar esse recurso. Para obter instruções, consulte a [página de suporte](https://support.microsoft.com/en-us/windows/record-a-game-clip-on-your-pc-with-xbox-game-bar-2f477001-54d4-1276-9144-b0416a307f3c#ID0EDD=Windows_11) da Microsoft.

macOS

## Como criar capturas de tela no macOS

Para criar uma captura de tela no macOS, você pode usar o aplicativo nativo de captura de tela. Você pode encontrá-lo na pasta Aplicativos ou usar o atalho do teclado **Command + Shift + 5** para abri-lo.

Depois que o aplicativo estiver aberto, selecione a área da captura: a tela inteira, uma janela selecionada ou uma parte da tela. Você também pode escolher onde salvar a captura de tela e se deseja definir um temporizador antes de tirá-la.

Depois de realizar a captura, ela vai aparecer como uma miniatura no canto inferior direito da tela. Você pode clicar nessa miniatura para editar ou salvar a captura ou pode arrastá-la direto para um aplicativo, como um e-mail ou documento.

Aqui estão mais alguns atalhos úteis. Todos eles salvam automaticamente uma captura de tela na mesa:

-   **Command + Shift + 3** tira instantaneamente uma captura de tela.
-   **Command + Shift + 4** abre uma ferramenta para selecionar e tirar uma captura de uma parte específica da tela.
-   **Command + Shift + 4 + Barra de espaço** tira uma captura de tela da janela aberta no momento (por exemplo, apenas do Safari ou do Visual Studio Code). Pressionar essa combinação de teclas transforma o cursor em um ícone de câmera e, ao clicar, uma captura de tela será tirada.

## Como criar gravações de tela no macOS

Fazer gravações de tela no macOS é bem fácil usando algumas ferramentas nativas. Usando a mesma combinação de antes, **Command + Shift + 5**, você verá uma barra de ferramentas aparecer com alguns ícones. Os ícones que mostram janelas retangulares com círculos sobrepostos são os de gravação. Você pode especificar se quer gravar uma parte da tela ou a tela inteira.

Depois de começar a gravar, você pode interromper a gravação clicando no ícone de parar na barra de menus do macOS no topo da tela (o ícone é ⏹️).

Por padrão, as gravações são salvas na mesa. No entanto, isso pode ser ajustado usando **Command + Shift + 5**, selecionando o menu suspenso "Opções" e escolhendo um destino.

Pergunta

## 🚀 Prática

1.  **Grave sua lição**: use o [Loom](http://loom.com) para criar uma gravação de tela da lição que você está fazendo. Mostre as seções principais e descreva o que você está aprendendo ao longo da lição.
2.  **Salve e copie o link**: após a gravação, salve seu vídeo no Loom, que gera automaticamente um link compartilhável.
3.  **Cole o link abaixo**: copie o link do seu vídeo e cole-o no campo abaixo para compartilhar sua gravação.

Enviar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-35-715Z.md
### Última modificação: 2025-05-28 18:26:36

# Ferramentas de videoconferência: Zoom e Google Meet - TripleTen

Capítulo

É bom ver você na TripleTen!

# Ferramentas de videoconferência: Zoom e Google Meet

Na TripleTen, você vai usar ferramentas de videoconferência para falar com tutores e participar de eventos ao vivo. E é muito provável que você também use esse tipo de ferramenta no seu futuro emprego! Imagine que você está prestes a participar de uma chamada importante, mas seu microfone não está funcionando ou você não consegue compartilhar a tela. Saber como navegar nessas ferramentas de maneira eficaz pode salvar você nessas situações.

Nesta lição, vamos dar uma visão geral para garantir que você saiba trabalhar com algumas das ferramentas de videoconferência mais populares, como o Zoom e o Google Meet.

## Fundamentos das chamadas de vídeo

### 1\. Testar sua configuração: microfone, câmera e compartilhamento de tela

Há algumas coisas a se considerar tanto no aplicativo quanto no navegador. Por exemplo, você precisa garantir que os aplicativos têm acesso ao microfone e à câmera antes de iniciar uma chamada. Também é importante verificar se as permissões de compartilhamento de tela estão configuradas.

Zoom

Após a instalação do Zoom, você pode testar a qualidade do microfone e da câmera participando de uma reunião de teste. Confira as instruções [aqui](https://support.zoom.us/hc/en-us/articles/115002262083-Joining-a-test-meeting). Participar de uma reunião de teste também é uma boa maneira de conhecer a interface do usuário.

Para testar a opção de compartilhamento de tela, você pode entrar em uma reunião de teste, conforme descrito acima. Uma vez na reunião, clique no botão "Compartilhar tela" na parte inferior. Você vai ver várias opções. Por enquanto, escolha qualquer uma delas. O importante é aceitar todas as permissões que aparecerem na tela.

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.10.a_PT.png)

Quando as permissões forem concedidas, uma mensagem destacada em verde informando que sua tela está sendo compartilhada vai aparecer na tela.

Google Meet

Na primeira vez que você participar de uma reunião do Google Meet, será solicitada permissão para acessar sua câmera e microfone. Clique em "Sempre permitir" e depois em "Concluído". Para testar as configurações, comece uma nova reunião clicando em "Nova reunião" e depois em "Iniciar uma reunião instantânea". Você vai entrar em uma reunião vazia, permitindo testar seu vídeo e áudio, além de conhecer melhor a interface.

Para testar a opção de compartilhamento de tela, você pode entrar em uma reunião instantânea, conforme descrito acima. Uma vez na reunião, clique no botão "Apresentar agora" na parte inferior (o retângulo com uma seta dentro). Você vai ver algumas opções. Por enquanto, o importante é apenas aceitar qualquer solicitação na tela relacionada a permissões.

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.10.b_PT.png)

Uma vez que as permissões tenham sido concedidas, você vai ver uma mensagem na parte inferior da tela dizendo que está compartilhando a tela, com o botão "Parar compartilhamento" ao lado dela.

### 💡

Dica: se a sua Internet estiver lenta, desligar o vídeo pode ajudar.

### 2\. Modo silencioso

Durante uma chamada de vídeo, pode ser útil ficar no mudo para evitar interromper quem está falando, principalmente em reuniões maiores, como webinars. Quando pressionado, esse botão muda para um ícone de microfone riscado e passa a exibir a opção de desativar o mudo. Isso significa que o microfone foi desligado.

Zoom

O botão "Ativar mudo" está localizado na parte inferior esquerda do painel de ferramentas.

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.10.c_PT.png)

### 💡

Dica: atalhos do Zoom

-   Pressione **Alt + A** se estiver usando o Windows/ChromeOS.
-   Pressione **Shift + ⌘ + A** em um Mac.

Google Meet

O botão "Desativar microfone" está localizado na parte inferior esquerda do painel de ferramentas.

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.10.d_PT.png)

### 💡

Dica: atalhos do Google Meet

-   Pressione **Ctrl + D** se você estiver usando o Windows/ChromeOS.
-   Pressione **⌘ + D** em um Mac.

### 3\. Enviar reações

As reações nas videochamadas são uma forma de expressar seus sentimentos ou indicar seu status sem interromper quem está falando. Você pode simplesmente reagir ao que a pessoa está dizendo com um emoji!

Zoom

Na barra inferior do aplicativo Zoom, você encontra o botão "Reações". Ao pressionar esse botão, você pode reagir à reunião usando um emoji (por exemplo, 👍🏼 ou 👏🏼). Essas reações ficam ativas por 10 segundos.

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.10.e_PT.png)

A qualquer momento durante uma reunião no Zoom, você também pode usar o botão "Levantar mão" para pedir a palavra. Essa opção permanece ativa até você clicar no botão "Abaixar mão".

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.10.f_PT.png)

Google Meet

O Google Meet funciona da mesma maneira. Na barra inferior, você encontra reações padrão e um botão separado para levantar e baixar a mão.

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/qa-sprint-0/PTBR/V8/0.10.g_PT.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/onboarding_data_pt/0.10.f_PT.png)

Ao seguir essas dicas, você pode tornar suas chamadas de vídeo mais eficientes e agradáveis. Lembre-se de manter a atenção, a educação e o respeito durante a chamada para ter uma experiência de videochamada bem-sucedida.

## 🚀 Prática

1.  Instale o aplicativo [Zoom](https://zoom.us/download) em seu computador caso não tenha feito isso na lição "Instalar e atualizar aplicativos".
2.  Abra o aplicativo Zoom e participe de uma reunião de teste.
3.  Uma vez na reunião, experimente as funções básicas:
4.  Desligue o microfone para controlar a entrada de áudio;
5.  Ligue e desligue a câmera para controlar a transmissão de vídeo;
6.  Explore a opção de compartilhamento de tela para apresentar conteúdo do seu dispositivo;
7.  Expresse reações usando emojis para dar um toque divertido e interativo à reunião;
8.  Teste seu microfone e câmera para garantir que estejam funcionando corretamente;
9.  Verifique se as permissões de compartilhamento de tela estão configuradas corretamente.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-37-000Z.md
### Última modificação: 2025-05-28 18:26:37

# Quiz: letramento digital - TripleTen

Capítulo

É bom ver você na TripleTen!

# Quiz: O que você sabe sobre letramento digital?

Esta é sua chance de mostrar o que você acabou de aprender sobre navegação, aplicativos populares e comunicação digital.

Responda uma pergunta de cada vez e divirta-se demonstrando suas habilidades. Boa sorte! 😎

Pergunta

Qual sistema operacional você escolheria em cada situação?

Você ama jogos e passa horas explorando mundos virtuais. Além dos jogos, você também usa o computador para navegar na Internet, assistir a vídeos e gerenciar documentos pessoais.

Você trabalha na área de design gráfico. Além disso, você prioriza uma experiência de usuário perfeita e intuitiva que permite focar no seu processo criativo.

Você é uma pessoa que entende de tecnologia e gosta de explorar e experimentar diferentes softwares, além de valorizar o controle e a liberdade das soluções de código aberto.

Você procura um sistema operacional que possa fornecer um ótimo desempenho em uma configuração de hardware modesta. A maior parte do seu tempo é gasta em um navegador web, e-mail e edição de documentos online.

Mostrar a resposta

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-39-763Z.md
### Última modificação: 2025-05-28 18:26:40

# Carta para o seu futuro eu - TripleTen

Capítulo

É bom ver você na TripleTen!

# Carta para o seu futuro eu

Agora você tem uma base sólida em letramento digital, com habilidades e conhecimentos essenciais para navegar com eficácia em diversas ferramentas digitais. Ao finalizarmos este curso, vamos dar um último passo que é um pouco mais pessoal e motivacional!

Manter o foco constante no seu objetivo final é a parte mais difícil ao alcançar metas de longo prazo.

Você provavelmente está transbordando motivação, energia e curiosidade agora. Porém, ingressar em uma nova profissão pode ser desafiador, e no final do percurso, você pode sentir que não tem mais forças para continuar. Isso é totalmente normal.

Para receber apoio no momento mais difícil, envie uma carta para o seu futuro eu. Você vai receber esta carta em alguns meses, e ela vai servir para se lembrar dos seus objetivos e expectativas em relação ao bootcamp.

Como fazer isso?

1.  Acesse o [serviço](https://www.futureme.org/) MailFuture.
2.  Comece a escrever uma carta. Não se esqueça: cabe a você decidir qual carta você vai receber em alguns meses. Ninguém vai ler a carta exceto você, então escreva o que estiver sentindo.
3.  Preencha seu nome e e-mail e programe a entrega para daqui a 2-3 meses.

### 💡

Quando você receber a sua carta na metade do bootcamp, recomendamos escrever outra. Programe a entrega para quando você estiver trabalhando no projeto final (geralmente são 5 meses após o início do curso). Essa etapa pode ser desafiadora, e a motivação adicional da carta vai ser essencial.

O que escrevo?

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-26-41-051Z.md
### Última modificação: 2025-05-28 18:26:41

# Feedback - TripleTen

Capítulo

É bom ver você na TripleTen!

# Feedback

Adoraríamos ouvir sua opinião sobre o curso. Seu feedback vai nos ajudar a melhorá-lo.

Como você avaliaria este curso de 1 a 5? (1 = Precisa Melhorar, 5 = Excelente):

⭐

⭐⭐

⭐⭐⭐

⭐⭐⭐⭐

⭐⭐⭐⭐⭐

Avançar

1 — 5

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-30-35-598Z.md
### Última modificação: 2025-05-28 18:30:36

# Introdução - TripleTen

Capítulo 1/3

Módulo 1. Análise de Dados

Opcional

# Introdução

Olá!

Você chegou ao início do nosso curso de Excel. Ficamos felizes por ter você aqui! Este curso não é obrigatório para as profissões de Analista de Dados e Cientista de Dados, mas recomendamos que você o complete antes do início das aulas no seu grupo. Se você é iniciante no trabalho com dados, este curso te ajudará a começar sua jornada com mais conforto. Se você já tem alguma experiência, ele será uma ótima maneira de atualizar suas habilidades. Divirta-se!

Olá!

Jonathas

Olá!

TripleTen

Deixa eu te contar um pouco mais sobre este curso e explicar como o processo de aprendizado é organizado.

Está bem!

Jonathas

Está bem!

## O que você vai aprender neste curso?

Este curso aborda as ferramentas básicas do Excel que podem ser usadas para resolver até 80% de tarefas diárias de trabalho.

Planilhas são uma ferramenta poderosa com tantos recursos que eles até assustam algumas pessoas. Elas acham mais fácil fazer as coisas manualmente do que tentar descobrir todas as opções e configurações.

Mas você ficará feliz em saber que você provavelmente não precisará de todas elas! A maioria de tarefas pode ser resolvida usando uma combinação de algumas ferramentas simples.

De quais recursos você está falando?

Jonathas

De quais recursos você está falando?

TripleTen

As seguintes funções e comandos:

Função

-   `SUM`, `SUMIFS`
-   `COUNT`, `COUNTA`, `COUNTIFS`
-   `MAX`, `MIN`, `AVERAGE`
-   `VLOOKUP`, `IFERROR`
-   `AND`, `OR`, `IF`
-   `VALUE`, `LEFT`, `RIGHT`
-   `DAY`, `MONTH`, `YEAR`
-   `UNIQUE`, `COUNTUNIQUE`
-   `IMPORTRANGE`, `QUERY`

Comandos:

-   "Tabela dinâmica" — permite resumir, filtrar e analisar os dados de forma interativa.”
-   "Validação de dados"
-   "AutoCalcular" — permite visualizar rapidamente os resultados de cálculos simples para um intervalo de células selecionado.
-   "Filtragem"
-   "Classificação"
-   "Colar especial"
-   "Localizar e substituir"
-   "Dividir texto em colunas"
-   "Remover duplicatas"
-   "Formatação condicional"
-   "Gráficos de linhas e gráficos de pizza"

É só isso? Mas eu poderia simplesmente pesquisar essas fórmulas no Google, né?

Jonathas

É só isso? Mas eu poderia simplesmente pesquisar essas fórmulas no Google, né?

Há muita informação sobre o Excel — você pode encontrar inúmeros artigos, tutoriais, manuais e vídeos explicativos no YouTube. Mas o problema é que trabalhar com planilhas não é só sobre saber as funções e comandos. É como aprender uma língua estrangeira — apenas conhecer as palavras não é o suficiente, você precisa combiná-las corretamente para formar frases significativas.

O objetivo deste curso é ajudá-lo a dominar o idioma de planilhas te ensinando a formular uma tarefa corretamente e decompô-la em etapas menores e mais simples.

Para isso, você precisará praticar resolvendo problemas de tabelas e se familiarizar com os seguintes conceitos: "tipo de dados" e "formato de dados", "referência absoluta" e "relativa", "intervalo", "critério", "argumento", "condição", "valor", "validação", "tabela dinâmica" e "função".

E isso me ajudará a cumprir todas as minhas tarefas?

Jonathas

E isso me ajudará a cumprir todas as minhas tarefas?

Nem todas, mas a maioria delas. Nós incluímos as ferramentas mais populares em nosso curso, mas não dizemos que abordamos todas elas. Para lidar com tarefas compostas especializadas, você precisará procurar por uma solução online. Ainda assim, depois deste curso, você não vai passar muito tempo pesquisando - você vai saber exatamente o que é preciso procurar e quais palavras-chave você deve usar.

Jonathas

Como é organizado o processo de aprendizagem?

## Como nós ensinamos

Nós não vamos iniciar o curso explicando o que é uma célula ou como adicionar dois valores. Em vez disso, você terá uma planilha com 1000 linhas logo na primeira lição, e nós te ensinaremos a fazer um relatório conciso e abrangente com base nela passo a passo. Desta forma, você não vai ficar cansado de explicações teóricas longas com que começa a maioria de manuais e livros didáticos.

Mas isso não significa que o curso não contém teoria básica. Você terá que estudar teoria, mas não desde o início. Nós iremos apresentá-la a você passo a passo, enquanto você se prepara para resolver cada tarefa grande.

Ao final do Módulo 1, você irá dominar várias maneiras de processar dados e descobrir como funcionam tabelas dinâmicas. Dê uma olhada no [resultado.](https://docs.google.com/spreadsheets/d/1FeCzaKyj7Ezpt6Yij5SFoYql_NdbNQmoUQTa9SqaAWw/edit?usp=sharing)

Ao final do Módulo 2, você aprenderá a "limpar" dados antes de análise, criar novos dados e adicionar dados de outras planilhas ou até de outros arquivos. Dê uma olhada no [resultado.](https://docs.google.com/spreadsheets/d/19oECGDmy7raZ1U0CCJSvT8Z_RdNtGLIE5ykAtlKFU6U/edit?gid=1664037298#gid=1664037298)

Espere, é o Google Planilhas, e não o Excel?

Jonathas

Espere, é o Google Planilhas, e não o Excel?

Nós chamamos este curso "Excel" porque é um nome mais óbvio. Há muitos editores de tabelas, mas eles compartilham os mesmos princípios de operação, e quase todos eles são compatíveis.

Microsoft Excel tem várias versões, e nós não sabemos qual delas você usa (se usar uma). Para livrar você de frustração desnecessária com um botão não estando no lugar onde você o está procurando, nós sugerimos trabalhar no Planilhas Google. Este programa é gratuito, e você pode abri-lo em uma aba de seu navegador. A coisa mais importante é que ele tem a mesma aparência para todo usuário.

Depois de concluir o curso, você não terá nenhum problema em ajustar seu conhecimento para qualquer programa particular que você decidir usar. Você pode usar algum outro programa, mas nós não recomendaríamos fazê-lo - isso dificultará seu processo de aprendizagem.

## Perguntas mais frequentes

Jonathas

A tabela acima contém dados relacionados a clientes, vendas e marketing. E se eu trabalho em outra área?

Todas as planilhas contêm dados sobre um salão de beleza que nós inventamos para o curso. Nós decidimos usar uma narrativa sobre uma startup para conectar todas as tarefas neste curso porque isso é um exemplo que você consegue transpor para diversos problemas do dia-a-dia.

Mas todos os dados são mais ou menos parecidos - eles são números ou texto, ou a combinação de ambos. Nós te ensinaremos a trabalhar com todos os casos, então você poderá aplicar seus novos conhecimentos a praticamente quaisquer dados que você possa encontrar na vida real.

Jonathas

E se o curso for muito difícil para mim?

O curso foi projetado tanto para iniciantes, como para as pessoas que já trabalham com planilhas. Se ficar difícil, você sempre pode consultar dicas ou planilhas com soluções.

No entanto, este curso provavelmente não será atraente para aqueles que se consideram profissionais do Excel.

Jonathas

Quantas horas vai levar para concluir o curso?

Em média, concluir este curso com todas suas tarefas demora 10 horas. Você pode estudar pouco a pouco. Dedicar 20–30 minutos por dia a estudos é ideal. Desta forma, em um mês, você já vai sentir confortável em trabalhar com planilhas.

Jonathas

Eu posso usar meu smartphone ou tablet para estudar?

Trabalhar com planilhas em smartphones ou tablets é muito difícil. Acredite, nós tentamos.

Se você está usando seu smartphone no momento, você pode ter notado que as tabelas são muito pequenas e sua visualização não é conveniente.

É por isso que nós recomendamos usar um computador ou um laptop. Será mais confortável completar tarefas no Planilhas Google.

Jonathas

Quem eu devo contatar se tiver uma pergunta?

Nós tentamos fazer todas as explicações tão claras como possível fornecendo demonstrações de vídeo e tabelas para autoavaliação.

Mas se você tiver qualquer pergunta, não hesite em entrar em contato com o Suporte do TripleTen. Ou você pode esperar pelo início das aulas e perguntar para seu tutor. Nós faremos nosso melhor para te ajudar.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-30-36-928Z.md
### Última modificação: 2025-05-28 18:30:37

# Mil linhas e um relatório - TripleTen

Capítulo 1/3

Módulo 1. Análise de Dados

Opcional

# Mil linhas e um relatório

Esta lição deve levar 5-7 minutos para ser concluída

Imagine que você e suas amigas decidiram abrir um salão de beleza para pessoas de cabelos cacheados. Seu negócio está na fase inicial de desenvolvimento. Você resolve todos os problemas por conta própria: não tem nem um contador, nem um analista, nem um especialista de marketing.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT_curly_excel_1_1663267258.png)

TripleTen

Neste caso, você vai trabalhar em tarefas diferentes e experimentar a maioria das ferramentas comuns do Excel.

Seu primeiro desafio é preparar um relatório mensal. Você precisa responder às seguintes perguntas:

-   Quanto de renda foi feita e em quê?
-   Qual é o total das despesas?
-   Qual categoria de despesa é a mais cara?

Você só tem uma planilha na qual sua amiga mais meticulosa registrou toda a renda e todas as despesas. Ela confessou que algumas vezes estava com pressa quando preencheu a planilha, então pode haver alguns erros de digitação. Você vai encontrá-los e corrigi-los processando os dados.

### Preparação

Crie [sua própria cópia](https://docs.google.com/spreadsheets/d/1ta5Oy8qST_EEsFKZjFJuuRvq7l-_mARRbGqwgH55iuw/copy) da planilha com renda e despesas.

O que fazer se o link não funcionar

Se você vir esta janela:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/google_prt_1.2.1_1663592782.png)

Isso significa que você ainda não tem uma conta do Google ou que você não fez login.

Se não tiver uma conta, crie uma. Caso contrário, você não vai poder trabalhar no Google Sheets e concluir as tarefas do curso. Se você já tem uma conta, insira suas informações de login.

Às vezes, pode aparecer esta janel

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.2.2_prt_1663612560.png)

Ela significa que você não tem espaço livre no seu Google Drive. Depois de você remover alguns arquivos desnecessários, tudo deve funcionar bem.

Pergunta

**Para qual período você tem os dados?**

o fim de junho e o mês inteiro de julho de 2021

junho e o início de julho de 2021

A tabela contém os registros de lucro e despesas para o mês inteiro de junho e os primeiros quatro dias de julho. Parece que o novo mês está apenas começando, então a tabela será atualizada.

os meses inteiros de junho e julho de 2021

Seu entendimento sobre o material é impressionante!

Sua amiga está pedindo que você a ajude a processar os dados e elaborar um relatório parecido com este:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.2.3_prt_1663593155.jpeg)

Ela diz que isto normalmente leva cerca de meia hora.

Como ela faz o relatório?

Jonathas

Como ela faz o relatório?

E fácil.

1.  Ela seleciona os dados necessários usando filtros.
2.  Depois, ela copia a coluna "Total" para uma planilha separada.
3.  E então ela soma os valores copiados usando a função `SUM`.

Todo o processo será assim:

Esses passos devem ser repetidos para cada categoria: 36 vezes.

Parece cansativo. Talvez haja uma maneira mais rápida de fazer isso?

Jonathas

Parece cansativo. Talvez haja uma maneira mais rápida de fazer isso?

Há. Sua amiga encontrou uma recentemente. Ela encontrou o painel de calculadora automática (Autocalculate ou AutoSoma) no canto inferior esquerdo. Ele exibe a soma de todas as células selecionadas. Então você pode apenas ajustar os filtros e se livrar de cópias e funções desnecessárias:

![](https://practicum-content.s3.amazonaws.com/resources/1.2.4PT_1685640418.png)

E talvez haja um jeito ainda mais rápido?

Jonathas

E talvez haja um jeito ainda mais rápido?

Há. Mas a sua amiga ainda não o descobriu. Você precisa aprendê-lo e depois ensinar para ela.

### 🛠️ Tarefa 1

1.  Dê dois cliques em uma célula em branco, insira a fórmula `=SUMIFS(L2:L1034;C2:C1034;"Renda";G2:G1034;6)`e depois dê enter. Você verá o resultado.
2.  Responda à pergunta.

Pergunta

**Qual número você obteve?**

47 253,25

Para copiar a fórmula, selecione-a e pressione em `Ctrl`+`C`. Abra a planilha, clique em qualquer célula em branco e pressione `Ctrl`+`V`. Se você usa MacOS, use `Cmd ⌘`+`C` e `Cmd ⌘`+`V`.

27 280

45 722,92

5 916

Fantástico!

Você acabou de usar a função `SUMIFS`, que executa todos os passos descritos no vídeo acima. Ela seleciona apenas os valores necessários e soma-os. Neste caso, ela somou os valores na coluna "Total" que foram registrados em junho.

Esse jeito é realmente mais rápido?

Jonathas

Esse jeito é realmente mais rápido?

Para vários cálculos repetitivos, essa fórmula é realmente mais rápida. Você pode fazer o seguinte:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Excel_1sprint_4/Illustration/PT/1.2.2_pt.gif)

E se sua planilha contiver dados para 6 meses ou mais, você também pode fazer assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Excel_1sprint_4/Illustration/PT/1.2.3_pt.gif)

TripleTen

Parece mágica, não é? Nós sabemos! Logo você aprenderá todos esses truques.

Nas próximas três lições, você vai usar a função `SUMIFS` para fazer o seguinte:

-   dominar as regras básicas de construção de fórmulas;
-   usar intervalos abertos, links para outras planilhas ativas e referências de células absolutas e relativas em fórmulas;
-   validar cálculos e encontrar e corrigir erros em dados de origem.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-30-38-235Z.md
### Última modificação: 2025-05-28 18:30:38

# Lição Bônus - TripleTen

Capítulo 1/3

Módulo 1. Análise de Dados

Opcional

# Lição Bônus

Se você já sabe como usar filtros, ordenação e a opção AutoCalculate (ou AutoSoma em português), pode avançar para a próxima lição. Se não, certifique-se de estudar essas ferramentas básicas. Elas serão bem úteis muito em breve.

💡 Você vai continuar usando a cópia da planilha com dados de renda e despesas que você já salvou no seu Drive.

Se você já fechou a tabela, pode encontrá-la na aba [Recentes](https://drive.google.com/drive/recent) no Google Drive ou digitando "Módulo 1. Lições 1–3" na [barra de pesquisa](https://drive.google.com/drive/my-drive).

Filtros, ordenação e AutoCalculate

### Filtros

Vamos considerar que toda planilha com mais de 20 linhas é grande. Ao trabalhar com planilhas grandes, você precisa configurar filtros o mais rápido possível.

Por que precisamos de filtros? Você precisa de filtros para selecionar apenas os valores que precisa no momento. Por exemplo, você pode visualizar toda a renda com vendas de produtos em junho usando filtros:

### Ordenação

A ordenação ajuda a organizar os dados em uma ordem específica: valores de texto podem ser organizados em ordem alfabética (`A → Z` ou `Z → A`), e números podem ser organizados em ordem crescente (menor para o maior) ou decrescente (maior para o menor).

### AutoCalculate

A opção AutoCalculate (calculadora automática) é a maneira mais fácil e rápida de calcular algo.

Ao simplesmente selecionar um intervalo ou uma coluna, você recebe o resultado do cálculo no canto inferior direito:

Soma do intervalo

![Soma do intervalo](https://practicum-content.s3.amazonaws.com/resources/1.3.1_1709295632.png)

![Soma da coluna](https://practicum-content.s3.amazonaws.com/resources/1.3.2_1709295663.png)

Clicar na área de AutoCalculate exibe mais opções de cálculo para o intervalo selecionado:

-   Média — a média de todos os valores no intervalo.
-   Max e Min — os valores máximo e mínimo no intervalo.
-   Contagem — o número de células preenchidas no intervalo.
-   Contagem de números — o número de células preenchidas com números (e não com texto).

### 🛠️ Tarefa

1.  Crie um filtro para cada coluna.
    
2.  Filtre os dados para visualizar apenas a renda (coluna “Operação”) juntamente com serviços (coluna “Tipo”) para junho (coluna “Mês”).
    
3.  Use a função AutoCalculate para descobrir qual a renda que foi obtida por seu salão com serviços em junho.
    
4.  Insira o número resultante abaixo e clique em "Verificar".
    

Você vai encontrar um dos erros de digitação sobre os quais sua amiga avisou executando essa tarefa. Na coluna "Tipo", há dois valores parecidos: `Serviço` e `Sevriço`. Certifique-se de selecionar ambos. Nas próximas lições, vamos ensinar a localizar rapidamente e corrigir esse tipo de erros.

Pergunta

**Qual é o número resultante? Digite sua resposta sem espaços.**

45723

Isso mesmo! Filtros corretos permitem obter uma resposta em apenas um par de cliques.

Fantástico!

**Vídeo para autoavaliação:**

TripleTen

Ótimo! Agora vamos para os capítulos principais.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-30-45-893Z.md
### Última modificação: 2025-05-28 18:30:46

# SUMIF: calcular uma soma com base em critérios - TripleTen

Capítulo 1/3

Módulo 1. Análise de Dados

Opcional

# SUMIF: calcular uma soma com base em critérios

💡 Abra a cópia da [planilha de receitas e despesas](https://docs.google.com/spreadsheets/d/1ta5Oy8qST_EEsFKZjFJuuRvq7l-_mARRbGqwgH55iuw/edit?usp=sharing) quevocê fez em seu computador. Se você já fechou a tabela, pode encontrá-la na aba [Recentes](https://drive.google.com/drive/recent) no [Google Drive](https://drive.google.com/drive/my-drive) no Google Drive ou digitando "Módulo 1.Lições 1–3" na barra de pesquisa".

Vamos!

Jonathas

Vamos!

Você vai aprender a usar a fórmula `SUMIFS` para resumir dados da planilha do salão para cabelos cacheados.

Essa é a primeira fórmula do curso, então vamos descrever como ela funciona em detalhes. Ao aprender `SUMIFS`, você também vai aprender algumas regras gerais que se aplicam a todas as fórmulas no Google Sheets e no Excel.

💡 No Google Sheets, você pode escrever fórmulas em letras latinas e alfabetos locais. Os layouts de teclados latinos são mais convenientes porque permitem digitar caracteres especiais mais rapidamente: aspas (devemos sempre usar aspas retas "…", e não tipográficas “…”), sinais matemáticos como =, <, >, o sinal $, etc.

Neste curso, vamos usar nomes de fórmulas em inglês, pois são mais curtos e mais rápidos de digitar. A função vai funcionar mesmo que o idioma esteja configurado como português.

A situação é um pouco mais complicada com o Excel. Nas versões localizadas do programa, as fórmulas só podem ser digitadas na língua do programa.

Como ativar o inglês para fórmulas no Google Sheets:

1.  Clique em `Arquivo` → `Configurações`;
2.  Clique na caixa de seleção ao lado de "Sempre usar os nomes das funções em inglês".

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/2_1664787147.png)

### O Conto do Pastor

Imagine que a fórmula `SUMIFS` é um pastor cuidando de um rebanho de diferentes alpacas: uma alpaca tem um sino pendurado no pescoço, outra tem um laço, e a terceira tem um lindo chapéu e uma tem tudo de uma vez só.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT_1.1.1_alpaca_1663267330.png)

O pastor se chama `SUMIFS`, em homenagem aos seus dois talentos.

"SUM" — que significa "soma", porque pode somar rapidamente diferentes números, como a idade ou o peso das alpacas. E seu segundo talento é memorizar literalmente tudo: o nome, idade, peso e acessórios favoritos de cada alpaca.

Mais importante ainda, o pastor sabe como usar ambos os seus talentos ao mesmo tempo. Você pode pedir que ele calcule somas muito específicas: por exemplo, a idade total de todas as alpacas com laços.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT_1.1.2_alpaca_1663267353.png)

Essa é a tarefa mais simples que o `SUMIFS` faz. Ele nos permite calcular uma soma com base em vários critérios.

Pergunta

**Vamos tentar ver como calcularíamos a mesma coisa sem `SUMIFS`. Calcule o peso total de todas as alpacas que usam laços e chapéus e têm mais de dois anos.**

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT_1.1.3_alpaca_1663267403.png)

35,1 kg

60,1 kg

Apenas duas alpacas atendem aos três critérios. Olhe para a foto abaixo.

95,2 kg

Muito bem!

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT_1.1.4_alpaca_1663267442.png)

_A propósito, uma alpaca real pesa cerca de 70 quilos._

Quando nosso conjunto de dados contém apenas informações sobre seis alpacas, é possível somar entradas por critérios manualmente. No entanto, se houver muitos dados, é melhor confiar os cálculos ao `SUMIFS`.

Como essa fórmula funciona?

Jonathas

Como essa fórmula funciona?

### Como funciona a fórmula

O objetivo principal da fórmula `SUMIFS` é somar valores. Ela também permite escolher valores seletivamente com base em critérios, e você pode usar quantos critérios desejar.

Veja como a fórmula é construída:

```
=SUMIFS(sum_range; criterion_range1; criterion1; criterion_range2; criterion 2; ...)
```

Se voltarmos ao nosso exemplo do rebanho de alpacas, primeiro precisamos apresentar os dados em uma tabela para aplicar a fórmula:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.4.2_prt_1663600563.png)

Agora estamos com tudo pronto para usar a fórmula:

💡 As fórmulas sempre começam com o sinal de igual. Dessa forma, o programa entende que você vai calcular, e não inserir novos dados na célula.

```
=SUMIFS(
```

Após o parêntesis de abertura, você indica primeiro o intervalo de soma, ou seja, as células que contêm valores a serem adicionados.

Por exemplo, o intervalo para somar as idades das alpacas é `B2:B7`,

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.4.3_prt_1663600584.png)

e se quiséssemos somar o peso em vez da idade, escreveríamos:

```
=SUMIFS(C2:C7;
```

Em seguida, precisamos decidir de quais alpacas queremos somar o peso. Por exemplo, estamos interessados em alpacas que atendam a dois critérios: que tenham sino e dois anos de idade.

Para cada critério na fórmula, especifique:

1.  Intervalo
2.  Valor desejado

O intervalo do critério "tem um sino" é `D2:D7`. O valor desejado é `"sim"`.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.4.4_prt_1663600617.png)

Veja como fica a fórmula:

```
=SUMIFS(C2:C7;D2:D7;"sim"
```

De maneira semelhante, especifique o restante dos pares "intervalo - valor do critério", separando-os com ponto e vírgula. No nosso caso, precisamos de mais um critério para escolher alpacas que também tenham dois anos.

```
=SUMIFS(C2:C7;D2:D7;"sim";B2:B7;2)
```

Se você copiar esta fórmula para uma célula, obterá zero como resultado. Isso acontece porque nenhuma das alpacas tem exatamente 2 anos!

💡 Observe que os valores de texto são escritos entre aspas e os valores numéricos não. Esta é uma regra universal que se aplica a todas as fórmulas.

Podemos “traduzir” a fórmula =`SUMIFS(C2:C7;D2:D7;"sim";B2:B7;2)` para a linguagem humana lendo-a de trás para a frente:

1.  Se houver alpacas com dois anos (`2`) no intervalo `B2:B7`,
2.  e se essas mesmas alpacas tiverem um sino (`"sim"`) no intervalo `D2:D7`,
3.  some o peso dessas alpacas do intervalo `C2:C7`.

Trabalhar com as alpacas facilitou um pouco. Vamos voltar para o salão para cabelos cacheados.

Jonathas

Trabalhar com as alpacas facilitou um pouco. Vamos voltar para o salão para cabelos cacheados.

Pergunta

**Na lição anterior, você usou a fórmula**

**`=SUMIFS(L2:L1034;C2:C1034;"Renda";G2:G1034;6)`.**

**Quais valores de coluna serão somados por essa fórmula?**

Coluna `L`

O intervalo de soma é `L2:L1034`. Isso significa que a fórmula vai adicionar os valores de 1.033 linhas da coluna `L`.

Coluna `C`

Da coluna `G`

Você conseguiu!

Pergunta

**`=SUMIFS(L2:L1034;C2:C1034;"Renda";G2:G1034;6)`**

**Quantos critérios tem esta fórmula?**

1

2

O primeiro intervalo de critério é `C2:C1034;`, com o valor desejado de `"Renda"`. O intervalo do segundo critério é `G2:G1034`, com o valor desejado de `6`.

3

4

Fantástico!

É hora de voltar para a planilha do salão para cabelos cacheados e aplicar a fórmula `SUMIFS` para calcular rendas e despesas.

### 🛠️ Tarefa

1.  Abra a planilha e localize a célula `Q1`. Ela contém um modelo para um resumo de relatório.
2.  Você já conhece a fórmula de cálculo da renda de junho. Cole-a na célula apropriada no relatório.
3.  Usando a fórmula `SUMIFS`, calcule as despesas do salão para junho, bem como quanto o salão rendeu com serviços e quanto rendeu com a venda de produtos.
4.  Inserimos as fórmulas de julho no relatório para referência. Se você estiver com problemas para fazer o cálculo de junho, consulte essas fórmulas e tente recriá-las.

❗ Se você sentir vontade de simplesmente copiar as fórmulas e alterar o número 7 para 6, aconselhamos que você pare. Isso vai impedir que você aprenda a usar a ferramenta.

Pergunta

**Qual é a receita total da venda de mercadorias em junho?**

**Escreva a resposta como um número inteiro.**

1530

Sim! Se você calculou esse parâmetro corretamente, não deve ter problemas com outros parâmetros. Por precaução, verifique por conta própria olhando a [planilha de solução](https://docs.google.com/spreadsheets/d/1SCusspFbd_g6gInJ-gI-gMne6IcJyzKPXzEpjEz4CQI/edit?usp=sharing).

Excelente!

Pergunta

**Agora use o painel AutoSoma para verificar se há algum erro nos cálculos.**

💡 A autoverificação é uma espécie de etiqueta do analista. Um erro de cálculo pode levar a salários incorretos, despesas subestimadas e outros erros tangíveis no trabalho.

**No relatório resultante, selecione duas células: receita de serviços e receita de venda de mercadorias. Verifique o valor no painel AutoSoma e veja se ele corresponde à soma dos rendimentos totais de cada mês. O que você pode concluir?**

Ambas as somas coincidem.

A soma de junho corresponde, mas a soma de julho **não**.

A soma de junho **não** corresponde, mas a soma de julho sim.

Aqui estão os resultados corretos: a soma da receita total de junho calculada pela fórmula é de $ 47.253, e a soma calculada usando o painel AutoSoma é de $ 47.034. Perdemos $ 219 em algum lugar. Para julho, a soma corresponde – são US$ 5.916.

Ambas as somas **não** coincidem.

Fantástico!

Aparentemente, há um ou mais erros ocultos entre 1.034 linhas dos dados de origem, então acabamos perdendo $ 208. Isso acontece quando as tabelas são preenchidas manualmente. Sua amiga avisou sobre isso.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT_curly_excel_2_2_1_1663267561.png)

Na próxima lição você vai aprender:

-   Como encontrar e corrigir rapidamente erros nos dados de origem
-   Como minimizar a probabilidade de erros em tabelas preenchidas manualmente

### Resumo

1.  A fórmula `SUMIFS` adiciona valores "seletivamente". Ou seja, soma apenas os valores que atendem às condições dadas. Pode haver qualquer número de condições.

2.  A sintaxe da fórmula: `=SUMIFS(intervalo_soma; criterio_intervalo1; criterio1; ...)`.
3.  Se o valor do critério for um número, você não vai precisar de aspas. Se for texto, certifique-se de usar as aspas duplas ("...").
    
4.  Você pode verificar convenientemente os cálculos feitos de acordo com as fórmulas usando o painel AutoCalculate.
    

![](https://practicum-content.s3.us-west-1.amazonaws.com/common/moved_example.svg)

2 — 2

**Caso queira fazer o curso completo**

Os cursos online gratuitos têm uma desvantagem significativa: as pessoas não os concluem.

Em 2012-2016, a Universidade de Harvard e o Instituto de Tecnologia de Massachusetts (MIT) realizaram um [grande estudo](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2889436) sobre as taxas de conclusão de cursos online gratuitos. A amostra incluiu 290 cursos e 4,5 milhões de estudantes. Os resultados foram bastante desanimadores: em média, apenas 7,7% dos alunos concluíram os cursos online.

Nos últimos cinco anos, a situação agravou-se ainda mais: [estudos recentes](https://www.classcentral.com/report/the-second-year-of-the-mooc/) mostram uma taxa média de conclusão de 3%.

Isso acontece em grande parte porque uma “cultura” de fazer cursos online ainda não se desenvolveu. Fazer um curso online é mais difícil do que ler artigos na internet, assistir a vídeos educacionais no YouTube ou ouvir podcasts educacionais. Além de motivação e interesse, você precisa de auto-organização e, acima de tudo, da capacidade de encontrar tempo para encaixar o estudo na sua agenda.

No TripleTen, queremos muito que nossos alunos terminem com sucesso os cursos online. Por isso, coletamos estatísticas e apresentamos duas recomendações simples para um aprendizado bem-sucedido:

1.  **Evite fazer longas pausas entre os estudos**. Dar o primeiro passo é tão importante quanto criar o hábito de estudar regularmente. Se você adiar sua próxima aula por mais de uma semana, a chance de continuar estudando diminui em 14%. É como fazer exercícios na academia: depois de uma longa pausa, é mais fácil parar do que ter dores musculares novamente.
2.  **Mantenha um ritmo constante**. Entre os estudantes que concluíram o curso, 34% terminaram nos dois primeiros dias após a matrícula e outros 32% nos primeiros sete dias após a matrícula. Mas não estamos dizendo para você mergulhar em seus estudos e ignorar o resto da sua vida. Muitos estudantes têm um emprego e muitas outras preocupações. É perfeitamente normal. A melhor estratégia é concluir algumas lições nos próximos dias e terminar o primeiro módulo no fim de semana.

TripleTen

Lembre-se: aprender coisas novas é sempre difícil.

Mas nós acreditamos no seu potencial!

Se você quer continuar estudando agora, não feche a tabela. Você vai precisar dela na próxima lição.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-30-47-235Z.md
### Última modificação: 2025-05-28 18:30:47

# Relatório avançado - TripleTen

Capítulo 1/3

Módulo 1. Análise de Dados

Opcional

# Relatório avançado

💡 Agora, você vai trabalhar com a [planilha de receitas e despesas](https://docs.google.com/spreadsheets/d/1jI36bhM4_04H0B06634B_A9nJoME-XhVnvf7D5Yrbi0/edit?usp=sharing).

Se você já fechou a tabela, pode encontrá-la na aba [Recentes](https://drive.google.com/drive/recent) no [Google Drive](https://drive.google.com/drive/my-drive) ou digitando "Módulo 1.Lições 5" na barra de pesquisa.

Nesta lição, você vai trabalhar com o relatório financeiro do salão para cabelos cacheados. Vamos ver como o relatório final deve ficar:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.2.3_prt_1663600699.jpeg)

Não é conveniente armazenar um grande relatório como este na mesma planilha com os dados de origem. Faz mais sentido separar os dados de origem e o relatório em planilhas diferentes.

💡 O arquivo com tabelas é geralmente chamado de pasta de trabalho, e as abas dentro da pasta são chamadas de _planilhas_.

Por padrão, cada planilha é simplesmente chamada de "Folha" seguida de seu número: Folha1, Folha2, Folha3, etc. Aconselhamos dar nomes autoexplicativos às suas planilhas assim que as criar. Por exemplo, um bom nome para uma planilha com dados de origem seria Export ou Dados.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Excel_1/pt/1gif.gif?etag=6061266bcb739d0f8761aeb8f4df4047)

Quando você faz um relatório em uma nova planilha, surge naturalmente a pergunta: como a tabela “vai entender” em qual planilha procurar os dados de origem?

Existem várias regras aqui:

-   Se os dados e a fórmula estiverem na mesma planilha, basta especificar o endereço da célula com sua coluna e linha, por exemplo `А1`. A letra indica a coluna, o número indica o número da linha.
-   Se a fórmula estiver em uma planilha e os dados de origem estiverem em outra, você vai precisar adicionar o endereço da planilha ao endereço da célula (por exemplo, `А1`). Por exemplo, se você precisar acessar a célula `А1` localizada na planilha Export, o endereço completo será: `Export!А1`. O mesmo se aplica aos intervalos: `Export!А1:А100`.
-   Para referenciar uma planilha com um nome contendo caracteres latinos, você pode usar aspas simples para envolver o nome da aba. Isso é necessário se o nome da aba contiver espaços, caracteres especiais ou acentos, por exemplo `'lição'!`.

### 🛠️ Tarefa 2

1.  Abra sua cópia da planilha de rendas e despesas. Ela tem uma planilha oculta: `Report`. Para abri-la, clique no quadrado listrado no canto inferior esquerdo da tela. Dica: ao passar o cursor sobre o quadrado que mencionamos, você verá o texto: “Todas as páginas".
2.  Existe um modelo de relatório na planilha `Report` que já contém algumas fórmulas. Elas estão todas corretas, mas falta um pequeno detalhe: não foi levado em consideração o fato de os dados para cálculos estarem em outra planilha.
    
3.  Corrija as fórmulas para que funcionem.
4.  Você não precisa escrever nenhuma outra fórmula ainda.

Pergunta

**Qual deve ser a fórmula corrigida para calcular a renda de junho?**

`=SUMIFS(Report!L:L;Report!C:C;"Renda";Report!G:G;6)`

`=SUMIFS('Export!'L:L;'Export!'C:C;"Renda";'Export!'G:G;6)`

`=SUMIFS(Export!L:L;Export!C:C;"Renda";Export!G:G;6)`

Os erros aparecem nas pequenas coisas. Observe a fórmula correta novamente e compare-a com a que você escreveu na tarefa.

`=SUMIFS('Export'!L:L;'Export'!C:C;"Renda";'Export'!G:G;6)`

Muito bem!

💡 As fórmulas que você acabou de editar usam intervalos abertos: `L:L`, `A:A`, `C:C`, `G:G`. Eles se referem a todas as células nas colunas `L`, `A`, `C`, `G`, respectivamente.

Certifique-se de usar intervalos abertos em suas fórmulas se você planeja adicionar novos dados à tabela original.

### Como detalhar o relatório e corrigir erros nos dados de origem

### 🛠️ Tarefa 2

1.  Vamos escrever uma nova fórmula na célula `A1` na planilha Report. Vamos usar a fórmula `UNIQUE` aplicada ao intervalo `Export!E:E`.
2.  Após fazer isso, confira o resultado e responda à pergunta abaixo.

Pergunta

**O que a fórmula `UNIQUE` faz?**

Conta o número de valores únicos no intervalo `Export!E:E`.

Retorna todos os valores exclusivos no intervalo `Export!E:E`.

A fórmula `UNIQUE` retorna todos os nomes das categorias de rendas e despesas. Com o cabeçalho, são 17 valores.

Conta o número de linhas não vazias no intervalo `Export!E:E`.

Retorna todos os valores numéricos exclusivos no intervalo `Export!E:E`.

Trabalho maravilhoso!

A fórmula `UNIQUE` analisa um intervalo com valores duplicados e retorna apenas seus valores únicos. Isso é conveniente: você não precisa escrever manualmente todas as categorias de rendas e despesas — você pode só copiá-las no relatório.

A desvantagem dessa fórmula é que ela não divide valores únicos relativos a despesas e rendas. Isso ainda terá que ser feito manualmente. Use atalhos para acelerar o processo:

-   Mantenha pressionada a tecla `Ctrl` e clique em todos os valores que deseja copiar.
-   Após selecionar todos os valores, pressione `Ctrl`+`C` para copiá-los para a área de transferência.
-   Mova o cursor para onde você deseja colar os valores copiados e pressione `Ctrl`+`V` — os valores da área de transferência serão colados ali mesmo.

💡 Se você usar o macOS, use `Cmd ⌘` em vez de `Ctrl`. Daqui em diante, vamos fornecer os atalhos de teclado para macOS entre parênteses.

### 🛠️ Tarefa 3

Use atalhos para adicionar todas as categorias de rendas e despesas ao relatório.

Feito

Jonathas

Feito

Veja o resultado da aplicação da fórmula `UNIQUE` novamente. Existem dois valores semelhantes na lista de valores únicos: “Corte de Cabelo Padrão” e “Corde de Cabelo Padrão”.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.5.2_pt_1663687980.png)

Parece que o administrador do salão cometeu um erro de digitação ao preencher a planilha Export. A receita da categoria “Corte de Cabelo Padrão” em seu relatório será menor que o valor real porque a fórmula não incluirá as linhas com o erro de digitação. Além disso, não sabemos quantas vezes esse erro de digitação é repetido, o que pode resultar em uma séria discrepância.

A última lição tinha o mesmo problema: a soma das receitas totais de junho foi menor que a AutoSoma das rendas de serviços e bens. A discrepância seria substancial para nosso pequeno salão para cabelos cacheados: $208.

Mas não se preocupe! Agora vamos aprender como encontrar e corrigir rapidamente imprecisões como essa nos dados de origem.

Existem duas maneiras de corrigir os dados iniciais: a maneira lenta e a maneira rápida.

**Forma lenta**

1.  bra a planilha Export e pressione `Ctrl`+`F` (`⌘`+`F`) para abrir o menu de pesquisa.
    
2.  Quando ele estiver aberto, insira o valor desejado (“Corde de Cabelo Padrão”).
    
3.  O valor desejado será destacado na tabela, e o menu mostrará quantas vezes ele é mencionado na planilha.
    
4.  Substitua manualmente todos os valores encontrados de “Corde de Cabelo Padrão” por “Corte de Cabelo Padrão” um por um.
    

No entanto, a forma rápida pode encontrar e substituir todos os erros de digitação de uma só vez. Existe uma ferramenta chamada `Localizar e substituir` que faz exatamente isso.

**Forma rápida**

1.  Clique em `Editar` → `Localizar e substituir` no menu ou use o atalho `Ctrl`+`H` (`⌘`+`Shift`+`H`).
    
2.  Quando o menu aparecer, digite “Corde de cabelo padrão” no campo Localizar e “Corte de cabelo padrão” no campo Substituir.
    
3.  Certifique-se de selecionar apenas "Esta planilha" no campo Pesquisar, para não substituir nada de outras planilhas.
    
4.  Clique em `Substituir tudo` e depois em `Concluído`
    

### 🛠️ Tarefa 4

1.  Usando Localizar e substituir, corrija o erro de digitação “Corde de Cabelo Padrão” nos dados de origem.
    
2.  Abra a planilha oculta List. Para abri-la, clique no quadrado listrado no canto inferior esquerdo da tela. Dica: ao passar o cursor sobre o quadrado que mencionamos, você verá o texto: “Todas as páginas".
    
3.  Na planilha List, aplique a fórmula `UNIQUE` às colunas indicadas para encontrar mais um erro de digitação nos dados de origem.
    
4.  Ao encontrar o erro de digitação, corrija-o nos dados de origem usando Localizar e substituir.
    

Pergunta

**Qual coluna continha o erro de digitação que causou a discrepância de renda na lição anterior?**

Tipo

Sim, a planilha tem o valor "Sevriço" ao invés de "Serviço". Use o AutoSoma para se certificar de que todas as rendas foram calculadas corretamente após a substituição.

Operação

Opção de pagamento

Mês

Excelente!

Vídeo para autoavaliação

## Listas suspensas

O relatório parece totalmente correto agora. No entanto, é provável que esses tipos de erros vão ocorrer de novo no futuro, e verificá-los manualmente toda vez se tornaria muito tedioso.

O recurso de lista suspensa vai simplificar sua vida e do administrador do salão que trabalha com a planilha Export todos os dias.

Essas listas são assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Excel/Module_1/pt/M1-L5_pt2.gif)

### 🛠️ Tarefa 5

1.  Abra a planilha Export e selecione a coluna `A`. Para selecionar a coluna inteira, basta clicar na parte superior da planilha, onde fica a letra `A`.
    
2.  Clique em `Dados` → `Validação de Dados`.
    
3.  No menu à direita, clique em “Adicionar uma regra” e altere o intervalo de `A1:A1040` para `A2:A` no campo “Aplicar a intervalo”. Isso é necessário para que a lista suspensa não capture o cabeçalho da coluna e para que as listas funcionem na coluna inteira, não apenas nas primeiras 1040 linhas.
    
4.  No campo "Critérios", selecione "Menu suspenso (de um intervalo)". Em seguida, escreva o intervalo `List!A3:A4` manualmente ou clique no ícone da tabela e clique no intervalo desejado na planilha List.
    
5.  Clique em `Concluído`.
    
6.  Configure as listas suspensas nas colunas "Operação", "Opção de pagamento" e "Categoria".
    

Verificar

Jonathas

Verificar

Vídeo para autoavaliação

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT_curly_excel_3_1_1663267621.png)

## Valores absolutos e variáveis

Os erros foram corrigidos e a possibilidade de sua recorrência foi eliminada. Agora podemos nos concentrar em terminar o relatório.

Na etapa anterior, você adicionou categorias de renda e despesa ao relatório. Agora, tudo o que resta é usar a fórmula `SUMIFS` para calcular a soma de cada categoria.

Podemos calcular quanto o salão ganhou em cortes de cabelo padrão em junho usando esta fórmula:

```
=SUMIFS(Export!L:L;Export!E:E;"Corte de Cabelo Padrão";Export!G:G;6)
```

Isso pode ser melhorado substituindo valores absolutos por variáveis.

💡 Vamos comparar:

Valor absoluto: encontre as linhas com o valor "Corte de Cabelo Padrão".

Variável: localize as linhas com o valor contido na célula `B11`.

No primeiro caso, o programa vai procurar imediatamente as linhas que contenham o valor "Corte de Cabelo Padrão". No segundo, ele vai olhar para a célula `B11` → vai encontrar o valor “Corte de Cabelo Padrão” lá → vai procurar por linhas com o valor “Corte de Cabelo Padrão”.

Por que deixar as coisas mais complicadas para o programa? Há uma boa razão:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Excel_1sprint_4/Illustration/PT/1.2.2_pt.gif)

Porque facilita as coisas para nós. Agora você pode apenas escrever a fórmula uma vez e arrastá-la para baixo — e a planilha vai calcular tudo sozinha!

A fórmula original “generalizada” ficou assim:

```
=SUMIFS(Export!L:L;Export!E:E;B11;Export!G:G;6)
```

💡 Observe que o valor `B11` é escrito sem aspas. As referências de células, como números, não exigem aspas.

### 🛠️ Tarefa 6

Preencha os campos restantes do relatório.

Verificar

Jonathas

Verificar

> [Planilha para autoavaliação](https://docs.google.com/spreadsheets/d/1meEn5zPj4HLy9pfwsLni9nyMi0Ww4DDPnBwsp4u9aS4/edit?usp=sharing)

Pergunta

**Os atalhos permitem que você trabalhe mais rápido com planilhas. Aconselhamos utilizá-los com a maior frequência possível. Quanto mais você os usar, mais fácil será lembrá-los.**

**Falamos sobre cinco atalhos nesta lição. Combine os atalhos de teclado com as ações que eles realizam.**

Se você usar o macOS, leia `Ctrl` como `⌘`.

Colar

Ctrl+V

Sim, esse é um clássico. Esse atalho funciona na maioria dos programas, não apenas no Google Sheets e no Excel.

Encontrar

Ctrl+F

Fácil de lembrar: F é a abreviação de "encontrar" (find).

Encontrar e substituir (Find and Replace)

Ctrl+H

`Ctrl`+`H` abre o menu Find and Replace. Isso funciona não apenas no Google Sheets e no Excel, mas também em editores de texto.

Copiar

Ctrl+C

Na verdade, é um atalho universal — funciona na maioria dos programas.

Selecionar

Ctrl + clique esquerdo do mouse

Use `Ctrl` sempre que precisar selecionar vários valores não consecutivos.

Fantástico!

### Resumo

1.  Mantenha os dados de origem em uma planilha e os cálculos em outra. Para que a fórmula encontre os dados de origem, você precisa adicionar o endereço da planilha ao endereço da célula (ou intervalo) assim: `Export!`, `'Export Data'!` ou `'Data'!`.
2.  Se você planejar atualizar os dados de origem, use intervalos abertos em suas fórmulas, por exemplo: `A:A`.
3.  Use a fórmula `UNIQUE` para obter uma lista de valores únicos de um intervalo com valores duplicados. Também é útil para encontrar erros de digitação em planilhas com dados homogêneos.
4.  A função de `Data Validation` permite inserir dados rapidamente, criando drop-down lists. A função `Find and Replace` permite editar dados rapidamente.

![](https://practicum-content.s3.us-west-1.amazonaws.com/common/moved_example.svg)

2 — 2

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-30-48-545Z.md
### Última modificação: 2025-05-28 18:30:48

# Como otimizar fórmulas do Excel - TripleTen

Capítulo 1/3

Módulo 1. Análise de Dados

Opcional

# Como otimizar fórmulas do Excel

Nas lições anteriores, você criou um relatório para dois meses. Você escreveu todas as fórmulas necessárias para o primeiro mês. Se você tentar aplicá-las no próximo mês, vai obter o seguinte:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Excel/Module_1/pt/1.6.1.gif)

Mas aqui está o que você deveria obter:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.6.1_1663678822.png)

Ao final desta lição, você não apenas vai saber a resposta para essa pergunta, mas também como fazer isso:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Excel/Module_1/pt/moved_1.6.2n.gif)

## Noções básicas de células

Vejamos este exemplo para entender a lógica de arrastar ou copiar células no Google Sheets.

Vamos escrever uma fórmula simples `=A1` na célula `E1` da tabela:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.4.16_1661896834.png)

Se arrastarmos essa fórmula para baixo e para a direita, veremos os seguintes valores:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.4.17_1661896851.png)

Em outras palavras, quando você arrasta a fórmula para baixo, o endereço da linha muda:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.4.18_1661896866.png)

E quando você arrasta a fórmula para a direita, o endereço da coluna muda:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.4.19_1661896890.png)

A mesma coisa acontece quando você copia uma fórmula. Os valores de referência de célula serão alterados no mesmo número de células nas quais a fórmula foi estendida.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.4.18_1661896916.gif)

Você já usou essa forma de mudar de endereço quando escreveu a fórmula, arrastou-a para baixo e viu imediatamente a soma das receitas e despesas de cada categoria. No entanto, esse recurso não funciona diretamente quando você arrasta a fórmula para a direita ou para a esquerda.

Por exemplo, a fórmula original para cálculo de renda ficou assim:

```
=SUMIFS(Export!L:L;Export!C:C;"Renda";Export!G:G;6)
```

Quando você arrasta uma célula para a direita, ela muda para isso:

```
=SUMIFS(Export!M:M;Export!D:D;"Renda";Export!H:H;6)
```

Todos os intervalos são alterados. É por isso que a fórmula retorna 0: o intervalo `Export!M:M` contém células vazias, `Export!D:D` não contém linhas de `"Renda"` e `Export!H:H` não possui linhas com valor `6`.

Para evitar isso, use a função de referência absoluta.

-   Uma célula fixa tem este formato: `$A$1`.
-   Um intervalo fixo tem o seguinte formato: `$L:$L` ou `$L$1:$L$100`.

O cifrão significa que a referência é fixa. Se a fórmula contém uma referência absoluta, não importa como você a arrasta ou copia. Ela sempre terá valores das células ou intervalos fixos.

### 🛠️ Tarefa 1

Cabeleireiros estrangeiros especializados em cabelos cacheados vêm ao seu salão dar aulas de tempos em tempos. Você precisa pagar a taxa do cabeleireiro, despesas de viagem e hospedagem. O problema é que todos os pagamentos são feitos em euros e você tem apenas dólares.

Copie a [planilha de tarefas](https://docs.google.com/spreadsheets/d/1zPVw5YUoF4ZDjJXRwdyHpOVQV6qBa8Dj4EwSCRsQsxI/copy) e siga as instruções da caixa vermelha na aba "Exercício 1" para converter euros em dólares.

Concluído

Jonathas

Concluído

> [Planilha para autoavaliação](https://docs.google.com/spreadsheets/d/19yendGS5sWfpD7QCa0nitO2_7SdQ5Ld0vxdnn7r-_LE/edit?usp=sharing)

Pergunta

**Você acabou de usar alguns atalhos. Combine os atalhos com suas funções.**

Esses são atalhos de teclado para Windows, mas quem usa macOS vai saber o que fazer com eles.

Ela preenche todo o intervalo com o valor da primeira linha

Ctrl+D

Sim! E se você selecionar uma célula vazia em vez de um intervalo e pressionar `Ctrl`+`D`, o valor ou fórmula da célula acima será copiado para a célula vazia.

Ele preenche a célula com os valores da célula à esquerda.

Esse atalho não estava na tarefa.

Certo, ainda não falamos sobre esse atalho útil. Você vai aprender sobre ele em breve.

Ele fixa uma célula.

F4

É muito mais conveniente usar essa chave do que inserir o sinal `$` duas vezes.

Ele seleciona um intervalo.

Shift

Está certo. Você pode selecionar o intervalo usando o mouse, mas às vezes o `Shift` é mais rápido.

Trabalho maravilhoso!

**O que exatamente você acabou de fazer?**

Como os cabeleireiros cobram preços diferentes, mas a taxa de câmbio do dólar é a mesma para todos, você usou cálculos com duas variáveis: `D7` (preço) e `$D$2` (taxa de câmbio do dólar).

`D7` é uma referência relativa. Em cada linha subsequente, ela altera o valor para `D8`, `D9` ... `D14`, e quando você copia a fórmula para as células `G7` e `I7`, ela muda para a coluna necessária, mudando para `F7` e `H7`, respectivamente.

`$D$2` é uma referência absoluta. Ela não muda seu endereço. É muito conveniente usar uma referência relativa nesses tipos de cálculos: quando a cotação do dólar muda, basta corrigir o valor na célula `D2` e todos os valores serão recalculados.

É sempre necessário manter a célula fixa como um todo?

Jonathas

É sempre necessário manter a célula fixa como um todo?

Não exatamente. A referência de célula é uma interseção de um nome de coluna e um número de linha. É quase como uma rua e um número de casa.

Você pode usar a referência de célula inteira ou apenas uma parte dela. Há três tipos de referências absolutas:

-   `$B$1` — Fixa o endereço completo. Pressione `F4` uma vez.
-   `B$1` — Fixa apenas o número de linha. Pressione `F4` duas vezes.
-   `$B1` — Fixa apenas o nome de coluna. Pressione `F4` três vezes.

### 🛠️ Tarefa 2

No documento da tarefa 1, abra a planilha oculta "Tarefa 2" e siga as instruções na caixa vermelha.

Para abrir a planilha, clique no quadrado listrado no canto inferior esquerdo da tela.

Concluído

Jonathas

Concluído

💡 Ok, você encontrou a chave que faltava.

Ela funciona como `Ctrl`+`D`, só que para a direita. Esse atalho preenche a célula por analogia com a célula à esquerda ou preenche todo o intervalo selecionado por semelhança com as células mais próximas à esquerda.

Pergunta

**Qual é a diferença entre a tabela com "Congelar linha" e a tabela com "Congelar coluna?"**

Não há diferença, os números são dispostos da mesma forma.

Os números são completamente diferentes, não há semelhança entre as tabelas.

Os números são os mesmos, mas localizados em células diferentes.

Os números realmente devem ser os mesmos, mas em células diferentes. Por exemplo, o número 2160 está localizado na célula `G8` da primeira tabela, mas na segunda está na célula `D21`.

Seu entendimento sobre o material é impressionante!

Certo, então é assim que o congelamento de parte de uma célula funciona.

Na fórmula `=B7*C$6`, a sexta célula é fixa. Na tabela "Congelar linha", o `C$6` refere-se a apenas cinco valores: `C6`, `D6`, `E6`, `F6`, `G6`. Ao arrastar a fórmula:

-   para baixo: O valor continua referente à célula `C6`.
-   para direita: Os nomes das colunas de `D6`, `E6`, `F6`, `G6` mudam.

Na fórmula `=$B17*C16`, a coluna `B` é fixa. Na tabela "Congelar coluna", `$B17` tem apenas cinco valores: `B17`, `B18`, `B19`, `B20`, `B21`. Ao arrastar a fórmula:

-   para baixo: os endereços de linha de `B18`, `B19`, `B20`, `B21` mudam.
-   para direita: o endereço da célula não muda e o valor continua referindo-se a `B17`.

💡 É fácil lembrar as regras para uma referência absoluta:

1.  Se $ estiver antes de uma letra, então a letra não muda.
    
2.  Se $ estiver antes de um número, o número não será alterado.
    

### 🛠️ Tarefa 3

1.  Abra a planilha "Tarefa 3".
2.  A tabela contém os turnos dos cabeleireiros em seu salão: 1 significa "o cabeleireiro trabalhou naquele dia", 0 significa "não trabalhou naquele dia". Também são dados os salários, ou seja, quanto o funcionário recebe por turno.
3.  Escreva uma fórmula com duas variáveis para calcular a renda de cada cabeleireiro em um determinado dia do mês.
4.  Importante! Você precisa escrever uma fórmula e arrastá-la para todas as células.
    

Pergunta

**Qual fórmula você obteve como resultado?**

`=D4*$B10`

A referência `D4` é totalmente relativa. Mas o valor `B10` contém uma coluna fixa, porque os preços padrão de cada cabeleireiro são fornecidos na coluna `B`.

`=D4*B$10`

`=D4*$B$10`

Você conseguiu!

### 🛠️ Tarefa Prática 1

Agora, vamos voltar ao relatório da primeira lição.

1.  Copie a [tabela com 6 meses de dados de receitas e despesas](https://docs.google.com/spreadsheets/d/1ZJ8yRYfRr85FKAP0iq0IT-9xyLyF6YEGpBsY0ysJItc/copy).
    
2.  Existe um modelo de relatório pronto na planilha Report. Ele tem até algumas fórmulas.
3.  Usando seu conhecimento de congelamento de endereços de células, altere as fórmulas para poderem ser arrastadas para a direita e preencha o relatório para todos os seis meses.

Verificar

Jonathas

Verificar

> [Planilha para autoavaliação](https://docs.google.com/spreadsheets/d/1A3xc2XMfFVo4yAGpAWfX8DNyqncIa5f-hKMena1MtTI/edit?usp=sharing)

### Resumo

1.  Para fazer muitos cálculos usando uma fórmula para valores diferentes, use variáveis como argumentos, não como valores absolutos.
2.  Uma referência pode ser feita relativa `C12`, totalmente absoluta `C$12` ou parcialmente absoluta: `$C12` é uma coluna fixa e `C$12` é uma linha fixa.
3.  Atalhos permitem que você trabalhe mais rápido com planilhas. Você pode encontrar a lista completa de atalhos do Google Sheets clicando em`Ajuda` → `Atalhos de teclado`.

💡 Curiosidade: o Planilhas Google tem um atalho para chamar uma lista de atalhos: `Ctrl`\+ `/` (`⌘`\+ `/`).

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/curly_excel_4_2_1663676110.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/common/moved_example.svg)

2 — 2

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-30-52-108Z.md
### Última modificação: 2025-05-28 18:30:52

# Fórmulas e funções numéricas básicas - TripleTen

Capítulo 1/3

Módulo 1. Análise de Dados

Opcional

# Fórmulas e funções numéricas básicas

Uma nova tarefa está esperando por você: criar um painel para ver as principais métricas online.

Uma de suas amigas anota diligentemente todas as suas despesas todos os dias. Você também faz perguntas a ela todos os dias, e não no final do mês: “Quanto dinheiro temos agora?”, “Oferecemos muitos descontos?”, “O que não vende muito bem e é possível fazer isso ter vendas melhores?”

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/curly_excel_5_1663679178.png)

É hora de começar a buscar respostas para essas perguntas e criar uma planilha mostrando os dados diários relevantes.

Ela vai ficar assim:

![](https://practicum-content.s3.amazonaws.com/resources/image_1_1704869005.png)

Uma amiga inseriu dados sobre despesas e receitas em uma planilha. Ela coletou mais de 3.000 linhas de dados em três meses, e acabou ficando inconveniente trabalhar com tudo isso. Então ela dividiu os dados em duas planilhas organizadas de forma semelhante:

-   All: contendo dados dos meses anteriores
    
-   Now: contendo dados para o mês atual
    

Quando o mês termina, ela move os dados de uma planilha para outra.

### 🛠️ Revisão

1.  Copie o [novo arquivo de receitas e despesas do salão](https://docs.google.com/spreadsheets/d/1czgNRMVJRbeNS2BmiGM1z_557BRo--05mqMVDkyqKtw/copy) e o coloque na pasta com todas as planilhas deste curso.
2.  Vá para a aba Dashboard. Preencha os campos destacados em amarelo com funções.

Pergunta

**O que gerou mais receita para o salão este mês?**

Corte de Cabelo Padrão

Corte de cabelo pelo Cabelereiro Sênior.

Até o momento, esse serviço rendeu $ 10.746. Verifique suas fórmulas na [tabela de gabaritos](https://docs.google.com/spreadsheets/d/1t9SjCN6Dw4u85eyHjKUCF3AbcnNjXkRq4zh6bSFVtl8/edit?usp=sharing).

Corte de Cabelo e Escova.

Xampu com Condicionador.

Você conseguiu!

Agora você precisa calcular o número de serviços prestados e mercadorias vendidas no mês atual. Você pode fazer isso usando a função `COUNTIFS`. Isso funciona de forma semelhante ao `SUMIFS`, mas retorna a contagem de células em vez da soma.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.1.5_alpaca_1663679225.png) _Na imagem, COUNTIFS conta quantas alpacas do rebanho usam tanto lenços (critério 1) quanto chapéus (critério 2). Havia apenas 3 alpacas com lenços e chapéus._

A sintaxe de `COUNTIFS` é semelhante a `SUMIFS`:

```
=COUNTIFS(criteria_range1; criterion1; criteria_range2; criterion2; ...)
```

Aqui você também precisa especificar dois argumentos:

-   intervalo de critérios
-   critérios a serem verificados

A única diferença é que `COUNTIFS` não tem o argumento sum\_range.

Por exemplo, a função para contar cortes de cabelo padrão se parece com isso:

```
=COUNTIFS(Export!E:E;"Corte de Cabelo Padrão")
```

Pode haver quantos critérios você quiser, assim como em `SUMIFS`.

### 🛠️ Tarefa 1

1.  Preencha os campos do painel onde você precisa calcular o número de serviços e bens específicos.
    
2.  Volte para a página do curso e responda à pergunta abaixo.
    

Pergunta

**Quantos frascos de xampu (sem contar "xampu e condicionador") o salão vendeu este mês? Escreva a resposta como um número.**

31

Sim! Essa métrica é necessária para evitar que você peça muitos produtos no próximo mês ou para incentivar os cabeleireiros a vender mais produtos aos clientes.

Muito bem!

## Acompanhamento da renda diária e mensal

Alguns cálculos podem ser feitos usando sinais matemáticos comuns sem funções padrão.

`+`: Adição

`-` : Subtração

`/`: Divisão

`*` : Multiplicação

Se houver vários símbolos na fórmula, use a ordem das operações:

1.  Primeiro, multiplicação e divisão
2.  Em seguida, adição e subtração
3.  Cálculos entre parênteses são feitos primeiro

O administrador do salão conhece a renda diária e mensal atual, bem como a previsão de renda para cada dia e mês. Você precisa escrever uma fórmula que responde à pergunta de qual porcentagem do plano foi concluída até agora.

Para calcular a porcentagem, você criou o seguinte esquema:

Porcentagem concluída = (Vendas / Plano) \* 100

Y — Z%

Se aplicar isso à tabela, vai obter o seguinte:

Porcentagem concluída hoje = 4,16%.

Porcentagem concluída para o mês = 58,66%.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.7.3_1663742674.png)

Pergunta

**Qual (quais) fórmula(s) pode calcular o percentual do plano diário? Escolha todas as opções que você considera corretas.**

Escolha quantas quiser

Vendas\*(100/Plano)

Essa fórmula também está correta.

100/Plano\*Vendas

Essa fórmula também está correta, embora a ordem das operações seja diferente.

Plano\*100/Vendas

Vendas\*100/Plano

A fórmula difere de outras opções corretas apenas na ordem das operações, e a resposta está correta!

Você conseguiu!

Você pode usar qualquer uma das três fórmulas, ou ainda mais fácil: `Vendas/Plano`. Se aplicarmos a fórmula à tabela:

```
 =D2/E2
```

O resultado é um decimal.

Basta pressionar o botão que altera o formato do número de fracionário para percentual e definir o número de casas decimais.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.7.4_1663743953.png)

## Como acompanhar os descontos

Agora, preencha o bloco com dados sobre os descontos deste mês.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.7.6_1663744121.png) _Ele está preenchido com valores incorretos de propósito. Você vai obter um resultado diferente._

Comece com os dados de seu interesse. Um bom nome de célula sempre responde a uma pergunta específica.

-   Quantos descontos foram dados usando o código promocional? — "Códigos promocionais usados"
-   Qual é o desconto máximo? — "Desconto máximo"
-   Qual é o desconto médio do salão? — "Desconto médio"
-   Quanto dinheiro no total foi descontado para os clientes? — "Quantidade total de descontos"

Agora é necessário escrever funções que dão uma resposta exata para cada pergunta.

**1\. Quantos descontos foram concedidos usando o código promocional?**

Precisamos de uma função que conte o número de códigos promocionais usados. A função `COUNT` serve. Ela retorna a contagem de células como `COUNTIFS`, mas sem as condições especificadas.

💡 Para aplicar a função `=COUNT()`, especifique entre parênteses o intervalo de células a serem contadas.

Nesta tarefa, os dados de desconto podem ser encontrados na coluna `B` "Código promocional". Portanto, para responder à pergunta, você precisa contar quantos códigos promocionais essa coluna possui.

Pergunta

**Como você escreve a função corretamente?**

\=COUNT("Código promocional")

\=COUNT(B2:B)

\=COUNT(Now!B2:B)

Você precisa contar o número de códigos promocionais no intervalo `B2:B` na planilha Now. A célula `B1` não está no intervalo: ela é um título.

\=COUNT(B:B)

\=COUNT(Now!B:B)

Fantástico!

Qual é o resultado?

Jonathas

Qual é o resultado?

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Excel_1/esp/7_pt.gif)

Por que 0? A função está correta!

Jonathas

Por que 0? A função está correta!

TripleTen

A função está correta. Mas não é adequada.

O que isso significa?

Jonathas

O que isso significa?

TripleTen

A função não pode calcular o formato de dados fornecido.

Você pode definir o formato de dados neste menu:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.7.7_1663745365.png)

Como você pode ver, o formato da coluna "Código promocional" é Texto simples, mas a função `COUNT` funciona apenas com números. Por isso retornou zero. Desta forma, `COUNT` retorna: "Este intervalo não contém células com um número."

Você precisa usar a função `COUNTA`, que conta o número de células não vazias no intervalo de dados. `COUNTA` conta qualquer célula não vazia, independentemente do tipo de informação que uma célula contenha, número ou texto.

Pergunta

**A função `COUNTA` vai ajudar você a calcular quantos códigos promocionais foram usados pelos clientes este mês. Anote sua resposta como um número na caixa abaixo.**

64

Muito bem! Agora, o painel vai atualizar os dados sobre o número de códigos promocionais usados todos os dias.

Muito bem!

E o próximo experimento vai mostrar se você consegue ou não enganar o Google Sheets.

### 🛠️ Experimento

Vamos tentar ver se você consegue enganar o Google Sheets.

1.  Na planilha Now, altere o formato de várias células contendo códigos promocionais de Texto simples para Número. Tente aplicar a função `COUNT` a essa coluna e veja se ela conta o número de códigos promocionais.
    
2.  Aplique a função `COUNT` à coluna do número do mês e lembre-se do resultado. Na coluna "Mês", altere o formato de vários números para Texto simples e veja se o resultado dos cálculos mudou
    
3.  Compartilhe suas descobertas respondendo às perguntas abaixo.
    

Pergunta

**O que a alteração dos formatos de dados mostrou?**

O texto pode ser transformado em número e um número em texto.

Um número pode se transformar em texto, mas o texto não pode se transformar em um número.

É útil lembrar dessa característica. Às vezes, a função não funciona corretamente porque alguns números estão formatados como texto.

O texto pode se transformar em um número, mas um número não pode se transformar em texto.

Um número não pode se transformar em texto e o texto não pode se transformar em um número.

Seu entendimento sobre o material é impressionante!

**2\. Qual é o desconto máximo?**

Para encontrar o maior número em um intervalo, use a função `MAX`. Como `COUNT`, essa é uma função numérica, então a sintaxe é a mesma. Você precisa especificar o intervalo entre colchetes.

Pergunta

**Use a função `MAX` para encontrar o maior desconto que o salão deu este mês. Anote sua resposta como um número inteiro, sem %.**

50

Isso é um bom desconto! Parece que seu salão ama seus clientes.

Excelente!

💡 Há também uma função `MIN`. Ela faz a mesma coisa que o `MAX`, mas ao contrário. Ela mostra o menor número no conjunto de dados.

**3\. Qual é o desconto médio que o salão oferece?**

Você pode usar outra função simples e popular para calcular isso: `AVERAGE`. Calcula a média aritmética? Faça o teste respondendo à seguinte pergunta.

Pergunta

**Se você tem 30 anos, um de seus amigos tem 44 e o outro tem 25, qual é a idade média?**

25

30

33

Da próxima vez, use o Google Sheets e a função `AVERAGE` para calcular a média. Assim, não tem erro.

44

Muito bem!

A função `AVERAGE` calcula a média para qualquer intervalo de dados. Ela soma tudo e divide pelo número de termos. Tente usá-la para o dashboard.

Pergunta

**Qual é o percentual do desconto médio deste mês? Anote a resposta completa sem %. Arredonde sua resposta para um dígito após o ponto decimal.**

16,4

Muito bem! Veja quantos dados foram adicionados no dashboard!

Você conseguiu!

**4\. Quanto dinheiro no total foi descontado para os clientes?**

Para responder a essa pergunta, precisamos de uma função `SUM` simples. Ela soma valores das células especificadas.

Por que precisamos dessa fórmula se temos um sinal "+"?

Jonathas

Por que precisamos dessa fórmula se temos um sinal "+"?

Essa função acelera o trabalho e melhora a precisão.

Primeiro, quando você tem milhares de linhas, leva muito tempo para escrever "célula + célula + célula +...". Em segundo lugar, as pessoas costumam esquecer de atualizar as fórmulas de adição escritas com um sinal de “+” quando uma nova linha aparece na tabela de origem.

E a função `SUM` vai adicionar todos os valores do intervalo especificado. Se um novo valor aparecer no intervalo, será adicionado automaticamente ao valor total.

Tente sempre usar a função `SUM` em vez do sinal "+". Assim, seus cálculos vão estar sempre corretos.

Pergunta

**Qual é a soma total dos descontos deste mês? Anote a resposta como um número na caixa abaixo.**

657,95

Quanta generosidade!

Seu entendimento sobre o material é impressionante!

Pergunta

**Combine os nomes das funções com suas descrições na página de ajuda do Google Sheets.**

Conta o número de valores numéricos em um conjunto de dados.

COUNT

Muito bem! Você lembra que `COUNT` conta apenas valores numéricos.

Encontra a soma de um conjunto de números ou um intervalo.

SUM

É uma simples operação de soma sem condições. Correto.

Conta o número de valores (células preenchidas) no conjunto de dados.

COUNTA

As células preenchidas não contêm apenas números. `COUNTA` conta todas as células preenchidas, não apenas as células preenchidas com números.

Retorna o valor máximo em um conjunto de números.

MAX

Claro, `MAX` é o máximo.

Calcula a média aritmética dos argumentos (excluindo o texto).

AVERAGE

`AVERAGE` é um valor médio. Correto!

Fantástico!

Você se deparou com os seguintes termos:

-   Um conjunto (de dados, números)
-   Uma série (de dados, números)
-   Argumentos (numéricos, texto)

_Argumento_ é o termo mais universal.

Os argumentos são os valores que a função, ou fórmula predefinida, usa para realizar cálculos.

Nas funções `COUNT`, `COUNTA`, `MAX`, `AVERAGE` e `SUM`, você usou intervalos de dados como argumentos: `COUNT(B2:B)`, `COUNTA(B2:B)`, `MAX(J:J)`, `AVERAGE(J:J)`, `SUM(I :I)`. No entanto, os argumentos podem assumir a forma de qualquer valor ou conjunto de valores, não apenas intervalos. Por exemplo, os argumentos das seguintes funções:

-   `SUM(13; 42; 2000)` — Valores absolutos;
-   `MAX(A11; C45; D100)` — Variáveis;
-   `AVERAGE(13; 42; A11; C45)` — Valores absolutos e variáveis;
-   `COUNT(E2:E99; F2:F; G:G)` — Vários intervalos;
-   `COUNTA(13; 42; A11; C45; E2:E99; F2:F)` — Valores absolutos, variáveis e intervalos

💡 Há mais uma opção. Outras fórmulas também podem ser argumentos.

Ainda há um campo em branco no dashboard: "Quantia em dinheiro". Para calcular esse valor, você precisa realizar três operações:

1.  Calcule quanto dinheiro o salão ganhou no mês atual.
2.  Calcule quanto dinheiro o salão gastou.
3.  Adicione a receita em dinheiro à quantia de dinheiro que você tinha no início do mês e subtraia as despesas em dinheiro.

O primeiro ponto já foi calculado, você sabe a quantidade de dinheiro no início do mês. Você só precisa calcular o resto.

Pergunta

**Qual fórmula vai calcular a quantidade atual de dinheiro? Escolha todas as opções que você considera corretas.**

**Célula `H7`: a quantia em dinheiro no início do mês.**

**Célula `H8`: renda em dinheiro.**

Escolha quantas quiser

`=H7 + H8 - SUMIFS(Now!L:L;Now!C:C;"Despesas";Now!D:D;"Dinheiro")`

Observe a estrutura de funções: `dinheiro em espécie` + `renda em dinheiro` - `gastos em dinheiro`. A única dificuldade é que as despesas também são uma função.

`= SUM(H7;H8;SUMIFS(Now!L:L;Now!C:C;"Despesas";Now!D:D;"Dinheiro"))`

`= SUM(H7;H8) - SUMIFS(Now!L:L;Now!C:C;"Despesas";Now!D:D;"Dinheiro")`

Observe a estrutura da função: `=SUM(dinheiro em espécie; renda em dinheiro)` - `gastos em dinheiro`. Muito simples.

`=SUMIFS(Now!L:L;Now!C:C;"Despesas";Now!D:D;"Dinheiro") + H7 + H8`

Muito bem!

O Dashboard está pronto!

Jonathas

O Dashboard está pronto!

> [Planilha para autoavaliação](https://docs.google.com/spreadsheets/d/1t9SjCN6Dw4u85eyHjKUCF3AbcnNjXkRq4zh6bSFVtl8/edit?usp=sharing)

Discutimos muitas funções nesta lição. Conclua a tarefa final para aprendê-las.

### 🛠️ Tarefa Prática 2

**Considerando:** [A planilha](https://docs.google.com/spreadsheets/d/1m_QdMk1RUsAOXCVID1xd-1RWhEIG3kbcVJpryEPqoDM/copy) com uma lista e parâmetros de todos os estabelecimentos de alimentação.

**Tarefa:** crie dois dashboards como esses abaixo. Se você tiver problemas para decidir qual função aplicar para uma tabela de dashboard específica, abra a planilha “Dica”.

**Como verificar:** Clique no botão "Verificar" e um link para um arquivo com a solução correta será aberto.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.7.8_1663754206.png)

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.7.9_1663754265.png)

Verificar

Jonathas

Verificar

> [Planilha para autoavaliação](https://docs.google.com/spreadsheets/d/1Ga8_LxExtNLqq3BQXdfJoMSoBcLisj1lBlDRngsTEkw/edit?usp=sharing)

### Resumo

1.  Use as operações matemáticas para escrever fórmulas simples: `+`, `–`, `/`, `*`, `()`.
2.  No Google Sheets, você pode alterar o formato de uma exibição de dados, por exemplo, alterar um número para uma porcentagem.
3.  Algumas funções funcionam com qualquer tipo de dados, como `COUNTA`, enquanto outras funcionam apenas com números: `COUNT`, `MAX`, `MIN`, `AVERAGE`, `SUM`.
4.  Os argumentos de função não precisam ser apenas intervalos; eles também podem ser valores absolutos e relativos, ou mesmo outras funções.

![](https://practicum-content.s3.us-west-1.amazonaws.com/common/moved_example.svg)

2 — 2

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-30-53-419Z.md
### Última modificação: 2025-05-28 18:30:53

# Tabelas dinâmicas - TripleTen

Capítulo 1/3

Módulo 1. Análise de Dados

Opcional

# Tabelas dinâmicas

Nesta lição, você vai aprender como criar tabelas dinâmicas. O Google Sheets fornece uma maneira rápida e fácil de criá-las. Talvez você já tenha adivinhado, mas a funcionalidade é chamada "Tabela dinâmica".

💡 Uma tabela dinâmica pode significar duas coisas:

1.  De modo mais abrangente, é uma planilha com dados agregados.
    
    Temos a planilha com dados brutos, por exemplo, com entradas de renda e despesas. É difícil tirar conclusões com base em tais planilhas porque elas têm excesso de informação.
    
    Por outro lado, uma tabela dinâmica contém dados brutos que são agregados aplicando um critério específico, por exemplo, o total de vendas com base em um nome de produto.
    
2.  "Tabela dinâmica" também é o nome de uma função específica no Google Sheets que proporciona a criação de tabelas dinâmicas.
    

Relatórios e dashboards ajudam a controlar a renda e as despesas do salão. Mas há ainda uma linha de despesas que se parece com uma caixa preta, porque ela mostra apenas as despesas totais para o "Orçamento publicitário". O que deixa várias perguntas em aberto, como por exemplo:

-   Este dinheiro é gasto de forma eficaz?
-   Ele pode ser gasto de um jeito mais sensato?

Você perguntou à sua amiga que trabalha com a publicidade do salão para tentar ter algumas respostas. Em resposta, ela enviou uma planilha com dados carregados de várias contas de publicidade. Você não sabe nada a respeito de marketing, mas já sabe como trabalhar com dados. Então, a análise do orçamento de marketing não deverá ser assustadora.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Excel_1sprint_4/Illustration/PT/PT_curly_excel_6_1.png)

Faça uma cópia da [planilha com os dados de marketing](https://docs.google.com/spreadsheets/d/1_V6SSuMfgt3E4e4HRaeFiPiTAOdsmSMu5ZCXlT5DHG4/copy), tente descobrir a lógica dos dados e responda a algumas perguntas.

Pergunta

**Qual é a relação entre os dados das colunas "Anúncio" e "Campanha"?**

Uma campanha pode consistir em vários anúncios.

Para ver como isso funciona, ordene os dados na coluna "Campanha". Feito isso, a estrutura dos dados vai ficar imediatamente clara.

Um anúncio consiste em várias campanhas.

O número de campanhas é igual ao de anúncios.

Excelente!

Pergunta

**Qual é a relação entre impressões e cliques?**

Sempre há mais impressões do que cliques.

Para ver isso, digite a fórmula "impressões menos cliques" em uma coluna separada. A diferença sempre será positiva, ou seja, há mais impressões do que cliques.

Sempre há mais cliques do que impressões.

Às vezes, há mais cliques do que impressões.

Fantástico!

Olhando para a estrutura dos dados, você já pode descobrir alguma coisa sobre os anúncios. Vamos adicionar algumas condições de entrada:

-   O objetivo principal de todo anúncio é vender um produto ou serviço.
-   As plataformas de publicidade podem cobrar dinheiro por impressões de anúncios ou por cliques neles. Nesta lição, vamos assumir que todas as plataformas de publicidade vendem cliques e não impressões.
-   O preço de um clique é dinâmico, ou seja, ele muda o tempo todo.
-   Podemos ter vários anúncios semelhantes em uma campanha para descobrir qual deles funciona melhor.

Pergunta

**Isto é uma tabela dinâmica ou uma planilha com dados brutos?**

Tabela dinâmica.

Planilha de dados brutos.

Você foi um pouco impaciente, mas logo vai criar algumas tabelas dinâmicas a partir de uma planilha de dados brutos.

Seu entendimento sobre o material é impressionante!

Isso é uma planilha com dados que precisam ser processados. Com base nos dados brutos, você pode criar uma planilha que mostra o número de anúncios em uma campanha.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.8.1_pt_1663737834.png)

Pergunta

**Escolha a combinação de fórmulas que você pode usar para criar uma planilha desse tipo.**

UNIQUE + SUMIFS + COUNTA

UNIQUE + SUM + COUNT

UNIQUE + COUNTIFS + SUM

`UNIQUE` vai mostrar os nomes de todas as campanhas, `COUNTIFS` vai calcular o número de anúncios em cada campanha, `SUM` vai somar os resultados do trabalho de `COUNTIFS` e visualizar o número total de anúncios.

UNIQUE + COUNTA + AVERAGE

Muito bem!

Você pode ter o mesmo resultado usando a funcionalidade de tabelas dinâmicas. Para construir a tabela abaixo, você vai precisar de 23 segundos (já contamos).

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.8.2_pt_1663737873.png) _Mais uma vantagem de uma tabela dinâmica sobre fórmulas é que ela formata os dados automaticamente para facilitar a leitura._

Ao final da lição, você vai saber como criar tabelas ainda mais complexas, como essa aqui:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.8.3_pt_1663737892.png)

Vamos começar!

Jonathas

Vamos começar!

## Sua primeira tabela dinâmica

Primeiro, você precisa selecionar o intervalo de dados brutos que você quer processar e acessar `Inserir` → `Tabela dinâmica`, e clicar `criar`.

Você verá a seguinte aba, que é um espaço em branco para sua futura tabela dinâmica.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1.8.4_pt_1663737934.png)

Cada tabela dinâmica é composta por Linhas, Colunas, Valores e Filtros. Você decide quais dados serão usados para as linhas e colunas da tabela e quais serão usados para calcular os valores, ou seja, quais dados ficam nas interseções das linhas e colunas.

Um exemplo de uma tabela dinâmica.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/pic_excel_1.8_pt_1663738385.png)

Aqui você pode ver diferentes meios de transporte organizados com base no fato de eles terem um motor ou não, e dependendo do ambiente no qual eles se movem.

As linhas indicam o uso de um motor.

As colunas são para o ambiente: água ou ar.

Os valores nas interseções das linhas e colunas indicam meios de transporte específicos.

E aqui está como seria uma planilha básica com essa informação.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Excel_1/pt/1.8.5_pt.png)

Se você reproduzir essa visualização usando tabelas dinâmicas, o processo vai ficar assim:

Uma tabela dinâmica serve para duas finalidades:

-   Ela modifica a representação de dados para torná-los mais fáceis de ler.
-   Ela executa as seguintes operações com dados:
    -   somar,
    -   calcular uma quantidade,
    -   encontrar a média,
    -   encontrar o valor máximo,
    -   encontrar o valor mínimo.
        
        É por isso que na tabela dinâmica resultante, você vê números, em vez dos nomes dos veículos. A tabela os calculou automaticamente.

### 🛠️ Tarefa 1

1.  Selecione as colunas de A a G da planilha com os dados de marketing e clique em `Inserir` → `Tabela dinâmica` → `Criar`.
2.  Adicione "Campanha" em Linhas, clicando em Adicionar ou arrastando a coluna.
3.  Adcione "Anúncio" em Valores, clicando em Adicionar ou arrastando a coluna.
    

Mostrar o resultado

Jonathas

Mostrar o resultado

A solução fica assim:

Pergunta

**Quais valores a tabela dinâmica resultante mostra?**

O número de anúncios únicos em cada campanha.

O número de todas as campanhas.

O número total de anúncios em cada campanha.

"Valores" contêm os dados sobre os anúncios. O campo "Resumir por" contém a fórmula COUNTA, ou seja, ele é sobre o número de anúncios.

Trabalho maravilhoso!

Com alguns cliques, você obteve uma tabela dinâmica que mostra o número de anúncios em cada campanha. Você poderia obter a mesma planilha com `COUNTIFS`, mas isso levaria muito mais tempo.

Experimente ligar um timer e comparar o tempo que você gastou para resolver a tarefa usando tabelas dinâmicas e usando fórmulas. Nossos resultados:

-   Usando uma tabela dinâmica: 23 segundos
-   Usando `COUNTIFS`: 2 minutos e 15 segundos

## Indicadores compostos

Há um problema de escassez de dados na planilha com os dados brutos de marketing. Por exemplo, você não pode criar uma tabela dinâmica com despesas mensais em anúncios publicitários porque você não tem uma coluna com esses dados. Há, claro, uma coluna com as datas. Mas esse dados não só mostram o mês, mas também o dia e o ano. O Google Sheets não vai separar o mês da data inteira por conta própria. Você precisa ajudar nisso.

💡 Ao trabalhar com dados, há um conceito chamado de indicadores compostos. Os indicadores compostos são as derivadas de outros valores da planilha. Essas derivadas são criadas especificamente para interpretar indicadores ou operar neles.

Por exemplo, se você tiver colunas com o preço de um pedido e a quantidade de produtos no pedido, você pode criar um indicador composto, ou seja, o preço médio de um produto. Para fazer isso, você precisa criar uma coluna com a fórmula `=Preço de um pedido/Quantidade de produtos`. Usando um indicador composto, você pode filtrar os pedidos com um preço médio específico. Você não poderia fazer isso na versão original da planilha.

Agora, adicione alguns indicadores compostos aos seus dados brutos.

### 🛠️ Tarefa 2

Copie [a nova versão da planilha](https://docs.google.com/spreadsheets/d/11ZOJr1JxNcGA8ud1um-T39Ixqp14qxb-1vrASCz7Pfg/copy) para o seu workspace. Preencha as colunas vazias com os indicadores compostos:

**CTR** (taxa de cliques) — é uma métrica de marketing igual à proporção de cliques para impressões expressa como uma porcentagem.

**CPC méd.** (custo por clique médio) — no nosso exemplo, as plataformas de publicidade não cobram dinheiro por impressões de anúncios, mas sim por cliques. O preço de um clique depende de muitos parâmetros e muda o tempo todo. É por isso que é mais fácil usar o preço médio.

A **taxa de conversão** é uma métrica de marketing que mostra a proporção dos usuários que fizeram uma compra em relação a todos os usuários que clicaram no anúncio. Essa é a métrica de porcentagem que mostra a eficiência de publicidade.

**Preço alvo** — Nem todos os usuários que clicam no anúncio compram algo. No entanto, a plataforma de publicidade cobra dinheiro por cada clique, o que inclui tanto os usuários que só acessaram o link, como aqueles que acessaram o link e depois compraram alguma coisa. É por isso que temos que introduzir um indicador para o preço médio de atrair um usuário que fez uma compra.

Verificar

Jonathas

Verificar

Pergunta

**Qual anúncio é o mais clicável, ou seja, tem o maior CTR?**

15% de desconto - Instagram

Verão Cacheado - 3

Brinde Grátis de Cosméticos - 1

Os especialistas de marketing invejariam esse resultado: 85,05%. Isso significa que 85 em cada 100 pessoas clicaram nesse anúncio após vê-lo. Fantástico! Você pode verificar o resto dos valores [na planilha](https://docs.google.com/spreadsheets/d/1ddsc7dOQXEAD6cP8eCUjM2eUOA6Y4x4gespQzplGPZ0/copy).

Estilo Pessoal - 1

Excelente!

## Linhas e valores

Agora a planilha tem dados suficientes para analisar e otimizar as despesas em campanhas de marketing. Para começar, você vai precisar criar algumas tabelas dinâmicas.

A primeira tabela dinâmica desta lição é muito simples. Ela contém apenas um valor: o número de anúncios nas campanhas. No entanto, as tabelas dinâmicas podem ter um número ilimitado de valores para quaisquer linhas.

Podemos ver um exemplo?

Jonathas

Podemos ver um exemplo?

Há 82 anúncios. Você adicionou uma métrica CTR a cada um deles. Quanto maior o CTR, melhor: o banner atrai a atenção das pessoas e elas clicam nele com muita frequência. Os anúncios com os CTRs mais altos são os mais valiosos. Você precisa continuar os exibindo. Podemos remover os anúncios com CTRs baixos.

A tabela dinâmica nos mostra o que precisa ser mantido e o que deve ser desativado.

### 🛠️ Tarefa 3

1.  Selecione a planilha com os dados básicos e clique no botão `Tabela dinâmica`.
2.  Insira os seguintes valores: "Anúncio" em Linhas e "CTR" em Valores.
3.  O "Anúncio" em Linhas tem dois parâmetros: `Ordem` e `Ordenar por`. Você pode usá-los para personalizar a aparência dos dados na tabela dinâmica. Defina os valores: em `Ordem` "Decrescente", em `Ordenar por` "`SUM` do parâmetro CTR por categoria".

Verificar

Assista ao tutorial sobre como realizar essa tarefa.

Você tem a classificação dos anúncios com base no CTR. Mas isso não é o suficiente. Você quer que as pessoas comprem seus serviços e não apenas cliquem nos anúncios. Alguns anúncios com um CTR alto podem ter vendas baixas? Talvez eles não estejam sendo exibidos para o público-alvo? Um anúncio com um CTR baixo pode ter vendas altas?

### 🛠️ Tarefa 4

Configure a tabela dinâmica de forma que ela mostre o número de compras que você obtém com um anúncio específico.

Verificar

Jonathas

Verificar

Pergunta

**Onde você adicionou o valor "Vendas por meio de anúncios"?**

Linhas

Colunas

Valores

Para ter uma tabela limpa e legível, adicione o valor "Vendas por meio de anúncios" a Valores.

Filtros

Você conseguiu!

Você então pode ver dois indicadores para cada anúncio: um CTR e a quantidade de vendas. Agora, você pode tomar uma decisão informada sobre quais anúncios são eficazes e quais não são. É sempre necessário se esforçar para ter dados completos e precisos, bem como para tirar conclusões sensatas.

Até agora, você usou apenas a primeira função de tabelas dinâmicas: a representação de dados. Você removeu todas as coisas irrelevantes e organizou os dados na ordem necessária. Agora, vamos aplicar a segunda funcionalidade de tabelas dinâmicas para fazer operações aritméticas.

Crie uma tabela de dados que mostra as taxas mínima, média e máxima de conversão entre todas as campanhas de publicidade.

### 🛠️ Tarefa 5

1.  Crie uma nova tabela dinâmica.
2.  Adicione "Campanha" a Linhas.
3.  Adicione o Indicador "Taxa de conversão" três vezes a Valores.
4.  Selecione `MIN` para o primeiro valor no parâmetro "Resumir por". Para o segundo e o terceiro valores, selecione `MAX` e `AVERAGE`, respectivamente.
5.  Selecione `AVERAGE` para o parâmetro "Ordenar por" em Linhas e "Decrescente" em "Ordem".

Pergunta

**Qual campanha tem a máxima taxa média de conversão?**

Publicações no Facebook.

Publicações no Instagram.

Stories no Instagram.

Anúncios no TikTok.

A taxa média de conversão da campanha é 71,87%. Isso é o indicador médio mais alto.

Muito bem!

Você tem uma tabela que verifica a taxa de conversão de todos os anúncios em cada campanha. A tabela escolhe os valores mínimo e máximo da conversão e calcula a média. Você poderia fazer a mesma coisa usando fórmulas, mas assim é muito mais rápido. Use a funcionalidade de tabelas dinâmicas para processar dados e chegar a conclusões mais rápido.

## Colunas

Você pode dizer: "Todas as tabelas dinâmicas anteriores tinham colunas". E você teria razão. Claro, toda planilha tem colunas. Mas as colunas `A`, `B`, `C` em uma planilha normal e as `Colunas` em uma tabela dinâmica são duas entidades diferentes.

Recentemente, você usou `Linhas` para selecionar um critério para agregar dados: anúncios ou campanhas. Dependendo do valor selecionado, as linhas mostraram a lista de todos os anúncios ou a lista das campanhas. Em tabelas dinâmicas, você precisa do campo `Colunas` para adicionar mais um critério, cujo valor não será representado nas linhas `1`, `2`, `3`, mas sim nas colunas `A`, `B`, `C`.

![](https://practicum-content.s3.amazonaws.com/resources/1.2.8PT_1685653408.png)

### 🛠️ Tarefa 6

1.  Crie uma tabela dinâmica com as despesas em todas as campanhas.
2.  Adicione um segundo indicador chamado "Mês" a Linhas.

Verificar

Jonathas

Verificar

Você deve obter a seguinte tabela:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Excel_1/pt/1.8.7_pt.png) _Quando você seleciona dois indicadores em Linhas, aparecem sinais de mais que escondem e exibem linhas dentro de um grupo._

Você pode ver as despesas mensais para cada campanha. Isso é útil, mas é difícil usar uma tabela tão grande: você precisa subir e descer para comparar despesas para diferentes campanhas.

Você pode adicionar o segundo critério de agregação usando Colunas. Veja o que acontece se você mover "Mês" de Linhas para Colunas.

## Filtro

Filtro é o último campo que você pode usar para configurar uma tabela dinâmica. Vamos ver como ele funciona.

O indicador composto "Custo por clique médio" mostra a proporção de despesas médias por anúncio para o número de cliques. Obviamente, esse indicador muda de um anúncio para outro. Ele também pode variar de mês para mês, já que as despesas diferem para meses diferentes.

Vamos criar uma tabela dinâmica que ajuda a analisar o preço médio de um clique com base em anúncios e meses.

### 🛠️ Tarefa 7

1.  Crie uma nova tabela dinâmica.
2.  Coloque os valores "Campanha" e "Anúncios" em Linhas. Ponha "Mês" em Colunas e "CPC méd. (Custo por clique médio)" em Valores.

Verificar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-30-54-745Z.md
### Última modificação: 2025-05-28 18:30:55

# Introdução - TripleTen

Capítulo 2/3 · Faltam 6 lições

Módulo 2. Trabalhando com Planilhas

Opcional

# Introdução

Neste módulo, você vai aprender a processar dados antes da análise, usar funções lógicas, combinar várias tabelas usando a função VLOOKUP, importar dados de outros arquivos e criar consultas SQL no Google Sheets.

Essas habilidades vão ajudar você a lidar com as seguintes tarefas:

-   Remover duplicados de bancos de dados sem excluir informação sobre pessoas com nomes idênticos.
-   Processar colunas com texto misto e dados numéricos em arquivos CSV importados.
-   Criar uma fórmula para dar notas aos alunos conforme o número de respostas corretas.
-   Definir critérios para cálculos de bônus.
-   Combinar informações sobre clientes de diferentes importações em uma única planilha.
-   Sincronizar tabelas para um trabalho em equipe eficaz.

O módulo consiste em cinco aulas e leva aproximadamente três horas para ser concluído.

Tenha em mente que não há necessidade de passar por todas as lições de uma só vez. Com base nas estatísticas, nossos alunos são mais bem-sucedidos na conclusão do curso quando estudam regularmente por intervalos de 15 a 30 minutos.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-30-56-964Z.md
### Última modificação: 2025-05-28 18:30:57

# Processamento de dados, parte 1 - TripleTen

Capítulo 2/3 · Faltam 5 lições

Módulo 2. Trabalhando com Planilhas

Opcional

# Processamento de dados, parte 1

Imagine que você decidiu criar um banco de dados de clientes para seu salão de beleza. Você quer usá-lo para newsletters, contatar clientes inativos e oferecer descontos exclusivos para clientes regulares.

Com essa finalidade, você pediu a um dos seus funcionários para exportar dados do sistema de agendamento. Ele envia uma tabela contendo as informações sobre todos os clientes que visitaram o salão pelo menos uma vez.

### 🛠️ Preparação

1.  Crie [a sua cópia do banco de dados](https://docs.google.com/spreadsheets/d/1npfcQw2B96gwHf6AVoAgYZedG65rvSKWA9BizBHupc0/copy).
2.  Você vai precisar desse arquivo para três lições, incluindo essa, então coloque-o em uma pasta do Google Drive onde você poderá acessar com facilidade depois.
3.  Examine a estrutura da tabela.

Você ficou feliz com a tabela após olhar de relance, mas depois descobriu que você precisava de uma tabela dinâmica com as seguintes colunas:

-   Classificação dos serviços indicando sua popularidade.
-   O número de novos clientes adquiridos em cada mês (em cada ano).
-   O perfil do cliente médio, incluindo cada serviço que ele usa, e qual a frequência, quanto de lucro ele traz e o desconto exclusivo que ele tem.

Isso é como você imagina o resultado ideal:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT2.2.1_1660221095.png)

Você está sem sorte, no entanto. Nenhuma das funções que você aprendeu no módulo anterior funcionaram. Você checou tudo duas vezes, mas ainda não conseguiu encontrar o erro. Então, você pediu ajuda a um dos seus colegas. Juntos, vocês chegaram à raiz do problema. As funções não são o problema, são os dados. Definitivamente alguma coisa está errada!

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PTcurly_excel_2.2_3_Z_1660222941.png)

No primeiro módulo, você trabalhou com dados que já estavam prontos para análise. Isso é realmente raro na vida real. Geralmente, os dados precisam ser pré-processados: remover valores duplicados, uniformizar, converter texto em número e estruturar tudo adequadamente.

Nessa, e nas lições futuras, você vai aprender a realizar todos esses procedimentos.

## Remover valores duplicados

A lógica de uma base de dados de clientes dita que cada linha contém informação sobre uma pessoa. Então, o número de linhas preenchidas deve coincidir com o número de clientes.

Pergunta

**Qual função conta o número de linhas preenchidas?**

-   **Você pode apenas dar uma olhada na última linha de número preenchida, mas em vez disso, vamos automatizar o processo! É muito mais fácil e rápido.**

`COUNT`

`COUNTA`

`COUNTIFS`

`SUMIFS`

Mostrar resposta

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-30-58-259Z.md
### Última modificação: 2025-05-28 18:30:58

# Processamento de dados, parte 2 - TripleTen

Capítulo 2/3 · Faltam 4 lições

Módulo 2. Trabalhando com Planilhas

Opcional

# Processamento de dados, parte 2

Vamos continuar processando o banco de dados de seus clientes. Se você fechou o arquivo, pode encontrá-lo em ["Recente”](https://drive.google.com/drive/u/0/recent) na página principal do Google Drive ou na caixa de pesquisa: "Lições 2.1–2.2. Tarefas 1–9.”

Nesta lição, você vai aprender a converter dados de um tipo para outro. Você também vai explorar vários tipos de dados e descobrir como eles diferem.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PTcurly_excel_2.3_1_1660226490.png)

## Conversão de dados: data e hora

Em nosso banco de dados, há uma coluna com a data da primeira visita. Não há nada de errado com essa coluna. Ele contém apenas datas e há apenas um valor por célula.

No entanto, se você decidir tentar criar uma tabela que mostre quantos clientes você adquiriu em cada mês dos dois anos anteriores, não vai conseguir.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT2.3.1_1660284553.png)

O Google Sheets não sabe "ler". Ou seja, se o formato de data for DD/MM/AA, você não vai conseguir calcular quantos clientes foram adquiridos em cada mês — e você precisa obter essa informação por um período de dois anos. Para fazer isso, você vai precisar dividir o mês e o ano em células diferentes.

Não se apresse em usar a função "separar texto em colunas" ainda — existe uma ferramenta muito mais conveniente para trabalhar com datas.

### 🛠️ Tarefa 6

**Converta as datas da primeira visita para analisar a dinâmica de aquisição de clientes.**

1.  Crie 3 colunas em branco à direita da coluna que contém a data da primeira visita. Para criar uma coluna, clique com o botão direito na coluna e em "inserir 1 coluna à direita".
2.  Preencha essas colunas com o dia, mês e ano da primeira visita de cada cliente com a ajuda das funções `=DAY()`, `=MONTH()`, e `=YEAR()` As funções devem receber a célula com a data como argumento.
3.  Na aba "Report", crie uma tabela mostrando quantos clientes você adquiriu por mês nos últimos dois anos.

Se essas três colunas mostrarem datas no formato DD/MM/AA, altere o formato de dados de `Automático` para `Número` e ajuste o número de dígitos após o ponto decimal.

Pergunta

**Quantos clientes seu salão conquistou no quinto mês de 2020?**

30

50

56

37

Mostrar resposta

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-30-59-574Z.md
### Última modificação: 2025-05-28 18:30:59

# Funções lógicas - TripleTen

Capítulo 2/3

Módulo 2. Trabalhando com Planilhas

Opcional

# Funções lógicas

Sua base de clientes está se expandindo rapidamente. Os clientes satisfeitos recomendam seu salão para amigos de cabelos cacheados, que marcam visitas e também recomendam o salão para os próprios amigos.

Para acomodar o fluxo de novos pedidos, você contratou novos cabeleireiros. Junto com os colegas mais experientes, eles formaram um sindicato profissional para discutir diversos assuntos e se apoiar mutualmente.

Vários meses depois, os representantes desse sindicato solicitaram uma reunião com você e informaram sobre uma preocupação. Seus funcionários não conseguem descobrir por que alguns deles recebem bônus e outros não. Os critérios de bônus parecem vagos e inespecíficos. Você concorda que o problema existe. Os bônus são calculados manualmente, então alguns funcionários têm sorte, enquanto outros não. Isso não pode continuar assim.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PTcurly_excel_2.4.1_1_1660291079.png)

Você quer que seus cabeleireiros se sintam confortáveis e gostem de trabalhar em seu salão. É por isso que você decidiu tornar o processo de cálculo de bônus transparente, fazendo a transição do cálculo manual para o automático.

Até agora, sua gerente calcula os bônus usando esta planilha:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_3_1712321371.png)

Ela preenche o campo "Objetivo atingido" à mão, depois considera a categoria do funcionário, seu salário, o número de turnos e decide se o funcionário merece ou não um bônus.

Para tornar o processo mais objetivo, você precisa de regras específicas que ditam como exatamente a categoria, o número de turnos e o cumprimento da meta afetam o cálculo do bônus. Em uma planilha, as regras são substituídas por funções que avaliam se uma determinada condição foi atendida.

💡 Essas são chamadas de funções lógicas. As mais populares são `AND`, `OR` e `IF`.

Nesta lição, você vai aprender como usá-las e verá quais outros problemas você pode resolver com a ajuda delas.

## Como funcionam as funções lógicas

A operação de funções lógicas são baseadas em dois conceitos: condição e validação.

**Condição**

Existem apenas seis operadores lógicos, e você já conhece todos eles:

№

Nome

Exemplo

Símbolo

1

Igual a

a = b, 2 = 2

`=`

2

Diferente de

a ≠ b, 2 ≠ 3

`<>`

3

Maior que

a > b, 2 > 1

`>`

4

Menor que

a < b, 2< 3

`<`

5

Maior ou igual a

a ≥ b, x ≥ 2, 2 ≥ y

`>=`

6

Menor ou igual a

a ≤b, x ≤ 2, y ≤ 3

`<=`

**Validação**

A afirmação resultante de uma condição pode ser verdadeira (true) ou falsa (false). "Verdadeiro" e "Falso" são os conceitos fundamentais da lógica.

💡 Eles também são os termos básicos da programação de software. Eles não têm nenhuma conotação positiva ou negativa, servem apenas para indicar se uma afirmação está correta ou não.

Pergunta

**Quais das seguintes afirmações são verdadeiras e quais são falsas?**

3 = 2

3 <> 2

999 > 111111111111

888 >= 777

14 <= 14

Mostrar resposta

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-31-02-856Z.md
### Última modificação: 2025-05-28 18:31:03

# VLOOKUP - TripleTen

Capítulo 2/3

Módulo 2. Trabalhando com Planilhas

Opcional

# VLOOKUP

Agora que você aprendeu a processar seus dados e obter os valores adicionais, pode passar para tarefas mais complexas. Muitas vezes, mesmo dados processados adequadamente não são suficientes para tirar conclusões. A informação necessária talvez esteja armazenada em outras planilhas ou arquivos.

As próximas duas lições são dedicadas a resolver esses problemas. No final do módulo, você vai saber como unir tabelas, compará-las e mover dados de uma tabela para outra.

Nesta lição, você vai aprender a operar talvez a função mais conhecida e cobiçada no Excel e no Google Sheets, `VLOOKUP`, que significa "pesquisa vertical".

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PTcurly_excel_2.5_1660302144.png)

## Procurar um valor em um conjunto de dados

Comparar dois conjuntos de dados não numéricos é uma tarefa comum. Por exemplo, você tem um conjunto de dados com os e-mails de todos os clientes que já marcaram algum horário no seu salão e outro conjunto de dados com os e-mails de quem marcou um horário este mês. Você decidiu realizar uma pesquisa para descobrir por que uma parte da base de clientes não visitou seu salão este mês.

Este seria o algoritmo para concluir esta tarefa manualmente:

1.  Pegue o conjunto de dados com os e-mails dos clientes que visitaram o salão este ano.
2.  Localize o primeiro e-mail.
3.  Verifique se esse e-mail aparece no conjunto de dados com os clientes do mês.
4.  Rotule o e-mail com "Sim" ou "Não".
5.  Repita as etapas de 1 a 4 com cada e-mail no conjunto de dados.

No entanto, pesquisar e rotular cem e-mails em mil é tedioso e demora muito. Esse processo realmente precisa ser automatizado.

### 🛠️ Tarefa 1

1.  Abra [a planilha com importações](https://docs.google.com/spreadsheets/d/1KSPmALa4n4uYE7dZQVTkiXvFf10kwZU67w2-xOVqxVo/copy). A planilha `year` contém os e-mails dos clientes que visitaram o salão desde a sua abertura, e a planilha `month` contém apenas aqueles que marcaram um horário este mês.
2.  Siga as instruções no arquivo.
3.  Responda à pergunta.

Pergunta

**O que a fórmula fez? Escolha todas as opções corretas.**

Escolha quantas quiser

Ela retornou os e-mails únicos.

Ela colocou o rótulo `#N/A` ao lado dos e-mails dos clientes que não visitaram o salão este mês.

Ela duplicou os e-mails dos clientes que visitaram o salão este mês.

Ela contou os e-mails repetidos.

Ela copiou todos os e-mails de uma planilha para outra.

Mostrar resposta

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-31-04-163Z.md
### Última modificação: 2025-05-28 18:31:05

# Importar dados de outras tabelas - TripleTen

Capítulo 2/3

Módulo 2. Trabalhando com Planilhas

Opcional

# Importar dados de outras tabelas

Nesta lição, você vai continuar aprendendo a trabalhar em várias tabelas. Vamos nos aprofundar mais no assunto. Na lição anterior, trabalhou com várias tabelas em uma planilha. Desta vez, você vai trabalhar com várias planilhas separadas.

Às vezes, os dados de que você precisa podem estar contidos em dois arquivos diferentes. Claro que você pode copiá-los e uni-los, mas isso pode causar alguns problemas irritantes.

1.  Quando houver cópias extras de arquivos com acesso compartilhado ou público, alguém pode começar a fazer alterações em apenas uma cópia, deixando as outras desatualizadas.
2.  Os arquivos ficam sobrecarregados com planilhas desnecessárias. O trabalho com eles torna-se inconveniente.

## Importar todos os dados

Vamos começar com a coisa mais importante: como unir duas tabelas diferentes.

Vamos supor que você está trabalhando com tabelas que contêm listas de funcionários de departamentos diferentes. Você quer criar uma única tabela para todos os dados. Claro que você pode apenas copiar as tabelas, mas existe uma maneira muito melhor de fazer isso.

### 🛠️ Tarefa 1

1.  Abra as duas tabelas: [a lista de funcionários](https://docs.google.com/spreadsheets/d/1hzOzoWq7NtjHtLE_crbPMoVGrozQnCPwRWV4YXdt8cw/edit?usp=sharing) e [a lista de administradores](https://docs.google.com/spreadsheets/d/1OO4GZrdA6H0eWixVAVGUKdJ8uk-gOaNSYn9ETZ7yKeA/copy).
2.  Observe as configurações de acesso: o primeiro arquivo está no modo somente leitura, já o segundo deve ser copiado e editado. Esses arquivos serão necessários apenas para esta tarefa, então você pode excluí-los depois.
3.  Na planilha "Administradores", escreva o nome e o sobrenome de seu personagem fictício favorito de um livro.
4.  Agora, observe o que aconteceu com a planilha "Administradores" no arquivo "Lista de funcionários".

Que lista incomum…

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-31-05-442Z.md
### Última modificação: 2025-05-28 18:31:05

# Introdução - TripleTen

Capítulo 3/3 · Faltam 7 lições

Módulo 3. Visualização de dados

Opcional

# Introdução

Você fez um grande progresso na análise de dados, na criação de tabelas vinculadas e na preparação de relatórios. Neste módulo, você vai explorar um conjunto abrangente de ferramentas para formatação e visualização de dados.

Tarefa relacionadas a dados costumam ter um objetivo específico. Por exemplo, você talvez precise criar um modelo financeiro para investidores, fazer um relatório para seu o chefe, desenvolver uma tabela de contabilidade de custos para um administrador de loja ou coletar análises de clientes para os seus colegas. É por isso que a formatação de dados é tão importante quanto garantir que todos os seus cálculos estejam corretos. E uma visualização adequada facilita que outras pessoas entendam e tirem conclusões dos dados apresentados.

A análise de dados é um processo de identificação de dependências e relacionamentos entre diferentes tipos de dados. Grandes quantidades de dados são difíceis de analisar, e uma formatação inadequada dificulta ainda mais esse processo. Vamos dar uma olhada em um exemplo. Responda à pergunta abaixo.

Pergunta

**Qual é o maior número nesta lista?**

999979993

883902221

999991993

983783367

99999899

Mostrar a resposta

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-31-08-155Z.md
### Última modificação: 2025-05-28 18:31:08

# Formatação de tabelas - TripleTen

Capítulo 3/3 · Faltam 6 lições

Módulo 3. Visualização de dados

Opcional

# Formatação de tabelas

Suponhamos que você criou um relatório de receitas e despesas para um salão de beleza especializado em cabelos cacheados. Você apresentou o seu trabalho aos seus colegas com a esperança de que ele fosse útil. Mas depois da reunião, todo mundo se esqueceu dele.

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT_gallery_1_3.1_1_1661519398.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT_gallery_1_3.1_2_1661519404.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/gallery_1_3.1_3_1661378965.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/gallery_1_3.1_4_1661378967.png)

Mas por quê? Pode ter sido porque a tabela continha muitos dados e seus colegas de trabalho simplesmente não descobriram como usar ela. Confira o arquivo aqui:

### Preparação

Crie uma [cópia do relatório](https://docs.google.com/spreadsheets/d/1iM3-YwSoqNECmQHC_dpI0e_0XB8yMmcm7AW6-jnNLZ8/copy) abaixo. Você vai trabalhar com ele nas próximas três lições.

Não se surpreenda ao ver dados de 2022 e 2023. O relatório é apenas um exemplo.

É... Há muitos números e eles não fazem muito sentido.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-31-09-469Z.md
### Última modificação: 2025-05-28 18:31:09

# Tipos de gráficos - TripleTen

Capítulo 3/3 · Faltam 5 lições

Módulo 3. Visualização de dados

Opcional

Na lição anterior, você aprendeu sobre formatação de tabela. É hora de passar para a visualização de dados, um tópico amplo que requer um curso separado só sobre ele para dominar o assunto. Neste módulo, vamos abordar o básico, e cabe a você escolher se deseja explorar mais esse campo ou não.

# Tipos de gráficos

Vamos voltar ao relatório de receitas e despesas da lição anterior. Ele parece mais limpo agora e é mais agradável de trabalhar com ele. Você e seus funcionários o utilizam com mais frequência para analisar o trabalho do seu salão. Você pode ver que seu negócio está indo muito bem.

De repente, você recebe uma ligação de um amigo próximo dizendo que ele está vendo muitas avaliações positivas do seu salão. Na verdade, ele está interessado em abrir uma nova filial do seu salão no bairro dele.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/curly_excel_1_3.2_1_1661379036.png)

Você e suas parceiras de negócios discutem a oferta e decidem vender uma franquia. Parece empolgante, né?

Sim! Mas por onde começar?

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-31-10-770Z.md
### Última modificação: 2025-05-28 18:31:11

# Configurações de gráficos - TripleTen

Capítulo 3/3 · Faltam 4 lições

Módulo 3. Visualização de dados

Opcional

# Configurações de gráficos

💡 Você vai trabalhar com a tabela que criou na primeira lição, sobre a formatação de tabelas.

Se você já fechou a tabela, pode encontrá-la na aba [Recentes](https://drive.google.com/drive/recent) no [Google Drive](https://drive.google.com/drive/my-drive) ou digitando "Tipos de Gráficos" na barra de pesquisa.

Na lição anterior, você aprendeu sobre a finalidade de três tipos comuns de gráficos. Nesta lição, vamos abordar as principais configurações dos gráfico no Google Sheets.

As configurações dos gráficos são similares em diferentes editores de planilhas. Por exemplo, se você estiver trabalhando no Excel, encontre as abas correspondentes na sua interface e aplique as mesmas configurações do Google Sheets.

Lembra que seu salão está planejando desenvolver uma franquia? Você precisa ter certeza de que os empresários de outras regiões terão interesse em comprar a sua franquia. Os dados mostram que seu modelo de negócios é bem-sucedido. No entanto, se você quiser que seus possíveis franqueados acreditem nisso, você precisa apresentar esses dados de forma visualmente atrativa.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/curly_excel_4_3.3_1_1661379111.png)

Você é responsável por fazer um gráfico para a apresentação que contém a proposta de negócio. É necessário destacar que a receita total do salão está aumentando a cada mês.

Pergunta

**Qual tipo de gráfico é mais adequado para essa tarefa?**

Gráfico de linhas

Gráfico de barras

Gráfico de pizza

Mostrar a resposta

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-31-13-741Z.md
### Última modificação: 2025-05-28 18:31:14

# Personalização de gráficos - TripleTen

Capítulo 3/3

Módulo 3. Visualização de dados

Opcional

# Personalização de gráficos

Na lição anterior, você criou gráficos para apresentar ao seu amigo. Esse foi um caso em que você precisou de uma visão geral resumida das suas tabelas para apresentar para alguém de fora. No entanto, é igualmente importante saber como adicionar gráficos aos dashboards nas suas planilhas.

O gerente do seu salão de cabeleireiro pediu que você adicionasse gráficos ao [dashboard que você criou no primeiro módulo deste curso](https://docs.google.com/spreadsheets/d/1kxFNw4csApPgkw3jZSd0VN_I1JQbZnB3NrhrIKfhLtg/edit?usp=sharing). Como você deve lembrar, o seu dashboard ficou assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/3.4.1_1662383052.png)

Analisar esse dashboard será muito mais rápido e fácil se você adicionar alguns gráficos a ele.

Nesta lição, você vai criar um dashboard contendo um gráfico de linhas e dois gráficos de barras interativos. O gráfico de linhas vai ajudar você e seus amigos a examinar a dinâmica de vendas do mês atual, facilitando a comparação com o período anterior e as vendas de referência.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/3.4.22_1662386950.png) _Clique na imagem para ampliar._

O primeiro gráfico de barras vai mostrar a distribuição de receita semanal. Se você alterar a data no campo `Hoje`, o gráfico será atualizado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Excel_sprint3/pt/4/Скрин-1.gif?etag=a132d5b723efd7d2554bdd9435ac1b56)

O segundo gráfico de barras vai mostrar a distribuição da receita total desde o primeiro dia do mês até a data de hoje. Por exemplo, se tivermos um valor de 9.09.2021 em `Hoje`, três barras vão aparecer no gráfico de barras, indicando a receita dos primeiros nove dias do mês atual, do mês anterior e a receita média. Você também vai poder atualizar esse gráfico de barras conforme a data indicada no dashboard.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Excel_sprint3/pt/4/Скрин-2.gif?etag=26602d0bcfbb80c9e31b6921a463a2e6)

O objetivo dessa lição é apresentar configurações avançadas do editor de gráficos. Além disso, você vai aprender a processar dados para criar gráficos e tabelas com atualização automática. Você já conhece as principais ferramentas de processamento de dados: as funções `SUMIFS`, `IF`, `VLOOKUP`, `UNIQUE`, funções de data e hora, bem como o recurso de validação de dados. Incluímos uma revisão delas para garantir que o conteúdo esteja fresco na sua memória.

Vamos lá!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-31-15-086Z.md
### Última modificação: 2025-05-28 18:31:15

# Avaliação Final - TripleTen

Capítulo 3/3

Módulo 3. Visualização de dados

Opcional

# Avaliação Final

Pergunta

1.  Qual indicador NÃO é exibido na área de AutoCalculate?

Média

Mediana

Soma

Min

Contagem de números

Mostrar a resposta

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-31-16-427Z.md
### Última modificação: 2025-05-28 18:31:16

# Conclusão - TripleTen

Capítulo 3/3

Módulo 3. Visualização de dados

Opcional

# Foi um desafio, mas você conseguiu!

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_curly_excel_7_1635991725.jpg)

Na verdade, é mais difícil completar cursos gratuitos do que pagos. Como as aulas ficam sempre à mão, você está convencido de que pode voltar aos estudos a qualquer momento, mas você achará isso cada vez mais difícil à medida que os dias passam. A vida, o trabalho e outras responsabilidades podem atrapalhar.

Ao contrário dos cursos pagos, não há prazos ou algum gerente de comunidade para entrar em contato com você para ter certeza de que está indo bem e lembrar de quaisquer lições que ainda não tenha concluído.

Os cientistas da Universidade de Harvard e do MIT realizaram um estudo sobre os cursos online abertos ao longo de quatro anos (2012-2016). [De acordo com o estudo](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2889436), em média, a taxa de conclusão foi de apenas 7,7%. Então, o fato de você ter chegado ao final deste curso é uma prova de sua dedicação e força de vontade!

TripleTen

Vamos pensar nas dicas que você pode dar para aqueles que estão com dificuldades para concluir o curso.

Nós vamos primeiro, e então você pode compartilhar sua experiência com a gente abaixo.

Reunimos algumas dicas com base em nossa experiência e adoraríamos receber as suas também. Na parte inferior da página, conte-nos como você conseguiu completar o curso com sucesso. Como você encontrou tempo, como organizou seu local de trabalho, como explicou a amigos e familiares que passará parte do seu tempo livre nos estudos. Como estudar de forma eficaz?

1.  **Não pare no começo.** Não é a melhor ideia marcar um curso esperando que você tenha mais tempo para ele algum dia. Esse dia parecerá estar sempre fora de alcance.
2.  **Evite fazer longas pausas entre os estudos**. Dar o primeiro passo é tão importante quanto criar o hábito de estudar regularmente. Se você adiar a próxima aula por mais de uma semana, é menos provável que retorne a ela com o mesmo nível de entusiasmo e motivação.
3.  **Mantenha um ritmo constante**. A melhor estratégia é completar algumas lições nos próximos dias e terminar com o primeiro módulo no fim de semana.

TripleTen

Compartilhe suas dicas sobre como se comprometer com um curso.

Pergunta

Como você organizou seu tempo e espaço para este curso? Como você explicou à sua família e amigos que você está ocupado e não pode ser incomodado? Você se recompensou por concluir cada módulo, como tomar uma xícara de café ou sua sobremesa favorita? Talvez você tenha um plano de autodesenvolvimento pessoal e os cursos online façam parte dele?

Gostaríamos muito de sabê-lo e vamos ler todos os seus comentários! 🖤

Enviar

Você fez um ótimo trabalho! Não desista e siga em frente!

Ficaremos felizes em vê-lo novamente.

Boa sorte!

🖤 Equipe do TripleTen

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-32-39-977Z.md
### Última modificação: 2025-05-28 18:32:40

# Conheça a sua nova profissão - TripleTen

Capítulo 1/9

Conheça a sua nova profissão

# Conheça a sua nova profissão

### Introdução

Daniel

Olá! É bom ter você por aqui. Meu nome é Daniel. Sou representante da TripleTen e vou ajudar você a dar os primeiros passos nessa incrível jornada rumo à sua nova carreira no mundo da ciência e da análise de dados. Nesse novo capítulo na sua vida, você vai iniciar uma caminhada cheia de oportunidades, descobertas e a chance de deixar a sua marca em diversas áreas.

Como cientista ou analista de dados em formação, você vai aprender a explorar todo o potencial dos dados para descobrir insights, prever tendências e contribuir com inovações que podem transformar vidas. Além de dominar a análise de dados, você vai aprender a interpretar e a comunicar suas descobertas de muitas formas que podem influenciar decisões e estratégias. Os dados são a chave para entender e melhorar o mundo, e nós estamos aqui para te ajudar a usá-los na construção de um futuro melhor.

Essa jornada vai ser desafiadora, mas também muito gratificante. Você vai encontrar problemas complexos que vão botar à prova suas habilidades analíticas, criatividade e persistência. Mas não se preocupe. Você não está sozinho nessa. Nossa equipe vai estar ao seu lado em cada etapa do caminho, e nossa comunidade de estudantes e profissionais dedicados vão estar a postos para ajudar, inspirar e crescer junto com você.

Boas-vindas aos programas de Ciência de Dados e de Análise de Dados da TripleTen! Mal posso esperar para ver tudo o que você vai conquistar!

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://youtube.com/embed/K5GZf4B829o"></iframe>

Script do vídeo

Boas-vindas ao mundo da ciência e da análise de dados! Ele está cheio de possibilidades e de insights esperando por você.

Você está mudando para um ramo que pode moldar o futuro. São carreiras em que suas ideias podem revolucionar setores, melhorar vidas e fomentar a inovação ao revelar padrões ocultos, prever tendências futuras e tomar decisões com base em dados.

Somos cientistas e analistas de dados.

O cientista de dados é um mestre do desconhecido, transformando dados brutos em insights úteis. Esse profissional cria modelos complexos, desenvolve algoritmos avançados e usa aprendizado de máquina para prever resultados e resolver problemas difíceis.

Por outro lado, o analista de dados é mestre em sua interpretação, fornecendo resultados analíticos e ajudando organizações a tomar decisões fundamentadas. Esse profissional "traduz" os dados, transformando conjuntos de dados complexos em descobertas úteis e fáceis de entender.

O começo dessas duas jornadas é compartilhado, pois o conhecimento básico necessário para ter sucesso em ambas as áreas é muito parecido, assim como as primeiras etapas em qualquer projeto de dados. Como tal, você vai trabalhar em conjunto com outros estudantes de ciência e de análise de dados no primeiro módulo do curso, desenvolvendo a base teórica necessária para depois seguir o conteúdo específico da sua carreira escolhida.

Tanto nos projetos de análise de dados (DA) quanto nos de ciência de dados (DS), seguir uma abordagem estruturada é essencial para chegar a descobertas precisas e relevantes. Seja você analista ou cientista de dados, o certo é que seguir um processo bem-definido garante uma análise abrangente que atenda às necessidades do cliente. É isso que vou ensinar a você no primeiro módulo do curso.

A primeira etapa em qualquer projeto de dados é compreender o que o cliente realmente precisa. Isso envolve discussões detalhadas com esse cliente para entender seus objetivos de negócio e identificar as perguntas para as quais precisamos de respostas.

Depois de entender bem esses requisitos, vamos à próxima etapa, que é coletar os dados relevantes. Isso envolve identificar fontes de dados, extrair e unir esses dados em um conjunto único e bem organizado.

Dados de alta qualidade são a base de qualquer análise. Combinar dados de diferentes fontes permite uma análise mais abrangente e garante que todas as informações relevantes sejam consideradas.

Em seguida, a etapa de preparação envolve limpar, formatar e padronizar os dados para garantir que eles estejam prontos para análise. Isso pode incluir lidar com valores ausentes, corrigir erros e converter dados para um determinado formato exigido.

Com os dados preparados, a etapa de análise começa. Isso envolve a aplicação de métodos estatísticos usando linguagens de programação como Python, além do uso de técnicas como testes de hipótese e segmentação de dados para extrair respostas que fazem sentido para o problema do cliente.

Você vai aprender a fazer tudo isso nas próximas semanas e meses, adicionando cada nova habilidade ao que eu chamo de “caixa de ferramentas” em comum. Essas ferramentas são a base do seu trabalho, seja você analista ou cientista de dados, pois ambas as profissões seguem essas etapas para garantir uma abordagem sistemática em projetos de dados.

Depois do primeiro módulo, a aprendizagem compartilhada vai continuar para aprendermos a usar SQL e outras ferramentas importantes. Elas ajudam a gerir dados com eficiência, além de executar consultas avançadas.

É aí que os caminhos começam a se diferenciar um pouco.

Como o principal objetivo do analista de dados é interpretá-los e fornecer insights úteis que ajudam empresas a tomar decisões mais certeiras, os estudantes de análise de dados vão aprender a usar ferramentas de business intelligence (BI) como o Tableau, além de aprofundar seus conhecimentos em Python para análises estatísticas e testes mais complexos.

Como analista de dados, você vai adicionar habilidades como análise de negócios, Tableau e teste A/B à sua caixa de ferramentas, já que são técnicas e recursos amplamente usados na área.

Já o cientista de dados tem como foco o uso dos dados para criar modelos e algoritmos preditivos capazes de prever tendências futuras. Com frequência, esse profissional lida com conjuntos de dados maiores e mais complexos, aplicando técnicas avançadas de estatística e aprendizado de máquina.

Para o cientista de dados, análise de regressão, séries temporais e redes neurais são ferramentas e técnicas essenciais. Portanto, em breve você vai adicionar tudo isso à sua caixa de ferramentas de cientista de dados!

Começar uma jornada na análise ou na ciência de dados é como abrir uma porta para um universo cheio de possibilidades. Ao iniciar essa caminhada incrível, lembre-se de que cada novo conjunto de dados é como um baú de tesouros, esperando para ser explorado. Então, mantenha a curiosidade ativa, encare os desafios de braços abertos e nunca pare de aprender. Nesse mundo movido por dados, precisamos de mentes criativas e analíticas como a sua. Mergulhe de cabeça e veja como é possível transformar dados brutos em histórias e soluções poderosas que podem impactar o mundo de verdade.

E não se preocupe: eu vou estar aqui para acompanhar você até que consiga seu primeiro emprego na área de dados!

### Resumo do vídeo

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/1.1._img1.1_1748437277.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/1.1._img1.2_1748437281.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/1.1._img1.3_1748437283.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/1.1._img1.4_1748437286.png)

Você aprendeu as principais etapas e habilidades necessárias para projetos de análise e de ciência de dados. As etapas em comum para analistas e cientistas de dados são entender as necessidades do cliente, reunir e combinar dados, além de preparar os dados com sua limpeza e formatação. Essa preparação permite analisar os dados com eficiência e encontrar respostas para perguntas importantes, revelando insights valiosos.

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/1.1._img2.1_1748437323.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/1.1._img2.2_1748437326.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/1.1._img2.3_1748437420.png)

Você descobriu as diferenças entre analistas de dados (DAs) e cientistas de dados (DSs): DAs têm foco na interpretação de dados para a tomada de decisões de negócios usando ferramentas como SQL e Tableau, ao passo que DSs criam modelos preditivos com técnicas avançadas, como aprendizado de máquina. Em ambas as carreiras, habilidades de Python, estatística e SQL são necessárias.

Este guia deu um roteiro para sua carreira no mundo dos dados, reforçando a importância de manter a curiosidade em alta, continuar aprendendo e aplicar seu conhecimento para tomar decisões importantes com base em dados.

### Próximos passos

Daniel

Agora que você deu esse primeiro passo fundamental, é hora de continuar avançando na sua jornada de aprendizagem e crescimento profissional. Lembre-se: você vai ter companhia nessa aventura. Eu, nossos tutores dedicados e nossa rede de profissionais de ponta estão a postos para ajudar você em todas as etapas dessa jornada. Seja para encarar desafios ou dividir conquistas, seus colegas também estão disponíveis para compartilhar experiências e ideias. Encare essa oportunidade com confiança e entusiasmo, sabendo que cada lição deixa você mais perto de atingir seus objetivos. Continue se esforçando, mantenha a curiosidade em alta e nunca deixe de pedir ajuda sempre que necessário. A seguir, você vai ver a introdução ao primeiro módulo do curso, todos os seus detalhes e muito mais! Sua jornada começa agora, e estamos todos aqui para garantir que ela seja uma jornada de sucesso.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-32-41-353Z.md
### Última modificação: 2025-05-28 18:32:41

# Módulo 1 – Introdução à análise de dados - TripleTen

Capítulo 1/9

Conheça a sua nova profissão

# Módulo 1 – Introdução à análise de dados

Daniel

Tudo pronto para encarar um novo desafio? Nesta lição, vou mostrar o que aguarda você no primeiro módulo do nosso programa. Você vai conhecer a estrutura do módulo, quanto tempo cada etapa vai levar, os eventos online incríveis de que vai participar e o ambiente de aprendizagem onde tudo isso acontece. Dessa maneira, vai ser possível planejar seu tempo com eficiência, manter o ritmo e aproveitar ao máximo a sua jornada de aprendizado.

Mas antes disso, que tal darmos uma olhada no primeiro grande marco da sua jornada rumo a uma carreira como analista de dados? Vejamos como vai ser o projeto do primeiro módulo.

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://youtube.com/embed/vdACa6c_NmU"></iframe>

Daniel

É muita informação de uma vez só, não é? Deixe eu resumir tudo para você. Este é o conjunto de habilidades do primeiro módulo. Aqui estão descritas todas as habilidades que você vai adquirir e as tarefas de análise de dados relacionadas:

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/1.1._img1.1_1748437692.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/1.1._img1.2_1748437694.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/1.1._img1.3_1748437696.png)

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/1.1._img1.4_1748437698.png)

Daniel

Talvez você se pergunte: "Só vou ficar treinando na plataforma por conta própria?" Não mesmo! Acreditamos na força da comunidade e no poder de aprender com quem tem experiência de verdade. Por isso, organizamos uma série de eventos online e oportunidades de coaprendizagem. Além disso, temos uma equipe de profissionais brilhantes para ajudar você ao longo da jornada.

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://youtube.com/embed/wBzorWhEi-c"></iframe>

Daniel

Agora você está com tudo pronto para começar a aprender! Lembre-se de verificar os anúncios no Discord e ficar de olho na agenda de eventos online na plataforma. Agora, vá até sua primeira lição sobre Python e aprenda sobre um assunto essencial: variáveis. Até mais!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-32-42-699Z.md
### Última modificação: 2025-05-28 18:32:43

# Introdução à sandbox e ao Jupyter Notebook - TripleTen

Capítulo 1/9

Conheça a sua nova profissão

# Introdução à sandbox e ao Jupyter Notebook

### Introdução

Boas-vindas à lição sobre a sandbox da TripleTen e o Jupyter Notebook, duas ferramentas essenciais na sua jornada em busca de dominar os dados. Nesta lição, vamos ver o básico do Jupyter Notebook, explorar por que ele é tão importante para a análise de dados e mostrar suas primeiras linhas de código Python. Também vamos apresentar a sandbox da TripleTen, mostrando como ela vai ajudar a sua experiência de aprendizagem prática. Esse conhecimento básico vai ser fundamental para as próximas sessões ao vivo e projetos futuros.

### O que você vai aprender

Ao final desta lição, você vai aprender o seguinte:

-   O que é Jupyter Notebook e por que ele é importante para a análise de dados;
-   Como navegar na interface do Jupyter e executar código Python;
-   Como escrever código Python em um notebook;
-   Como usar células Markdown para escrever notas e documentação no notebook;
-   O que é sandbox e como ela pode auxiliar a sua experiência de aprendizagem.

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://youtube.com/embed/44gpdNjH5oo"></iframe>

### Teoria

## Introdução à sandbox

A sandbox é um ambiente poderoso e versátil desenvolvido para melhorar a sua experiência de aprendizagem e programação.

Ela tem dois recursos principais:

-   Seu IDE pessoal;
-   Seu notebook Jupyter pessoal.

O primeiro recurso, _seu IDE pessoal_, é um ambiente de desenvolvimento integrado online em que é possível escrever, testar e depurar snippets pequenos de código Python em tempo real. Ele é ótimo para fazer experimentos com conceitos de programação específicos, testar funções individuais ou resolver problemas das lições. Por exemplo, se estiver aprendendo a implementar algoritmos de ordenação ou se estiver testando um novo método, o IDE é o lugar ideal para executar e ajustar rapidamente seu código. Ele foi feito para testes rápidos, em que é possível ver resultados imediatos e fazer iterações rapidamente. Dessa forma, é a ferramenta perfeita para fazer experimentos em pequena escala ou para depurar problemas específicos.

Por outro lado, o segundo recurso, _seu notebook Jupyter pessoal_, é mais adequado a tarefas maiores e mais estruturadas. Pense nele como uma ferramenta para "contar uma história" com seu código. Notebooks Jupyter permitem escrever códigos em partes (células) que podem ser combinadas com rich text, equações e visualizações. Por exemplo, ao analisar um conjunto de dados, você pode começar carregando os dados em uma célula, aplicar uma função de limpeza de dados na próxima, visualizar tendências em outra célula e assim por diante, além de documentar cada descoberta entre uma etapa e outra. Portanto, essa ferramenta é ideal para quando é necessário experimentar com partes diferentes de uma análise complexa ou para quando queremos apresentar nosso código, resultados e pensamentos de uma maneira coesa e de fácil compreensão.

Em resumo, o IDE é perfeito para experimentos rápidos com partes de código individual, e o Notebook Jupyter é ideal para quando é necessário criar, testar e documentar fluxos de trabalho mais complexos baseados em dados de forma estruturada e interativa.

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/1.3.img1PT_1735821296.png)

A sandbox pode ser encontrada no fim da lista de conteúdos como um módulo adicional. Você pode acessá-la diretamente ao abrir os links.

-   [Link do seu IDE pessoal](https://tripleten.com/trainer/data-analyst/lesson/f280f523-f9c4-449a-a7f6-ec5974fd3815/task/be8f7293-9caa-4028-bf31-e6905ca57e52/).
-   [Link do seu notebook Jupyter pessoal](https://tripleten.com/trainer/data-analyst/lesson/0d12fac3-d2ad-4b8b-b87b-22a045384fc3/task/39eec815-e0cd-4946-861c-c8cb3d48baad/).

Favorite os links e deixe sua opção preferida aberta em uma aba separada. Assim, você pode testar e praticar seu código com facilidade enquanto estuda! Dessa forma, é possível ter acesso rápido ao seu IDE ou notebook Jupyter quando a inspiração bater à porta, permitindo que você faça experimentos, aprimore suas ideias e reforce o que está aprendendo em tempo real.

Tanto o IDE quanto o notebook Jupyter favorecem uma abordagem iterativa e prática de aprendizagem. É possível experimentar livremente, ajustar seu código, testar hipóteses e ver resultados imediatos. Essa mentalidade de executar e testar é fundamental para desenvolver uma compreensão mais aprofundada de conceitos de programação. É como se você tivesse uma tela em branco para explorar novas ideias sem medo de errar, o que é essencial para evoluir como profissional de dados.

## Introdução ao Jupyter Notebook

Boas-vindas à introdução ao Jupyter Notebook, uma das ferramentas mais usadas em análise de dados. Nesta lição, vamos explicar o que é Jupyter Notebook e mostrar suas funções básicas. Vamos abordar como usar os dois principais tipos de células (células de código e Markdown), e você vai aprender a escrever seus primeiros comandos em Python na interface do notebook. Também vamos descobrir como salvar e compartilhar seu trabalho, algo essencial para projetos colaborativos de dados.

Vamos começar entendendo a principal finalidade do Jupyter Notebook.

O Jupyter Notebook é um aplicativo web de código aberto que permite que os usuários criem documentos contendo código ativo, equações, visualizações e texto. Ele é muito usado na análise e na ciência de dados devido à sua natureza interativa, que faz com que seja fácil fazer experimentos e documentar seu processo em tempo real.

Imagine que você recebeu a tarefa de analisar um conjunto de dados e apresentar seus insights na forma de visualizações. Onde fazer isso? Se você respondeu "Jupyter Notebook", acertou! Com sua capacidade de executar código Python em "células de código" e exibir os resultados diretamente abaixo, o ambiente permite escrever e executar sua análise, bem como gerar gráficos e visualizar dados em um único documento. Além disso, é possível documentar seu processo com rich text formatado em "células Markdown". Assim, é muito fácil explicar suas conclusões, descrever as etapas e apresentar um relatório abrangente e compartilhável.

Essa combinação de execução de código, visualização e documentação faz com que o Jupyter Notebook seja a ferramenta ideal para tarefas baseadas em dados e apresentações de projetos.

Os notebooks são organizados em células que podem conter código ou Markdown (para texto formatado).

**Células de código:** executam código Python de modo interativo. **Células Markdown:** formatam texto usando a sintaxe Markdown.

Por exemplo:

-   **Célula de código** `python 2 + 2` O resultado será `4`.
-   **Célula Markdown** `markdown # Meu primeiro projeto de análise de dados` Isso vai criar um cabeçalho no seu notebook.

Nos notebooks Jupyter, há dois modos para cada célula. Elas podem estar no modo de edição, em que é possível fazer mudanças no conteúdo, ou no modo de comando, em que é possível executar o código e fazer alterações na célula inteira, como copiar, excluir etc.

No modo de comando, há uma borda cinza com uma barra azul. Já no modo de edição, a borda é verde, e o cursor fica piscando.

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/1.3.img2PT_1735821352.png)

Você vai usar notebooks Jupyter com frequência na sua jornada de aprendizagem, solucionando projetos e programando nas sessões ao vivo. Então, dedique-se para conhecer bem essa ferramenta essencial!

Para ver mais informações sobre o Jupyter Notebook e todas as suas possibilidades, confira o vídeo, se ainda não fez isso, e dê uma olhada na [documentação oficial](https://jupyter-notebook.readthedocs.io/en/latest/).

### Resumo

Nesta lição, exploramos o **Jupyter Notebook**, uma ferramenta fundamental para a análise de dados. Você aprendeu sobre seus recursos interativos, incluindo células de código e Markdown, e praticou a escrita de código Python. Também demonstramos como o Jupyter Notebook é essencial para analisar e documentar seu trabalho com dados.

Além disso, você descobriu o que é um ambiente de sandbox, como acessá-lo e por que esse é um recurso tão importante para aprender a programar. Transforme a sandbox em uma parte essencial da sua jornada de aprendizagem, pois é uma ferramenta-chave que vai ajudar a reforçar suas habilidades e aumentar sua confiança à medida que você avança!

### Terminologia

-   **Célula de código**: uma célula no Jupyter em que é possível escrever e executar código.
-   **Célula Markdown**: uma célula usada para escrever texto formatado, cabeçalhos e notas.
-   **Notebook Jupyter**: um ambiente interativo usado para programar, fazer visualizações e escrever texto em um só lugar.

### Próximos passos

Experimente! Na próxima lição, vamos dar acesso ao notebook que você acabou de ver para que possa fazer experimentos com ele.

Teste alguns códigos básicos e execute-os usando os atalhos. Tente criar células e escrever em Markdown. Essas habilidades serão úteis no projeto, quando você estiver trabalhando em um ambiente de notebook Jupyter.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-32-50-994Z.md
### Última modificação: 2025-05-28 18:32:51

# Experimente o notebook Jupyter - TripleTen

Experimente o notebook Jupyter

Tarefa

# Experimente o notebook Jupyter

### Introdução

Experimente! Aqui está o notebook que você acabou de visualizar. Agora, é possível fazer seus próprios experimentos.

Teste alguns códigos básicos e execute-os usando os atalhos. Tente criar células e escrever em Markdown. Essas habilidades serão úteis no projeto, quando você estiver trabalhando em um ambiente de notebook Jupyter.

<iframe class="page-lesson-jupyter__frame" title="Jupyter" src="https://jupyterhub.tripleten-services.com/user/user-3-f213ea5c-94ca-4f29-9c46-4771fbdad1ea/notebooks/ad8a3516-db36-4c5b-8aee-b5c3c13de881.ipynb?token=6258b16329b64fd5a8cab1e809b5241a"></iframe>

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-32-57-606Z.md
### Última modificação: 2025-05-28 18:32:58

# Experimente o notebook Jupyter - TripleTen

Experimente o notebook Jupyter

Tarefa

# Experimente o notebook Jupyter

### Introdução

Experimente! Aqui está o notebook que você acabou de visualizar. Agora, é possível fazer seus próprios experimentos.

Teste alguns códigos básicos e execute-os usando os atalhos. Tente criar células e escrever em Markdown. Essas habilidades serão úteis no projeto, quando você estiver trabalhando em um ambiente de notebook Jupyter.

<iframe class="page-lesson-jupyter__frame" title="Jupyter" src="https://jupyterhub.tripleten-services.com/user/user-3-f213ea5c-94ca-4f29-9c46-4771fbdad1ea/notebooks/ad8a3516-db36-4c5b-8aee-b5c3c13de881.ipynb?token=6258b16329b64fd5a8cab1e809b5241a"></iframe>

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-33-00-264Z.md
### Última modificação: 2025-05-28 18:33:00

# Por que Python? - TripleTen

Capítulo 2/9

Variáveis, Tipos de Dados, e Operações Aritméticas

# Por que Python?

<iframe class="base-markdown-iframe__iframe" id="player-PKgAzRuOZYk" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Why Python" width="640" height="360" src="https://www.youtube.com/embed/PKgAzRuOZYk?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Ff64c5dc1-3e3c-4519-8656-2f4647b1fcb5%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

O Python é uma das linguagens de programação mais usadas no mundo, principalmente entre profissionais da área de dados. No projeto do programa, você vai usar o Python para concluir cada uma das tarefas. Estamos falando sério quando dizemos que ele é muito importante! E no final desta lição, você vai escrever sua primeira linha de código Python.

A linguagem Python, criada por Guido van Rossum, um desenvolvedor holandês, foi chamada assim em homenagem ao grupo humorístico britânico "Monty Python".

## O que faz o Python tão especial?

Muitas pessoas gostam do Python devido a sua sintaxe, que se aproxima da linguagem humana. Em outras palavras, escrevemos instruções no Python de forma parecida como conversamos no dia-a-dia. Seria difícil encontrar uma ferramenta mais concisa para a expressão de regras e instruções.

Vamos tomar como exemplo clássico a tarefa de mostrar na tela a frase "Olá, mundo!" (Em inglês, "Hello, world"). Tudo o que você precisa é de uma linha de código.

Basta colocar o texto `Olá, mundo!` entre aspas abaixo, então você tem a seguinte linha de código: `print('Olá, mundo!')`. Em seguida, clique no botão **Verificar**.

CódigoPYTHON

9

1

print('Olá, mundo!')

Dica

Mostrar a soluçãoValidar

A propósito, suas próximas práticas em Python serão semelhantes à que você acabou de fazer. Sempre vamos fornecer uma breve descrição da tarefa e, em seguida, solicitaremos que você conclua a tarefa. Se o resultado passar no teste, ele é aceito. Se tiver algum erro, sempre exibimos um aviso, dizendo exatamente o que está acontecendo para que você possa corrigir e reenviar a solução. Muito legal, não é?

Por enquanto, vamos continuar aprendendo mais sobre Python.

## O que mais torna o Python tão popular?

O Python é popular devido a sua flexibilidade e versatilidade. Quer enviar e-mails em horário programado, coletar dados de uma rede social ou obter textos de sites? O Python tem soluções prontas e ferramentas eficientes para praticamente qualquer tarefa.

Além disso, ele é constantemente melhorado e ampliado, o que o torna um instrumento dinâmico e capaz de enfrentar uma grande variedade de desafios.

Graças a essas vantagens, o Python pode ser empregado em diversas áreas: desenvolvimento web e de jogos, aprendizado de máquina, análise de dados, gerenciamento de dados e muito mais. Até mesmo alguns sites populares, como Instagram, Facebook, Spotify, Pinterest e Netflix, foram escritos parcialmente em Python.

Em outras palavras, dominar a linguagem Python significa mais oportunidades de emprego. Neste momento, o site [indeed.com](http://indeed.com/) possui mais de cem mil vagas que têm Python como requisito!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-36-57-568Z.md
### Última modificação: 2025-05-28 18:36:58

# Por que Python? - TripleTen

Capítulo 2/9

Variáveis, Tipos de Dados, e Operações Aritméticas

# Por que Python?

<iframe class="base-markdown-iframe__iframe" id="player-PKgAzRuOZYk" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Why Python" width="640" height="360" src="https://www.youtube.com/embed/PKgAzRuOZYk?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Ff64c5dc1-3e3c-4519-8656-2f4647b1fcb5%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=6"></iframe>

Teoria

O Python é uma das linguagens de programação mais usadas no mundo, principalmente entre profissionais da área de dados. No projeto do programa, você vai usar o Python para concluir cada uma das tarefas. Estamos falando sério quando dizemos que ele é muito importante! E no final desta lição, você vai escrever sua primeira linha de código Python.

A linguagem Python, criada por Guido van Rossum, um desenvolvedor holandês, foi chamada assim em homenagem ao grupo humorístico britânico "Monty Python".

## O que faz o Python tão especial?

Muitas pessoas gostam do Python devido a sua sintaxe, que se aproxima da linguagem humana. Em outras palavras, escrevemos instruções no Python de forma parecida como conversamos no dia-a-dia. Seria difícil encontrar uma ferramenta mais concisa para a expressão de regras e instruções.

Vamos tomar como exemplo clássico a tarefa de mostrar na tela a frase "Olá, mundo!" (Em inglês, "Hello, world"). Tudo o que você precisa é de uma linha de código.

Basta colocar o texto `Olá, mundo!` entre aspas abaixo, então você tem a seguinte linha de código: `print('Olá, mundo!')`. Em seguida, clique no botão **Verificar**.

CódigoPYTHON

9

1

print('Olá, mundo!')

Dica

Mostrar a soluçãoValidar

A propósito, suas próximas práticas em Python serão semelhantes à que você acabou de fazer. Sempre vamos fornecer uma breve descrição da tarefa e, em seguida, solicitaremos que você conclua a tarefa. Se o resultado passar no teste, ele é aceito. Se tiver algum erro, sempre exibimos um aviso, dizendo exatamente o que está acontecendo para que você possa corrigir e reenviar a solução. Muito legal, não é?

Por enquanto, vamos continuar aprendendo mais sobre Python.

## O que mais torna o Python tão popular?

O Python é popular devido a sua flexibilidade e versatilidade. Quer enviar e-mails em horário programado, coletar dados de uma rede social ou obter textos de sites? O Python tem soluções prontas e ferramentas eficientes para praticamente qualquer tarefa.

Além disso, ele é constantemente melhorado e ampliado, o que o torna um instrumento dinâmico e capaz de enfrentar uma grande variedade de desafios.

Graças a essas vantagens, o Python pode ser empregado em diversas áreas: desenvolvimento web e de jogos, aprendizado de máquina, análise de dados, gerenciamento de dados e muito mais. Até mesmo alguns sites populares, como Instagram, Facebook, Spotify, Pinterest e Netflix, foram escritos parcialmente em Python.

Em outras palavras, dominar a linguagem Python significa mais oportunidades de emprego. Neste momento, o site [indeed.com](http://indeed.com/) possui mais de cem mil vagas que têm Python como requisito!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-36-58-881Z.md
### Última modificação: 2025-05-28 18:36:59

# Variáveis - TripleTen

Capítulo 2/9

Variáveis, Tipos de Dados, e Operações Aritméticas

# Variáveis

<iframe class="base-markdown-iframe__iframe" id="player-M6N18rKyOsU" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Variables" width="640" height="360" src="https://www.youtube.com/embed/M6N18rKyOsU?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F3f94ff1e-5da5-472c-8910-6b9b447d5170%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Na lição anterior, imprimimos a frase `Olá, mundo!` na tela, mas e se quisermos não apenas imprimi-la, mas também salvá-la para usar no futuro? Nesse caso, não há necessidade de digitar de novo a frase toda vez que quisermos imprimi-la.

Para evitar redigitar a frase várias vezes, precisamos aprender o conceito de variáveis. Antes, você imprimiu `'Olá, mundo!'` na tela passando-o diretamente para a função `print()`. Em vez disso, poderíamos ter armazenado esse valor em uma variável e depois usado essa variável para imprimir a mensagem.

```
message = 'Olá, mundo!'
print(message) # imprimindo a variável
```

```
Olá, mundo!
```

No código acima, salvamos a frase na variável `message` e usamos essa variável para imprimir a frase. Observe que `# imprimindo a variável` é um comentário que você vai ver muito. Comentários são as partes do código que não são levadas em consideração quando um código é executado. Então, tudo o que vem depois do caractere `#` é ignorado. Vamos usar comentários para fins explicativos no código que mostrarmos, facilitando seu entendimento. Agora, vamos voltar às variáveis.

No final desta lição, você será capaz de explicar o que são variáveis e como atribuir valores a elas. Armazenar dados em variáveis é essencial para escrever código limpo e eficaz, então vamos começar.

### Questionário 1

Vamos conferir seu entendimento agora com um pequeno quiz.

Pergunta

Observe o código abaixo. O que você acha que este código vai exibir na tela? Digite o resultado que você espera.

```
My_Var = 'oi'
my_var = 'olá'
print(My_Var)
```

oi

`My_var` e `my_var` são variáveis completamente diferentes, embora tenham nomes parecidos.

Trabalho maravilhoso!

Teoria

## Como criar variáveis

Nas linguagens de programação, variáveis funcionam como contêiners para outros valores (texto, números, etc.). Elas nos permitem salvar esses valores para fazer facilmente referências a eles no futuro.

Por exemplo, e se quisermos trabalhar com os primeiros 15 dígitos de `pi`? Seria bem difícil digitar esse número cada vez que fossemos usá-lo, sem falar que teríamos mais chances de cometer um erro. Em vez disso, podemos criar uma variável para armazenar esse número e então usar o nome da variável para usar o valor posteriormente no código:

```
pi = 3.14159265358979
print(pi + 1)
```

```
4.14159265358979
```

No exemplo acima, chamamos a variável de `pi` e usamos o **operador de atribuição** `=` para armazenar o valor na variável. Em outras palavras, _atribuímos_ o valor 3.14159… à variável `pi`. E então usamos `pi` para fazer o cálculo. Só precisamos digitar o valor uma vez — muito conveniente!

### Questionário 2

Outra oportunidade para praticar:

Pergunta

Veja o código abaixo. O que será impresso na tela? Selecione a resposta correta

```
spendings = 42
print(spendings + 10)
```

42

52

Está correto!

32

Seu entendimento sobre o material é impressionante!

Teoria

Em Python, não podemos criar uma variável sem atribuir um valor inicial a ela. Tudo o que precisamos fazer é escrever o nome da variável seguido pelo operador de atribuição `=` e então escrever o valor que queremos armazenar na variável.

![](https://practicum-content.s3.amazonaws.com/resources/1.2.3PT_1690197069.png) _Atribuindo um valor a uma variável_

Se colocarmos a variável nos parênteses do comando `print()` e executarmos o código, o valor da variável será exibido na tela. Em termos de programação, isso se chama passar a variável para a função `print()`:

```
person = 'Maxwell'
print(person)
```

```
Maxwell
```

Observe que todos os nomes de variáveis que usamos até agora são descritivos. É sempre necessário se esforçar para usar nomes de variáveis descritivos, para que outras pessoas que vão ler seu código (incluindo você no futuro) possam concluir facilmente o significado dos valores armazenados nas variáveis.

## Como alterar o valor de uma variável

Variáveis têm esse nome porque os valores armazenados nelas podem ser alterados. Para alterar o valor de uma variável, basta atribuir um novo valor a uma variável existente:

```
person = 'Maxwell'
person = 'Max'
print(person)
```

```
Max
```

É importante saber que reatribuir uma variável significa que o valor antigo será excluído. Agora `person` contém `'Max'`; o valor antigo `'Maxwell'` não pode mais ser acessado em linhas futuras do código.

Agora é sua vez. A variável `pets` armazena o número de animais de estimação que Max tem:

```
pets = 2
```

### Tarefa

Mas Max acabou de adotar mais dois gatos de um abrigo. Atribua um novo valor à variável `pets` para que o código abaixo imprima `4`.

CódigoPYTHON

9

1

2

3

pets \= 4

\# escreva seu código aqui

print(pets)

Dica

Mostrar a soluçãoValidar

### Questionário 3

Até agora, criamos variáveis atribuindo valores específicos a elas. Mas também podemos criar novas variáveis usando outras variáveis.

Pergunta

```
first_variable = 42
second_variable = first_variable
print(second_variable)
```

O que você acha que este código vai exibir na tela?

42

Atribuímos o valor `42` a `first_variable` e, depois, o conteúdo dela a `second_variable`. Assim, o resultado é `42`.

Excelente!

Teoria

## Variable syntax

**Sintaxe** é o conjunto das regras que definem como um código será escrito e interpretado. Se seu código não tiver a sintaxe apropriada, você provavelmente terá um erro quando tentar executá-lo. O Python segue as seguintes regras de sintaxe para nomear variáveis:

-   Nomes de variáveis só podem começar com letras ou um sublinhado `_`
-   Nomes de variáveis podem conter apenas letras, números e sublinhados; outros caracteres especiais não são permitidos

Uma coisa a notar sobre nomes de variáveis no Python é que há diferenças entre minúsculas e maiúsculas. Então, por exemplo, Python consideraria `My_Var` e `my_var` como nomes de variáveis diferentes.

Pergunta

### Questionário 4

Qual dos seguintes nomes de variáveis está conforme as regras de sintaxe do Python? Selecione todas as opções que se aplicam.

Escolha quantas quiser

`_super_mario`

Está tudo bem com esse nome; nomes de variáveis podem começar com `_`.

`2supermario`

`super_mario_3`

Está tudo bem com este nome; nomes de variáveis podem conter sublinhados e números.

`superMario4`

Está tudo bem com este nome; nomes de variáveis podem conter tanto letras maiúsculas quanto minúsculas.

`$uperMario`

`mario*bros`

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-00-224Z.md
### Última modificação: 2025-05-28 18:37:00

# Impressão - TripleTen

Capítulo 2/9

Variáveis, Tipos de Dados, e Operações Aritméticas

# Impressão

<iframe class="base-markdown-iframe__iframe" id="player-bx1ql8XJxAY" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Printing" width="640" height="360" src="https://www.youtube.com/embed/bx1ql8XJxAY?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fa8c4a275-34a7-4d8d-a34a-40c4d369c0db%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Agora que sabemos como criar e usar variáveis em Python, é hora de aprender mais sobre como exibir informação na tela. Usamos a função integrada `print()` para exibir valores na última lição. Agora vamos aprender mais sobre como essa função funciona.

## Como `print()` funciona

Você pode ter notado que `print()` é um pouco diferente das outras variáveis que viu. Ela contém parênteses, ao contrário de nomes de variáveis. Isso acontece porque `print()` é uma **função**.

Quando chamamos uma função no código, os valores entre parênteses são chamados de **argumentos**. A função `print()` exibe na tela os valores de quaisquer argumentos que passarmos para ela.

![](https://practicum-content.s3.amazonaws.com/resources/2.4PT_1697031759.png)

Você se lembra da primeira lição, como imprimimos a frase `'Olá, mundo!'`? Fizemos isso ao passar essa frase como argumento para a função `print()`:

```
print('Olá, mundo!')
```

```
Olá, mundo!
```

Para imprimir um texto, precisamos colocá-lo entre aspas simples ou duplas; do contrário, o Python não vai o reconhecer. Isso significa que ambas as opções `"Olá, mundo!"` e `'Olá, mundo!'` são aceitas.

No entanto, observe que `print()` exibe o texto sem aspas.

A função `print()` pode aceitar valores de qualquer tipo como entrada: texto, números e outros tipos de dados são permitidos. Por exemplo, se quisermos imprimir um número, `print(3)` vai exibir esse número:

```
print(3)
```

```
3
```

Como você pode ver, para passar um número como argumento, não precisamos de aspas.

Por último, mas não menos importante, precisamos mencionar que a função `print()` aceita qualquer número de parâmetros. Para ilustrar isso, examine o seguinte código e então o execute para ver o que acontece.

CódigoPYTHON

9

1

print("O número favorito de Max é", "...", 3)

Mostrar a soluçãoExecutar

Como você pode ver, podemos passar vários argumentos separados por vírgulas para a função `print()`, e ela vai exibir todos esses argumentos separados por espaços.

### Tarefa 1

Agora é hora de praticarmos um pouco. Use a instrução `print()` para imprimir a sua idade. Para fazer isso, passe três entradas para a função `print`: `"Eu tenho"` , `25` e `"anos."` separados com vírgulas.

CódigoPYTHON

9

1

print("Eu tenho", 25, "anos.")

Dica

Mostrar a soluçãoValidar

Teoria

## Quebras de linha

Cada vez que você chama a função `print()`, o conteúdo dela é impresso em uma nova linha. Se chamarmos `print()` sem argumentos, ela vai exibir apenas uma linha em branco.

Execute o seguinte código para ver como funciona.

CódigoPYTHON

9

1

2

3

print("O número favorito de Max é...")

print()

print(3)

Mostrar a soluçãoExecutar

## Variáveis

Como já vimos antes, você também pode passar uma variável para a função `print()`. Nesse caso, o _valor_ dela será exibido. Já que _nomes_ de variáveis não são strings, certifique-se de não usar aspas ao passar variáveis para a função `print()`.

Abaixo estão duas variáveis que armazenam o título e o ano de lançamento de um dos primeiros filmes da história do cinema. Ele se chama _L'Arrivée d'un train en gare de La Ciotat_ (A chegada do trem na estação) e foi lançado em 1896. Vamos imprimir os valores dessas duas variáveis passando-as para `print()`.

Você pode usar duas instruções `print()` separadas para imprimir o título e o ano de lançamento em duas linhas separadas ou usar uma única chamada da função `print()` passando a ela o título e o ano de lançamento como várias entradas.

### Tarefa 2

Abaixo estão duas variáveis que armazenam o título e o ano de lançamento de um dos primeiros filmes da história do cinema. Ele se chama _L'Arrivée d'un train en gare de La Ciotat_ (A chegada do trem na estação) e foi lançado em 1896. Vamos imprimir os valores dessas duas variáveis passando-as para `print()`.

Você pode usar duas instruções `print()` separadas para imprimir o título e o ano de lançamento em duas linhas separadas ou usar uma única chamada da função `print()` passando a ela o título e o ano de lançamento como várias entradas.

CódigoPYTHON

9

1

2

3

4

5

6

first\_movie\_title \= 'L\\'Arrivée d\\'un train en gare de La Ciotat'

first\_movie\_year \= 1896

  

print(first\_movie\_title)

print(first\_movie\_year)

#imprima o ano de lançamento aqui

Mostrar a soluçãoValidar

### Questionário

Agora um pouco mais de prática para você!

Pergunta

Veja o seguinte código:

```
metric = 34
print('metric:', metric)
```

O que será impresso na tela?

34: 34

metric: metric

metric:34

metric: 34

👍

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-03-582Z.md
### Última modificação: 2025-05-28 18:37:03

# Operações aritméticas - TripleTen

Capítulo 2/9

Variáveis, Tipos de Dados, e Operações Aritméticas

# Operações aritméticas

<iframe class="base-markdown-iframe__iframe" id="player-WyY_Y_IzXns" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Arithmetic Operations" width="640" height="360" src="https://www.youtube.com/embed/WyY_Y_IzXns?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F6e8c03d5-dcd2-48fa-81fb-32bf6ec51e77%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Agora que sabemos como criar variáveis e exibir os valores usando a função `print()`, é hora de começar a usar variáveis para fazer cálculos.

Nesta lição, vamos aprender a criar variáveis usando operadores aritméticos básicos e usar essas novas habilidades para responder a perguntas sobre um conjunto de dados sobre sites e idiomas. Dê uma olhada nos dados abaixo antes de começarmos:

![](https://practicum-content.s3.amazonaws.com/resources/2.5PT_1706797086.png)

Vamos usar esses dados para responder às seguintes perguntas nesta lição:

-   Quantas pessoas falam russo ou inglês?
-   Qual é a parcela de falantes nativos de italiano no número total de falantes de italiano?
-   Quantos falantes nativos de japonês existem a mais do que falantes nativos de francês?

## Operadores aritméticos

Para responder a essas perguntas com o Python, precisamos usar alguns operadores aritméticos básicos:

Operador

Descrição

Exemplo

`+`

adição

3 + 5 = 8

`-`

subtração

6 - 3 = 3

`*`

multiplicação

3 \* 2 = 6

`**`

`exponenciação`

3 \*\* 2 = 9

`/`

divisão

5 / 2 = 2.5

`//`

`divisão inteira`

5 // 2 = 2

`%`

`módulo`

14 % 3 = 2

Esses operadores funcionam exatamente como o esperado se você estivesse usando uma calculadora. Execute o código abaixo e veja por conta própria. Sinta-se à vontade para alterar os números e brincar com o código.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

\# adição

print(1.5 + 10)

  

\# subtração

print(1.5 \- 10)

  

\# multiplicação

print(1.5 \* 10)

  

\# exponenciação

print(1.5 \*\* 10)

  

\# divisão

print(15 / 10)

  

\# divisão inteira

print(15 // 10)

  

\# módulo

print(15 % 10)

Mostrar a soluçãoExecutar

Você percebeu como podemos colocar expressões aritméticas dentro de `print()` para exibir o resultado? Muito legal, não é?

![](https://practicum-content.s3.amazonaws.com/resources/1-1_1687782634.png)

## Ordem das operações

Com frequência, precisamos fazer cálculos que envolvem vários termos e vários operadores aritméticos. A sintaxe do Python para a ordem das operações segue a ordem clássica PEMDAS: parênteses, exponenciação, multiplicação e divisão, adição e subtração.

Veja se você consegue encontrar o resultado das operações abaixo antes de executar o código.

CódigoPYTHON

9

1

print( (2 \*\* 3 + 12 / 2) / (5 \* 4 \- 13) )

Mostrar a soluçãoExecutar

## Cálculos com variáveis

Agora sabemos os princípios básicos dos cálculos aritméticos em Python. Mas o verdadeiro poder vem da possibilidade de usar variáveis para armazenar nossos resultados, que poderemos usar em cálculos posteriores.

Vamos voltar para a tabela com os dados que apresentamos no início da lição. Por exemplo, podemos usar variáveis para calcular o número total dos falantes de francês ou alemão somando os valores da coluna `'Falantes do idioma (mi)'`.

```
french_speakers = 284.9
german_speakers = 132.0

total_speakers = french_speakers + german_speakers

print("Milhões de falantes de francês ou alemão:", total_speakers)
```

```
Milhões de falantes de francês ou alemão: 416.9
```

Usamos variáveis para calcular e exibir o nosso resultado: 416,9 milhões de pessoas falam francês ou alemão (ou ambos os idiomas).

### Tarefa 1

Agora tente responder à primeira pergunta: quantas pessoas falam russo ou inglês?

Vamos conferir mais uma vez a tabela que mostramos no início da lição. Aqui está:

![](https://practicum-content.s3.amazonaws.com/resources/2.5PT_1706797086.png)

Confira a coluna "Falantes do idioma (mi)" acima. Qual é o valor do inglês? Atribuir esse valor à variável `english_speakers`. Faça o mesmo para o russo, atribuindo o valor à variável `russian_speakers`.

Quando os valores forem atribuídos para inglês e russo, crie uma nova variável chamada `total_speakers` que armazena o resultado da adição.

Por último, imprima essa variável.

CódigoPYTHON

9

1

2

3

4

5

english\_speakers \= 1121.0\# encontre o valor na tabela e coloque-o aqui

russian\_speakers \= 264.3\# encontre o valor na tabela e coloque-o aqui

  

total\_speakers \= english\_speakers + russian\_speakers\# crie a variável total\_speakers e faça os cálculos abaixo desta linha

print(total\_speakers)\# imprima o resultado da adição

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Agora que respondemos à primeira pergunta, vamos lidar com as duas últimas.

A segunda questão da pesquisa era encontrar a parcela de falantes nativos de italiano no número total de falantes do idioma. Mais uma vez, aqui está a tabela:

![](https://practicum-content.s3.amazonaws.com/resources/2.5PT_1706797086.png)

Atribua os valores às variáveis fornecidas no pré-código. Ou seja:

-   atribua quantos falantes (mil) falam italiano à variável `total_italian`
-   atribua quantos falantes nativos (mil) falam italiano à variável `native_italian` variable

Em seguida, faça os cálculos e atribua o resultado a uma variável chamada `share_native`. Imprima essa variável.

CódigoPYTHON

9

1

2

3

4

5

total\_italian \= 67.8\# procure quantos falantes (mil) falam italiano

native\_italian \= 64.8\# procure quantos falantes nativos (mil) falam italiano

  

share\_native \= native\_italian/total\_italian\# crie a variável share\_native e faça os cálculos abaixo desta linha

print(share\_native)

Dica

Mostrar a soluçãoValidar

### Tarefa 3

Muito bem, você está se tornando mestre da aritmética em Python! Você consegue usar suas novas habilidades para responder à última pergunta: quantos falantes nativos de japonês existem a mais do que falantes nativos de francês?

Aqui está a tabela:

![](https://practicum-content.s3.amazonaws.com/resources/2.5PT_1706797086.png)

Use as variáveis fornecidas e atribua o resultado a uma nova variável chamada `speaker_diff`, depois imprima esse valor.

CódigoPYTHON

9

1

2

3

4

5

native\_japanese \= 128.2\# encontre o valor na tabela e coloque-o aqui

native\_french \= 76.7\# encontre o valor na tabela e coloque-o aqui

  

speaker\_diff\= native\_japanese\-native\_french\# termine seu código aqui embaixo

print(speaker\_diff)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-04-917Z.md
### Última modificação: 2025-05-28 18:37:05

# Tipos de dados - TripleTen

Capítulo 2/9

Variáveis, Tipos de Dados, e Operações Aritméticas

# Tipos de dados

<iframe class="base-markdown-iframe__iframe" id="player-YXrqovgKW6I" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Data Types" width="640" height="360" src="https://www.youtube.com/embed/YXrqovgKW6I?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fd06fcac1-c78f-4eec-b2f8-b6195910605c%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Até agora, falamos bastante sobre valores e variáveis. Você provavelmente já percebeu que existem diferentes tipos de valores em Python: por exemplo, valores de texto que imprimimos na tela e valores numéricos que salvamos nas variáveis.

Nesta lição, vamos estudar em mais detalhes os tipos de dados de Python, para que, no final da lição, você seja capaz de explicar como operadores diferentes se comportam com cada tipo de dados.

## Objetos e seus tipos

Chamamos os dados armazenados em uma variável de **objeto**, e cada objeto tem um determinado **tipo de dados**.

Você não precisa indicar o tipo de dados quando cria um objeto: o Python faz isso para você. Mesmo assim, precisamos ter tipos de dados em mente ao escrever programas; caso contrário, podemos introduzir erros no código.

Os tipos mais comuns incluem números inteiros, números de ponto flutuante, strings e valores booleanos (true/false). Números inteiros e de ponto flutuante são tipos de dados numéricos, enquanto strings são um tipo textual.

Abaixo estão exemplos dos quatro tipos de dados que listamos acima:

![](https://practicum-content.s3.amazonaws.com/resources/2.6_1PT_1690982139.png)

O tipo de dados determina quais tipos de operações são possíveis com um valor, e algumas operações se comportam diferentemente dependendo dos tipos de dados envolvidos.

## Aplicação: dados sobre idiomas

Vamos usar a tabela dos dados sobre idiomas de novo para conhecer as operações disponíveis para os quatro principais tipos de dados. Aqui está a tabela mais uma vez, para relembrar:

![](https://practicum-content.s3.amazonaws.com/resources/2.5PT_1706797086.png)

Observe que adicionamos uma nova coluna — "Maioria nativa", para indicar se a proporção de falantes nativos supera a de não falantes nativos do idioma.

Vamos dar uma olhada mais de perto nos tipos de dados na tabela. O Python tem a função `type()`, usada para determinar tipos de dados. Vamos mostrar como ela funciona.

### Tarefa 1

No pré-código abaixo, veja que armazenamos os dados sobre o idioma chinês em algumas variáveis. Aplique a função `type()` e termine o código para imprimir o tipo de cada variável.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

language \= 'Chinese'

website\_share \= 0.017

native\_speakers \= 908.7

total\_speakers \= 1107

majority\_native \= True

  

print(type(language))

print(type(website\_share))

print(type(native\_speakers))

print(type(total\_speakers))\# imprima o tipo de dados para total\_speakers

print(type(majority\_native))\# imprima o tipo de dados para majority\_native

Mostrar a soluçãoValidar

Teoria

Os resultados são aqueles que você estava esperando? Aqui estão os tipos de dados que temos:

-   `language` — string (`str`)
-   `website_share` e `native_speakers` — números de ponto flutuante (`float`)
-   `total_speakers` — número inteiro (`int`)
-   `majority_native` — valor booleano (`bool`)

### O operador de adição

Agora vamos ver como a adição funciona com vários tipos de dados. Vamos criar mais algumas variáveis para os dados sobre inglês. Depois, vamos usar o operador de adição nas variáveis de cada tipo. Execute o seguinte código para ver como ele funciona.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

\# strings

ch\_language \= 'Chinese'

en\_language \= 'English'

  

\# números de ponto flutuante

ch\_website\_share \= 0.017

en\_website\_share \= 0.539

  

\# números inteiros

ch\_total\_speakers \= 1107

en\_total\_speakers \= 1121

  

\# valores booleanos

ch\_majority\_native \= True

en\_majority\_native \= False

  

print(ch\_language + en\_language)

print(ch\_website\_share + en\_website\_share)

print(ch\_total\_speakers + en\_total\_speakers)

print(ch\_majority\_native + en\_majority\_native)

Mostrar a soluçãoExecutar

Ok, parece que podemos tirar algumas conclusões sobre a adição:

-   Quando somamos strings, elas são **concatenadas**, o que significa que elas são unidas em uma só string.
-   Se somarmos apenas números inteiros, o resultado será um número inteiro; se somarmos apenas números de ponto flutuante, o resultado também será um número de ponto flutuante.
-   Se somarmos um número inteiro e um número de ponto flutuante, o resultado será um número de ponto flutuante; na verdade, qualquer operação aritmética com um número de ponto flutuante vai resultar em um número de ponto flutuante.
-   Se somarmos valores booleanos, o resultado será um número inteiro; True é igual a 1, e False é igual a 0; é por isso que, ao somar valores booleanos, basta contar todos os valores True.

### Tarefa 2

Agora vamos praticar. Seu objetivo é usar as variáveis abaixo juntamente com o operador de adição para imprimir a string `'the cat in the hat'`.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

str1 \= 'hat'

str2 \= 'cat'

str3 \= 'the'

str4 \= 'in'

str5 \= ' '

  

\# escreva seu código dentro da função print()

print(str3,str2,str4,str3,str1)

Dica

Mostrar a soluçãoValidar

Podemos também concatenar strings multiplicando-as por números inteiros (mas não por números de ponto flutuante). O que você acha que vai acontecer quando executar o código abaixo? Sinta-se à vontade para praticar alterando a string e o número inteiro.

CódigoPYTHON

9

1

print(' 123ABC ' \* 6)

Mostrar a soluçãoExecutar

Teoria

Algumas das operações funcionam apenas para determinados tipos de dados. Por exemplo, você não pode somar um número a uma string. Se você tentar fazer isso, o Python vai gerar um erro:

```
result = 'Chinês é falado por ' + 1107
```

```
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: can only concatenate str (not "int") to str
```

Essa é uma instância de um erro `TypeError`, que significa que os tipos de dados são incompatíveis com a operação. Uma string só pode ser somada a outra string, ou um número a outro número, mas não podemos somar números com strings. Vamos falar sobre erros em mais detalhe mais adiante neste capítulo.

É muito importante saber os tipos dos dados. Em caso de dúvida, basta usar a função `type()`!

Pergunta

### Questionário

Para as variáveis abaixo, selecione todas as operações que são permitidas em Python:

```
a = 17
b = 3.2
c = 'oi'
```

Escolha quantas quiser

`a + b`

Não há problema em somar números inteiros e números de ponto flutuante.

`c * a`

Você pode multiplicar strings por números inteiros para obter uma concatenação maluca.

`b * c`

`c / a`

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-06-252Z.md
### Última modificação: 2025-05-28 18:37:06

# Conversão de tipos de dados - TripleTen

Capítulo 2/9

Variáveis, Tipos de Dados, e Operações Aritméticas

# Conversão de tipos de dados

<iframe class="base-markdown-iframe__iframe" id="player-7xCrM6d65ho" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Converting Data Types" width="640" height="360" src="https://www.youtube.com/embed/7xCrM6d65ho?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F5a06d99e-b43c-4e3f-bf9b-d4c3d746ad4b%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Como aprendemos na última lição, os tipos de dados dos objetos Python determinam quais operações são permitidas e como essas operações se comportam:

-   strings podem ser concatenadas usando `+`:
    
    ```
      print('a' + 'b')
      
    ```
    
    ```
      ab
      
    ```
    
-   strings podem até ser multiplicadas (`*`) por um número:
    
    ```
      print('a' * 3)
      
    ```
    
    ```
      aaa
      
    ```
    
-   números (que são representados em Python por números de ponto flutuante e inteiros) podem ser somados, subtraídos, multiplicados e divididos:
    
    ```
      print(2 + 2)
      print(2 - 2)
      print(2 * 2)
      print(2 / 2)
      
    ```
    
    ```
      4
      0
      4
      1
      
    ```
    

Assim, há momentos em que precisamos converter um valor de um tipo em outro para realizar uma determinada operação.

Por exemplo, um número pode ser armazenado como uma string: `'24'`. Como você sabe, para somar esse número a outro número, precisamos primeiro converter a string em um tipo numérico, como `int` ou `float`. No final desta lição, você será capaz de converter números de ponto flutuante em números inteiros, converter strings em números e vice-versa, e também reconhecer erros de conversão.

Vamos analisar as seguintes funções nesta lição:

-   `int()` transforma um objeto em um tipo inteiro
-   `float()` transforma um objeto em um tipo de ponto flutuante
-   `str()` transforma um objeto em um tipo string

Vamos usar mais dados do conjunto de dados sobre sites e idiomas para ver como a conversão de tipos de dados funciona em Python. Para refrescar a memória, aqui está a tabela:

![](https://practicum-content.s3.amazonaws.com/resources/2.5PT_1706797086.png)

## Conversão de números de ponto flutuante em números inteiros

Dos 10 milhões de sites mais populares da internet, podemos calcular o número de sites que estão em inglês:

```
en_website_share = 0.539
total_sites = 10000000

en_sites = total_sites * en_website_share

print(en_sites)
print(type(en_sites))
```

```
5390000.0
<class 'float'>
```

Python retorna o resultado como um número de ponto flutuante porque `en_website_share` armazena um valor de ponto flutuante. Mas, na verdade, talvez seja melhor representar `en_sites` como um número inteiro, já que não podemos ter uma fração de um site. Assim, o tipo de dados vai corresponder melhor ao que o valor representa na vida real.

Para converter um valor de ponto flutuante em um número inteiro, usamos a função `int()`:

```
en_website_share = 0.539
sites = 10000000

en_sites = sites * en_website_share
en_sites = int(en_sites)
print(en_sites)
print(type(en_sites))
```

```
5390000
<class 'int'>
```

A variável `en_sites` foi convertida de `float` para `int`. O valor antigo foi substituído após atribuirmos o resultado da chamada de `int()` de volta à variável `en_sites`.

No exemplo acima, não perdemos nenhuma informação ao converter `en_sites` em um número inteiro, porque havia apenas 0 após o ponto decimal. Aplicar a função `int()` a um valor `float` simplesmente descarta a parte fracionária, então qualquer número diferente de zero após o ponto decimal será perdido.

### Tarefa 1

Em contraste, agora vamos dar uma olhada em outro exemplo. Atribua o número total de falantes de tcheco à variável `cz_total_speakers` e imprima seu valor usando o código já fornecido. Em seguida, escreva seu próprio código para converter a variável `cz_total_speakers` para o tipo `int` e imprima seu valor de novo.

CódigoPYTHON

9

1

2

3

4

5

cz\_total\_speakers \= 10.6\# encontre o valor na tabela e coloque-o aqui

print(cz\_total\_speakers)

  

cz\_total\_speakers \= int(cz\_total\_speakers)

print(cz\_total\_speakers)\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

Teoria

Qualquer valor de ponto flutuante pode ser convertido em um número inteiro, mas sempre perdemos toda a informação após o ponto decimal. Às vezes, essa perda pode ser significativa.

## Conversão de strings em números e vice-versa

Nem sempre recebemos dados no formato que esperamos. Às vezes, valores que parecem ser números para nós, estão, na verdade, armazenados como strings em Python.

Digamos que queremos fazer um cálculo usando um número armazenado como uma string.

```
ch_total_speakers = '1107'

ch_total_speakers = ch_total_speakers + 10
```

```
...

TypeError: can only concatenate str (not "int") to str
```

Ocorre o erro `TypeError`. Os tipos de dados não correspondem à operação solicitada porque não podemos adicionar uma string a um número em Python. Embora seja óbvio para nós que `'1107'` representa um número, Python não sabe isso.

Para realizar uma operação aritmética com `ch_total_speakers`, precisamos converter a string em um tipo numérico. Nesse caso, vamos convertê-la em um número inteiro:

```
ch_total_speakers = '1107'
ch_total_speakers = int(ch_total_speakers)

ch_total_speakers = ch_total_speakers + 10
print(ch_total_speakers)
print(type(ch_total_speakers))
```

```
1117
<class 'int'>
```

Agora as strings foram convertidas em um tipo de dados com o qual podemos trabalhar.

### Tarefa 2

Agora é sua vez de fazer conversão de dados. Execute o código abaixo e examine a mensagem de erro. E então converta a variável `share_english` em um número de ponto flutuante e execute o código novamente antes de enviar a resposta.

CódigoPYTHON

9

1

2

3

4

5

6

share\_english \= '0.539'

  

share\_english \= float(share\_english)\# escreva seu código aqui

  

print(type(share\_english))

print(share\_english + 0.1)

Dica

Mostrar a soluçãoValidar

Teoria

Você fez um ótimo trabalho convertendo strings em números. Mas há situações em que queremos fazer o contrário. Por exemplo, talvez seja melhor representar dados de códigos postais ou números de identificação como strings, e não como números inteiros, já que esses números não são valores quantitativos e, portanto, não seriam usados em operações aritméticas.

Como você pode ter adivinhado, podemos usar a função `str()` para fazer isso:

```
zipcode = 90210
print(type(zipcode))

zipcode = str(zipcode)
print(type(zipcode))
```

```
<class 'int'>
<class 'str'>
```

## Erros de conversão

É claro que nem todo objeto pode ser convertido de um tipo em outro. Por exemplo, vamos tentar converter a string `'Olá, mundo!'` em um número de ponto flutuante. Veja o que vai acontecer:

```
float('Olá, mundo!')
```

```
...

ValueError: could not convert string to float: 'Olá, mundo!'
```

A verdade é que apenas determinadas strings podem ser convertidas em tipos numéricos. As strings precisam conter números para que a conversão funcione. Letras ou símbolos especiais farão com que Python retorne um erro.

Pergunta

### Questionário

Selecione todas as conversões que, na sua opinião, _são_ permitidas em Python para as seguintes variáveis:

```
my_str   = 'TripleTen'
my_int   = 123456
my_float = 123.456
```

Escolha quantas quiser

`str(my_float)`

Isso mesmo, podemos converter qualquer float ou inteiro em uma string sem problemas.

`str(my_str)`

Está correto, a string pode ficar como string sem problemas.

`int(my_str)`

`float(my_int)`

Isso mesmo, podemos converter qualquer número inteiro em um número de ponto flutuante.

`int(my_float)`

Está certo, podemos converter qualquer número de ponto flutuante em um número inteiro; mas podemos perder os dígitos após o ponto decimal.

`float(my_str)`

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-07-551Z.md
### Última modificação: 2025-05-28 18:37:07

# Mensagens de erro - TripleTen

Capítulo 2/9

Variáveis, Tipos de Dados, e Operações Aritméticas

# Mensagens de erro

<iframe class="base-markdown-iframe__iframe" id="player-Rsn9ogwHyBE" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Error Messages" width="640" height="360" src="https://www.youtube.com/embed/Rsn9ogwHyBE?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F302c2602-473c-48fb-82cc-331d70b1398c%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Ao longo deste capítulo, mencionamos repetidamente vários tipos de erros que podem aparecer quando escrevemos código em Python. Na programação, encontrar e corrigir erros constitui grande parte do trabalho.

Há muitos tipos de erros, mas a maioria deles são causados por uma destas três razões:

1.  Sintaxe inapropriada
2.  Operações proibidas
3.  Referência a variáveis inválidas

Se o código tiver qualquer uma dessas coisas, o Python não vai saber como proceder e vai retornar um erro. Antes de analisarmos exemplos específicos, queremos salientar que erros são uma parte completamente normal da programação. Na verdade, receber erros pode ser uma coisa boa, já que isso ajuda a resolver problemas ao trabalhar no código.

No final desta lição, você será capaz de identificar e corrigir a maioria dos erros comuns.

## Erros de sintaxe

Se seu código tiver uma sintaxe inapropriada, o Python vai retornar um erro.

Por exemplo, vamos ver o que acontece se esquecermos de pôr um parêntese:

```
print('Oi'
```

```
...
        print('Oi'
              ^
SyntaxError: incomplete input
```

Como esperado, temos um erro de sintaxe `SyntaxError`. As mensagens de erro são grandes aliadas, porque elas dizem exatamente qual é o problema e onde o erro aconteceu. Você pode ver que o código terminou na linha com o erro de sintaxe e as chamadas de `print()` não foram executadas.

### Tarefa 1

Veja abaixo um trecho de código que resulta em um erro. Você consegue adivinhar o que está errado? O erro leva a um erro devido ao nome da variável que começa com um número. Seu objetivo é corrigir o erro para podermos imprimir o texto `'Maria'`. Crie um novo nome de sua escolha.

CódigoPYTHON

9

1

2

name \= 'Maria'

print(name)

Dica

Mostrar a soluçãoValidar

Teoria

## Erros de operações

Também podemos encontrar erros quando tentamos executar operações "ilegais". Por exemplo, dividindo por zero. Execute o código a seguir para entender.

CódigoPYTHON

9

1

print(777 / 0)

Mostrar a soluçãoExecutar

Vemos um erro porque não é possível dividir nenhum número por zero.

Outro exemplo seria adicionar texto a um número. Execute o código a seguir para ver o que é um erro.

CódigoPYTHON

9

1

2

numbers \= 'um dois três quatro' + 5

print(numbers)

Mostrar a soluçãoExecutar

Mais uma vez, o Python diz claramente qual é o problema.

## Erros de namespace

Outro motivo comum para a ocorrência de erros no código são erros de namespace. Se você tentar usar uma variável que ainda não foi criada, o Python não vai saber o que fazer, já que a variável ainda não existe no namespace do Python. Aqui está um exemplo:

```
apple_price = 1.25
banana_price = 0.99

print("Preço total:", apple_price + banana_price + kiwi_price)
```

```
...

NameError: name 'kiwi_price' is not defined
```

### Tarefa 2

O Python não sabe o que queremos dizer com `kiwi_price`. Para corrigir esse erro, precisamos criar a variável `kiwi_price` antes de usá-la na função `print()`.

Abaixo está um código que resulta em um erro. Tente encontrar um erro. Se encontrar, o corrija. Caso contrário, execute o código, leia a mensagem de erro e tente corrigir o erro novamente.

CódigoPYTHON

9

1

2

3

4

5

6

apple\_price \= 1.25

banana\_price \= 0.99

kiwi\_price \= 1.49

  

total\_price \= apple\_price + banana\_price + kiwi\_price

print("Preço total:", total\_price)

Dica

Mostrar a soluçãoValidar

Teoria

## Processamento de erros

Quando o Python encontra um erro, ele fecha o programa. É isso o que acontece quando você está jogando um videogame ou usando um aplicativo e o programa fecha de repente.

Podemos evitar erros usando o operador de tratamento de erros, o `try-except`:

```
try:
    print(777 / 0)
except:
    print('Não é possível dividir por zero!')
    print('Seguimos adiante!')
```

```
Não é possível dividir por zero!
Seguimos adiante!
```

O Python vai tentar executar o código do bloco `try`. Se houver um erro, ele vai executar o código do bloco `except`.

`try` e `except` são ambos seguidos por dois pontos. Depois deles, os blocos de instruções são escritos. Cada linha dentro dos blocos é indentada por quatro espaços.

Python utiliza dois pontos e indentação para distinguir onde acaba um bloco e começa o outro. Um espaço ausente ou extra vai resultar em um erro.

### Tarefa 3

Experimente por conta própria na tarefa abaixo.

Primeiro, execute o programa abaixo para ver se o erro `ZeroDivisionError` aparece.

Depois, altere o código para que ele contenha um bloco `try-except`. Se aparecer um erro, imprima a mensagem `'Divisão por zero!'`.

Você não precisa alterar os valores das variáveis.

CódigoPYTHON

9

1

2

3

4

5

6

7

ja\_total\_speakers \= 0

ja\_native\_speakers \= 128.2

  

try:

print((ja\_total\_speakers \- ja\_native\_speakers) / ja\_total\_speakers)

except:

print('Divisão por zero!')

Dica

Mostrar a soluçãoValidar

Agora que você sabe como tratar erros em Python, o capítulo está quase concluído! Passe para a próxima lição para resumir o que você aprendeu.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-08-878Z.md
### Última modificação: 2025-05-28 18:37:09

# Atividade prática do capítulo - TripleTen

Capítulo 2/9

Variáveis, Tipos de Dados, e Operações Aritméticas

# Atividade prática do capítulo

Na lição anterior, você aprendeu muito sobre erros em Python e como lidar com eles. Agora você conhece alguns dos tipos comuns de erros e os blocos `try-except` para tratamento de erros. Essa era a última coisa que tínhamos para mostrar neste capítulo. Sabemos que foi um desafio aprender esses novos conceitos. Vamos apenas ver o que já abordamos:

-   variáveis e a sintaxe usada para nomeá-las.
-   a função `print()`, usada para exibir informações na tela.
-   diferentes tipos de dados, como números inteiros, de ponto flutuante, strings, booleanos e operações aritméticas (`+`, `-`, `*`, `/`) que podem ser aplicadas a eles.
-   como converter de um tipo de dados para outro com funções como `int()`, `float()` e `str()`.

Para ter certeza de que você está confortável com tudo o que aprendeu até agora, vamos praticar um pouco mais. Então, boas-vindas ao seu primeiro teste sobre Python!

Criamos este quiz para ajudar a identificar quais lições você precisa revisar antes de seguir em frente.

Preparamos 5 perguntas para você, então vamos lá.

Pergunta

Nomes de variáveis em Python têm regras de sintaxe que precisamos seguir. Escolha todos os nomes de variáveis escritos corretamente:

Escolha quantas quiser

Arbutus

Está certo, nomes de variáveis em Python só podem conter letras, números e sublinhados. Nomes não podem começar com números, mas não há problemas em usar sublinhados. Nomes de variáveis também não podem conter espaços ou outros caracteres especiais.

\_oak

Está certo, nomes de variáveis em Python só podem conter letras, números e sublinhados. Nomes não podem começar com números, mas não há problemas em usar sublinhados. Nomes de variáveis também não podem conter espaços ou outros caracteres especiais.

0hazel

willow$

\_7fir

Está certo, nomes de variáveis em Python só podem conter letras, números e sublinhados. Nomes não podem começar com números, mas não há problemas em usar sublinhados. Nomes de variáveis também não podem conter espaços ou outros caracteres especiais.

white album

Fantástico!

Imprima o seguinte texto "Vou me tornar mestre em Python":

CódigoPYTHON

9

1

print("Vou me tornar mestre em Python")\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

Altere o valor de `white_album`: primeiro, subtraia 13 dele e divida o resultado por 3. E então imprima o valor resultante de `white_album`.

CódigoPYTHON

9

1

2

3

4

5

white\_album \= 40

  

white\_album \= (white\_album \-13)/3\# atualize o valor da variável aqui

print(white\_album)

\# imprima o valor da variável aqui

Dica

Mostrar a soluçãoValidar

Vamos calcular a altura média da população nos Países Baixos. Antes de fazermos isso, vamos primeiro imprimir os tipos das variáveis `avg_height_men_cm` (altura média dos homens em cm) e `avg_height_women_cm` altura média das mulheres em cm):

CódigoPYTHON

9

1

2

3

4

5

avg\_height\_men\_cm \= 183.78

avg\_height\_women\_cm \= '170.36'

print(type(avg\_height\_men\_cm))

print(type(avg\_height\_women\_cm))

\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

Agora calcule a `média` de altura dos homens e mulheres e armazene o resultado em uma variável `average_height` (altura média). Observe que primeiro você pode precisar alterar um tipo de dados para uma das variáveis. Imprima o resultado:

CódigoPYTHON

9

1

2

3

4

5

6

avg\_height\_men\_cm \= 183.78

avg\_height\_women\_cm \= '170.36'

avg\_height\_women\_cm \= float(avg\_height\_women\_cm)

average\_height \= avg\_height\_men\_cm/2+avg\_height\_women\_cm/2

print(average\_height)

\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-10-207Z.md
### Última modificação: 2025-05-28 18:37:10

# Conclusão - TripleTen

Capítulo 2/9

Variáveis, Tipos de Dados, e Operações Aritméticas

# Conclusão

Parabéns! Você terminou seu primeiro capítulo! Agora você está um passo mais perto de se tornar um profissional da área de dados.

Após concluir este capítulo, você será capaz de:

-   Escrever código em Python Se você ainda tiver dúvidas, pode revisar a lição [Por que o Python?](https://tripleten.com/trainer/data-analyst/lesson/dec0f325-f1ba-4505-9fbd-52c4d2c83fd2/)
-   Explicar o que são variáveis Se você ainda tiver dúvidas, pode revisar a lição sobre [Variáveis](https://tripleten.com/trainer/data-analyst/lesson/ac519953-09fc-477c-8e2a-17b02f2dbd4a/).
-   Criar variáveis Se você ainda tiver dúvidas, pode revisar a lição sobre [Variáveis](https://tripleten.com/trainer/data-analyst/lesson/ac519953-09fc-477c-8e2a-17b02f2dbd4a/).
-   Imprimir informações na tela Se você ainda tiver dúvidas, pode revisar a lição sobre [Impressão](https://tripleten.com/trainer/data-analyst/lesson/76cedb9d-f80b-440a-be8a-b8ae1660d5d8/).
-   Fazer operações aritméticas em Python Se você ainda tiver dúvidas, pode revisar a lição sobre [Operações aritméticas](https://tripleten.com/trainer/data-analyst/lesson/1e38a527-fcca-4893-9722-ef0be2518816/).
-   Identificar e corrigir erros comuns (de sintaxe, operações e namespace) Se você ainda tiver dúvidas, pode revisar a lição sobre [Mensagens de erro](https://tripleten.com/trainer/data-analyst/lesson/e65d66bb-5fec-4e84-a220-dfd46899df27/).
-   Explicar como operadores diferentes se comportam com cada tipo de dados Se você ainda tiver dúvidas, pode revisar a lição sobre [Tipos de dados](https://tripleten.com/trainer/data-analyst/lesson/3b71c74f-c426-484c-a3f8-aa95a9e44617/).
-   Converter números de ponto flutuante em números inteiros, converter strings em números e vice-versa Se você ainda tiver dúvidas, pode revisar a lição sobre [Conversão de tipos de dados](https://tripleten.com/trainer/data-analyst/lesson/78c00b98-013b-40ce-b52f-2eb4aeeda1ba/).
-   Perceber quando erros de conversão acontecem Se você ainda tiver dúvidas, pode revisar a lição sobre [Conversão de tipos de dados](https://tripleten.com/trainer/data-analyst/lesson/78c00b98-013b-40ce-b52f-2eb4aeeda1ba/).

## O que vem a seguir

No próximo capítulo, você vai ver mais de perto o tipo de dados string e aprender sobre outros conceitos fundamentais de Python.

Antes de prosseguirmos, queremos aproveitar o momento para te deixar algumas palavras de motivação.

<iframe class="base-markdown-iframe__iframe" id="player-3Z08FppEHO8" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Pacing Yourself" width="640" height="360" src="https://www.youtube.com/embed/3Z08FppEHO8?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fc8d0caa8-2af3-47c9-af8e-98774f07e473%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

[A estrutura do curso](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/ptbr-syllabus/DA_PT_Syllabus_V5.pdf)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-12-193Z.md
### Última modificação: 2025-05-28 18:37:12

# Introdução - TripleTen

Capítulo 3/9

Strings

# Introdução

Você aprendeu que existem muitos tipos de dados diferentes em Python. Alguns deles são numéricos (flutuantes e inteiros) e outros são usados para definir valores True ou False (booleanos). Por último, temos strings usadas para armazenar texto. Aqui está uma tabela que mostramos antes:

![](https://practicum-content.s3.amazonaws.com/resources/2.6_1PT_1697105638.png)

Neste capítulo, vamos aprender mais sobre o tipo de dados string.

![](https://practicum-content.s3.amazonaws.com/resources/3.1PT_1690199953.png)

Já que conjuntos de dados frequentemente incluem textos, endereços, etc., esse tipo de dados é muito importante.

Após concluir este capítulo, você será capaz de:

-   Definir as características básicas de strings
-   Calcular comprimentos de strings
-   Usar caracteres de escape
-   Criar strings de múltiplas linhas
-   Formatar strings
-   Usar índices para obter um elemento específico em uma string
-   Processar strings usando métodos especiais de string

Aprender tudo isso deve levar cerca de 1-2 horas. Então vamos lá!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-13-551Z.md
### Última modificação: 2025-05-28 18:37:13

# Propriedades básicas das strings - TripleTen

Capítulo 3/9

Strings

# Propriedades básicas das strings

<iframe class="base-markdown-iframe__iframe" id="player-ut49PVkAY_4" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Basic Properties of Strings" width="640" height="360" src="https://www.youtube.com/embed/ut49PVkAY_4?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F3367693f-c6a5-453e-8dfa-4ca77854515f%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Talvez você não tenha percebido, mas estamos trabalhando com strings desde a primeira lição. Basicamente, strings são dados de texto. Muitos dados importantes, como nomes de usuários, endereços de e-mail e avaliações de produtos, podem ser armazenados como objetos string em Python. Isso significa que você vai trabalhar _muito_ com strings como profissional da área de dados.

Ao final desta lição, você será capaz de criar strings em Python e calcular seu comprimento. Você também vai aprender como usar caracteres de escape e criar strings de múltiplas linhas. Todas essas habilidades são necessárias para analisar e processar dados do tipo string.

## Como criar strings

Vamos começar criando um objeto string em Python. Para fazer isso, basta colocar qualquer sequência de caracteres entre aspas (simples ou duplas) e atribuir essa sequência a uma variável:

```
my_string_1 = '1 string, 2 strings, string vermelha, string azul!'
my_string_2 = "1 string, 2 strings, string vermelha, string azul!"

print(my_string_1)
print(my_string_2)
```

```
1 string, 2 strings, string vermelha, string azul!
1 string, 2 strings, string vermelha, string azul!
```

Não importa se usamos aspas simples ou duplas, as strings serão iguais. Podemos tirar vantagem disso usando aspas duplas para strings que contêm apóstrofos.

### Tarefa 1

Veja a string abaixo `'Quero um copo d'água, por favor!'`. Se colocarmos essa string na função `print()` e executarmos o código, ele vai resultar em um erro:

```
print('Quero um copo d'água, por favor!')
```

```
print('Quero um copo d'água, por favor!')
                                     ^
SyntaxError: unterminated string literal (detected at line 1)
```

Para corrigir o erro, precisamos colocar `Quero um copo d'água, por favor!` entre aspas duplas:

```
print("Quero um copo d'água, por favor!")
```

```
Quero um copo d'água, por favor!
```

Como você pode ver, não há erro agora!

Experimente por conta própria agora. Mude de aspas simples para duplas para evitar o erro.

CódigoPYTHON

9

1

print("Estude Python e tome um copo d'água")

Dica

Mostrar a soluçãoValidar

Aqui vamos nós… A seguir, aprenderemos sobre caracteres de escape.

Teoria

Como indicado na mensagem de erro, nossa primeira string começou com uma aspa simples e terminou com o apóstrofo de "d'água", o que fez com que o Python não conseguisse reconhecer o que veio depois disso. Mas quando usamos aspas duplas, o Python entende onde a string começa e onde termina, interpretando o apóstrofo corretamente.

---

## Caracteres de escape

Você pode estar se perguntando: "E se eu quiser escrever uma string que contenha aspas simples _e_ duplas?" O Python tem uma maneira de fazer isso (e outras coisas) usando caracteres de escape nas strings.

Para usar um caractere de escape, precisamos colocar uma contrabarra antes do caractere especial que queremos usar na nossa string. Isso nos permite imprimir a string do exercício anterior sem usar aspas duplas:

```
print('Quero um copo d\'água, por favor!')
```

```
Quero um copo d'água, por favor!
```

Nesse caso, o caractere de escape foi `\'`. Observe no resultado que Python não inclui `\` na string resultante; a contrabarra simplesmente está lá para informar ao Python que queremos que a string inclua o caractere `'`. Da mesma forma, podemos usar `\"` para incluir o caractere `"` em uma string escrita entre aspas duplas.

### Tarefa 2

Execute o código abaixo e examine a mensagem de erro. Em seguida, use caracteres de escape para corrigir a sintaxe.

CódigoPYTHON

9

1

print("E então ela disse \\"Peguei!\\" ao agarrar a galinha d'angola.")

Dica

Mostrar a soluçãoValidar

Teoria

Além de aspas simples e duplas, há muitos outros caracteres de escape. A maioria deles é usada para formatar o texto; os caracteres mais comuns são os seguintes:

-   `\n` — o caractere de nova linha, adiciona uma quebra de linha fazendo com que a string continue na nova linha
-   `\t` — o caractere de tabulação, adiciona tabulação à string

Aqui está um exemplo que usa ambos os caracteres na mesma string

```
print("O filme favorito do Pedro é:\t\t o último que ele assistiu\n:D")
```

```
O filme favorito do Pedro é:         o último que ele assistiu
:D
```

A string inicial parece confusa e difícil de ler, mas quando a exibimos com `print()`, uma bela formatação é aplicada.

### Tarefa 3

Vamos praticar! Formate a sequência abaixo de modo que cada item da lista de compras comece em uma nova linha, começando com o primeiro item. Mantenha as vírgulas como estão agora.

CódigoPYTHON

9

1

2

print("Minha lista de compras: \\n2 pacotes de leite, \\ncereais, \\novos, \\npão")

  

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-14-893Z.md
### Última modificação: 2025-05-28 18:37:15

# Continuação das propriedades básicas das strings - TripleTen

Capítulo 3/9

Strings

# Continuação das propriedades básicas das strings

<iframe class="base-markdown-iframe__iframe" id="player-cVj5zwcnhf0" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Basic Properties of Strings Continued" width="640" height="360" src="https://www.youtube.com/embed/cVj5zwcnhf0?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F90cd399d-0e7d-4db1-801c-11fae1725af1%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Na lição anterior você aprendeu:

-   sobre como criar strings com aspas simples (`’ ‘`) ou duplas (`” “`)
-   sobre caracteres de escape como `\`, `\n` e `\t`, usados para tratar uma string do jeito certo ou para formatá-la corretamente.

Agora, vamos seguir em frente e aprender sobre outras propriedades importantes. A primeiro que queremos apresentar é a capacidade de usar multilinhas.

## Strings de múltiplas linhas

Computadores podem não ligar para o quão longa é uma linha de texto, mas as pessoas acham muito mais fácil de ler um texto em que cada ideia nova começa em uma nova linha.

Embora essa string da tarefa anterior seja exibida de forma bonita quando a passamos para a função `print()`, seria bem difícil para outras pessoas entenderem o código (especialmente se a lista de compras for longa).

Seria ótimo se fosse possível escrever o código com quebras de linha reais, mas, recebemos um erro ao fazer isso:

```
shopping_list = "MInha lista de compras:
2 pacotes de leite,
cereais, 
ovos, 
pão"

print(shopping_list)
```

```
...

SyntaxError: unterminated string literal (detected at line 1)
```

Felizmente, o Python permite criar strings de múltiplas linhas. Para fazer isso, coloque três aspas simples `’’’` ou duplas `”””` no início e no final da string. Aqui está um exemplo:

```
shopping_list = """MInha lista de compras
2 pacotes de leite,
cereais, 
ovos, 
pão"""

print(shopping_list)
```

```
MInha lista de compras
2 pacotes de leite,
cereais, 
ovos, 
pão
```

A string de múltiplas linhas é exibida por `print()` exatamente da mesma maneira que a digitamos no código, mesmo sem usarmos nenhum caractere de escape.

## Comprimento de uma string

Com frequência, precisamos saber o comprimento de uma string durante o processamento e análise de dados. O comprimento de uma string é igual ao número de caracteres que ela tem. As strings podem ter qualquer comprimento, a única limitação é a memória do computador.

Python tem uma função chamada `len()` que podemos usar para calcular facilmente o comprimento de qualquer string.

Aqui está `len()` em ação:

```
sonnet = "De tudo, ao meu amor serei atento\nAntes, e com tal zelo, e sempre, e tanto\nQue mesmo em face do maior encanto\nDele se encante mais meu pensamento.\nQuero vivê-lo em cada vão momento\nE em louvor hei de espalhar meu canto\nE rir meu riso e derramar meu pranto\nAo seu pesar ou seu contentamento.\nE assim, quando mais tarde me procure\nQuem sabe a morte, angústia de quem vive\nQuem sabe a solidão, fim de quem ama\nEu possa me dizer do amor (que tive):\nQue não seja imortal, posto que é chama\nMas que seja infinito enquanto dure."

print(sonnet)
print()
print(len(sonnet))
```

```
De tudo, ao meu amor serei atento
Antes, e com tal zelo, e sempre, e tanto
Que mesmo em face do maior encanto
Dele se encante mais meu pensamento.
Quero vivê-lo em cada vão momento
E em louvor hei de espalhar meu canto
E rir meu riso e derramar meu pranto
Ao seu pesar ou seu contentamento.
E assim, quando mais tarde me procure
Quem sabe a morte, angústia de quem vive
Quem sabe a solidão, fim de quem ama
Eu possa me dizer do amor (que tive):
Que não seja imortal, posto que é chama
Mas que seja infinito enquanto dure.

521
```

Consegue imaginar ter que contar o número de caracteres nessa string manualmente? Usando a função `len()`, vemos que o soneto contém 521 caracteres.

É importante observar que cada caractere de escape ou espaço também conta como um caractere quando medimos o comprimento de uma string:

```
string_1 = "OiAté"
string_2 = "Oi Até"
string_3 = "Oi\nAté"
print(len(string_1))
print(len(string_2))
print(len(string_3))
```

```
5
6
6
```

### Tarefa

Abaixo, há uma string um pouco mais interessante. Use `len()` para calcular o comprimento da `string` e atribua o resultado a uma variável chamada `length`. Em seguida, imprima o valor de `length`.

CódigoPYTHON

9

1

2

3

string \= 'Ela pediu um copo d\\'água.'

length \= len(string)

print(length)\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

Por fim, precisamos mencionar strings vazias — elas frequentemente aparecem como espaços reservados para valores ausentes em tabelas e arquivos de dados. Esta é uma string especial de comprimento zero que pode ser criada usando aspas simples ou duplas, sem caracteres entre elas:

```
print(len(''))
print(len(""))
```

```
0
0
```

Muito bem, agora você sabe como formatar uma string e calcular seu comprimento. Mas e se quisermos consultar apenas um elemento específico da nossa string? Vamos ver como fazer isso na próxima lição.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-17-306Z.md
### Última modificação: 2025-05-28 18:37:17

# Índices - TripleTen

Capítulo 3/9

Strings

# Índices

<iframe class="base-markdown-iframe__iframe" id="player-bHkSwynSMG0" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Indices" width="640" height="360" src="https://www.youtube.com/embed/bHkSwynSMG0?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F0284c121-6f2a-4615-9ff4-4940a66b1955%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Agora sabemos que toda string tem seu comprimento, mesmo que esteja vazia. Podemos verificar isso com a função `len()`. Vamos agora ver o que podemos fazer com cada caractere de uma string.

## Introdução aos índices

Em Python, strings podem ser vistas como sequências de caracteres, em que cada caractere recebe um **índice** de zero (o caractere no início de uma string) ao valor do comprimento da string menos um (caractere final da string).

Um índice é nada mais que um número que identifica a posição daquele caractere na string. Pode parecer estranho, mas o primeiro caractere tem o índice 0, não 1. Esse é um comportamento que você pode encontrar em muitas outras linguagens de programação.

![](https://practicum-content.s3.amazonaws.com/resources/3.3PT_1_1697106043.png)

No final desta lição, você será capaz de acessar qualquer elemento específico de uma string usando o índice. Você também vai conseguir explicar como evitar erros comuns e vai praticar usando indexação positiva e negativa.

## Como acessar caracteres em uma string

Você pode acessar qualquer caractere de uma string fazendo referência ao seu índice. Tudo o que precisa fazer é colocar o índice entre colchetes depois da variável contendo a string. Mas preste atenção: como mostra a imagem acima, a indexação começa em zero. Veja este exemplo: Execute o código a seguir para ver como funciona a indexação.

CódigoPYTHON

9

1

2

3

word \= 'supercalifragilisticexpialidocious'

letter \= word\[5\] \# extraindo o 6º caractere de uma string

print(letter)

Mostrar a soluçãoExecutar

Aqui obtivemos o sexto caractere da palavra (não se esqueça de que começamos a contar de zero).

## Indexação negativa

Python nos dá uma maneira prática para indexar começando do final da string também. Isso é chamado de **indexação negativa** e pode ser feito usando números negativos como índice. O último caractere da string assume -1 como índice, o anterior tem -2 como índice e assim por diante até chegar ao primeiro caractere da string. Execute o código a seguir para ver como funciona a indexação negativa.

CódigoPYTHON

9

1

2

3

4

5

word \= 'supercalifragilisticexpialidocious'

  

last\_letter \= word\[\-1\] \# extraindo a última letra

third\_from\_end \= word\[\-3\] \# extraindo o terceiro caractere do final

print(last\_letter, third\_from\_end)

Mostrar a soluçãoExecutar

Aqui obtivemos o último (-1) e o antepenúltimo (-3) caractere na string. É importante perceber que apesar da indexação positiva começar em 0, a indexação negativa começa em -1. Pense da seguinte maneira: não existe um zero negativo.

### Questionário 1

Agora, vamos praticar.

Pergunta

Como você extrairia a letra `i` da variável `word` abaixo usando indexação positiva?

```
word = 'side'
```

`word[2]`

`word[1]`

Está correto!

`word[0]`

Muito bem!

### Questionário 2

Pergunta

Como você extrairia a letra `g` da variável `phrase` abaixo usando indexação negativa?

```
phrase = 'The other side of Olivia\'s rectangle is 2.' 
```

`phrase[-7]`

`phrase[-9]`

Exatamente!

`phrase[-6]`

Excelente!

Teoria

## Index errors

Se você tentar acessar um caractere fora do intervalo dos índices da string, você terá um erro `IndexError`:

```
word = 'word'
print(word[8])
```

```
...
IndexError: string index out of range
```

E se não soubermos qual é o último índice da string? Como podemos evitar esses erros?

Vamos ensinar sobre técnicas mais avançadas no futuro. Por enquanto, para ter certeza que seu código não vai tentar acessar nenhum caractere não existente, você pode manter-se entre 0 e o valor do comprimento da string menos 1.

Você consegue dizer por que menos 1? Apenas verifique o último exemplo: `len('word')` daria 4, mas o último caractere tem o índice 3 porque a contagem começou com 0.

Se você pensar nisso, o mesmo é válido para a indexação negativa, mas, nesse caso, o primeiro caractere terá o índice `-len('word')`. É simples assim!

### Tarefa

Vamos praticar usando a função `len()` e um pouco de matemática. Imprima o último caractere da string armazenada na variável `phrase` usando a função `len()` e índice positivo, de forma que o índice seja um número positivo, e a letra `g` seja impressa da palavra `duckling`. Depois, imprima o primeiro caractere da string armazenada na variável `phrase` usando a função `len()` e índice negativo. Isso significa que seu índice precisa ser um número negativo que acessa a letra `O` de `Olivia`.

CódigoPYTHON

9

1

2

3

phrase \= 'Olivia loves her little yellow duckling'

print(phrase\[len(phrase)\-1\])\# imprima aqui o último caractere usando a função len() e indexação positiva

print(phrase\[\-len(phrase)\])\# imprima aqui o último caractere usando a função len() e indexação negativa

Dica

Mostrar a soluçãoValidar

Teoria

### Como alterar a string

Se você tentar acessar um caractere de uma string pelo seu índice e redefinir o caractere, você terá outro erro:

```
word = 'bbc'
word[0] = 'a'
```

```
...
TypeError: 'str' object does not support item assignment
```

A razão para isso é que as strings são um `tipo de dados imutável`. Mas você pode substituir uma string antiga por uma nova:

```

word = 'bbc'
word = 'abc'
```

Mais adiante neste capítulo, vamos discutir como trabalhar com esse problema e alterar dinamicamente algumas partes da sua string.

Por enquanto, veremos como podemos usar os índices para acessar um grupo de elementos na nossa string.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-18-606Z.md
### Última modificação: 2025-05-28 18:37:18

# Fatias - TripleTen

Capítulo 3/9

Strings

# Fatias

<iframe class="base-markdown-iframe__iframe" id="player-DIEXN1LAYG4" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Slices" width="640" height="360" src="https://www.youtube.com/embed/DIEXN1LAYG4?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fb7719634-9de9-4dc5-ba9b-51cadd2fece1%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Na lição anterior, você viu como podemos indexar cada caractere em uma string usando indexação positiva ou negativa. Além disso, vimos que strings não podem ser modificadas via índices porque são imutáveis.

E se quisermos extrair não só um caractere de uma string, mas vários caracteres? É aí que as fatias entram em cena!

## Fatias permitem acessar múltiplos caracteres

O uso de fatias permite selecionar vários caracteres de uma string, em vez de apenas um, de modo que possamos acessar nossos dados mais rapidamente e trabalhar de forma mais eficiente.

No final desta lição, você será capaz de usar fatias para acessar qualquer subconjunto de caracteres de uma string.

Para fatiar uma string, também usamos a indexação, mas para fazer isso, precisamos ir um pouco além do que aprendemos na lição anterior. Para as fatias, precisamos colocar os índices inicial _e_ final entre colchetes, separados por dois pontos. Execute o seguinte código para ver como isso funciona.

CódigoPYTHON

9

1

2

3

city \= 'Rio de Janeiro, Brasil'

substring \= city\[7:14\]

print(substring)

Mostrar a soluçãoExecutar

Se contar, você vai perceber que o caractere no índice `14` não está incluso no resultado. Isto acontece porque **o último caractere do intervalo não faz parte do resultado final**. Então, o número de caracteres em uma substring sempre será igual ao índice final menos o índice inicial. Saber essa propriedade de fatiamento em Python vai ajudar você a ter certeza de que seu código retorna as fatias esperadas.

Brinque com a fatia da string abaixo para melhorar a sua compreensão sobre como isso funciona.

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Widgets/Data/array/pt_br/array.html" style="min-width: 112%; min-height: 565px;"></iframe>

Você viu na última lição que se tentarmos indexar um caractere fora do intervalo de uma string, Python retorna um erro. Com fatias, isso não é um problema; o intervalo da fatia pode ser maior que o número de caracteres em uma string. Execute o seguinte código para ver como ele funciona.

CódigoPYTHON

9

1

2

3

4

5

city \= 'Rio de Janeiro, Brasil'

  

print(city\[4:1000\])

print(city\[\-15:500\])

print(city\[4:0\])

Mostrar a soluçãoExecutar

O Python ignora valores de índice de fora do intervalo real:

-   A substring `city[4:1000]` vai conter os caracteres a partir do índice `4` até o final da string.
-   `city[-15:500]` vai conter a substring começando de `J` (que tem o índice `-15`) e indo até o final da string.
-   Em `city[4:0]`, como o primeiro índice é maior que o segundo, a fatia vai retornar uma string vazia.

Para fatiar uma substring começando do primeiro caractere, você pode deixar o primeiro índice em branco:

```
city = 'Rio de Janeiro, Brasil'
print(city[:14])  # em vez de city[0:14]
```

```
Rio de Janeiro
```

De forma parecida, você pode omitir o índice final se quiser que sua substring inclua tudo até o final da string:

```
city = 'Rio de Janeiro, Brasil'
print(city[16:])  # em vez de city[16:22]
```

```
Brasil
```

### Questionário

Agora vamos praticar!

Pergunta

A variável `word` armazena a string `'sentence'`. Como você fatiaria essa string para extrair `ten`?

```
word = 'sentence'
```

`word[4:7]`

`word[3:6]`

Está correto!

Trabalho maravilhoso!

### Tarefa

e um pouco mais de prática!

Obtenha a substring dos primeiros quatro caracteres da variável `spell` e a imprima.

CódigoPYTHON

9

1

2

spell \= 'abracadabra'

print(spell\[:4\])\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

Na próxima lição, você vai aprender como tornar suas strings mais interativas e alterar dinamicamente seus elementos usando formatação especial.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-19-913Z.md
### Última modificação: 2025-05-28 18:37:20

# Strings formatadas - TripleTen

Capítulo 3/9

Strings

# Strings formatadas

<iframe class="base-markdown-iframe__iframe" id="player-Fdza6K4cW_I" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Formatted Strings" width="640" height="360" src="https://www.youtube.com/embed/Fdza6K4cW_I?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fec7c3092-e07a-4dd3-9e84-68fdab3a8f45%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Agora que você sabe como criar e até mesmo fatiar strings, é hora de ir um pouco mais fundo e aprender como torná-las mais dinâmicas usando formatação especial.

Ao final desta lição, você será capaz de explicar como f-strings podem produzir strings que contêm variáveis e vai saber como incorporar suas próprias variáveis em f-strings para deixá-las muito mais convenientes de imprimir.

## Incorporação de variáveis em strings

Suponha que queremos imprimir uma string que diz o nome de uma pessoa, bem como sua idade e altura. Podemos fazer algo assim:

```
message = "Maggie tem 23 anos e 157 cm de altura"
print(message)
```

```
Maggie tem 23 anos e 157 cm de altura
```

Não há nada de errado com esse código, mas se um dia os dados da Maggie mudarem ou se quisermos exibir informações de alguma outra pessoa, teremos que modificar a string. Quando seu código for longo, vai ser quase inevitável cometer erros ao fazer isso, então é muito melhor usar variáveis.

Podemos dividir a string anterior em strings separadas e concatená-las com variáveis nos lugares em que queremos imprimi-las. Para fazer isso, usamos o sinal +, como vimos antes.

Brinque com os valores das variáveis abaixo para ver como isso afeta o resultado quando você executar o código:

CódigoPYTHON

9

1

2

3

4

5

6

name \= 'Maggie'

age \= 23

height \= 157

  

message \= name + " tem " + str(age) + " anos e " + str(height) + " cm de altura"

print(message)

Mostrar a soluçãoExecutar

Esse código é melhor, mas ainda não é o ideal. Criar a variável `message` dá muito trabalho, e precisamos ter muito cuidado ao converter os nossos números em strings. Strings formatadas tornam esse processo _muito_ mais fácil.

Há duas maneiras de criar strings formatadas:

1.  Usando f-strings
2.  Usando o método `format()`

## F-strings

É bem simples criar f-strings: tudo o que precisamos fazer é adicionar à nossa string a letra `f` (de "`f`ormatada") antes da aspa de abertura. Nomes de variáveis em uma f-string são indicados entre chaves `{}`.

![](https://practicum-content.s3.amazonaws.com/resources/3.5_1PT_1690202303.png)

Execute o código abaixo para ver como as f-strings funcionam. Altere as variáveis e veja como isso afeta o resultado.

CódigoPYTHON

9

1

2

3

4

5

6

name \= 'Maggie'

age \= 23

height \= 157

  

message \= f"{name} tem {age} anos e {height} cm de altura"

print(message)

Mostrar a soluçãoExecutar

Esse código é muito mais limpo e fácil de entender do que o de antes, não é?

Também podemos realizar operações aritméticas dentro das chaves. Por exemplo:

```
name = 'Maggie'
age = 23

message = f"No ano que vem, {name} terá {age + 1} anos"
print(message)
```

```
No ano que vem, Maggie terá 24 anos
```

### Tarefa 1

Agora é sua vez de criar uma f-string. Use as variáveis no código abaixo para criar uma f-string que vai exibir a mensagem abaixo: basta colocar o caractere `f` antes da string e as variáveis entre chaves.

```
Cinco anos atrás, Olívia tinha 20 anos
```

Observe que a variável `age` armazena `25` como valor, mas a string acima que pedimos para você imprimir mostra `20`. Use uma operação aritmética dentro da f-string para calcular a idade.

CódigoPYTHON

9

1

2

3

4

5

6

7

name \= 'Olívia'

age \= 25

  

\# preencha os espaços em branco abaixo

message \= f"Cinco anos atrás, {name} tinha {age\-5} anos"

  

print(message)

Dica

Mostrar a soluçãoValidar

Teoria

## O método `format()`

Também podemos usar o método string `format()` para obter os mesmos resultados que acabamos de obter usando f-strings. Métodos são funções especiais que podem ser chamados para objetos individuais. Vamos aprender mais sobre métodos aplicados a strings mais adiante neste capítulo.

Por enquanto, confira como usar o método `format()` e depois vamos falar sobre como ele funciona:

```
name = 'Maggie'
age = 23
height = 157

message = "{} tem {} anos e {} cm de altura".format(name, age, height)
print(message)
```

```
Maggie tem 23 anos e 157 cm de altura
```

Podemos chamar o método `format()` em uma string colocando um ponto entre a string e o nome do método. A string contém chaves, e `format()` contém as variáveis como argumentos, um argumento para cada conjunto de chaves. Os valores das variáveis substituem as chaves na ordem na qual os argumentos aparecem em `format()`.

![](https://practicum-content.s3.amazonaws.com/resources/3.5_2PT_1690202329.png)

Como alternativa, podemos usar os nomes dos argumentos dentro das chaves da mesma forma que fizemos com f-strings. Nesse caso, precisamos especificar os valores dos argumentos dentro de `format()`, mas os argumentos podem ser passados em qualquer ordem.

```
name = 'Maggie'
age = 23
height = 157

message = "{n} tem {a} anos e {h} cm de altura".format(h=height, n=name, a=age)
print(message)
```

```
Maggie tem 23 anos e 157 cm de altura
```

### Tarefa 2

Use as variáveis fornecidas e o método `format()` para exibir a seguinte mensagem:

```
Olívia tem 25 anos
```

Com isso em mente, vamos praticar!

CódigoPYTHON

9

1

2

3

4

5

6

name \= 'Olívia'

age \= 25

  

\# crie uma string formatada abaixo

message \= "{} tem {} anos".format(name,age)

print(message)

Dica

Mostrar a soluçãoValidar

Agora que você tem um método de string no seu conjunto de ferramentas, isso significa que você está com tudo pronto para aprender mais alguns. Na próxima lição, vamos discutir métodos específicos de string que costumam ser usados para processar e limpar dados de texto antes da análise.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-21-946Z.md
### Última modificação: 2025-05-28 18:37:22

# Métodos de strings - TripleTen

Capítulo 3/9

Strings

# Métodos de strings

<iframe class="base-markdown-iframe__iframe" id="player-bWYefBywKto" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="String Methods" width="640" height="360" src="https://www.youtube.com/embed/bWYefBywKto?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F032ba782-0228-4ab9-ac42-f342396d8490%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Na lição anterior, você aprendeu sobre o método específico para strings `format()`, e agora podemos chamá-lo em objetos string para resolver o problema de incorporação de variáveis em strings. Acontece que as strings vêm com uma infinidade de métodos, e cada um serve para processar strings de uma maneira diferente.

Usar métodos de strings para processar strings é uma parte muito importante da análise e modelagem de dados de texto. No final desta lição, você será capaz de usar os seguintes métodos em seu código:

-   `upper()` — converte todos os caracteres para maiúsculas
-   `lower()` — converte todos os caracteres para minúsculas
-   `replace()` — substitui algumas partes de uma string por outra string
-   `strip()` — remove espaços no início e no final da string

## Ajustes de maiúsculas e minúsculas

Já que strings diferenciam maiúsculas de minúsculas em Python, com frequência precisamos prestar atenção a esse aspecto durante as etapas de limpeza e processamento. Em outras palavras, Python considera que as strings `'Olá'`, `'olá'` e `'OLÁ'` são valores distintos.

Para contornar isso, podemos padronizar os textos transformando todas as letras em maiúsculas ou minúsculas. Isso torna muito mais fácil encontrar valores duplicados ou agrupar dados de texto por categorias únicas.

Temos dois métodos para fazer isso: `upper()` e `lower()`. O método `upper ()` converte todas as letras para maiúsculas, e o método `lower()` converte todas as letras para minúsculas.

Aqui está um exemplo de `upper()`:

```
message = "Oi, meu nome é Han Solo!"

print(message)
message_new = message.upper()
print(message_new)
```

```
Oi, meu nome é Han Solo!
OI, MEU NOME É HAN SOLO!
```

Pronto, toda a nossa string está em maiúsculas. É magia!

### Tarefa 1

O método `lower()` funciona da mesma maneira que `upper()`. Use `lower()` para converter a lista de compras abaixo para minúsculas. Reatribua a variável `shopping_list` abaixo com a lista convertida.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

shopping\_list \= '''MAÇÃS

laranjas

leite

Maçãs

pão

Sorvete

maçãs

ÓLEO

Pão'''

  

shopping\_list \= shopping\_list.lower()\# converta a variável shopping\_list para minúscula aqui

print(shopping\_list)

Dica

Mostrar a soluçãoValidar

Teoria

## Substituição de uma substring

Outra etapa comum de processamento de dados de texto é remoção ou substituição de partes de strings. Por exemplo, precisamos corrigir erros de digitação em números de telefone de nossos clientes ou remover nomes de domínio de seus endereços de e-mail. Como você sabe, objetos string são `imutáveis`, então se quisermos fazer alterações, precisamos reatribuir a string inteira de novo. Felizmente, podemos usar o método `replace()` para remover ou substituir **substrings** (partes de strings).

O método `replace()` leva dois argumentos:

1.  A substring que queremos substituir
2.  A string pela qual queremos substituir a string inicial

Vamos usar `replace()` para corrigir alguns erros de digitação em outra lista de compras em que escrevemos "óleo" errado várias vezes:

```
shopping_list = '''óelo de canola
óleo de girassol
óleo de abacate
óelo de amendoim'''

print(shopping_list.replace('óelo', 'óleo'))
```

```
óleo de canola
óleo de girassol
óleo de abacate
óleo de amendoim
```

Todas as instâncias de "óelo" na string `shopping_list` foram substituídas por "óleo".

Também podemos usar `replace()` para remover substrings sem substituí-las. Para fazer isso, simplesmente passamos a string vazia `''` como o segundo argumento. Vamos usar `replace()` para remover os sublinhados de uma string:

```
user_ids = "_151234_, _792051_, _955247_"
user_ids = user_ids.replace('_', '')

print(user_ids)
```

```
151234, 792051, 955247
```

Agora, vamos praticar:

### Tarefa 2

Use `replace()` para substituir todas as ocorrências de `'maçã'` por `'laranja'` na seguinte string e atribua a string modificada à variável `new_string`. Imprima `new_string`.

CódigoPYTHON

9

1

2

3

4

5

old\_string \= "Eu tenho uma maçã, ela tem uma maçã, ele tem uma maçã, nós todos temos maçãs."

new\_string \= old\_string.replace("maçã", "laranja")\# escreva seu código aqui

  

print(new\_string)

  

Dica

Mostrar a soluçãoValidar

Teoria

## Remoção de espaços em branco

Por fim, outro procedimento comum de processamento de dados de texto é remover espaços em branco do início e do final de uma string.

Por exemplo, quando dados são exportados entre sistemas, artefatos como espaços à esquerda podem ser introduzidos em nomes de colunas. Isso pode dificultar escrever e compartilhar código que usa os mesmos dados.

O método `strip()` remove todos os caracteres de espaço no início e no final de uma string. Todos os espaços no meio da string são mantidos. Espaços em branco são os caracteres de espaços, tabulação e linhas novas.

Aqui está um exemplo:

```
column_name = '     data de compra  '

print(column_name)
print(len(column_name))
print()
print(column_name.strip())
print(len(column_name.strip()))
```

```
     data de compra 
20

data de compra
13
```

Podemos ver que todos os espaços em branco foram removidos, o que é refletido nos valores de comprimento das strings com e sem chamar `strip()`.

### Tarefa 3

Você recebeu uma string `sentence` que contém espaços extras antes do texto. Use o método `strip()` para remover esses espaços. Quando removido, substitua todos os espaços da string por `-`.

Atribua a string modificada a uma nova variável chamada `new_sentence` e imprima-a.

CódigoPYTHON

9

1

2

3

4

sentence \= " Python é uma linguagem de programação interpretada de alto nível, de uso geral. "

new\_sentence \= sentence.strip().replace(' ', '-')

print(new\_sentence)

  

Dica

Mostrar a soluçãoValidar

Essa foi a última lição deste capítulo! Passe para a próxima lição para testar seus conhecimentos.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-23-339Z.md
### Última modificação: 2025-05-28 18:37:23

# Atividade prática do capítulo - TripleTen

Capítulo 3/9

Strings

# Atividade prática do capítulo

Chegou a hora de testar seu conhecimento de strings com exercícios práticos. Vamos recapitular brevemente o que aprendemos até agora:

-   Strings possuem propriedades. Por exemplo, agora sabemos sobre caracteres de escape como `\`, `\n` e `\t`, usados para ignorar um caractere, iniciar uma nova linha ou adicionar uma aba, respectivamente. A função `len()` é aquela que precisamos para verificar o comprimento de uma string.
-   Strings podem ser indexadas por meio de indexação positiva (por exemplo, `word[3]` ) ou negativa (por exemplo, `word[-1]`), e se tentarmos extrair um índice que exceda o comprimento da string, vai ocorrer um erro.
-   Strings são imutáveis, portanto não podemos alterá-las com indexação.
    
    ```
      word = 'bbc'
      word[0] = 'a'
      
    ```
    
    ```
      ...
      TypeError: 'str' object does not support item assignment
      
    ```
    
-   Strings podem ser fatiadas passando o índice inferior e superior para colchetes (`[]`) separados por dois pontos (`:`).
    
    ```
      city = 'Rio de Janeiro, Brasil'
      substring = city[7:14]
      print(substring)
      
    ```
    
    ```
      Janeiro
      
    ```
    
-   Formatar strings não é mais um problema porque agora sabemos sobre F-strings:
    
    ```
      name = 'Maggie'
      age = 23
      
      message = f"No ano que vem, {name} terá {age + 1} anos"
      print(message)
      
    ```
    
    ```
      No ano que vem, Maggie terá 24 anos
      
    ```
    
    e o método `format()`:
    
    ```
      name = 'Maggie'
      age = 23
      height = 157
      
      message = "{} tem {} anos e {} cm de altura".format(name, age, height)
      print(message)
      
    ```
    
    ```
      Maggie tem 23 anos e 157 cm de altura
      
    ```
    
-   Por último, mas não menos importante, métodos como `upper()`, `lower()`, `replace()` e `strip()` permitem alterar strings facilmente em uma única linha de código, convertendo todos os caracteres para maiúsculas/minúsculas, substituindo partes de uma string por outra string ou removendo espaços desnecessários.
    

No primeiro quiz deste capítulo prático, você vai precisar corrigir o código abaixo para não receber uma mensagem de erro de sintaxe. Lembre-se: quando Python vê diversos pares de aspas simples e duplas, ele pode ficar confuso e gerar um erro. Confira duas estratégias para lidar com esses erros:

1.  Cite uma string entre aspas duplas e coloque apenas aspas simples na string. Por exemplo, `'Quero um copo d'água!'` gera um erro, mas `"Quero um copo d'água!"` não gera.
2.  Use caracteres de escape. Por exemplo, `'Quero um copo d'água!'` gera um erro, mas `'Quero um copo d\'água!'` não gera.

Agora, vamos colocar isso em prática.

Pergunta

Veja a citação abaixo. Seu objetivo é selecionar uma resposta que sugira mudanças corretas.

```
quote = 'John Johnson disse, 'Primeiro, resolva o problema. Em seguida, escreva o código.'
```

`quote = "John Johnson disse, \'Primeiro, resolva o problema. Em seguida, escreva o código.'"`

`quote = "John Johnson disse, 'Primeiro, resolva o problema. Em seguida, escreva o código.'"`

Está correto!

Fantástico!

No próximo exercício, use `len()` para contar o comprimento da `string` e imprimir o resultado:

CódigoPYTHON

9

1

2

3

string \= 'In some ways, programming is like painting. You start with a blank canvas and certain basic raw materials. You use a combination of science, art, and craft to determine what to do with them'

  

print(len(string))\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

A seguir, obtenha o 4º elemento da string a partir do início consultando seu índice e imprima-o. Em seguida, obtenha o 4º elemento a partir do final usando indexação negativa e imprima.

CódigoPYTHON

9

1

2

3

4

string \= 'In some ways, programming is like painting. You start with a blank canvas and certain basic raw materials. You use a combination of science, art, and craft to determine what to do with them'

  

print(string\[3\])\# obtenha e imprima o quarto elemento pelo índice aqui

print(string\[\-4\])\# obtenha e imprima o 4º elemento usando indexação negativa aqui

Dica

Mostrar a soluçãoValidar

Substitua a substring `'somente'` pela substring `‘apenas’` na string `quote`. Imprima `quote` ao fazer essa alteração.

CódigoPYTHON

9

1

2

3

quote \= 'A vida é curta demais para não se divertir; estamos aqui somente por um curto período de tempo, em comparação ao Sol e a Lua e tudo isso'

quote \= quote.replace('somente', 'apenas')

print(quote)\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

Suponha que você esteja criando um programa que aceita uma entrada de usuário e a salva em um banco de dados. No entanto, você quer ter certeza de que todas as letras na entrada sejam maiúsculas e ela não contenha espaços em branco extras.

Complete o seguinte pré-código para obter a string de entrada corrigida:

CódigoPYTHON

9

1

2

3

4

5

6

user\_input \= " MaRaDoNa "

  

clean\_input \= user\_input.strip().upper()\# escreva seu código aqui

  

print(clean\_input)

  

Dica

Mostrar a soluçãoValidar

Pergunta

Quais das seguintes opções são válidas para a formatação da string? Escolha as opções que se aplicam.

Escolha quantas quiser

```
city = 'Dhaka'
country = 'Bangladesh'
phrase = f'{city} é a capital de {country}'
```

Usamos formatação para incluir variáveis de qualquer tipo em nossas strings. Não é errado usar o método `format()`, mas seria melhor usar uma f-string.

```
city = 'Dhaka'
country = 'Bangladesh'
phrase = f'(city) é a capital de (country)'
```

```
city = 'Dhaka'
country = 'Bangladesh'
phrase = '{} é a capital de {}'.format(city, country)
```

Usamos formatação para incluir variáveis de qualquer tipo em nossas strings. Não é errado usar o método `format()`, mas seria melhor usar uma f-string.

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-24-652Z.md
### Última modificação: 2025-05-28 18:37:25

# Conclusão - TripleTen

Capítulo 3/9

Strings

# Conclusão

Você concluiu dois capítulos. Não é pouca coisa, muito bem!

![](https://practicum-content.s3.amazonaws.com/resources/3.9_PT_1690202680.png)

Vamos ver o que você aprendeu neste capítulo.

Ao concluir este capítulo, você será capaz de:

-   Definir as características básicas de strings Se ainda tiver dúvidas, revise a lição sobre [Propriedades básicas de strings](https://tripleten.com/trainer/data-analyst/lesson/3367693f-c6a5-453e-8dfa-4ca77854515f/).
-   Calcular o comprimento das strings, usar caracteres de escape e criar strings de várias linhas Se ainda tiver dúvidas, revise a lição sobre [Propriedades básicas de strings](https://tripleten.com/trainer/data-analyst/lesson/3367693f-c6a5-453e-8dfa-4ca77854515f/).
-   Usar índice positivo e negativo para obter um elemento da string Se ainda tiver dúvidas, revise a lição sobre [Índices](https://tripleten.com/trainer/data-analyst/lesson/0284c121-6f2a-4615-9ff4-4940a66b1955/).
-   Formatar strings usando f-strings e o método `format()` Se ainda tiver dúvidas, revise a lição sobre [Strings formatadas](https://tripleten.com/trainer/data-analyst/lesson/ec7c3092-e07a-4dd3-9e84-68fdab3a8f45/).
-   Usar métodos integrados para processar strings Se ainda tiver dúvidas, revise a lição sobre [Métodos de string](https://tripleten.com/trainer/data-analyst/lesson/032ba782-0228-4ab9-ac42-f342396d8490/).

## O que vem a seguir?

No próximo capítulo, você vai aprender sobre outro tipo de dados fundamental do Python: listas. É uma estrutura de dados muito potente e versátil que você vai usar muitas vezes no seu trabalho com Python, então vamos lá!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-26-916Z.md
### Última modificação: 2025-05-28 18:37:27

# Introdução - TripleTen

Capítulo 4/9

Listas

# Introdução

Você já aprendeu muito até agora. Começamos com variáveis, operações básicas e funções como `print()`. Em seguida, seguimos em frente e nos concentramos mais nas strings e examinamos não apenas suas propriedades, mas também como indexar, fatiar e formatá-las. Com essas habilidades, você pode enfrentar alguns dos desafios reais que profissionais de dados enfrentam todos os dias. No projeto do final deste sprint, tudo isso vai ser útil.

Agora, vamos aprofundar nosso conhecimento em Python e aprender sobre outra estrutura de dados comum: as listas, que usamos extensivamente para armazenar diferentes tipos de informações.

Ao final deste capítulo, você será capaz de:

-   criar e adicionar dados a listas
-   ordenar, fatiar e realizar outras modificações básicas em listas
-   trabalhar com listas aninhadas
-   usar métodos de lista para pré-processar strings

---

Serão necessárias de 1,5 a 2 horas para concluir este capítulo. Se você sentir que está ficando para trás, entre em contato com a Equipe de Orientação para pedir ajuda.

![](https://practicum-content.s3.amazonaws.com/resources/image_1687945434.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-28-213Z.md
### Última modificação: 2025-05-28 18:37:28

# Propriedades básicas das listas - TripleTen

Capítulo 4/9

Listas

# Propriedades básicas das listas

<iframe class="base-markdown-iframe__iframe" id="player-bDWiYUNq4xo" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Basic Properties of Lists" width="640" height="360" src="https://www.youtube.com/embed/bDWiYUNq4xo?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F8eee3001-25d5-432c-87c4-cc22c92695ea%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Até agora, trabalhamos apenas com tipos de dados de Python com um único valor. As listas são diferentes: elas permitem armazenar uma coleção de valores em uma única estrutura de dados. Isso é útil quando temos uma grande quantidade de dados. Se tivéssemos uma tabela com milhares de valores, armazenar cada um em uma variável individual se tornaria um grande desafio. Em vez disso, podemos armazenar todos na mesma lista e então escolher aqueles com os quais precisamos trabalhar.

![](https://practicum-content.s3.amazonaws.com/resources/4.2PT_1691045940.png)

Ao final desta lição, você será capaz de criar listas e calcular o número de itens em uma lista.

## Como criar uma lista

As listas são criadas usando colchetes. Podemos criar uma lista vazia usando colchetes, basta não colocar valores entre eles:

```
my_list = [] # criando uma lista

print(my_list) # imprima a lista
print(type(my_list)) # verificando o tipo
print(len(my_list))  # verificando o comprimento
```

```
[]
<class 'list'>
0
```

A variável `my_list` tem um tipo de dados que ainda não vimos — **list** (lista). A lista está vazia, então é de comprimento zero. Uma lista vazia não é muito interessante nesse momento, então vamos criar uma com alguns valores reais:

```
favorite_films = ['The Graduate', 'Reservoir Dogs', 'Fear and Loathing in Las Vegas']

print(favorite_films)
print(type(favorite_films))
print(len(favorite_films))
```

```
['The Graduate', 'Reservoir Dogs', 'Fear and Loathing in Las Vegas']
<class 'list'>
3
```

Assim é mais interessante! A nossa lista tem três **itens** (que também são chamados de **elementos** ou **valores**). Cada item é separado por uma vírgula. Embora os itens sejam strings, a própria lista tem o tipo de dados `list`. O comprimento da lista é 3 porque ela contém três itens.

Observe também que chamamos a variável da lista de `favorite_films` porque os itens na lista são nomes de filmes; podemos chamar a lista como quisermos, mas é uma boa prática de programação dar nomes que descrevem os dados contidos nas variáveis.

Todos os itens na lista `favorite_films` têm o mesmo tipo de dados, mas isso não é uma regra rígida. Listas podem conter itens de tipos diferentes, e itens de qualquer tipo de dados podem fazer parte de uma lista — até mesmo outras listas:

```
my_list = ['<3', 77, 3.89, True, False, ['sub', 'list', 0.2]]

print(my_list)
print(len(my_list))
```

```
['<3', 77, 3.89, True, False, ['sub', 'list', 0.2]]
6
```

Na prática, geralmente não precisamos misturar vários tipos de dados em uma lista. Mas isso pode ser útil, por exemplo, se quisermos representar as linhas de uma tabela como uma lista. Observe que o comprimento de `my_list` é 6. Embora a sublista contenha três itens, ela conta como apenas um item em `my_list`.

### Tarefa

Agora você sabe o suficiente para criar suas próprias listas. Abaixo estão algumas variáveis (`title`, `year`, `genre`, etc.) que armazenam informações sobre o filme Fight Club (Clube da Luta). Em vez de ter essas informações na forma de variáveis, queremos que você crie uma lista que armazene todas essas informações.

Abaixo criamos uma variável chamada `movie_info`. Seu objetivo é a preencher de forma que a lista tenha a seguinte ordem de itens: título, ano, gênero, duração e classificação (title, year, genre, duration e rating).

Após criá-la, a imprima.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

9

title \= 'Fight Club'

year \= 1999

genre \= \['thriller', 'drama', 'crime'\]

duration \= 139

rating \= 8.644

  

\# adicione itens à lista movie\_info:

movie\_info \= \[title, year, genre, duration ,rating\]

print(movie\_info)\# imprima a lista

Dica

Mostrar a soluçãoValidar

### Questionário

Pergunta

Que comprimento tem a lista `movie_info` que você acabou de criar?

5

A sublista conta como apenas um item.

Trabalho maravilhoso!

Aprendemos como criar listas, e isso é ótimo. Mas você pode estar se perguntando como podemos acessar os itens de uma lista depois de colocá-los nela. A resposta está na indexação. Vamos discutir isso com mais detalhes na próxima lição.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-29-561Z.md
### Última modificação: 2025-05-28 18:37:30

# Índices e fatias - TripleTen

Capítulo 4/9

Listas

# Índices e fatias

<iframe class="base-markdown-iframe__iframe" id="player-t3DZoyt32Yc" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Indices and Slices" width="640" height="360" src="https://www.youtube.com/embed/t3DZoyt32Yc?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F111211db-ba82-4d92-be6d-12ba5ce6aaaa%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Agora sabemos como criar listas. Mas essa não é a única operação que podemos realizar com listas: há muitas outras. Nesta lição, vamos falar sobre as operações mais importantes: indexação e fatiamento.

## Indexação de listas

Assim como strings, listas são coleções _ordenadas_ de itens. Isso significa que cada item possui um "lugar na fila", ou seja, um índice. A indexação da lista começa em 0 e o índice aumenta em 1 para cada item subsequente.

![](https://practicum-content.s3.amazonaws.com/resources/4.3PT_1690202920.png)

Esses índices nos permitem acessar elementos e fatiar partes de uma lista. Primeiro, vamos examinar como podemos acessar elementos de listas.

### Como acessar um item

Para acessar um item em um determinado índice, colocamos o número do índice entre colchetes após a lista (exatamente da mesma forma que fazemos com strings):

```
movie_info = ['Fight Club', 1999, ['thriller', 'drama', 'crime'], 139, 8.644]

print(movie_info[0])
print(type(movie_info[0]))
```

```
Fight Club
<class 'str'>
```

Selecionamos o primeiro item na lista (ou seja, o item no índice 0) e confirmamos isso imprimindo seu valor e seu tipo de dados.

### Questionário 1

Agora é sua vez.

Pergunta

Como você acessaria o terceiro item da lista `movie_info` abaixo:

```
movie_info = ['Fight Club', 1999, ['thriller', 'drama', 'crime'], 139, 8.644]
```

`movie_info[2]`

Você está pegando o jeito da indexação, certo?

`movie_info[3]`

`movie_info[4]`

Seu entendimento sobre o material é impressionante!

### Questionário 2

Outro quiz para você:

Pergunta

O que você acha que vai acontecer se tentarmos acessar um valor de índice que não existe?

```
movie_info = ['Fight Club', 1999, ['thriller', 'drama', 'crime'], 139, 8.644]

print(movie_info[5])
```

O Python vai retornar ‘0’

O Python vai retornar o último elemento existente

O Python vai lançar um erro IndexError: list index out of range

Exatamente! Assim como com strings, o código vai retornar um erro.

Excelente!

### Questionário 3

A lista `movies_info` tem apenas 5 itens, então 4 é o seu índice máximo. Uma tentativa de acessar um item maior que 4 vai gerar um erro de índice; tenha em mente o comprimento da lista para evitar isso.

E o último antes de prosseguirmos:

Pergunta

Na sua opinião, como podemos usar indexação negativa para imprimir o quarto item (`139`) em `movie_info`?

```
movie_info = ['Fight Club', 1999, ['thriller', 'drama', 'crime'], 139, 8.644]
```

`movie_info[-2]`

Isso é exatamente o que você precisa fazer se quiser extrair `139`.

`movie_info[-1]`

`movie_info[-3]`

Excelente!

Teoria

## Fatiamento

Acessar elementos individuais em listas é algo feito com frequência. Mas e se quisermos pegar vários de uma vez só? Para obter vários itens de uma vez, podemos usar indexação e fatiamento.

A sintaxe geral para fatiar uma lista é colocar o primeiro e o último índice da fatia entre colchetes após a lista, separando-os por dois pontos.

![](https://practicum-content.s3.amazonaws.com/resources/4.3_2PT_1690203023.png)

É muito mais fácil mostrar isso com um exemplo:

```
movie_info = ['Fight Club', 1999, ['thriller', 'drama', 'crime'], 139, 8.644]

print(movie_info[1:4])
print(type(movie_info[1:4]))
print(len(movie_info[1:4]))
```

```
[1999, ['thriller', 'drama', 'crime'], 139]
<class 'list'>
3
```

Acabamos de fazer uma fatia da lista `movie_info` do índice 1 ao 4. Há algumas coisas a serem observadas sobre o resultado:

-   O item no índice superior _não_ está incluso na lista
-   O tipo de dados de uma fatia da lista também é uma lista
-   O comprimento da fatia é igual à diferença entre o índice superior e o inferior

Boas notícias: com fatias, ao contrário de trabalhar com um único elemento, podemos especificar um índice superior que está fora do intervalo do índice sem causar um erro. Em vez disso, obtemos todos os elementos até e incluindo o último elemento da lista:

```
movie_info = ['Fight Club', 1999, ['thriller', 'drama', 'crime'], 139, 8.644]

print(movie_info[2:100])
print(len(movie_info[2:100]))
```

```
[['thriller', 'drama', 'crime'], 139, 8.644]
3
```

Há uma notação abreviada para isso. Se omitirmos um índice superior completamente, vamos receber todos os itens da lista começando do item inferior:

```
movie_info = ['Fight Club', 1999, ['thriller', 'drama', 'crime'], 139, 8.644]

print(movie_info[1:])
print(len(movie_info[1:]))
```

```
[1999, ['thriller', 'drama', 'crime'], 139, 8.644]
4
```

### Tarefa 1

Isso também se aplica ao contrário. Podemos omitir o limite inferior para receber todos os itens começando do início da lista _até, mas não incluindo_ o item no índice superior. Tente usar esta forma de indexação para imprimir fatias de `movies_info` com os intervalos apropriados conforme os comentários.

CódigoPYTHON

9

1

2

3

4

5

6

7

movies\_info \= \['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\]

\# fatie os elementos do índice 1 ao 3:

print(movies\_info\[1:4\])

\# fatiando os elementos do índice 0 ao 2:

print(movies\_info\[:3\])

\# fatiando os elementos do índice 2 até o último:

print(movies\_info\[2:\])

Dica

Mostrar a soluçãoValidar

Parabéns! Agora você sabe como criar listas, encontrar índices de elementos específicos e até mesmo fatiar listas para obter vários elementos de uma vez. Agora é hora de discutir como modificar listas.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-33-508Z.md
### Última modificação: 2025-05-28 18:37:33

# Modificação de listas - TripleTen

Capítulo 4/9

Listas

# Modificação de listas

<iframe class="base-markdown-iframe__iframe" id="player-3XZRJyiBes8" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Modifying Lists" width="640" height="360" src="https://www.youtube.com/embed/3XZRJyiBes8?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F5fce1a26-a0e8-475d-809a-27ba5345027f%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

![](https://practicum-content.s3.amazonaws.com/resources/3-4_1687945873.png)

Certo, sabemos como indexar e fatiar listas. Isso é praticamente o mesmo que fizemos antes com strings, mas há alguma diferença importante entre listas e strings? Sim! Vamos aprender sobre essas diferenças agora.

## Como modificar listas

Vemos que existem muitas semelhanças entre strings e listas, mas há uma diferença significativa: as listas podem ser modificadas. Em termos de Python, listas são objetos \*\*mutáveis**, enquanto strings são** imutáveis\*\*, o que significa que seus valores não podem ser atualizados.

Ao final desta lição, você será capaz de modificar listas usando os métodos `append(),` `extend()`, `insert()` e `pop()`. Esses métodos nos permitem adicionar ou remover dados de uma lista existente de uma maneira simples. É muito mais fácil alterar elementos em uma lista do que criar a lista do zero sempre que quisermos modificá-la.

## Como adicionar elementos a uma lista

Há três maneiras de adicionar elementos a listas:

-   Adicionar um elemento ao final de uma lista
-   Adicionar vários elementos ao final de uma lista
-   Adicionar um elemento em um índice específico

Vamos conferir cada uma das maneiras e ver como elas funcionam.

### Adicionar um elemento ao final de uma lista

Uma maneira simples de adicionar um elemento a uma lista é usando o método `append()`.

Podemos chamar `append()` em uma lista e passar o elemento que queremos adicionar como argumento. Então `append()` adiciona aquele elemento no final da lista, e assim, aumenta o seu comprimento em 1.

### Tarefa 1

Vamos atualizar a lista `shawshank_movie` adicionando o nome do diretor do filme. O nome do diretor é `'Frank Darabont'`.

CódigoPYTHON

9

1

2

3

shawshank\_movie \= \['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\]

shawshank\_movie.append('Frank Darabont') \# escreva seu código aqui

print(shawshank\_movie)

Dica

Mostrar a soluçãoValidar

Teoria

Observe que `append()` modifica a lista em que foi chamado; ele não retorna um novo objeto de lista.

### Adicionar vários elementos ao final de uma lista

Com `append()`, podemos adicionar apenas um elemento. Para adicionar vários, precisamos chamar o método `extend()`.

Esse método é chamado em uma lista e passamos outra lista cujos elementos queremos adicionar como argumento. Assim como `append()`, `extend()` modifica a lista original, em vez de retornar uma nova lista.

Aqui está um exemplo de adição de dois itens ao final da lista:

```
godfather_movie = ['The Godfather', 'USA', 1972]
# adicionando os nomes dos diretores e compositores ao final da lista
godfather_movie.extend(['Francis Ford Coppola', 'Nino Rota'])
print(godfather_movie)
```

```
['The Godfather', 'USA', 1972, 'Francis Ford Coppola', 'Nino Rota']
```

Como você pode ver, `extend()` adicionou cada um dos elementos da lista que passamos como novos itens à lista original. Se usarmos `append()` em vez disso, o resultado será completamente diferente. Execute o exemplo abaixo que ilustra isso:

CódigoPYTHON

9

1

2

3

row \= \[1, 2\]

row.append(\[3, 4\])

print(row)

Mostrar a soluçãoExecutar

Aqui chamamos o método `append()` na lista. Agora é hora de verificar sua compreensão.

Como podemos ver, o resultado será diferente: o método `append()` adiciona apenas um elemento, então ele incorpora uma **nova lista** na lista antiga: `[1, 2 [3, 4]]`. O método `extend()` adiciona os **valores da lista**, não a própria lista: `[1, 2, 3, 4]`.

Pergunta

### Questionário

O que vai acontecer se chamarmos o método `extend()` na mesma lista passando `[3, 4]` como argumento?

```
row = [1, 2]
row.extend([3, 4])
print(row)
```

O resultado será idêntico àquele onde usamos o método `append()`: `[1, 2 [3, 4]]`.

O resultado vai ser `[1, 2, 3, 4]`

Exatamente. O método `extend()` adiciona mais dois elementos à lista original, estendendo-a.

Temos `[3, 4, 1, 2]` como resultado.

Excelente!

### Tarefa 2

É hora de praticar! Abaixo você vai encontrar uma variável chamada `titanic_movie` que contém informações sobre o filme Titanic: nome, país de origem e ano de lançamento. Como você pode ver, as informações são armazenadas como uma lista `list`. Nosso objetivo é adicionar a informação sobre o gênero do filme (’drama’) e sua duração (194) à lista:

CódigoPYTHON

9

1

2

3

titanic\_movie \= \['Titanic', 'USA', 1997\]

titanic\_movie.extend(\['drama', 194\]) \# adicione o gênero e a duração ao final da lista

print(titanic\_movie)

Dica

Mostrar a soluçãoValidar

Teoria

### Adicionar um elemento em um índice específico

O método `append()` tem duas limitações:

1.  Ele apenas adiciona um elemento de cada vez.
2.  Ele apenas adiciona o elemento ao final da lista.

O método `extend()` remove a primeira limitação. Mas e quanto à segunda?

Para adicionar um elemento em alguma outra posição, não ao final de uma lista, você precisa chamar o método `insert()`. Esse método aceita dois argumentos: o índice onde o novo elemento será inserido e o novo valor que será inserido.

Vamos adicionar o ano de lançamento do filme _The Dark Knight_ (Batman: O Cavaleiro das Trevas) à lista, logo após o país. Dado que o país está no índice 1, o nosso novo elemento deve ser inserido no índice 2:

```
dark_knight_movie = ['The Dark Knight', 'USA', 'fantasy, action, thriller', 152]
dark_knight_movie.insert(2, 2008)
print(dark_knight_movie)
```

```
['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152]
```

O ano de lançamento (2008) agora está no índice 2, enquanto os seguintes elementos foram movidos um índice para a direita.

Observe que o método `insert()`, assim como o método `append()`, insere apenas um elemento. Se nós lhe passarmos uma lista, ele vai inserir a lista inteira como um elemento.

### Tarefa 3

Adicione o nome do diretor (’James Cameron’) antes do gênero do filme:

CódigoPYTHON

9

1

2

3

titanic\_movie \= \['Titanic', 'USA', 1997, 'drama', 194\]

titanic\_movie.insert(3,'James Cameron')\# adicione o nome do diretor antes do gênero do filme

print(titanic\_movie)

Dica

Mostrar a soluçãoValidar

Teoria

A tabela abaixo vai ajudar a distinguir entre os três métodos:

Método

Um elemento

Vários elementos

No final da lista

Em qualquer parte da lista

append()

+

\-

+

\-

extend()

+

+

+

\-

insert()

+

\-

+

+

## Remover um elemento

Saber como adicionar elementos a uma lista é apenas uma parte do trabalho. Às vezes, precisamos fazer o contrário: remover um elemento. O método `pop()` pode ser usado para isso.

Quando o aplicamos sem argumentos, esse método remove o último elemento da lista. Aqui está um exemplo:

```
movies = ['Matrix', 'Matrix 2', 'Matrix 3']
movies.pop()
print(movies)
```

```
['Matrix', 'Matrix 2']
```

Podemos fornecer um índice exato como argumento para especificar um determinado elemento da lista que queremos remover.

```
movies = ['Matrix', 'Matrix 2', 'Matrix 3']
movies.pop(1)
print(movies)
```

```
['Matrix', 'Matrix 3']
```

Por falar nisso, `movies.pop(1)` retorna um resultado que podemos armazenar se quisermos:

```
movies = ['Matrix', 'Matrix 2', 'Matrix 3']
popped_value = movies.pop(1)
print(popped_value)
```

```
Matrix 2
```

Como você pode ver, salvamos o resultado de `pop` na variável `popped_value` e imprimimos a variável. Isso exibiu o elemento removido da lista.

### Tarefa 4

Agora é hora de praticar usando o método `pop()`.

Recentemente, adicionamos o nome do diretor (’James Cameron’) antes do gênero do filme. Agora queremos voltar para a versão original. Para fazer isso, precisamos remover o nome do diretor.

CódigoPYTHON

9

1

2

3

titanic\_movie \= \['Titanic', 'USA', 1997, 'James Cameron', 'drama', 194\]

titanic\_movie.pop(3)\# remova o nome do diretor da lista

print(titanic\_movie)

Mostrar a soluçãoValidar

Agora você pode modificar as listas o tanto que quiser! No entanto, ao trabalhar com listas longas, classificar os elementos antecipadamente pode ser útil. Vamos descobrir como fazer isso na próxima lição.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-34-825Z.md
### Última modificação: 2025-05-28 18:37:35

# Ordenação de listas - TripleTen

Capítulo 4/9

Listas

# Ordenação de listas

<iframe class="base-markdown-iframe__iframe" id="player-lQ5-7kPgd24" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Sorting Lists" width="640" height="360" src="https://www.youtube.com/embed/lQ5-7kPgd24?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F38eed9ad-e4cf-4d56-8319-2f9381ab6087%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Vamos recapitular brevemente o que sabemos sobre modificação de listas até agora:

-   `append()` pode ser usado para adicionar um único elemento no final de uma lista
-   `extend()` é usado para adicionar vários elementos no final de uma lista
-   `insert()` é o que você precisa para inserir um elemento em qualquer parte de uma lista
-   `pop()` permite remover qualquer elemento de uma lista.

O que mais podemos fazer com listas? Uma operação importante é a ordenação.

## Como ordenar listas

Assim como strings, listas são coleções ordenadas de elementos. Mas, ao contrário de strings, podemos alterar a ordem dos elementos em uma lista graças ao fato de que listas são objetos `mutáveis`.

Ao final desta lição, você será capaz de usar o método `sort()` e a função integrada `sorted()` em Python para reorganizar os elementos em uma lista. Além disso, você será capaz de explicar as diferenças entre essas duas técnicas. A ordenação é uma ferramenta útil para organizar por ordem alfabética dados string, entender intervalos de dados numéricos e muito mais.

## O método `sort()`

Vamos pegar uma lista com os anos de lançamento de filmes e imprimir a lista antes e após ordenar com o método `sort()`:

```
years = [1994, 1972, 2008, 1993, 2003, 1994, 1966, 1999, 1962, 1997]
print(years)

years.sort()
print(years)
```

```
[1994, 1972, 2008, 1993, 2003, 1994, 1966, 1999, 1962, 1997]
[1962, 1966, 1972, 1993, 1994, 1994, 1997, 1999, 2003, 2008]
```

Há duas coisas a serem observadas sobre o nosso resultado:

1.  O método `sort()` modifica a lista original sem criar uma nova.
2.  Por padrão, esse método ordena elementos em ordem crescente, do menor para o maior.

Para ordená-los em ordem decrescente, passe o parâmetro `reverse=` para o método e defina seu valor como `True`. Aqui está um exemplo:

```
years = [1994, 1972, 2008, 1993, 2003, 1994, 1966, 1999, 1962, 1997]

years.sort(reverse=True)
print(years)
```

```
[2008, 2003, 1999, 1997, 1994, 1994, 1993, 1972, 1966, 1962]
```

Quando o Python ordena dados, ele compara os valores de cada elemento. Para números, o processo é bastante óbvio. Entretanto, strings são um pouco mais complicadas.

Strings são ordenadas na **ordem lexicográfica**, que considera regras de ordenação para caracteres não alfabéticos e capitalização.

Aqui está a prioridade para a ordenação lexicográfica em ordem crescente:

1.  Sinais de pontuação
2.  Números
3.  A-Z
4.  a-z

Este é o tipo de comparação que ocorre quando strings em uma lista são ordenadas:

```
movies = ['The Shawshank Redemption', 'The Godfather', 'The Dark Knight', 'Schindler\'s List']
movies.sort()
print(movies)
```

```
["Schindler's List", 'The Dark Knight', 'The Godfather', 'The Shawshank Redemption']
```

Para ilustrar a ordem lexicográfica na ordenação, vamos alterar `'The Dark Knight'` para `'the Dark Knight'` convertendo a primeira letra da string em minúscula. O método `sort()` vai se comportar assim:

```
movies = ['The Shawshank Redemption', 'The Godfather', 'the Dark Knight', 'Schindler\'s List']
movies.sort()
print(movies)
```

```
["Schindler's List", 'The Godfather', 'The Shawshank Redemption', 'the Dark Knight']
```

Observe que o filme `'the Dark Knight'` foi movido da segunda posição para a última.

Ou seja, o método `sort()` coloca nossos dados em ordem crescente. Também podemos usar o parâmetro `reverse=` para ordenar uma lista de strings em ordem decrescente:

```
movies = ['The Shawshank Redemption', 'The Godfather', 'the Dark Knight', 'Schindler\'s List']
movies.sort(reverse=True)
print(movies)
```

```
['the Dark Knight', 'The Shawshank Redemption', 'The Godfather', "Schindler's List"]
```

O resultado é exatamente aquele que esperávamos: invertemos a ordem. Agora é hora de praticar!

### Tarefa 1

Utilize o método `sort()` para ordenar a lista `movies` em ordem crescente. Imprima o resultado.

CódigoPYTHON

9

1

2

3

4

movies \= \['The Good, the Bad and the Ugly', 'Pulp Fiction', 'Fight Club', 'Harakiri'\]

  

movies.sort()\# ordene a lista aqui

print(movies)\# imprima a lista aqui

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Ordene a lista `movies_duration` (duração dos filmes) em ordem decrescente usando o método `sort()`. Imprima o resultado.

CódigoPYTHON

9

1

2

3

movies\_duration \= \[142, 175, 152, 195, 201, 154, 178, 139\]

movies\_duration.sort(reverse\=True)

print(movies\_duration) \# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

Teoria

## Uso da função `sorted()`

Há mais de uma maneira de concluir uma determinada tarefa, como costuma ser o caso em programação. O Python tem uma função integrada chamada `sorted()`, que também pode ser usada para ordenar listas, assim como outros tipos de dados. Ela é diferente de `sort()` em dois aspectos importantes:

-   `sort()` é um **método** de lista que só pode ser chamado em objetos de lista, enquanto `sorted()` é uma **função**.
-   O método `sort()` modifica a lista, enquanto a função `sorted()` produz uma nova lista ordenada.

Para ilustrar isso, vamos comparar o resultado da aplicação de ambas as técnicas. Vamos começar ordenando uma lista com a função `sorted()`:

```
years = [1994, 1972, 2008, 1993, 2003, 1994, 1966, 1999, 1962, 1997]

years_sorted = sorted(years)

print(years_sorted)
```

```
[1962, 1966, 1972, 1993, 1994, 1994, 1997, 1999, 2003, 2008]
```

Agora vamos comparar com o resultado do método `sort()`:

```
years = [1994, 1972, 2008, 1993, 2003, 1994, 1966, 1999, 1962, 1997]

years.sort()

print(years)
```

```
[1962, 1966, 1972, 1993, 1994, 1994, 1997, 1999, 2003, 2008]
```

Veja o que acontece se tentarmos atribuir o resultado produzido pelo método `sort()` a uma nova variável:

```
years = [1994, 1972, 2008, 1993, 2003, 1994, 1966, 1999, 1962, 1997]

years_sort = years.sort()

print(years_sort)
```

```
None
```

Recebemos `None`, porque o método `sort()` apenas modifica a lista original, mas não retorna uma nova lista para que possa ser atribuída a uma nova variável.

Se você quer ordenar uma lista e atribuir o resultado a uma nova variável, use a função `sorted()`. Se você já tiver uma variável de tipo lista que quer manter, mas também quer ordená-la, use o método `sort()`. Ambos os métodos ordenam listas em ordem crescente por padrão, e ambos podem ordenar elementos em ordem decrescente se passarmos a eles o argumento `reverse=True`:

```
years = [1994, 1972, 2008, 1993, 2003, 1994, 1966, 1999, 1962, 1997]

years_sorted = sorted(years, reverse=True)

print(years_sorted)
```

**Resultado:**

```
[2008, 2003, 1999, 1997, 1994, 1994, 1993, 1972, 1966, 1962]
```

### Tarefa 3

Ordene a lista `movies_duration` em ordem decrescente, mas desta vez use a função `sorted()`. Armazene o resultado da classificação na variável `movies_duration_sorted` e imprima-o.

CódigoPYTHON

9

1

2

3

movies\_duration \= \[142, 175, 152, 195, 201, 154, 178, 139\]

movies\_duration\_sorted \= sorted(movies\_duration, reverse\=True)\# classificar a lista movies\_duration usando sorted()

print(movies\_duration\_sorted)\# imprima a lista resultante aqui

Dica

Mostrar a soluçãoValidar

É fácil confundir `sort()` com `sorted()`, mas você não caiu na armadilha! Agora é hora de abordar o último tópico sobre listas deste capítulo – listas aninhadas.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-36-137Z.md
### Última modificação: 2025-05-28 18:37:36

# Listas aninhadas - TripleTen

Capítulo 4/9

Listas

# Listas aninhadas

<iframe class="base-markdown-iframe__iframe" id="player-EnVVX1KmfCo" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Nested Lists" width="640" height="360" src="https://www.youtube.com/embed/EnVVX1KmfCo?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F76869bf0-4c4c-4a25-b80d-9a83587b9b5d%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Lembra como criamos listas separadas para armazenar dados sobre filmes nas lições anteriores? E se quisermos aumentar a nossa pequena coleção de filmes para que ela contenha 10 títulos? Manipular cada lista individualmente levaria muito tempo. Felizmente, podemos combinar todas as listas em uma só — uma lista aninhada.

No final desta lição, você será capaz de explicar o que é uma lista aninhada e como acessar os seus elementos.

![](https://practicum-content.s3.amazonaws.com/resources/5-3_1687946734.png)

## Listas aninhadas

Listas aninhadas são, de maneira simples, listas de listas: o primeiro nível é a lista principal, seus elementos são listas separadas (sublistas), e os elementos dessas sublistas são objetos mais simples, como números ou strings.

![](https://practicum-content.s3.amazonaws.com/resources/4.6_2PT_1698756945.png)

Com essas estruturas, você pode criar, armazenar e exibir informações organizadas (como tabelas) em Python. Armazenar informação na forma de uma tabela pode tornar a análise mais rápida e melhorar processos de tomada de decisão. Como especialista da área de dados, você vai trabalhar com tabelas com frequência.

Vamos dar uma olhada em um exemplo específico. Execute o código abaixo que imprime uma lista aninhada.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

print(movies\_info)

Mostrar a soluçãoExecutar

Embora o resultado da função `print()` possa ser difícil de ler à primeira vista, ele deixa claro que categorizamos itens em sublistas. Em outras palavras, os dados de cada filme estão armazenados na sua própria sublista, e a informação sobre todos os filmes está armazenada na lista principal.

---

### Como acessar dados de uma lista aninhada

Quando uma tabela é representada como uma lista de listas, o primeiro índice indica uma linha (ou uma sublista) e o segundo indica uma coluna (um índice de uma sublista). Podemos usar essas duas coordenadas para acessar qualquer elemento individual.

Vamos usar o primeiro índice para recuperar sublistas, que são as linhas da nossa tabela:

```
print(movies_info[0]) # acessando a primeira linha
print(movies_info[-1]) # acessando a última linha
```

```
['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111]
['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077]
```

Agora vamos acessar os elementos de uma sublista indicando um segundo índice entre colchetes:

```
print(movies_info[0][2]) # acessando o terceiro valor (year) da primeira linha
print(movies_info[-1][0]) # acessando o primeiro valor (title) da última linha
```

```
1994
Good Will Hunting
```

O aninhamento pode ir ainda mais fundo criando listas de listas de listas, mas vamos focar em listas com dois níveis, já que elas têm a maior utilidade prática.

### Tarefa

Acesse a lista `movies_info` e imprima:

-   O país de produção de _Good Will Hunting_
-   O ano de lançamento de _Pulp Fiction_
-   A avaliação de _The Godfather_ (`8.730`)

Obtenha os elementos usando seus índices e imprima cada um deles em uma nova linha.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

print(movies\_info\[\-1\]\[1\])\# imprima o país de produção de Good Will Hunting aqui

print(movies\_info\[5\]\[2\])\# imprima o ano de lançamento de Pulp Fiction aqui

print(movies\_info\[1\]\[\-1\])\# imprima a avaliação de The Godfather aqui

Dica

Mostrar a soluçãoValidar

Neste capítulo, você aprendeu sobre vários tipos de dados: strings e números, listas e estruturas aninhadas feitas de listas. Listas são ferramentas poderosas que também podem ser usadas para processar strings. Como? Descubra na próxima lição!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-38-440Z.md
### Última modificação: 2025-05-28 18:37:38

# Processamento de strings - TripleTen

Capítulo 4/9

Listas

# Processamento de strings

<iframe class="base-markdown-iframe__iframe" id="player-piWGhDC64T4" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Processing Strings" width="640" height="360" src="https://www.youtube.com/embed/piWGhDC64T4?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fd435ac80-14b4-48da-9f51-377d2f1b9cd5%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Listas aninhadas são excelentes, mas vamos retornar às listas convencionais e explorar como elas podem ser aplicadas no processamento de strings.

As listas possuem alguns métodos integrados únicos que são muito úteis no processamento de strings, então analistas têm o costume de transformar strings em listas para calcular e processar textos. Isso é útil quando o texto deve ser modificado: por exemplo, se você quer remover palavras repetidas ou substituir certos fragmentos.

Separar sua string em uma lista de elementos permite aproveitar métodos e técnicas potentes de listas para processar e analisar dados de texto, e então transformar a lista de volta em uma string para a futura impressão ou análise.

Nesta lição, você vai aprender sobre o método `split()`, que converte uma string em uma lista. Para converter uma lista de volta em uma string, vamos apresentar também o método `join()`.

### Como dividir uma string em uma lista

Para transformar uma string em uma lista, podemos usar o método `split()`.

![](https://practicum-content.s3.amazonaws.com/resources/4.7PT_1690203509.png)

Por padrão, `split()` divide uma string sempre que encontrar um espaço em branco. Os caracteres entre os espaços se tornam os elementos da lista:

```
phrase = 'vamos dividir ou não vamos dividir'
words = phrase.split()
print(words)
```

```
['vamos', 'dividir', 'ou', 'não', 'vamos', 'dividir']
```

Observe que cada elemento da lista inclui todos os caracteres entre espaços, mas os próprios espaços não estão incluídos em nenhum dos elementos da lista.

Agora podemos fazer algumas coisas legais como, por exemplo, calcular o número de palavras usando a função `len()`:

```
phrase = 'vamos dividir ou não vamos dividir'
words = phrase.split()
print(len(words))
```

```
6
```

Não precisamos usar apenas espaços para dividir strings; qualquer coisa pode ser usada como separador. Para especificar um separador, passe-o como uma string para o método `split()`:

```
phrase = 'Brilha-brilha-estrelinha-lá-no-céu-pequenininha'
words = phrase.split('-')
print(words)
```

```
['Brilha', 'brilha', 'estrelinha', 'lá', 'no', 'céu', 'pequenininha']
```

### Tarefa 1

Separe a string `phrase` em uma lista de nomes de personagens da Disney. Armazene o resultado na variável `words` e imprima-a.

CódigoPYTHON

9

1

2

3

phrase \= 'Aladdin#Esmeralda#Hércules#Mulan'

words \= phrase.split("#")\# escreva seu código aqui

print(words)

Dica

Mostrar a soluçãoValidar

Teoria

### Como combinar elementos de uma lista em uma string

É comum dividirmos uma string para fazer modificações nela. Mais adiante no curso, você vai aprender sobre todas as operações que podem ser realizadas. Por enquanto, vamos supor que fizemos todas as alterações necessárias. Já que todas as alterações necessárias estão feitas, a lista pode ser convertida de volta em uma string usando o método `join()`.

O método `join()` espera uma lista como entrada e requer que um objeto seja chamado. Esse objeto é a string que será inserida entre cada elemento da lista. Normalmente separamos as palavras em strings usando espaços, então é comum usar `' '` como esse objeto. Aqui está um exemplo em que todos os elementos da lista `words` são unidos com espaços entre eles:

```
words = ['Meu', 'filme', 'favorito', 'é', 'The', 'Graduate.']
phrase = ' '.join(words) # forma uma string a partir dos elementos, separados por espaços
print(phrase)
```

```
Meu filme favorito é The Graduate.
```

### Tarefa 2

Junte os nomes dos super-heróis da lista `heroes` em uma string com o separador `>`. Atribua o resultado à variável `comparison` e imprima-a.

CódigoPYTHON

9

1

2

3

heroes \= \['Homem-Aranha', 'Batman', 'Super-Homem', 'Homem de Ferro'\]

comparison \= ">".join(heroes)\# escreva seu código aqui

print(comparison)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-39-727Z.md
### Última modificação: 2025-05-28 18:37:40

# Atividade prática do capítulo - TripleTen

Capítulo 4/9

Listas

# Atividade prática do capítulo

Listas são amplamente utilizadas no processamento de dados. Vamos recapitular o que aprendemos até agora:

-   Semelhante às strings, os elementos de uma lista podem receber indexação positiva ou negativa.
    
    ```
      movie_info = ['Fight Club', 1999, ['thriller', 'drama', 'crime'], 139, 8.644]
      
      print(movie_info[4])
      print(movie_info[-1])
      
    ```
    
    ```
      8.644
      8.644
      
    ```
    
-   Para obter uma fatia de uma lista, precisamos passar o limite inferior e superior do que queremos incluir.
    
    ```
      movie_info = ['Fight Club', 1999, ['thriller', 'drama', 'crime'], 139, 8.644]
      
      print(movie_info[2:4])
      
    ```
    
    ```
      [['thriller', 'drama', 'crime'], 139]
      
    ```
    
    O limite superior pode até ser removido se quisermos incluir todos os elementos, começando pelo limite inferior:
    
    ```
      movie_info = ['Fight Club', 1999, ['thriller', 'drama', 'crime'], 139, 8.644]
      
      print(movie_info[2:])
      
    ```
    
    ```
      [['thriller', 'drama', 'crime'], 139, 8.644]
      
    ```
    
    O mesmo funciona para o limite inferior.
    
-   `append()`, `extend()` e `insert()` são os métodos usados para adicionar elementos a uma lista.
    
    A tabela abaixo vai ajudar a distinguir entre os três métodos:
    
    Método
    
    Um elemento
    
    Vários elementos
    
    No final da lista
    
    Em qualquer parte da lista
    
    append()
    
    +
    
    \-
    
    +
    
    \-
    
    extend()
    
    +
    
    +
    
    +
    
    \-
    
    insert()
    
    +
    
    \-
    
    +
    
    +
    
-   O método `pop()` é aquele que precisamos para remover um elemento de uma lista.
    
    ```
      movies = ['Matrix', 'Matrix 2', 'Matrix 3']
      movies.pop(1)
      print(movies)
      
    ```
    
    ```
      ['Matrix', 'Matrix 3']
      
    ```
    
-   A função `sorted()` e o método `sort()` são usados para ordenar elementos em uma lista
    
    ```
      movies_duration = [142, 175, 152, 195, 201, 154, 178, 139]
      movies_duration = sorted(movies_duration, reverse=True)
      
      print(movies_duration)
      
    ```
    
    ```
      [201, 195, 178, 175, 154, 152, 142, 139]
      
    ```
    
    ou
    
    ```
      movies_duration = [142, 175, 152, 195, 201, 154, 178, 139]
      movies_duration.sort(reverse=True)
      
      print(movies_duration)
      
    ```
    
    ```
      [201, 195, 178, 175, 154, 152, 142, 139]
      
    ```
    
-   Se usarmos uma lista para representar uma tabela, os elementos dessa lista costumam ser listas aninhadas. Lembre-se de que o primeiro índice se refere a uma linha (ou sublista) e o segundo índice indica uma coluna (um índice de uma sublista)
    
    ```
      movies_info = [
          ['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111],
          ['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730],
          ['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499],
          ["Schindler's List", 'USA', 1993, 'drama', 195, 8.818],
          ['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625],
          ['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619],
          ['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521],
          ['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644],
          ['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106],
          ['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077]
      ]
      
      print(movies_info[9][1])
      print(movies_info[5][2])
      print(movies_info[1][5])
      
    ```
    
    ```
      USA
      1994
      8.73
      
    ```
    
-   Por fim, o método `split()` é usado para dividir uma string em uma lista. O método `join()` faz o oposto.
    

Para ilustrar isso e praticar operações comuns de listas, vamos apresentar um cenário da vida real. O banco ABC acabou de lançar um serviço bancário privilegiado para seus clientes com grande patrimônio.

O objetivo principal é organizar o armazenamento de dados dos novos clientes. Em seguida, vamos realizar uma análise dos primeiros clientes para entender melhor quem eles são.

## Tarefas

### Tarefa 1

A informação para cada novo cliente é fornecida na seguinte ordem: ID, nome, idade, receita líquida anual e ocupação. Aqui está uma lista dos dados de Jack: `32456`, `'Jack Wilson'`, `32`, `150000`, `'Healthcare'`. Crie a variável `client_info` que inclui as informações sobre Jack e imprima-a.

CódigoPYTHON

9

1

2

client\_info \= \[32456, 'Jack Wilson', 32, 150000, 'Healthcare'\]\# escreva seu código aqui

print(client\_info)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Para organizar o armazenamento de dados de vários clientes, criamos a variável `clients` e atribuímos uma lista vazia a ela.

Os dados de Jack estão armazenados na variável `client_info`. Nosso objetivo é adicionar essas informações à lista `clients` existente como uma lista aninhada. Por favor, faça isso. Quando terminar, não se esqueça de imprimir a lista de `clients`.

CódigoPYTHON

9

1

2

3

4

5

6

clients \= \[\]

client\_info \= \[32456, 'Jack Wilson', 32, 150000, 'Healthcare'\]

  

clients.append(client\_info)\# escreva seu código aqui

  

print(clients)

Dica

Mostrar a soluçãoValidar

### Tarefa 3

As informações sobre uma nova cliente, Nina Brown, foram adicionadas à lista `clients`. O banco está interessado na receita líquida anual dela, que ocupa a 4ª posição da lista. Nosso objetivo é extrair essas informações, atribuir à variável `client_income` e imprimir.

CódigoPYTHON

9

1

2

3

4

5

6

7

clients \= \[

\[32456, 'Jack Wilson', 32, 150000, 'Healthcare'\],

\[34591, 'Nina Brown', 45, 250000, 'Telecom'\]

\]

  

client\_income \= clients\[1\]\[3\]

print(client\_income)

Dica

Mostrar a soluçãoValidar

### Tarefa 4

O número de clientes está aumentando rapidamente. Outros dois clientes acabaram de se juntar ao serviço, e agora seus dados estão visíveis na lista `clients`. Entretanto, um funcionário do banco cometeu um erro tipográfico ao adicionar a informação do cliente, especificamente no campo de ocupação de Brian Perez (`'Transportatiion'`). Já que essa entrada é inválida, ela deve ser removida da lista `clients`. Seu objetivo é remover de `clients` a lista onde está o erro e imprimir o resultado.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

clients \= \[

\[32456, 'Jack Wilson', 32, 150000, 'Healthcare'\],

\[34591, 'Nina Brown', 45, 250000, 'Telecom'\],

\[37512, 'Alex Smith', 39, 210000, 'IT'\],

\[39591, 'Brian Perez', 29, 340000, 'Transportatiion'\]

\]

  

clients.pop(\[\-1\]\[\-1\])\# escreva seu código aqui

  

print(clients)

Dica

Mostrar a soluçãoValidar

### Tarefa 5

As informações sobre Brian Perez foram atualizadas, e mais dois clientes se juntaram ao serviço recentemente. Agora o banco quer conduzir uma análise dos clientes: mais especificamente, determinar a distribuição da idade do valor mais alto para o mais baixo.

No pré-código, você vai encontrar a variável `ages` que contém as idades de todos os clientes atuais. Nosso objetivo é ordenar essa lista na ordem decrescente e imprimi-la. Para ordenação, use o método `sort()`.

CódigoPYTHON

9

1

2

3

4

5

ages \= \[32, 45, 39, 29, 25, 32\]

  

ages.sort(reverse\=True)\# escreva seu código aqui

  

print(ages)

Dica

Mostrar a soluçãoValidar

### Tarefa 6

O departamento de segurança do banco pediu para fornecermos uma lista com os nomes dos nossos primeiros clientes. Essa informação está atualmente disponível como uma string, em que todos os nomes são separados por vírgulas e estão armazenados na variável `names`.

Para ajudar, converteremos a string em uma lista chamada `names_split`, onde cada elemento representa o nome de um cliente. Não se esqueça de imprimir a lista resultante.

CódigoPYTHON

9

1

2

3

4

5

names \= 'Jack Wilson,Nina Brown,Alex Smith,Brian Perez,David Martinez,John Kim'

  

names\_split \= names.split(',')\# escreva seu código aqui

  

print(names\_split)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-37-41-089Z.md
### Última modificação: 2025-05-28 18:37:41

# Conclusão - TripleTen

Capítulo 4/9

Listas

# Conclusão

Parabéns por aprender sobre listas, esse é o primeiro passo na aprendizagem de ferramentas potentes usadas por profissionais da área de dados diariamente!

![](https://practicum-content.s3.amazonaws.com/resources/3-conc_1687947256.png)

Vamos dar uma olhada no que você já aprendeu.

Agora você é capaz de:

-   Criar listas, calcular o número de itens nelas e acessar os valores de uma lista por meio de indexação e fatiação Se ainda tiver dúvidas sobre esses tópicos, revise a lição sobre as [Propriedades básicas de listas](https://tripleten.com/trainer/data-analyst/lesson/ad3a78d1-ff67-4b1b-9aea-965910b7df54/).
-   Modificar listas usando `append ()`, `extend()` ,`insert()` e `pop()`. Se ainda tiver dúvidas sobre esses tópicos, revise a lição [Modificação de listas](https://tripleten.com/trainer/data-analyst/lesson/429934b9-d514-42da-9289-a316b21faa1f/).
-   Usar o método de lista `sort()` e a função integrada `sorted()` para reorganizar os elementos em uma lista Se ainda tiver dúvidas, revise a lição [Ordenação de listas](https://tripleten.com/trainer/data-analyst/lesson/c988fbd8-ec9d-4ed3-8b44-d52967be30d9/).
-   Criar e recuperar elementos de listas aninhadas Se ainda tiver dúvidas, revise a lição [Listas aninhadas](https://tripleten.com/trainer/data-analyst/lesson/b99b4cc5-c6ae-4731-8648-e0ce321c582b/).
-   Usar os métodos `split()` e `join()` para processar strings como listas Se ainda tiver dúvidas, revise a lição [Processamento de strings](https://tripleten.com/trainer/data-analyst/lesson/42ccf618-ada9-4dd4-a8fc-dccecdbea7fd/).

## O que vem a seguir

No próximo capítulo, você vai aprender sobre ciclos, que permitem aos programadores automatizar operações que seriam tediosas ou impossíveis para meros humanos realizarem.

Vamos lá!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-01-441Z.md
### Última modificação: 2025-05-28 18:54:02

# Introdução - TripleTen

Capítulo 5/9

Ciclos

# Introdução

Agora que você aprendeu sobre listas, é hora de conhecer uma ferramenta potente para acessar e manipular os dados dentro delas: os ciclos `for` e `while`.

Ao final deste capítulo, você será capaz de explicar o que são os loops `for` e `while`, demonstrar como usar loops `for` para percorrer elementos de lista e seus índices, construir loops para resolver várias tarefas e aplicar `sum()`, `max()` e `min()` ao seu código.

Serão necessárias de 1 a 2 horas para concluir este capítulo.

![](https://practicum-content.s3.amazonaws.com/resources/4-1_1687950079.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-02-777Z.md
### Última modificação: 2025-05-28 18:54:03

# Operadores de atribuição composta e funções integradas - TripleTen

Capítulo 5/9

Ciclos

# Operadores de atribuição composta e funções integradas

<iframe class="base-markdown-iframe__iframe" id="player-2fp-W7ZK0Lc" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Augmented Assignment Operators and Build in Functions" width="640" height="360" src="https://www.youtube.com/embed/2fp-W7ZK0Lc?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F99931ea0-59f6-463c-b5d5-6097df51b308%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Antes de começarmos a falar sobre ciclos, queremos apresentar dois conceitos que costumam ser usados em conjunto deles para simplificar e agilizar o código: **atribuição composta** e **funções integradas**, como `min()`, `max()` e `sum()`.

Ao usar a atribuição composta, você pode atualizar facilmente o valor de uma variável sem ter que duplicar seu nome, enquanto as funções integradas fornecem soluções prontas para cálculos e operações comuns. Entender esses conceitos vai ajudar você a escrever um código Python mais eficiente.

Ao final desta lição, você será capaz de modificar valores armazenados em variáveis usando atribuição composta e calcular min, max e sum usando funções integradas.

Vamos lá!

## Atualização de valores

Não é comum o uso de Python como uma simples calculadora para somar ou subtrair números. Em vez disso, os valores costumam ser vinculados a variáveis, e eles podem mudar à medida que recebemos e processamos mais dados. Por exemplo, o número de falantes de cada idioma certamente muda de ano para ano.

Vamos tomar o número total de falantes de francês (284,9 milhões de pessoas) e armazená-lo na variável `fr_speakers`. Imagine que recentemente as coisas mudaram e agora mais 2,5 milhões de pessoas falam francês. Precisamos atualizar a variável.

Claro que podemos definir a variável de novo:

```
fr_speakers = 284.9
fr_speakers = 284.9 + 2.5
print(fr_speakers)
```

**Resultado:**

```
287.4
```

Mas essa não é a melhor solução. Quando corrigimos valores desatualizados manualmente, isso pode resultar em erros humanos. Por exemplo, se seu código for grande e o valor for alterado várias vezes no código, você não vai saber qual valor ele terá quando precisar alterá-lo.

Afinal, só precisamos somar `2.5` ao valor atual usando a própria variável:

```
fr_speakers = 284.9
fr_speakers = fr_speakers + 2.5
print(fr_speakers)
```

**Resultado:**

```
287.4
```

Temos o mesmo resultado!

---

## Atribuição composta

Mesmo usando o método acima, você ainda duplica o nome da variável e cria a possibilidade de gerar erros humanos. Basta um erro de digitação e `fr_speakers` pode se transformar em `fi_speakers`, e você vai ter dificuldade em descobrir por que todos aqueles falantes de francês desapareceram! Talvez seja culpa do inverno finlandês, e não de um erro de digitação!

Para evitar esses erros, existe um método mais curto e útil chamado **atribuição composta**. Em Python, ela fica assim:

CódigoPYTHON

9

1

2

3

fr\_speakers \= 284.9

fr\_speakers += 2.5

print(fr\_speakers)

Mostrar a soluçãoExecutar

E funcionou novamente! Os operadores de atribuição composta funcionam da mesma forma que outros operadores aritméticos. Confira por conta própria. Execute o código abaixo para ver como as atribuições compostas alteram o valor original de `items`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

items \= 10

  

items += 5 \# como items = items + 5

print(items)

  

items \-= 5 \# como items = items - 5

print(items)

  

items \*= 3 \# como items = items \* 3

print(items)

  

items /= 5 \# como items = items / 5

print(items)

Mostrar a soluçãoExecutar

Menos digitação, resultados iguais!

### Tarefa 1

Experimente por conta própria na tarefa abaixo.

Altere o valor de `it_speakers`: adicione 5 a ele usando atribuição composta. Imprima o novo valor.

CódigoPYTHON

9

1

2

3

it\_speakers \= 67.8

it\_speakers += 5\# atualize o valor da variável aqui

print(it\_speakers)\# imprima o valor da variável aqui

Dica

Mostrar a soluçãoValidar

Teoria

## Funções integradas

![](https://practicum-content.s3.amazonaws.com/resources/5.2PT_1690203819.png)

Uma grande vantagem do Python são as funções integradas. É comum que analistas precisem encontrar a soma ou os valores mínimo e máximo dos dados, e, portanto, desenvolvedores criaram funções prontas para esses cálculos.

-   A função `sum()` recebe uma lista como argumento e retorna a soma de seus elementos.
-   A função `max()` é usada para encontrar o valor máximo.
-   A função `min()` é usada para encontrar o valor mínimo da lista.

Veja como eles funcionam executando o código abaixo

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

movies\_duration \= \[142, 175, 152, 195, 201, 154, 178, 139, 133, 126\]

  

total\_duration \= sum(movies\_duration) #soma todos os elementos da lista

max\_duration \= max(movies\_duration) #obtêm o maior elemento da lista

min\_duration \= min(movies\_duration) #obtêm o menor elemento da lista

  

  

print(total\_duration)

print(max\_duration)

print(min\_duration)

Mostrar a soluçãoExecutar

Para concluir: as funções integradas do Python são incrivelmente úteis para simplificar operações e cálculos complexos. Analistas podem aproveitar essas funções para calcular métricas importantes com rapidez e facilidade para os dados. Você vai usá-las muito em sua futura profissão!

Agora vamos passar para o tópico principal desse capítulo: ciclos.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-04-120Z.md
### Última modificação: 2025-05-28 18:54:04

# Ciclos For - TripleTen

Capítulo 5/9

Ciclos

# Ciclos For

<iframe class="base-markdown-iframe__iframe" id="player-A8xoGQ0bRGM" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="For Loops" width="640" height="360" src="https://www.youtube.com/embed/A8xoGQ0bRGM?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F92f22f2d-1f21-426e-b6ff-18b8f1acf729%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

## Automações economizam tempo (e sanidade)

Imagine que você tem uma lista de 1.000 elementos e deseja executar a mesma operação em cada um deles. Fazer isso à mão levaria uma eternidade. Por exemplo, se quisermos imprimir os elementos da lista `film_genres` um por um, precisaremos usar `print()` para cada um deles:

```
film_genres = ['scifi','drama','thriller','comedy','action']
print(film_genres[0])
print(film_genres[1])
print(film_genres[2])
print(film_genres[3])
print(film_genres[4])
```

**Resultado:**

```
scifi
drama
thriller
comedy
action
```

Não parece muito eficiente, não é? Felizmente, o Python nos ajuda a evitar isso usando **ciclos** (também conhecidos como loops ou laços) — uma ferramenta que permite repetir instruções quantas vezes quisermos.

Ao final desta lição, você será capaz de explicar o que é um ciclo `for`, descrever a estrutura dos ciclos e escrever seus próprios ciclos `for`.

## Ciclos `for` em Python

O tipo mais comum de ciclo em Python é o `for`, que começa com a palavra-chave `for`. Ele percorre os elementos de uma sequência e realiza operações similares em cada um deles.

![](https://practicum-content.s3.amazonaws.com/resources/5.3_1PT_1690205716.png)

A sequência que usamos pode ser qualquer objeto ordenado, como listas ou strings.

Mas vamos voltar à lista `film_genres` e escrever um pequeno ciclo para imprimir os elementos:

```
film_genres = ['scifi','drama','thriller','comedy','action']
for value in film_genres:
    print(value)
```

**Resultado:**

```
scifi
drama
thriller
comedy
action
```

Isso muda tudo!

## Componentes de um ciclo

Vamos olhar para a estrutura de um ciclo e fazer uma análise:

![](https://practicum-content.s3.amazonaws.com/resources/5.3_2PT_1690205737.png)

-   A palavra-chave `for`.
-   A **variável de ciclo** `element`. A cada iteração do ciclo, ela recebe o valor de um elemento da sequência e permite realizar operações sobre ele. Você pode usar qualquer nome para essa variável, mas é melhor usar um que seja claro e descritivo.
-   A palavra-chave `in`.
-   A lista `your_list`. É o nome da lista que o ciclo está processando.
-   Os dois pontos `:` são um elemento essencial para separar as duas partes do ciclo.
-   A função `print(element)`. Isso constitui o **corpo** do ciclo. O corpo contém comandos que o ciclo vai executar a cada iteração. Cada comando no corpo do ciclo é precedido por indentação de quatro espaços, que indica que o comando deve ser executado dentro do ciclo, separado do resto do código.

É fácil de lembrar, é como se estivesse escrito em um livro. Para cada elemento da lista: faça todas essas ações.

### Tarefa 1

Experimente. Escreva um ciclo que vai imprimir na tela todos os elementos da lista `payers`.

CódigoPYTHON

9

1

2

3

payers \= \[56, 65, 64, 63\]

for numeros in payers:

print(numeros)\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

Teoria

## Erros de sintaxe

Não se esqueça da indentação e dos dois pontos. A sintaxe é crucial: deixe de fora qualquer elemento e o programa não vai funcionar.

### Tarefa 2

Vamos lidar com essa situação no código abaixo. Estude as mensagens de erro e corrija o código para que ele funcione.

CódigoPYTHON

9

1

2

3

actors \= \['John Travolta', 'Bonnie Hunt', 'Jason Bateman', 'Idris Elba'\]

for actor in actors:

print(actor)

Dica

Mostrar a soluçãoValidar

### Tarefa 3

## Listas diferentes

Os ciclos funcionam com todo o tipo de lista. Escreva um ciclo que vai imprimir todos os elementos da lista `elements`.

CódigoPYTHON

9

1

2

3

4

elements \= \[42, 'hello', 0.5, \-999, 'world'\]

  

for actor in elements:

print(actor)\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

Teoria

## Ciclos com strings

Você também pode usar o ciclo `for` com strings. Ciclos podem iterar sobre todos os caracteres do texto, assim como eles iteram sobre todos os elementos de uma lista:

```
review = 'Emoji: O Filme deixou muito a desejar. Duas estrelas é bem generoso.'

for symbol in review:
    print(symbol)
```

### Tarefa 4

Crie um ciclo que imprima todos os caracteres na string `message_japanese` .

CódigoPYTHON

9

1

2

3

4

message\_japanese \= '機械翻訳は通常良くありません'

  

for letras in message\_japanese:

print(letras)\# escreva o código aqui

Dica

Mostrar a soluçãoValidar

Muito bem! Saber usar ciclos vai permitir que você automatize muitas operações chatas.

Na próxima lição, você vai aprender como usar ciclos com listas aninhadas.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-07-464Z.md
### Última modificação: 2025-05-28 18:54:07

# Percorrer listas aninhadas - TripleTen

Capítulo 5/9

Ciclos

# Percorrer listas aninhadas

<iframe class="base-markdown-iframe__iframe" id="player-bHfI1sxbN04" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Looping Over Nested Lists" width="640" height="360" src="https://www.youtube.com/embed/bHfI1sxbN04?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fcd6ccc65-2ad1-48fd-9e2e-67fb6155dbec%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

## Ciclos em listas aninhadas

Agora que você sabe como os ciclos processam listas comuns, passando por seus elementos e executando instruções para cada um deles, é hora de aprender como usamos ciclos com listas aninhadas.

As listas aninhadas nada mais são do que uma lista de listas, e os ciclos funcionam com elas da mesma forma que com listas simples.

Veja a tabela a seguir como exemplo.

Movie Title

Country of Origin

Year Released

Genre

Runtime (min)

IMDB Rating

The Shawshank Redemption

USA

1994

drama

142

9.111

The Godfather

USA

1972

drama, crime

175

8.730

The Dark Knight

USA

2008

fantasy, action, thriller

152

8.499

Schindler's List

USA

1993

drama

195

8.818

The Lord of the Rings: The Return of the King

New Zealand

2003

fantasy, adventure, drama

201

8.625

Pulp Fiction

USA

1994

thriller, comedy, crime

154

8.619

The Good, the Bad and the Ugly

Italy

1966

western

178

8.521

Fight Club

USA

1999

thriller, drama, crime

139

8.644

Harakiri

Japan

1962

drama, action, history

133

8.106

Good Will Hunting

USA

1997

drama, romance

126

8.077

Podemos representá-la em Python como uma lista aninhada, com cada linha sendo um elemento:

```
movies_info = [
    ['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111],
    ['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730],
    ['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499],
    ["Schindler's List", 'USA', 1993, 'drama', 195, 8.818],
    ['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625],
    ['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619],
    ['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521],
    ['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644],
    ['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106],
    ['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077]
]
```

Um ciclo regular pode acessar as informações de cada filme. Execute o código abaixo para ver como funciona.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

for row in movies\_info:

print(row) \# vai imprimir uma linha inteira em cada iteração

Mostrar a soluçãoExecutar

Vamos ver mais uma vez a estrutura de um ciclo `for`:

![](https://practicum-content.s3.amazonaws.com/resources/1.5.4_pt_1697703482.png)

A cada iteração, o ciclo armazena um elemento da lista na variável `row`. Esse elemento é outra lista, mas você não precisa se preocupar com isso. A função `print()` no corpo do ciclo imprime os elementos da lista um por um até imprimir a lista inteira, assim como fez para uma lista simples.

O bom de obter outra lista como elemento do ciclo é que você pode acessar o conteúdo por meio dos índices.

Vamos dar uma olhada em como isso é feito. Aqui vamos acessar os títulos dos filmes e os imprimir:

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

for row in movies\_info:

print(row\[0\]) \# imprime os títulos. Queremos a 1ª coluna, então o índice é 0

Mostrar a soluçãoExecutar

Um ciclo como esse percorre toda a tabela, mas imprime apenas uma coluna.

Agora vamos tentar juntar tudo!

### Tarefa 1

Já imprimimos os títulos dos filmes antes. Vamos tentar imprimir os gêneros dos filmes da tabela `movies_info`. Cada valor deve estar em uma nova linha.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

for generos in movies\_info:

print(generos\[3\])\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

### Tarefa 2

A duração de todos os filmes está em minutos. Para converter em horas, itere sobre a lista `movies_info`, extraia o tempo de duração e o armazene na variável `runtime`. Em seguida, use [atribuição composta](https://tripleten.com/trainer/data-analyst/lesson/b333811d-a06d-428d-b1d3-5150d8ee9102/) para converter a variável `runtime` de minutos para horas. Feito isso, imprima a duração em horas.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

for row in movies\_info:

runtime \= row\[\-2\]\# extraia o tempo de duração aqui

runtime /= 60 \# converter o tempo de duração extraído para horas

print(runtime)

  

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-08-789Z.md
### Última modificação: 2025-05-28 18:54:09

# Ciclos While - TripleTen

Capítulo 5/9

Ciclos

# Ciclos While

<iframe class="base-markdown-iframe__iframe" id="player-oSqd548m4bo" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="While Loops" width="640" height="360" src="https://www.youtube.com/embed/oSqd548m4bo?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F7f24776e-a60d-4d24-b1f8-ff7a56223a0a%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Existem vários tipos de ciclos, incluindo ciclos `for` e ciclos `while`. Cada um deles serve para resolver problemas diferentes. Como vimos, os ciclos `for` são ótimos quando você sabe de antemão que vai precisar executar um conjunto de instruções um determinado número de vezes, como, por exemplo, o número de elementos de uma lista.

No entanto, quando não sabemos quantas vezes precisaremos repetir um pedaço de código, é aí que os ciclos `while` entram em cena. Por exemplo, eles podem ser muito úteis nestes casos:

-   Quando um usuário preenche um formulário, pode haver um ciclo que processa os dados à medida que são inseridos. O ciclo deve continuar iterando até que todos os dados tenham sido inseridos corretamente.
-   Se uma webcam reage a um movimento, um ciclo pode transmitir dados até que o movimento pare.

Em ciclos `while`, a condição é verificada antes de cada iteração. Se essa condição for `True`, o algoritmo executa as instruções dentro do corpo do ciclo. Assim que a expressão lógica na condição retornar `False`, o ciclo será interrompido.

Vamos ver outro exemplo. Um elevador inteligente pode medir o peso total dos passageiros e compará-lo com a própria capacidade. Cada vez que um novo passageiro entra no elevador, o ciclo `while` verifica se o peso máximo foi atingido.

Nesse experimento, assumiremos que o peso em quilogramas de cada novo passageiro é um valor aleatório de 30 a 120. Valores aleatórios podem ser atribuídos a variáveis usando a função `randint()` da biblioteca `random`. A função `randint` gera números inteiros aleatórios dentro de um intervalo especificado. Portanto, vamos declarar o intervalo de 30 a 120 e simular o peso de uma pessoa aleatória. Vamos falar sobre a geração de números aleatórios e bibliotecas com mais detalhes posteriormente. Por enquanto, vamos supor que podemos importar `randint()` de `random` e usá-la como uma ferramenta para o que precisamos.

Execute o código abaixo e vamos analisar os detalhes.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

\# Valores aleatórios podem ser atribuídos a variáveis usando a função randint() da biblioteca random.

\# A função randint gera números inteiros aleatórios dentro de um intervalo especificado.

\# Portanto, vamos declarar o intervalo de 30 a 120 e simular o peso de uma pessoa aleatória.

  

from random import randint \# importamos a função randint da biblioteca random

  

capacity \= 400 \# capacidade do elevador em kg

total\_weight \= 0 \# variável que armazena o peso total

  

while total\_weight < capacity: \# enquanto o peso total é menor do que a capacidade máxima

person\_weight \= randint(30, 120) \# geramos um número aleatório de 30 a 120

total\_weight += person\_weight \# o peso gerado é adicionado ao peso total que será impresso

print(f'Uma pessoa entra. Carga do elevador: {total\_weight}')

print('Desculpe! O elevador está lotado. Você terá que esperar pelo próximo.')

Mostrar a soluçãoExecutar

Antes de cada etapa, o ciclo verifica a variável `total_weight`. Se estiver abaixo do limite `capacity`, o ciclo executa as instruções de seu corpo. Depois, ele volta para a primeira linha e verifica de novo se o peso total é menor que a capacidade máxima. Quando o limite for atingido ou excedido, o ciclo será interrompido e a impressão localizada fora do corpo do ciclo será impressa.

O sinal `<` no ciclo `while`é um operador de comparação que verifica se o valor à direita é maior que o valor à esquerda, igual ao operador matemático. No contexto de um ciclo `while`, ele é usado para verificar se uma determinada condição é `True` ou `False`. Se a condição for true, o código dentro do ciclo será executado. Falaremos mais sobre esses operadores de comparação no próximo capítulo.

Não sabemos quantas etapas o ciclo vai executar. Isso depende da rapidez com que o peso total atinge o limite, mas quando isso acontece, o ciclo será encerrado.

## Uso de contadores

`while` pode ser usado com uma variável contadora, que serve para parar o ciclo, ou você pode usá-la simplesmente para saber quantas vezes o ciclo se repetiu.

Vamos dar uma olhada no exemplo do elevador de novo, mas desta vez queremos evitar que mais de 10 pessoas entrem no elevador.

Execute o código abaixo e vamos analisar o caso.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

num\_people \= 0 \# começa em 0 e não há ninguém dentro

capacity \= 10 \# variável que armazena o limite de pessoas

  

while num\_people < capacity: \# enquanto o número de pessoas é menor que a capacidade máxima

num\_people += 1 \# uma pessoa entra

print(f'Uma pessoa entra. Carga do elevador: {num\_people}')

print('Desculpe! O elevador está lotado. Você terá que esperar pelo próximo.')

Mostrar a soluçãoExecutar

Veja que, a cada iteração, o código imprime o número de pessoas no elevador. O último número impresso é 10. Logo em seguida, não há números impressos, significando que o ciclo foi encerrado.

A seguir, vamos conferir como podemos descobrir quantas pessoas foram necessárias para o elevador atingir a carga máxima de peso. Execute o código várias vezes e você verá a função `randint()` em ação!

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

from random import randint

  

capacity \= 400 \# capacidade do elevador em kg

total\_weight \= 0 \# variável que armazena o peso total

num\_people\= 0 \# começa em 0, não tem ninguém dentro

  

while total\_weight < capacity: \# enquanto o peso total é menor do que a capacidade máxima

person\_weight \= randint(30, 120) \# geramos um número aleatório de 30 a 120

total\_weight += person\_weight \# o peso gerado é adicionado ao peso total

num\_people += 1 \# uma pessoa entra

print(f'Uma pessoa entra. Carga do elevador: {total\_weight}')

print(f'Desculpe! O elevador está lotado. Você terá que esperar pelo próximo. \\n{num\_people} pessoas já estão no elevador')

Mostrar a soluçãoExecutar

## Ciclos infinitos

Observe que o uso de ciclos `while` tem um problema: se a comparação no cabeçalho nunca resultar em `False` vai ocorrer um ciclo infinito. O corpo do ciclo continua sendo executado sem fim:

```
sales = 1000 # definindo o valor de vendas para 1000

while sales > 100: # desde que o valor em sales seja superior a 100,
    sales += 1 # aumentamos o valor de sales em 1
    print(sales) # e imprimimos
```

```
1000
1001
1002
1003
1004
1005
...
```

A condição `sales > 100` nunca vai retornar `False`, então acabamos com um contador eterno.

Pergunta

## Questionário 1

Quantas vezes o ciclo será executado?

```
start = 1
while start < 100:
    start -= 20
```

5

0

Ciclo infinito

O ciclo para quando o valor `start` se tornar maior ou igual a 100, mas isso nunca vai acontecer: ele começa em 1 e a cada etapa subtraímos 20.

Excelente!

Pergunta

## Questionário 2

Quantas vezes o ciclo será executado?

```
start = 100
while start < 30:
    start += 10
```

20

0

Certo, `start` sempre será maior que 30, inclusive na primeira iteração.

Ciclo infinito

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-10-137Z.md
### Última modificação: 2025-05-28 18:54:10

# Atividade prática do capítulo - TripleTen

Capítulo 5/9

Ciclos

# Atividade prática do capítulo

Você viu como os ciclos `for` e `while` podem ser incríveis. Por exemplo, os ciclos `for` permitem iterar sobre uma tabela (uma lista de listas) e modificar valores em linhas e colunas específicas como esta:

```
movies_info = [
    ['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111],
    ['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730],
    ['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499],
    ["Schindler's List", 'USA', 1993, 'drama', 195, 8.818],
    ['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625],
    ['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619],
    ['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521],
    ['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644],
    ['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106],
    ['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077]
]

for row in movies_info:
    runtime = row[4]
    runtime /= 60
    print(runtime)
```

```
2.3666666666666667
2.9166666666666665
2.533333333333333
3.25
3.35
2.566666666666667
2.966666666666667
2.316666666666667
2.216666666666667
2.1
```

Ciclos `while`, por outro lado, são incrivelmente úteis quando não sabemos de quantas iterações vamos precisar. Assim como no exemplo dos elevadores, fizemos um ciclo até chegar ao limite.

Vamos continuar nossa prática do último capítulo e resolver mais algumas tarefas do Banco ABC.

O banco reviu a sua lista de clientes e encontrou mais alguns clientes interessantes para o serviço bancário privilegiado. Eles foram adicionados à lista anterior.

```
clients = [
    [32456, "Jack Wilson", 32, 150000, "Healthcare"],
    [34591, "Nina Brown", 45, 250000, "Telecom"],
    [37512, "Alex Smith", 39, 210000, "IT"],
    [39591, "Brian Perez", 29, 340000, "Transportation"],
    [45123, "Sarah Lee", 28, 120000, "Marketing"],
    [47635, "David Kim", 36, 180000, "Finance"],
    [49571, "Samantha Chen", 42, 220000, "Retail"],
    [50391, "Juan Rodriguez", 31, 160000, "Architecture"],
]
```

## Tarefas

### Tarefa 1

Os departamentos de segurança do banco não ficaram satisfeitos com como apresentamos a lista de clientes da última vez. Então, desta vez, eles pediram para fornecermos os nomes dos clientes imprimindo cada nome em uma linha separada.

Percorra a lista de clientes com um ciclo e imprima os nomes dos clientes.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

clients \= \[

\[32456, "Jack Wilson", 32, 150000, "Healthcare"\],

\[34591, "Nina Brown", 45, 250000, "Telecom"\],

\[37512, "Alex Smith", 39, 210000, "IT"\],

\[39591, "Brian Perez", 29, 340000, "Transportation"\],

\[45123, "Sarah Lee", 28, 120000, "Marketing"\],

\[47635, "David Kim", 36, 180000, "Finance"\],

\[49571, "Samantha Chen", 42, 220000, "Retail"\],

\[50391, "Juan Rodriguez", 31, 160000, "Architecture"\],

\]

  

for nome in clients:

print(nome\[1\])\# escreva seu código aqui

  

Dica

Mostrar a soluçãoValidar

### Tarefa 2

O departamento de marketing do Banco ABC tem interesse em saber a idade média dos clientes. A idade média é apenas a soma das idades de todos os clientes dividida pelo número de clientes. Use o ciclo for para iterar na lista de `clients` e atualize as variáveis `total_age` e `num_clients` em cada iteração do ciclo.

A variável `total_age` deve armazenar a soma das idades, enquanto a variável `num_clients` é usada para armazenar o número total de clientes.

As variáveis `total_age` e `num_clients` já estão declaradas para você fazer seus cálculos com elas. Após revisar a lista e atualizar essas duas variáveis, calcule a idade média de todos os clientes, a armazene na variável `average_age` e a imprima.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

clients \= \[

\[32456, "Jack Wilson", 32, 150000, "Healthcare"\],

\[34591, "Nina Brown", 45, 250000, "Telecom"\],

\[37512, "Alex Smith", 39, 210000, "IT"\],

\[39591, "Brian Perez", 29, 340000, "Transportation"\],

\[45123, "Sarah Lee", 28, 120000, "Marketing"\],

\[47635, "David Kim", 36, 180000, "Finance"\],

\[49571, "Samantha Chen", 42, 220000, "Retail"\],

\[50391, "Juan Rodriguez", 31, 160000, "Architecture"\]

\]

  

total\_age \= 0

num\_clients \= len(clients)

for idade in clients:

total\_age += idade\[2\]

average\_age \= total\_age / num\_clients

print(average\_age)

\# escreva seu código aqui

  

  

Dica

Mostrar a soluçãoValidar

### Tarefa 3

A gerência está interessada em saber quando um cliente atinge um milhão em receita total e pediu que você escreva um código que calcule o número de anos necessários para um cliente atingir esse número. Para começar, ela pediu para escrever um código teste para `'Jack Wilson'`, que ganha `150000` por ano. Escreva um ciclo `while` que soma a receita anual total de Jack até que ele atinja 1 milhão. Assim que chegar lá, imprima o número de anos que levou para atingir essa receita total.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

annual\_income \= 150000 \# Receita anual de Jack

target\_income \= 1000000 \# meta de receita

  

total\_income\_sum \= 0 \# receita total que será atualizada a cada ano

years\_to\_million \= 0 \# número de anos que também será atualizado

  

while total\_income\_sum < target\_income:

total\_income\_sum += annual\_income\# escreva seu código aqui

years\_to\_million += 1\# escreva seu código aqui

  

print(years\_to\_million)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-12-761Z.md
### Última modificação: 2025-05-28 18:54:13

# Conclusão - TripleTen

Capítulo 5/9

Ciclos

# Conclusão

Ótimo trabalho! Você dominou os ciclos `for` e `while`. Além disso, agora você compreende a atribuição composta e aprendeu sobre funções de listas integradas, como `max()`, `min()` e outras.

Agora você é capaz de:

-   Explicar para que servem os ciclos `for` e `while`. Se ainda não tiver certeza, revise a lição sobre [Ciclos `for`](https://tripleten.com/trainer/data-analyst/lesson/ce74d34f-f2e4-4387-8865-df8c0d74f716/) e [Ciclos `while`](https://tripleten.com/trainer/data-analyst/lesson/d5d6f6f3-6013-4331-8e0b-f5f04c6a6b70/).
-   Usar ciclos com listas aninhadas Se ainda não tiver certeza, revise a lição sobre [Percorrer listas aninhadas](https://tripleten.com/trainer/data-analyst/lesson/4c617bc7-a872-4f5f-8995-48bc2849445b/).
-   Usar operadores de atribuição composta com ciclos Se ainda não tiver certeza, revise a lição sobre [Operadores de atribuição composta e funções integradas](https://tripleten.com/trainer/data-analyst/lesson/b333811d-a06d-428d-b1d3-5150d8ee9102/).
-   Calcular valores mínimo, máximo e soma de elementos em uma lista usando funções integradas Se ainda não tiver certeza, revise a lição sobre [Operadores de atribuição composta e funções integradas](https://tripleten.com/trainer/data-analyst/lesson/b333811d-a06d-428d-b1d3-5150d8ee9102/).

Essas funções são úteis quando estamos realizando operações em listas, mas como veremos mais adiante, elas também são úteis em conjuntos maiores de dados.

![](https://practicum-content.s3.amazonaws.com/resources/4-conc_1686217976.png)

## O que vem a seguir

Você vai aprimorar ainda mais suas habilidades com listas enquanto aprende a trabalhar com instruções condicionais. Não se assuste com o termo: a lógica é a mesma que você já conhece da matemática — instruções condicionais apenas abrem a porta para o tipo de manipulação de dados que você verá quando trabalhar com conjuntos de dados do mundo real.

Quando estiver com tudo pronto, vamos lá!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-14-047Z.md
### Última modificação: 2025-05-28 18:54:14

# Introdução - TripleTen

Capítulo 6/9

Instruções Condicionais

# Introdução

No capítulo anterior, enquanto estudava os ciclos `while`, você viu como a condição era verificada a cada iteração do ciclo. Em particular, verificamos se `total_weight` era menor (`<`) que `capacity` e, se esse era o caso, o ciclo continuava.

Agora é hora de aprender mais sobre instruções condicionais como `<` que usamos no ciclo `while`.

### Instruções condicionais

Além dos ciclos `while`, você trabalhou ativamente no capítulo anterior na criação de sua própria biblioteca de filmes: `movies_info`. Você já consegue fazer muito com ela: ordenar filmes por diferentes parâmetros, adicionar e remover entradas, acessar apenas valores específicos e muito mais.

Nesse capítulo, você vai escrever programas que tomam decisões com base na entrada, ou seja, nos dados que recebem – com a ajuda de instruções condicionais. Após dominar o uso de instruções condicionais, você vai estar realmente com tudo pronto para entrar na análise de dados.

Ao final deste capítulo, você será capaz de reconhecer e definir expressões lógicas, usar expressões condicionais para filtrar tabelas, usar as instruções condicionais `if` e `else` para criar ramificações no código, usar `elif` para tratar mais de duas possíveis ramificações, formar expressões compostas com `and`, `or` e `not` e escrever filtros que armazenam, em uma variável vazia, linhas que atendem a certos critérios.

Serão necessárias de 1 a 3 horas para concluir o capítulo. Também veremos alguns exemplos simples da nossa lista aninhada `movies_info`.

```
movies_info = [
    ['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111],
    ['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730],
    ['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499],
    ["Schindler's List", 'USA', 1993, 'drama', 195, 8.818],
    ['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625],
    ['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619],
    ['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521],
    ['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644],
    ['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106],
    ['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077]
]
```

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-15-381Z.md
### Última modificação: 2025-05-28 18:54:16

# Expressões lógicas e operadores de comparação - TripleTen

Capítulo 6/9

Instruções Condicionais

# Expressões lógicas e operadores de comparação

<iframe class="base-markdown-iframe__iframe" id="player-YZg18BOQ3lk" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Logical Expressions and Comparison Operators" width="640" height="360" src="https://www.youtube.com/embed/YZg18BOQ3lk?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fb10b7d89-938c-4a3f-becd-282872ff411a%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Até agora, você criou programas simples, nos quais as linhas de código eram apenas executadas uma após a outra. No entanto, às vezes precisamos adicionar flexibilidade ao comportamento do programa. Podemos fazer isso usando instruções condicionais.

Vamos supor que estamos no outono. Você está tentando decidir se precisa levar um guarda-chuva quando for sair de casa.

![](https://practicum-content.s3.amazonaws.com/resources/7.2PT_1690208897.png)

Se estiver chovendo, você leva um guarda-chuva, se não estiver chovendo, você leva um chapéu.

O resultado da verificação da condição (está chovendo?) determina qual ramo do algoritmo será executado (com guarda-chuva ou sem guarda-chuva).

A estrutura básica de qualquer **instrução condicional** é: IF <instrução é True/verdadeira>, THEN <faça isso>.

Para definir a primeira parte da instrução condicional, usamos expressões lógicas.

## Expressões lógicas

Existem vários tipos de expressões em Python que retornam `True` ou `False:`

-   Expressões contendo operadores de comparação: `==`, `!=`, `>`, `<`, `>=`, `<=`
-   Expressões contendo operadores lógicos: `and`, `or`, `not`
-   Funções predicado

**Operadores de comparação**

Digamos que queremos verificar se o filme 'The Shawshank Redemption' foi lançado depois de 'Titanic':

```
shawshank_release_year = 1994
titanic_release_year = 1997

shawshank_release_year > titanic_release_year
```

**Resultado:**

```
False
```

'The Shawshank Redemption' was released a few years before!

Aqui está uma lista dos operadores lógicos usados em Python:

Operador

Significado

Exemplo

Resultado

\==

Igual

5 == 5

True

!=

Diferente

5 != 2

True

\>

Maior que

5 > 2

True

<

Menos que

5 < 2

False

\>=

Maior ou igual a

5 >= 2

True

<=

Menor ou igual a

5 <= 5

True

Ao usar operadores de comparação, devemos saber que não podemos comparar tipos de dados completamente diferentes.

Ao usar operadores de comparação, podemos comparar `float` com `int`. Na verdade, o Python é capaz de converter automaticamente `int` em `float` quando necessário, mas você não pode comparar tipos de dados completamente diferentes. Por exemplo, você não pode comparar números com strings.

## Operadores lógicos

Os operadores lógicos são ferramentas bastante úteis na programação, pois permitem combinar e avaliar várias condições em uma expressão lógica. Em Python, existem três operadores lógicos bastante usados, sendo eles "and" (e), "or" (ou) e "not" (não), que podem ser usados para criar instruções condicionais mais complexas que podem lidar com várias condições.

O operador "and" é usado para verificar se ambas as condições em uma instrução são verdadeiras. Se ambas as condições forem verdadeiras, então toda a afirmação é verdadeira. Por outro lado, se uma ou ambas as condições forem falsas, toda a afirmação será falsa.

O operador "or", por outro lado, verifica se uma ou ambas as condições em uma instrução são verdadeiras. Se pelo menos uma das condições for verdadeira, então toda a afirmação é verdadeira. A afirmação só será falsa se ambas as condições forem falsas.

Por fim, o operador "not" é um operador unário usado para inverter o valor verdadeiro de uma condição. Se uma condição for verdadeira, então "not" a tornará falsa e vice-versa.

Esses operadores podem ser usados em combinação com outras instruções condicionais e ciclos para criar programas mais complexos e dinâmicos. Ao entender como usar operadores lógicos, você pode criar um código mais eficiente, que pode lidar com uma ampla variedade de situações.

Aqui está uma tabela para os operadores lógicos usados em Python:

Operador

Significado

Exemplo

Resultado

and

Ambas as condições devem ser verdadeiras

5 > 2 and 3 < 5

True

or

Pelo menos uma condição deve ser verdadeira

5 > 2 or 3 > 5

True

not

Inverte o valor verdadeiro de uma condição

not (5 > 2)

False

Vamos voltar ao banco de dados de filmes e conferir se ‘Fight Club’ foi lançado entre 1996 e 1998. Observe o código abaixo e o execute para ver qual é o resultado.

CódigoPYTHON

9

1

2

3

movie\_info \= \['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\]

  

print(movie\_info\[2\] \> 1996 and movie\_info\[2\] < 1998)

Mostrar a soluçãoExecutar

O código retornou `False` porque uma das condições não foi atendida (Data de lançamento de Fight Club <1998).

Caso você queira verificar se apenas uma das condições é `True`, use `or` em vez de `and`. O código vai ficar assim se trocarmos `and` por `or`. Execute o código para ver o resultado:

CódigoPYTHON

9

1

2

3

movie\_info \= \['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\]

  

print(movie\_info\[2\] \> 1996 or movie\_info\[2\] < 1998)

Mostrar a soluçãoExecutar

Não é de surpreender que o resultado seja `True`. Isso ocorre porque `movie_info[2] > 1996` retornou `True`.

Vamos praticar.

Pergunta

### Questionário 1

O que você acha que esse código vai imprimir?

```
print('10' > 2)
```

`True`

`False`

Ele vai retornar um erro

Exatamente! O Python não pode comparar strings com números. `int` e `float`, porém, podem ser comparados, visto que ambos representam números.

Trabalho maravilhoso!

### Questionário 2

Vamos praticar.

Pergunta

Confira a lista `movie_info` abaixo e selecione as opções que imprimem `True` apenas se a classificação estiver entre 8 e 9.

```
movie_info = ['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644]
```

Escolha quantas quiser

`print(movie_info[5] > 8 or movie_info[5] < 9)`

`print(movie_info[5] > 8 and movie_info[5] < 9)`

Isso está certo!

`print(movie_info[-1] > 8 and movie_info[-1] < 9)`

Isso está correto! Aqui usamos indexação negativa para extrair a classificação de um filme.

Muito bem!

Teoria

E se você quiser inverter o resultado que obtivemos antes? Para isso, podemos usar `not`:

CódigoPYTHON

9

1

2

3

4

5

6

7

movie\_info \= \['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\]

  

\# primeiro, vamos escrever uma condição como fizemos antes e imprimir o resultado

print(movie\_info\[2\] \> 1996 and movie\_info\[2\] < 1998)

  

\# depois, vamos reverter com 'not' e também imprimir o resultado

print(not(movie\_info\[2\] \> 1996 and movie\_info\[2\] < 1998))

Mostrar a soluçãoExecutar

O código retornou `False` porque uma das condições não foi atendida (Data de lançamento de Fight Club < 1998) e então transformamos em `True`.

Vamos praticar `not` agora.

### Questionário 3

Vamos praticar `not` agora.

Pergunta

O que você acha que esse código vai imprimir?

```
movie_info = ['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644]

print(not(movie_info[-1] > 8 and movie_info[-1] < 9))
```

`True`

`False`

Exatamente! Isso ocorre porque `True` é retornado por `movie_info[-1] > 8 and movie_info[-1] < 9`, então o resultado é invertido por `not`.

Ele vai retornar um erro

Trabalho maravilhoso!

Teoria

## Funções predicado

Uma função predicado é qualquer função que retorne `True` ou `False`. Por exemplo, aqui estão alguns métodos de string que verificam se os caracteres em uma string atendem a certas condições:

-   `islower()` retorna `True` se todos os caracteres da string alfabética forem minúsculos
-   `isdigit()` retorna o valor `True` se a string tiver apenas números
-   `isalpha()` retorna `True` se uma string contém apenas letras. Se uma string contiver sinais de pontuação ou espaços, será retornado `False`.

Abaixo apresentamos strings e funções predicado. Altere as strings, execute o código e veja como o resultado muda.

CódigoPYTHON

9

1

2

3

print('olá'.islower())

print('777'.isdigit())

print('string contém espaços, bem como sinais de pontuação'.isalpha())

Mostrar a soluçãoExecutar

### Questionário 4

Agora é sua hora de brilhar! Confira o quiz abaixo.

Pergunta

O que você acha que esse código vai imprimir?

```
print('Eu amo python'.islower()) 
print('doze'.isdigit()) 
print('Instruções condicionais são ótimas'.isalpha())
```

Opção 1

```
False
False
False
```

Exatamente. Todas as linhas retornam `False`.

Opção 2

```
False
False
True
```

Opção 3

```
True
False
True
```

Muito bem!

As instruções condicionais são essenciais para a criação de programas que possam tomar decisões com base em diferentes entradas ou dados. Elas permitem que você escreva programas mais flexíveis e potentes que podem lidar com uma ampla gama de situações. Vamos praticar escrever sua primeira condição na próxima lição!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-18-005Z.md
### Última modificação: 2025-05-28 18:54:18

# Instruções if e else - TripleTen

Capítulo 6/9

Instruções Condicionais

# Instruções if e else

<iframe class="base-markdown-iframe__iframe" id="player-CNjaioMADoA" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="if and else Statements" width="640" height="360" src="https://www.youtube.com/embed/CNjaioMADoA?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F80490863-ee0e-4fd4-9c40-5c18fb0a5d39%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Resumindo, existem dois tipos gerais de expressões lógicas:

-   Expressões contendo operadores de comparação: `==`, `!=`, `>`, `<`, `>=`, `<=`
-   Expressões contendo operadores lógicos: `and`, `or`, `not`

Além disso, também aprendemos sobre funções predicado, que podem ser aplicadas a strings. São elas: `islower()`, `isdigit()` e `isalpha()`.

Agora que você pode reconhecer e identificar uma expressão lógica que retorne `True` ou `False`, é hora de aprender sobre operadores condicionais.

Ao final dessa lição, você será capaz de usar as instruções `if` e `else` para criar ramificações no código.

Você se lembra do exemplo da lição anterior? Veja aqui uma versão simplificada dele:

![](https://practicum-content.s3.amazonaws.com/resources/7.3_1PT_1690208925.png)

Uma instrução `if` é um operador condicional que permite executar um bloco de código somente se uma determinada condição for verdadeira. Se a condição for falsa, o bloco de código será ignorado.

Vamos ver como programar essa lógica em Python:

![](https://practicum-content.s3.amazonaws.com/resources/7.3_2PT_1690208947.png)

Uma instrução condicional contém um cabeçalho e um corpo:

-   O cabeçalho possui a condição (uma expressão lógica)
-   O corpo possui instruções (ramificações de código), contendo indentação de quatro espaços

Se a expressão lógica retornar `True`, a instrução `if` vai executar o código no corpo da condição.

Brinque com o código abaixo alterando o valor da variável `weather` e veja como isso funciona.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

\# mude o estado de weather para ver como ele afeta a condição

weather \= 'chuva'

  

\# condição

if weather \== 'chuva':

print('leve um guarda-chuva')

  

print('vamos lá!')

Mostrar a soluçãoExecutar

### Questionário 1

Se a expressão lógica retornar `False`, o Python vai simplesmente pular o código que está dentro da construção condicional. Agora é hora de praticar!

Pergunta

Selecione o código que vai imprimir `'Não perturbe'` se `time` (horário) for > `10`.

Opção 1

```
if time == 10:
    print('Não perturbe')
```

Opção 2

```
if time > 10:
print('Não perturbe')
```

Opção 3

```
if time > 10:
    print('Não perturbe')
```

Isso está certo!

Excelente!

Teoria

No entanto, em algumas situações, pode ser necessário definir a ação alternativa a ser tomada caso a condição retorne `False`. Por exemplo: SE NÃO estiver chovendo, ENTÃO leve um chapéu.

![](https://practicum-content.s3.amazonaws.com/resources/7.3_3PT_1690209020.png)

Em Python, após uma instrução `if`, uma instrução `else` pode ser usada para executar código quando a condição da instrução `if` não for verdadeira.

A instrução `else` deve ser indentada com quatro espaços para separar ela do bloco de código anterior. Esse é um recurso útil do Python que permite um fluxo de controle mais complexo em programas. Além disso, é importante manter a atenção, pois a instrução `else` é opcional e pode ser omitida se não for necessária.

![](https://practicum-content.s3.amazonaws.com/resources/7.3_4PT_1690209052.png)

Vamos alterar o status do clima para `'sol'` e adicionar uma instrução `else` ao código acima para ver como isso afeta o resultado.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

weather \= 'sol'

  

\# condição

if weather \== 'chuva':

print('leve um guarda-chuva')

else:

print('leve um chapéu')

print('vamos lá!')

Mostrar a soluçãoExecutar

Como não está chovendo, Python recomendou levar um chapéu, como pedimos! Ok, vamos fazer isso por conta própria agora.

### Questionário 2

Pergunta

Selecione o código que vai imprimir `'Não perturbe'` se `time` (horário) for > `10`. Caso contrário, ele vai imprimir `'Estou disponível'`.

Opção 1

```
if time > 10:
    print('Não perturbe')
else:
print('Estou disponível')
```

Opção 2

```
if time > 10:
    print('Não perturbe')
else:
    print('Estou disponível')
```

Isso está certo!

Fantástico!

### Tarefa

Ok, agora vamos escrever algum código!

Use uma declaração condicional para verificar a classificação do filme _The Godfather_ (O Poderoso Chefão) fornecida no pré-código. Ela está armazenado na variável `movie_rating`.

Se for **maior ou igual** a 8,5, imprima a mensagem `'Avaliação alta'`. Caso contrário, imprima `'Avaliação média'`.

CódigoPYTHON

9

1

2

3

4

5

movie\_rating \= 8.73 \# A avaliação de The Godfather

if movie\_rating \>= 8.5:

print('Avaliação alta')

else:

print('Avaliação média')\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

Teoria

### Verificação de substring

Em Python, você pode usar o operador `in` para determinar se existe uma substring em uma string maior. Essa é uma ferramenta bastante útil que pode ser usada em vários contextos. Por exemplo, você pode usar esse operador para verificar se uma determinada palavra aparece em uma frase ou para identificar se uma sequência de caracteres específica está presente em um arquivo de texto.

Para usar esse operador, basta escrever a substring que você está procurando seguida de `in` e, depois disso, a string maior na qual você deseja pesquisar. Se a substring estiver presente na string maior, o operador vai retornar `True`. Caso contrário, vai retornar `False`.

Dizem que "Não existe eu em equipe", vamos conferir:

```
if "estou" in "na equipe":
    print("Aqui estou eu na equipe.")
else:
    print("É verdade, não há eu em equipe.")
```

```
É verdade, não há eu em equipe.
```

Vamos tentar de novo com um exemplo mais complexo. Aqui está uma citação conhecida escrita por George Bernard Shaw, vamos procurar algo engraçado nela:

```
quote = "O progresso é impossível sem mudança, e aqueles que não conseguem mudar suas mentes não conseguem mudar nada."
if "ogres" in quote:
    print("Onde há progresso, há ogres!")
else:
    print("Aqui não, docinho de coco!")
```

```
Onde há progresso, há ogres!
```

Com `if` e `else`, podemos criar um algoritmo bidirecional. E se quisermos considerar mais de duas possibilidades? Descobriremos na próxima lição, mas, por enquanto, vamos praticar!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-19-333Z.md
### Última modificação: 2025-05-28 18:54:19

# Criação de várias ramificações com else e elif - TripleTen

Capítulo 6/9

Instruções Condicionais

# Criação de várias ramificações com else e elif

<iframe class="base-markdown-iframe__iframe" id="player-7FtFZx0Jq80" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Creating Multiple Branches with else and elif" width="640" height="360" src="https://www.youtube.com/embed/7FtFZx0Jq80?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Facba7bdc-70c0-4cdf-a366-aadcda16efbd%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Ok, agora sabemos como verificar uma condição usando `if` e `else`, mas e se precisarmos verificar múltiplas condições? Como podemos fazer isso em Python?

## Múltiplas condições

É comum ter múltiplas instruções condicionais em Python. A instrução `elif` é uma abreviação de "else if" e permite verificar condições adicionais após uma instrução `if` inicial.

Ao final desta lição, você será capaz de usar `elif` em seu código para criar qualquer número de ramificações.

Vamos refinar nosso algoritmo para lidar com o clima e adicionar mais algumas opções:

-   **Se estiver chovendo**, leve um guarda-chuva.
-   **Se estiver ensolarado**, leve um chapéu.
-   **Se estiver nevando**, coloque um gorro e um cachecol.
-   **Caso contrário,** não leve nada.

![](https://practicum-content.s3.amazonaws.com/resources/7.4PT_1690209866.png)

Como você vê no diagrama, o Python verifica as condições sequencialmente, uma após a outra.

No caso de um algoritmo tão complexo, uma estrutura `else-if` simples não será suficiente. Execute o exemplo abaixo para verificar por conta própria.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

weather \= 'chuva'

  

if weather \== 'chuva':

item\_to\_take \= 'guarda-chuva'

if weather \== 'sol':

item\_to\_take \= 'chapéu'

if weather \== 'neve':

item\_to\_take \= 'gorro e um cachecol'

else:

item\_to\_take \= 'nada'

  

print(weather)

print(item\_to\_take)

Mostrar a soluçãoExecutar

A variável `weather` no código acima armazena o clima atual, enquanto `item_to_take` armazena o item que devemos vestir ou levar conosco.

O programa não está funcionando como deveria: ele nos diz para não pegar nada quando o valor for `'chuva'` ou `'sol'`. Por que isso acontece? Vamos conferir o desempenho do código acima se o valor de `weather` for `'chuva'`:

1.  `if weather == 'chuva'`: define `item_to_take` como `'guarda-chuva'` e é exatamente isso que queremos.
2.  `if weather == 'sol'`: não faz nada, pois essa condição retorna `False`.
3.  `if weather == 'neve'`: retorna `False` e dentro da cláusula `else 'nada'` é definido como um valor para `item_to_take`.

Podemos facilmente resolver esse problema usando a palavra-chave `elif`. Ela nos permite juntar várias condições em uma estrutura: `if-elif-else`.

Vamos reescrever o código usando instruções `elif`:

```
weather = 'chuva'

if weather == 'chuva':
    item_to_take = 'guarda-chuva'
elif weather == 'sol':
    item_to_take = 'chapéu'
elif weather == 'neve':
    item_to_take = 'gorro e um cachecol'
else:
    item_to_take = 'nada'

print(weather)
print(item_to_take)
```

**Resultado:**

```
chuva
guarda-chuva
```

Voilà! Agora o programa é executado corretamente. Confirme por conta própria substituindo o código acima com a versão contendo `elif`.

Podemos adicionar qualquer quantidade de ramificações usando a palavra-chave `elif`. Porém, apenas uma ramificação será executada: aquela cuja condição for atendida primeiro. Todas as outras ramificações serão ignoradas.

É por isso que é importante ordenar os ramos corretamente, com as condições mais específicas primeiro e as mais gerais por último. Ao fazer isso, garantimos que o código se comporte conforme esperado, e que todos os casos possíveis sejam cobertos.

## Condições com ciclo for

Combinadas com um ciclo `for`, as instruções condicionais se tornam uma ferramenta indispensável para especialistas em dados. Com a ajuda delas, você pode verificar cada elemento da lista em busca de uma condição.

Dê uma olhada na lista `years`, que armazena os anos de lançamento de filmes. Para cada ano, vamos imprimir a década correspondente. Para fazer isso, vamos verificar vários intervalos abrangendo os anos de 1960 a 2009:

```
years = [1994, 1972, 2008, 1993, 2003, 1994, 1966, 1999, 1962, 1997]

for year in years: 
    if 1960 <= year <= 1969: 
        print('O filme foi lançado nos anos 60.')
    elif 1970 <= year <= 1979:
        print('O filme foi lançado nos anos 70.') 
    elif 1980 <= year <= 1989: 
        print('O filme foi lançado nos anos 80.')
    elif 1990 <= year <= 1999: 
        print('O filme foi lançado nos anos 90.')
    elif 2000 <= year <= 2009:
        print('O filme foi lançado nos anos 2000.')
    else: 
        print('Ano não definido.')
```

```
O filme foi lançado nos anos 90.
O filme foi lançado nos anos 70.
O filme foi lançado nos anos 2000.
O filme foi lançado nos anos 90.
O filme foi lançado nos anos 2000.
O filme foi lançado nos anos 90.
O filme foi lançado nos anos 60.
O filme foi lançado nos anos 90.
O filme foi lançado nos anos 60.
O filme foi lançado nos anos 90.
```

Assim, obtivemos todas as informações necessárias sobre cada elemento da lista.

### Tarefa 1

Agora tente você!

No pré-código, você tem uma lista `countries` que armazena os países onde determinados filmes foram lançados. Escreva um ciclo `for` que vai iterar sobre elementos de `countries` e, para cada elemento, imprimir a mensagem relevante:

-   Para os `USA`, vamos imprimir `'The movie was released in the USA.'`
-   Para a França: `'Le film est sorti en France.'`
-   Para a Itália: `'Il film e stato rilasciato in Italia.'`
-   Para qualquer outro país: `'País não definido.'`

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

countries \= \['France','Italy' , 'New Zealand', 'Italy', 'France', 'USA'\]

  

for pais in countries:

if pais \== 'USA':

print('The movie was released in the USA.')

elif pais \== 'France':

print('Le film est sorti en France.')

elif pais \== 'Italy':

print('Il film e stato rilasciato in Italia.')

else :

print('País não definido.')\# comece a escrever seu ciclo for aqui

Dica

Mostrar a soluçãoValidar

### Tarefa 2

As classificações de filmes são armazenadas na variável `ratings`. Queremos que você converta essas classificações de um sistema de 100 pontos para um sistema de 5 pontos. Para fazer isso, itere sobre a lista `ratings` original e use instruções condicionais para converter a escala original para o sistema de 5 pontos de acordo com esta lógica:

-   imprimir 2 se a classificação original estiver na faixa de 0 a 59 pontos, incluindo o último valor
-   imprimir 3 se a classificação estiver na faixa de 60 a 72 pontos, incluindo o último valor
-   imprimir 4 se a classificação for de 73 a 84 pontos, incluindo o último valor
-   imprimir 5 se a classificação for de 85 a 100 pontos, incluindo o último valor

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

ratings \= \[91, 35, 65, 89, 78, 93\]

  

for numero in ratings:

if numero \>= 0 and numero <= 59:

print('2')

elif numero \>= 60 and numero <= 72:

print('3')

elif numero \>= 73 and numero <= 84:

print('4')

else:

print(5)\# comece a escrever seu loop for aqui

Dica

Mostrar a soluçãoValidar

Nas próximas lições, vamos mostrar como instruções condicionais podem ser usadas para modificar e filtrar dados em tabelas.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-20-667Z.md
### Última modificação: 2025-05-28 18:54:21

# Modificações em tabelas - TripleTen

Capítulo 6/9

Instruções Condicionais

# Modificações em tabelas

<iframe class="base-markdown-iframe__iframe" id="player-MYLVsmlZmNk" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Modifying Tables" width="640" height="360" src="https://www.youtube.com/embed/MYLVsmlZmNk?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F2c23e510-ec9f-4b98-9dac-9ef832c2a212%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

![](https://practicum-content.s3.amazonaws.com/resources/7.5PT_1690209886.png)

As tabelas são parte integrante do processamento e análise de dados. Em Python, as tabelas são geralmente implementadas como listas de listas ou usando bibliotecas como a pandas, que veremos no futuro.

A modificação de tabelas é uma etapa crucial no pré-processamento de dados, e os ciclos são uma maneira eficaz de fazer isso. Nesta lição, vamos aprender como modificar tabelas usando ciclos em Python.

## Como modificar valores em uma tabela

Modificar valores em uma tabela é uma operação comum no processamento de dados. Podemos usar ciclos para fazer isso. Imagine que temos uma tabela de filmes no seguinte formato:

```
movies = [
    ["The Shawshank Redemption", 1994, "Frank Darabont"],
    ["The Godfather", 1972, "Francis Ford Coppola"],
    ["The Dark Knight", 2008, "Christopher Nolan"],
    ["12 Angry Men", 1957, "Sidney Lumet"],
    ["Schindler's List", 1994, "Steven Spielberg"],
    ["The Lord of the Rings: The Return of the King", 2003, "Peter Jackson"]
]
```

Imagine que queiramos corrigir o ano de lançamento de "Schindler's List" de 1994 a 1993. Podemos usar o seguinte código:

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

movies \= \[

\["The Shawshank Redemption", 1994, "Frank Darabont"\],

\["The Godfather", 1972, "Francis Ford Coppola"\],

\["The Dark Knight", 2008, "Christopher Nolan"\],

\["12 Angry Men", 1957, "Sidney Lumet"\],

\["Schindler's List", 1994, "Steven Spielberg"\],

\["The Lord of the Rings: The Return of the King", 2003, "Peter Jackson"\]

\]

  

movie\_name \= "Schindler's List"

correct\_year \= 1993

  

for movie in movies: \# itere sobre todos os filmes

if movie\[0\] \== movie\_name: \# se o título do filme é o que queremos corrigir

movie\[1\] \= correct\_year \# corrija o valor que queremos

  

  

for movie in movies: \# itere sobre todos os filmes

print(movie)

Mostrar a soluçãoExecutar

Lembre-se de que tais modificações são possíveis porque as listas são mutáveis.

### Tarefa

Agora que acertamos os anos, tente fazer alterações você mesmo nesta tarefa:

Imagine que você criou uma lista de filmes que gostaria de assistir com seus amigos. Aqui está a lista:

```
movies = [
    ["The Shawshank Redemption", 1994, "Frank Darabont"],
    ["The Godfather", 1972, "Francis Ford Coppola"],
    ["The Dark Knight", 2008, "Christopher Nolan"],
    ["12 Angry Men", 1957, "Sidney Lumet"],
    ["Schindler's List", 1993, "Steven Spielberg"],
    ["The Lord of the Rings: The Return of the King", 2003, "Peter Jackson"]
]
```

Você a mostrou aos seus amigos e perguntou o que eles achavam. Em geral, eles adoram, mas seus amigos disseram que, antes de assistir "O Senhor dos Anéis: O Retorno do Rei", seria ótimo assistir a primeira parte, que é "O Senhor dos Anéis: A Sociedade do Anel".

Você concorda que a primeira parte do filme deve ser assistida primeiro. Então, seu objetivo agora é modificar a lista `movies` que você criou antes. Em particular, você quer substituir “O Senhor dos Anéis: O Retorno do Rei” por “O Senhor dos Anéis: A Sociedade do Anel”, lançado em 2001. Peter Jackson foi o diretor do filme.

Para fazer isso, escreva um ciclo `for` que itere sobre os elementos da lista `movies` e, quando você chegar a uma sublista que tenha `"The Lord of the Rings: The Return of the King"` como nome de filme, substitua o nome do filme pela variável `new_movie` e o ano de lançamento pela variável `new_year`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

movies \= \[

\["The Shawshank Redemption", 1994, "Frank Darabont"\],

\["The Godfather", 1972, "Francis Ford Coppola"\],

\["The Dark Knight", 2008, "Christopher Nolan"\],

\["12 Angry Men", 1957, "Sidney Lumet"\],

\["Schindler's List", 1993, "Steven Spielberg"\],

\["The Lord of the Rings: The Return of the King", 2003, "Peter Jackson"\]

\]

  

movie\_to\_change \= "The Lord of the Rings: The Return of the King"

new\_movie \= "The Lord of the Rings: The Fellowship of the Ring"

new\_year \= 2001

  

for movie in movies:

if movie\[0\] \== movie\_to\_change: \# defina uma condição aqui

movie\[0\] \= new\_movie \# substitua o nome do filme se a condição retornar True

movie\[1\] \= new\_year \# substitua o ano de lançamento se a condição retornar True

  

\# não modifique o código abaixo. Ele imprime o resultado

for movie in movies:

print(movie)

  

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-23-077Z.md
### Última modificação: 2025-05-28 18:54:23

# Filtragem de tabelas - TripleTen

Capítulo 6/9

Instruções Condicionais

# Filtragem de tabelas

<iframe class="base-markdown-iframe__iframe" id="player-TOvbXGTMLYA" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Filtering Tables" width="640" height="360" src="https://www.youtube.com/embed/TOvbXGTMLYA?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F81feba3e-e81e-4e76-baea-93cfb9cb4793%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Nos últimos capítulos, você aprendeu três conceitos importantes:

-   listas aninhadas, que podem ser usadas para representar tabelas
-   ciclos, que podem ser usados para percorrer sublistas ou valores dentro delas
-   e operadores condicionais, que alteram o comportamento do código dependendo de determinados critérios

Quando juntamos esses três conceitos, obtemos um novo: um filtro

## Como usar filtros

![](https://practicum-content.s3.amazonaws.com/resources/7.6PT_1690210084.png)

Um **filtro** é uma ferramenta útil que podemos usar para processar tabelas, selecionando apenas dados que atendam a um conjunto de condições e usando essas condições para construir novas tabelas.

Ao final desta lição, você vai conseguir descrever como um filtro funciona e aplicá-lo para extrair tabelas mais curtas que contenham apenas os dados relevantes, tornando a análise e a manipulação dos dados mais simples.

Vamos dar outra olhada na nossa lista aninhada sobre filmes:

```
movies_info = [
    ['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111],
    ['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730],
    ['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499],
    ["Schindler's List", 'USA', 1993, 'drama', 195, 8.818],
    ['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625],
    ['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619],
    ['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521],
    ['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644],
    ['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106],
    ['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077]
]
```

Vamos criar um filtro que armazena filmes com mais de 180 minutos em uma nova lista aninhada. Execute o código abaixo e vamos analisar o caso.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

movies\_filtered \= \[\] \# lista vazia para armazenar o resultado

  

for movie in movies\_info: \# iterando sobre as linhas da tabela original

if movie\[4\] \> 180: \# se um filme dura mais do que 180 min

movies\_filtered.append(movie) \# adicione a linha à lista movie\_filtered

  

for movie in movies\_filtered: \# imprimindo o conteúdo da lista movies\_filtered

print(movie)

Mostrar a soluçãoExecutar

Primeiro criamos a variável chamada `movies_filtered` e armazenamos uma lista vazia nela. Em seguida, usamos um ciclo `for` para iterar as sublistas da lista `movies_info`. `if movie[4] > 180` é uma condição que nos permite extrair a duração do filme e verificar se ela é maior que 180. Se sim, usamos o método `append()`, que adiciona o filme à lista `movies_filtered`.

O que acabamos de fazer? Filtramos a tabela `movies_filtered` original com base na duração do filme. Acabamos de criar um filtro.

## Partes de um filtro

Um filtro para uma tabela consiste em três partes:

-   **Uma variável de lista vazia** para anexar linhas correspondentes.
-   **Um ciclo** que percorre a tabela original.
-   **Uma** **instrução condicional** `if`. Se uma sublista atender à condição, ela será anexada à variável que armazena os elementos que atendem ao filtro.

Vamos fazer outro filtro para armazenar os filmes japoneses em uma nova lista. Examine o código abaixo e o execute para ver o resultado.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

movies\_filtered \= \[\] \# lista vazia para armazenar o resultado

Mostrar a soluçãoExecutar

Não há muitos filmes japoneses no nosso banco de dados. E filmes americanos? Tente encontrá-los.

### Tarefa

Obtenha todos os filmes dos EUA da tabela `movies_info` usando um filtro. Armazene o resultado na lista `movies_filtered`. Imprima essa lista quando terminar.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

movies\_filtered \= \[\] \# lista vazia para armazenar o resultado

  

  

for movies in movies\_info:

if movies\[1\] \== 'USA':

movies\_filtered.append(movies)\# escreva seu código aqui

  

  

\# imprimir o resultado

print(movies\_filtered)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-24-400Z.md
### Última modificação: 2025-05-28 18:54:24

# Operadores lógicos e filtros - TripleTen

Capítulo 6/9

Instruções Condicionais

# Operadores lógicos e filtros

<iframe class="base-markdown-iframe__iframe" id="player-j0v9N24M-3s" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Logical Operators and Filters" width="640" height="360" src="https://www.youtube.com/embed/j0v9N24M-3s?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Ff12f8ed9-878d-4f5a-94b8-30d5de2fd555%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Como acabamos de aprender, um filtro para uma tabela sempre consiste em três partes:

-   **Uma variável de lista vazia** para anexar linhas correspondentes.
-   **Um ciclo** que percorre a tabela original.
-   **Uma** **instrução condicional** `if`. Se uma sublista atender à condição, ela será anexada à variável que armazena os elementos que atendem ao filtro.

## Operadores lógicos e filtros

![](https://practicum-content.s3.amazonaws.com/resources/2__764x513_1687954582.jpg)

Para criar algoritmos de filtragem mais sofisticados, você pode usar várias condições unidas por operadores lógicos. Ao fazer isso, você pode filtrar os dados com base em mais de um critério.

Nessa lição, você vai aprender como combinar várias condições usando operadores lógicos.

Vamos obter os filmes lançados nos Estados Unidos depois de 1990 verificando duas condições. Examine o código abaixo e o execute. Em seguida, vamos analisá-lo.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

movies\_filtered \= \[\]

  

for movie in movies\_info: \# percorrendo cada filme com um ciclo

if movie\[2\] \> 1990 and movie\[1\] \== 'USA': \# se o filme foi lançado depois do ano 1990 e nos EUA,

movies\_filtered.append(movie) \# nós o adicionamos à nova lista

  

for movie in movies\_filtered:

print(movie)

Mostrar a soluçãoExecutar

A linha principal no código acima é `if movie[2] > 1990 and movie[1] == 'USA'`. Vamos discutir o que ele faz, pois é assim que um filtro verifica uma condição. Em primeiro lugar, vemos claramente que ele consiste em duas partes:

1.  `movie[2] > 1990`
2.  `movie[1] == 'USA'`

Essas duas partes são unidas por um operador lógico `and`.

Se as condições `movie[2] > 1990` e `movie[1] == 'USA'` retornarem `True`, então, como temos `and` como operador lógico, a linha inteira (`if movie[2] > 1990 and movie[1] == 'USA'`) retorna `True`. Como resultado, executamos `movies_filtered.append(movie)` e o filme é anexado à lista `movies_filtered`.

### Tarefa

Parece que você está se tornando um especialista em filtragem de filmes! Continue explorando e filtrando filmes para encontrar seus favoritos.

Agora tente por conta própria:

Crie a lista vazia chamada `movies_filtered`. Em seguida, adicione a `movies_filtered` filmes que foram lançados em 1994 ou possui uma classificação inferior a 8,5.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

movies\_filtered \= \[\]

  

for movie in movies\_info:

if movie\[2\] \== 1994 or movie\[5\] < 8.5:

movies\_filtered.append(movie)

  

for movie in movies\_filtered:

print(movie)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-25-723Z.md
### Última modificação: 2025-05-28 18:54:26

# Atividade prática do capítulo - TripleTen

Capítulo 6/9

Instruções Condicionais

# Atividade prática do capítulo

Neste capítulo, aprendemos muito, começando pelas instruções condicionais com `if`, `else` e `elif` e terminando em sua implementação real para a criação de filtros complexos para tabelas.

Vamos praticar o uso de instruções condicionais. Elas são essenciais para qualquer programador e serão certamente usadas com frequência no seu código.

Vamos garantir que você entende completamente como usar `if`, `else` e `elif` para criar ramificações no seu código, bem como usar operadores lógicos como `and` e `or` para criar condições mais complexas. Boa sorte!

Vamos continuar melhorando os sistemas do banco ABC dos capítulos anteriores:

## Tarefas

### Tarefa 1

O Banco ABC criou uma nova Conta Elite feita sob medida para clientes de alto rendimento líquido que ganham mais de $200.000 anualmente. Escreva um código para iterar sobre os clientes, verificar se a receita anual deles excede US$ 200.000 e adiciona a sublista de clientes qualificados à variável `elite_clients`, que é a lista de clientes ricos. Por fim, imprima a variável `elite_clients`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

clients \= \[

\[32456, "Jack Wilson", 32, 150000, "Healthcare"\],

\[34591, "Nina Brown", 45, 250000, "Telecom"\],

\[37512, "Alex Smith", 39, 210000, "IT"\],

\[39591, "Brian Perez", 29, 340000, "Transportation"\],

\[45123, "Sarah Lee", 28, 120000, "Marketing"\],

\[47635, "David Kim", 36, 180000, "Finance"\],

\[49571, "Samantha Chen", 42, 220000, "Retail"\],

\[50391, "Juan Rodriguez", 31, 160000, "Architecture"\]

\]

  

elite\_clients \= \[\] \# adicione clientes elite aqui

  

for s in clients:

if s\[3\] \> 200000:

elite\_clients.append(s)\# escreva seu código aqui

  

  

print(elite\_clients)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

O departamento de marketing quer uma lista de todos os clientes do banco, divididos em quatro segmentos de contas. Eles pediram que você crie essa lista. Divida todos os clientes do Banco ABC em quatro segmentos: conta Standard para clientes com receita inferior a $100.000 (excluindo esse número), conta Plus para receitas de US$ 100.000 (incluindo esse número) a US$ 200.000 (excluindo esse número), conta Elite para receitas de US$ 200.000 (incluindo esse número) a US$ 300.000 (excluindo esse número) e conta Executive para receitas de US$ 300.000 (incluindo esse número) ou mais.

No pré-código abaixo, você vai encontrar quatro variáveis com as listas: `standard`, `plus`, `elite` e `executive`. Escreva o código para iterar sobre os clientes, verifique em qual categoria a receita se enquadra e adicione-os a uma lista apropriada. Quando terminar, imprima a lista de clientes Executive.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

clients \= \[

\[32456, "Jack Wilson", 32, 150000, "Healthcare"\],

\[34591, "Nina Brown", 45, 250000, "Telecom"\],

\[37512, "Alex Smith", 39, 210000, "IT"\],

\[39591, "Brian Perez", 29, 340000, "Transportation"\],

\[45123, "Sarah Lee", 28, 120000, "Marketing"\],

\[47635, "David Kim", 36, 180000, "Finance"\],

\[49571, "Samantha Chen", 42, 220000, "Retail"\],

\[50391, "Juan Rodriguez", 31, 160000, "Architecture"\],

\[34556, "Lucas Hernandez", 37, 75000, "Education"\],

\[64291, "Jessica Li", 25, 125000, "IT"\],

\[74512, "Emma Davis", 47, 197000, "Finance"\],

\[83191, "Sophia Perez", 34, 225000, "Transportation"\],

\[91023, "Liam Kim", 29, 98000, "Retail"\],

\[96435, "Ava Chen", 31, 175000, "Marketing"\],

\[100571, "Noah Rodriguez", 28, 85000, "Architecture"\],

\[101321, "Olivia Wilson", 44, 310000, "Telecom"\],

\[104556, "William Brown", 38, 289000, "Finance"\],

\[105491, "Emily Smith", 29, 193000, "Healthcare"\],

\[107512, "Michael Perez", 53, 415000, "Transportation"\]

\]

  

\# listas vazias para adicionar clientes

standard \= \[\]

plus \= \[\]

elite \= \[\]

executive \= \[\]

o \= 100000

for q in clients:

if q\[3\] < 1\*o:

standard.append(q)

elif q\[3\] <= 2\*o:

plus.append(q)

elif q\[3\] < 3\*o:

elite.append(q)

else:

executive.append(q) \# escreva seu código aqui

  

  

print(executive)

Dica

Mostrar a soluçãoValidar

### Tarefa 3

É o Dia da Juventude, e a equipe de marketing deseja enviar um e-mail direcionado aos nossos clientes jovens.

Para melhorar a solução anterior, precisamos atualizar os critérios de idade para identificação de clientes jovens.

Atualizamos o código anteriores com as novas categorias de jovens. No entanto, será necessário adicionar os filtros **de idade** ao filtro anterior de **renda** para que tudo funcione como o esperado.

As regras são as seguintes:

`standard_young`:

Renda anua inferior a $ 100.000 (excluindo esse número) **E** idade inferior a 40 anos (excluindo esse número).

`plus_young`:

Renda anual de $ 100.000 (incluindo esse número) a $ 200.000 (excluindo esse número) **E** idade inferior a 35 anos (excluindo esse número).

`elite_young`:

Renda anual de $ 200.000 (incluindo esse número) a $ 300.000 (excluindo esse número) **E** idade inferior a 35 anos (excluindo esse número).

`executive_young`:

Renda anual a partir de $ 300.000 (incluindo esse número) **E** idade inferior a 35 anos (excluindo esse número).

Forneça listas atualizadas com base nessas regras e reimprima apenas a lista de clientes executive\_young.

O comentário no pré-código abaixo vai guiar você nas alterações necessárias. Preste atenção nele. Se tiver dificuldades, confira a dica que preparamos.

O comentário no pré-código abaixo vai guiar você nas alterações necessárias. Preste atenção nele. Se você tiver dificuldades, não hesite em conferir a dica.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

clients \= \[

\[32456, "Jack Wilson", 32, 150000, "Healthcare"\],

\[34591, "Nina Brown", 45, 250000, "Telecom"\],

\[37512, "Alex Smith", 39, 210000, "IT"\],

\[39591, "Brian Perez", 29, 340000, "Transportation"\],

\[45123, "Sarah Lee", 28, 120000, "Marketing"\],

\[47635, "David Kim", 36, 180000, "Finance"\],

\[49571, "Samantha Chen", 42, 220000, "Retail"\],

\[50391, "Juan Rodriguez", 31, 160000, "Architecture"\],

\[34556, "Lucas Hernandez", 37, 75000, "Education"\],

\[64291, "Jessica Li", 25, 125000, "IT"\],

\[74512, "Emma Davis", 47, 197000, "Finance"\],

\[83191, "Sophia Perez", 34, 225000, "Transportation"\],

\[91023, "Liam Kim", 29, 98000, "Retail"\],

\[96435, "Ava Chen", 31, 175000, "Marketing"\],

\[100571, "Noah Rodriguez", 28, 85000, "Architecture"\],

\[101321, "Olivia Wilson", 44, 310000, "Telecom"\],

\[104556, "William Brown", 38, 289000, "Finance"\],

\[105491, "Emily Smith", 29, 193000, "Healthcare"\],

\[107512, "Michael Perez", 53, 415000, "Transportation"\]

\]

  

standard\_young \= \[\]

plus\_young \= \[\]

elite\_young \= \[\]

executive\_young \= \[\]

  

for client in clients:

if client\[3\] < 100000 and client\[2\] < 40: \# atualize a condição aqui

standard\_young.append(client)

elif client\[3\] \>= 100000 and client\[3\] < 200000 and client\[2\] < 35: \# atualize a condição aqui

plus\_young.append(client)

elif client\[3\] \>= 200000 and client\[3\] < 300000 and client\[2\] < 35: \# atualize a condição aqui

elite\_young.append(client)

elif client\[3\] \>= 300000 and client\[2\] < 35: \# substitua 'else' por outra condição elif aqui

executive\_young .append(client)

  

\# não modifique abaixo desta linha

print(executive\_young)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-27-855Z.md
### Última modificação: 2025-05-28 18:54:28

# Conclusão - TripleTen

Capítulo 6/9

Instruções Condicionais

# Conclusão

Bom trabalho! Você concluiu mais um capítulo!

Agora você sabe como:

-   Reconhecer e definir expressões lógicas Se ainda não tiver certeza, revise a lição sobre [Expressões lógicas e operadores de comparação](https://tripleten.com/trainer/data-analyst/lesson/58421052-3904-42dc-8312-447e40aae9ed/).
-   Usar as instruções condicionais `if` e `else` para criar ramificações no código Se ainda não tiver certeza, revise a lição sobre [Declarações `if` e `else`](https://tripleten.com/trainer/data-analyst/lesson/c6750243-1829-4966-ae0a-e356ce92a167/).
-   Usar `elif` no código para criar qualquer número de ramificações Se ainda não tiver certeza, revise a lição [Criação de várias ramificações com `else` e `elif`](https://tripleten.com/trainer/data-analyst/lesson/f5b2a608-5c79-42e8-8afb-df566792ff13/).
-   Modificar o conteúdo das tabelas Se ainda não tiver certeza, revise a lição [Modificações em tabelas](https://tripleten.com/trainer/data-analyst/lesson/de5f6dfc-355b-47ce-84ce-821fce12ebdc/).
-   Escrever filtros para extrair dados específicos de tabelas Se ainda não tiver certeza, revise a lição [Filtragem de tabelas](https://tripleten.com/trainer/data-analyst/lesson/84e12b32-3732-4d01-bed8-fcd9ebbf8620/).

## O que vem a seguir?

No próximo capítulo, vamos apresentar um ambiente onde profissionais de dados trabalham. Ele é chamado de Jupyter Notebook. Siga em frente para descobrir porque todos adoram esse ambiente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-29-184Z.md
### Última modificação: 2025-05-28 18:54:29

# Introdução ao Jupyter Notebook - TripleTen

Capítulo 7/9

Uma Olhadinha Rápida no Jupyter Notebook

# Introdução ao Jupyter Notebook

O Jupyter Notebook é uma ferramenta poderosa para profissionais de dados. Ele permite que desenvolvedores incluam texto, código e gráficos para descrever e explicar o fluxo de pesquisa.

A melhor maneira de aprender o Jupyter é visualmente. Preparamos um vídeo que demonstra as funcionalidades e recursos disponíveis no ambiente Jupyter Notebook. Ele fornece uma introdução amigável sobre o que é o Jupyter e como ele funciona.

<iframe class="base-markdown-iframe__iframe" id="player-0Xeigdry0bA" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Intro to Jupyter Notebooks" width="640" height="360" src="https://www.youtube.com/embed/0Xeigdry0bA?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F8eec51b5-e0c6-4178-bf39-0579e4e01b17%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-30-476Z.md
### Última modificação: 2025-05-28 18:54:30

# Seu primeiro caderno de teste - TripleTen

Seu primeiro caderno de teste

Tarefa

Experimente! Vamos fornecer acesso ao notebook que você acabou de visualizar para que você possa testar como ele funciona.

Teste alguns códigos básicos e execute-os usando os atalhos. Tente criar novas células e escrever marcações. Essas habilidades serão úteis no projeto, quando você estiver trabalhando em um ambiente de notebook Jupyter.

<iframe class="page-lesson-jupyter__frame" title="Jupyter" src="https://jupyterhub.tripleten-services.com/user/user-3-f213ea5c-94ca-4f29-9c46-4771fbdad1ea/notebooks/c0fc384f-a375-41b9-9081-b3d94217da90.ipynb?token=7d89c13ac6bb4414a369467e80a184ef"></iframe>

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-32-916Z.md
### Última modificação: 2025-05-28 18:54:33

# Seu primeiro projeto - TripleTen

Capítulo 8/9

Projeto Final

# Seu primeiro projeto

Você passou por um longo caminho desde o início da sua jornada no Python. Agora você está com tudo pronto para aplicar suas novas habilidades em um pequeno projeto.

Nele, você vai receber um conjunto de tarefas para resolver. Seus projetos serão feitos no Jupyter Notebook.

Nos sprints finais, o trabalho será mais independente. Dessa vez, para ajudar você a começar, vamos providenciar um notebook contendo alguns detalhes:

-   A estrutura do projeto
    -   Todas as tarefas terão a própria célula na qual você deve programar

### 💡

Ao finalizar o projeto, envie-o para revisão. Você vai receber comentários em até 48 horas. Faça mudanças conforme o indicado pelo revisor e entregue uma versão atualizada. Você pode receber novos comentários. É normal ter várias versões e fazer mudanças antes que o projeto seja aceito.

Quando o revisor aceitar o seu projeto, ele será considerado concluído.

## Diretrizes

Você vai encontrar uma descrição detalhada do projeto na próxima lição. Aqui vamos falar apenas dos pontos principais:

-   **Esse é o seu código**. Mas se você aprendeu uma nova maneira interessante de resolver a tarefa e deseja usá-la, adicione-a como comentário ao código e use o que aprendeu no programa até aqui. Isso vai ajudar você a solidificar todo o conhecimento que adquiriu até agora. Certifique-se de seguir as instruções e cumprir o prazo.
-   **O formato importa**. O estilo de programar é fundamental. Preste atenção na nomenclatura do seu código e mantenha a consistência com as práticas recomendadas. Lembre-se: como programador, você deve escrever um código que todos possam ler e entender com facilidade.
-   **Comentários ajudam na revisão do projeto**. Não remova os comentários que você vê nas células de código. Lembre-se de que todos os comentários começam com o sinal `#` e incluem algum comentário textual após o sinal. Em vez disso, apenas adicione os seus comentários. Isso vai ajudar o revisor a ler rapidamente e entender a sua solução e sugerir melhorias.
-   **Você já é um sucesso**. Você já concluiu muitas tarefas e demonstrou que está com tudo pronto para esse trabalho. É normal achar alguns detalhes difíceis de entender, já que você está apenas começando a explorar sua nova profissão. Se você tiver alguma dúvida, entre em contato com seu gerente de sucesso de carreira.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-34-243Z.md
### Última modificação: 2025-05-28 18:54:35

# Sprint 1 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 8/9

Projeto Final

# Descrição do projeto

Parabéns! Você está prestes a começar a trabalhar em seu primeiro projeto. Você deve estar sentindo bastante orgulho. Vamos fazer uma recapitulação rápida do que aprendemos até agora.

-   Agora você conhece diferentes tipos de dados, incluindo os básicos como `int`, `float` e `bool`, bem como tipos mais complexos como _strings_ e _listas_. Você também aprendeu como converter um tipo de dados em outro e realizar operações aritméticas com valores numéricos.
-   O tratamento de erros não é mais um problema, pois você consegue facilmente implementar o bloco `try-except` para evitar travamentos.
-   A formatação de strings ficou fácil com o método `format()` ou com f-strings. Além disso, é facil mudar qualquer string para maiúsculas ou minúsculas usando os métodos `upper()` ou `lower()`. Se houver espaços desnecessários, o método `strip()` pode removê-los. Para finalizar, agora você consegue substituir uma parte de uma string usando o método `replace()`.
-   As listas são uma ótima maneira de armazenar uma coleção de valores em uma única estrutura de dados. Os elementos de uma lista podem ser fatiados passando os índices desejados entre colchetes.
-   Ciclos podem poupar muito tempo. Agora você sabe como iterar sobre elementos em uma lista usando o ciclo `for`. O ciclo `while`, por outro lado, é útil quando queremos continuar a iteração até que uma determinada condição seja atendida.
-   Você consegue usar instruções condicionais para criar ramificações no código.

Tendo isso em mente, vamos avançar para o projeto, onde você pode praticar suas habilidades e conhecimentos recém-adquiridos.

## Instruções para concluir o projeto

Preparamos um modelo de notebook, em que você pode escrever seu código. Para concluir o projeto, preencha cada célula de código no modelo.

Antes de você começar, vamos dar uma olhada nas etapas do projeto:

**Etapa 1:** ler as instruções do projeto. O notebook contém células prontas com instruções sobre que tipo de código escrever, então é uma boa ideia ler todas as tarefas e obter uma visão geral do que você vai fazer no projeto.

**Etapa 2:** resolver as tarefas. Nesta etapa, você vai resolver cada tarefa escrevendo seu próprio código na célula apropriada. Não se esqueça de adicionar comentários a seu código para tornar o processo de revisão mais fácil e rápido.

**Etapa 3:** revisão. Percorra todas as etapas de programação mais uma vez e se certifique de que tudo foi resolvido, o código foi bem escrito e contém comentários adequados. Adicione tudo o que você achar útil antes de enviar o projeto para revisão.

No vídeo abaixo, abordamos alguns pontos essenciais nos quais você deve prestar atenção enquanto trabalha no seu projeto.

<iframe class="base-markdown-iframe__iframe" id="player-IUgDrFD_dS0" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Project Instructions" width="640" height="360" src="https://www.youtube.com/embed/IUgDrFD_dS0?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F5a3c8074-0943-4c32-8d67-0dbd0afc55c3%2Ftask%2F911c9770-6422-479e-8713-ad7e863cf3b8%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

### Checklist

Antes de enviar o projeto, certifique-se que:

-   você concluiu todas as etapas do projeto e preencheu todas as células.
-   a estrutura lógica do projeto segue a estrutura do modelo.

![](https://practicum-content.s3.amazonaws.com/resources/14.3PT_1690218323.png)

Quando a revisão do projeto for concluída, você vai receber um e-mail no seu endereço de e-mail cadastrado na plataforma. Você vai precisar abrir o projeto e fazer correções, se necessário.

![](https://practicum-content.s3.amazonaws.com/resources/Bildschirmfoto_2024-01-16_um_16.25.26_pt_1706170870.png)

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-4090beb0-a116-43c3-b575-e0f879091890.containerhub.tripleten-services.com/doc/tree/5a3c8074-0943-4c32-8d67-0dbd0afc55c3.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-37-237Z.md
### Última modificação: 2025-05-28 18:54:37

# Sprint 1 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 8/9

Projeto Final

# Descrição do projeto

Parabéns! Você está prestes a começar a trabalhar em seu primeiro projeto. Você deve estar sentindo bastante orgulho. Vamos fazer uma recapitulação rápida do que aprendemos até agora.

-   Agora você conhece diferentes tipos de dados, incluindo os básicos como `int`, `float` e `bool`, bem como tipos mais complexos como _strings_ e _listas_. Você também aprendeu como converter um tipo de dados em outro e realizar operações aritméticas com valores numéricos.
-   O tratamento de erros não é mais um problema, pois você consegue facilmente implementar o bloco `try-except` para evitar travamentos.
-   A formatação de strings ficou fácil com o método `format()` ou com f-strings. Além disso, é facil mudar qualquer string para maiúsculas ou minúsculas usando os métodos `upper()` ou `lower()`. Se houver espaços desnecessários, o método `strip()` pode removê-los. Para finalizar, agora você consegue substituir uma parte de uma string usando o método `replace()`.
-   As listas são uma ótima maneira de armazenar uma coleção de valores em uma única estrutura de dados. Os elementos de uma lista podem ser fatiados passando os índices desejados entre colchetes.
-   Ciclos podem poupar muito tempo. Agora você sabe como iterar sobre elementos em uma lista usando o ciclo `for`. O ciclo `while`, por outro lado, é útil quando queremos continuar a iteração até que uma determinada condição seja atendida.
-   Você consegue usar instruções condicionais para criar ramificações no código.

Tendo isso em mente, vamos avançar para o projeto, onde você pode praticar suas habilidades e conhecimentos recém-adquiridos.

## Instruções para concluir o projeto

Preparamos um modelo de notebook, em que você pode escrever seu código. Para concluir o projeto, preencha cada célula de código no modelo.

Antes de você começar, vamos dar uma olhada nas etapas do projeto:

**Etapa 1:** ler as instruções do projeto. O notebook contém células prontas com instruções sobre que tipo de código escrever, então é uma boa ideia ler todas as tarefas e obter uma visão geral do que você vai fazer no projeto.

**Etapa 2:** resolver as tarefas. Nesta etapa, você vai resolver cada tarefa escrevendo seu próprio código na célula apropriada. Não se esqueça de adicionar comentários a seu código para tornar o processo de revisão mais fácil e rápido.

**Etapa 3:** revisão. Percorra todas as etapas de programação mais uma vez e se certifique de que tudo foi resolvido, o código foi bem escrito e contém comentários adequados. Adicione tudo o que você achar útil antes de enviar o projeto para revisão.

No vídeo abaixo, abordamos alguns pontos essenciais nos quais você deve prestar atenção enquanto trabalha no seu projeto.

<iframe class="base-markdown-iframe__iframe" id="player-IUgDrFD_dS0" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Project Instructions" width="640" height="360" src="https://www.youtube.com/embed/IUgDrFD_dS0?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F5a3c8074-0943-4c32-8d67-0dbd0afc55c3%2Ftask%2F911c9770-6422-479e-8713-ad7e863cf3b8%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

### Checklist

Antes de enviar o projeto, certifique-se que:

-   você concluiu todas as etapas do projeto e preencheu todas as células.
-   a estrutura lógica do projeto segue a estrutura do modelo.

![](https://practicum-content.s3.amazonaws.com/resources/14.3PT_1690218323.png)

Quando a revisão do projeto for concluída, você vai receber um e-mail no seu endereço de e-mail cadastrado na plataforma. Você vai precisar abrir o projeto e fazer correções, se necessário.

![](https://practicum-content.s3.amazonaws.com/resources/Bildschirmfoto_2024-01-16_um_16.25.26_pt_1706170870.png)

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-4090beb0-a116-43c3-b575-e0f879091890.containerhub.tripleten-services.com/doc/tree/5a3c8074-0943-4c32-8d67-0dbd0afc55c3.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-40-541Z.md
### Última modificação: 2025-05-28 18:54:40

# Sprint 1 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 8/9

Projeto Final

# Descrição do projeto

Parabéns! Você está prestes a começar a trabalhar em seu primeiro projeto. Você deve estar sentindo bastante orgulho. Vamos fazer uma recapitulação rápida do que aprendemos até agora.

-   Agora você conhece diferentes tipos de dados, incluindo os básicos como `int`, `float` e `bool`, bem como tipos mais complexos como _strings_ e _listas_. Você também aprendeu como converter um tipo de dados em outro e realizar operações aritméticas com valores numéricos.
-   O tratamento de erros não é mais um problema, pois você consegue facilmente implementar o bloco `try-except` para evitar travamentos.
-   A formatação de strings ficou fácil com o método `format()` ou com f-strings. Além disso, é facil mudar qualquer string para maiúsculas ou minúsculas usando os métodos `upper()` ou `lower()`. Se houver espaços desnecessários, o método `strip()` pode removê-los. Para finalizar, agora você consegue substituir uma parte de uma string usando o método `replace()`.
-   As listas são uma ótima maneira de armazenar uma coleção de valores em uma única estrutura de dados. Os elementos de uma lista podem ser fatiados passando os índices desejados entre colchetes.
-   Ciclos podem poupar muito tempo. Agora você sabe como iterar sobre elementos em uma lista usando o ciclo `for`. O ciclo `while`, por outro lado, é útil quando queremos continuar a iteração até que uma determinada condição seja atendida.
-   Você consegue usar instruções condicionais para criar ramificações no código.

Tendo isso em mente, vamos avançar para o projeto, onde você pode praticar suas habilidades e conhecimentos recém-adquiridos.

## Instruções para concluir o projeto

Preparamos um modelo de notebook, em que você pode escrever seu código. Para concluir o projeto, preencha cada célula de código no modelo.

Antes de você começar, vamos dar uma olhada nas etapas do projeto:

**Etapa 1:** ler as instruções do projeto. O notebook contém células prontas com instruções sobre que tipo de código escrever, então é uma boa ideia ler todas as tarefas e obter uma visão geral do que você vai fazer no projeto.

**Etapa 2:** resolver as tarefas. Nesta etapa, você vai resolver cada tarefa escrevendo seu próprio código na célula apropriada. Não se esqueça de adicionar comentários a seu código para tornar o processo de revisão mais fácil e rápido.

**Etapa 3:** revisão. Percorra todas as etapas de programação mais uma vez e se certifique de que tudo foi resolvido, o código foi bem escrito e contém comentários adequados. Adicione tudo o que você achar útil antes de enviar o projeto para revisão.

No vídeo abaixo, abordamos alguns pontos essenciais nos quais você deve prestar atenção enquanto trabalha no seu projeto.

<iframe class="base-markdown-iframe__iframe" id="player-IUgDrFD_dS0" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Project Instructions" width="640" height="360" src="https://www.youtube.com/embed/IUgDrFD_dS0?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F5a3c8074-0943-4c32-8d67-0dbd0afc55c3%2Ftask%2F911c9770-6422-479e-8713-ad7e863cf3b8%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

### Checklist

Antes de enviar o projeto, certifique-se que:

-   você concluiu todas as etapas do projeto e preencheu todas as células.
-   a estrutura lógica do projeto segue a estrutura do modelo.

![](https://practicum-content.s3.amazonaws.com/resources/14.3PT_1690218323.png)

Quando a revisão do projeto for concluída, você vai receber um e-mail no seu endereço de e-mail cadastrado na plataforma. Você vai precisar abrir o projeto e fazer correções, se necessário.

![](https://practicum-content.s3.amazonaws.com/resources/Bildschirmfoto_2024-01-16_um_16.25.26_pt_1706170870.png)

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-4090beb0-a116-43c3-b575-e0f879091890.containerhub.tripleten-services.com/doc/tree/5a3c8074-0943-4c32-8d67-0dbd0afc55c3.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-54-57-868Z.md
### Última modificação: 2025-05-28 18:54:58

# Sprint 1 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 8/9

Projeto Final

# Descrição do projeto

Parabéns! Você está prestes a começar a trabalhar em seu primeiro projeto. Você deve estar sentindo bastante orgulho. Vamos fazer uma recapitulação rápida do que aprendemos até agora.

-   Agora você conhece diferentes tipos de dados, incluindo os básicos como `int`, `float` e `bool`, bem como tipos mais complexos como _strings_ e _listas_. Você também aprendeu como converter um tipo de dados em outro e realizar operações aritméticas com valores numéricos.
-   O tratamento de erros não é mais um problema, pois você consegue facilmente implementar o bloco `try-except` para evitar travamentos.
-   A formatação de strings ficou fácil com o método `format()` ou com f-strings. Além disso, é facil mudar qualquer string para maiúsculas ou minúsculas usando os métodos `upper()` ou `lower()`. Se houver espaços desnecessários, o método `strip()` pode removê-los. Para finalizar, agora você consegue substituir uma parte de uma string usando o método `replace()`.
-   As listas são uma ótima maneira de armazenar uma coleção de valores em uma única estrutura de dados. Os elementos de uma lista podem ser fatiados passando os índices desejados entre colchetes.
-   Ciclos podem poupar muito tempo. Agora você sabe como iterar sobre elementos em uma lista usando o ciclo `for`. O ciclo `while`, por outro lado, é útil quando queremos continuar a iteração até que uma determinada condição seja atendida.
-   Você consegue usar instruções condicionais para criar ramificações no código.

Tendo isso em mente, vamos avançar para o projeto, onde você pode praticar suas habilidades e conhecimentos recém-adquiridos.

## Instruções para concluir o projeto

Preparamos um modelo de notebook, em que você pode escrever seu código. Para concluir o projeto, preencha cada célula de código no modelo.

Antes de você começar, vamos dar uma olhada nas etapas do projeto:

**Etapa 1:** ler as instruções do projeto. O notebook contém células prontas com instruções sobre que tipo de código escrever, então é uma boa ideia ler todas as tarefas e obter uma visão geral do que você vai fazer no projeto.

**Etapa 2:** resolver as tarefas. Nesta etapa, você vai resolver cada tarefa escrevendo seu próprio código na célula apropriada. Não se esqueça de adicionar comentários a seu código para tornar o processo de revisão mais fácil e rápido.

**Etapa 3:** revisão. Percorra todas as etapas de programação mais uma vez e se certifique de que tudo foi resolvido, o código foi bem escrito e contém comentários adequados. Adicione tudo o que você achar útil antes de enviar o projeto para revisão.

No vídeo abaixo, abordamos alguns pontos essenciais nos quais você deve prestar atenção enquanto trabalha no seu projeto.

<iframe class="base-markdown-iframe__iframe" id="player-IUgDrFD_dS0" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Project Instructions" width="640" height="360" src="https://www.youtube.com/embed/IUgDrFD_dS0?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F5a3c8074-0943-4c32-8d67-0dbd0afc55c3%2Ftask%2F911c9770-6422-479e-8713-ad7e863cf3b8%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

### Checklist

Antes de enviar o projeto, certifique-se que:

-   você concluiu todas as etapas do projeto e preencheu todas as células.
-   a estrutura lógica do projeto segue a estrutura do modelo.

![](https://practicum-content.s3.amazonaws.com/resources/14.3PT_1690218323.png)

Quando a revisão do projeto for concluída, você vai receber um e-mail no seu endereço de e-mail cadastrado na plataforma. Você vai precisar abrir o projeto e fazer correções, se necessário.

![](https://practicum-content.s3.amazonaws.com/resources/Bildschirmfoto_2024-01-16_um_16.25.26_pt_1706170870.png)

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-4090beb0-a116-43c3-b575-e0f879091890.containerhub.tripleten-services.com/doc/tree/5a3c8074-0943-4c32-8d67-0dbd0afc55c3.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-55-06-526Z.md
### Última modificação: 2025-05-28 18:55:06

# Feedback do Sprint 1 - TripleTen

Capítulo 9/9

Conclusão

# Compartilhe sua opinião

**Olá!**

Você acabou de concluir um sprint e enviar seu projeto. Parabéns por alcançar este marco! 🚀

Agora é o momento perfeito para dar seu feedback sobre o sprint. Sua opinião vai nos ajudar a aprimorar ainda mais o processo de aprendizagem.

Este questionário curto vai levar apenas 2 minutos. Todas as respostas vão ser confidenciais: não vamos compartilhá-las de forma que identifique você.

Agradecemos por nos ajudar a evoluir com você!

Como você avalia a experiência com os materiais didáticos, as tarefas e os projetos neste sprint?

1

2

3

4

5

Respostas: 1 – Muito insatisfatória; 5 – Muito satisfatória.

Avançar

1 — 7

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-55-07-851Z.md
### Última modificação: 2025-05-28 18:55:08

# Conclusão - TripleTen

Capítulo 9/9

Conclusão

# Conclusão

Parabéns! Você chegou ao final do primeiro sprint do programa e aprendeu muita coisa!

![](https://practicum-content.s3.amazonaws.com/resources/Artboard_1-100_1698046305.jpg)

O conhecimento prático de Python é essencial para alguém que planeja construir uma carreira na área de dados. É claro que ainda há muita coisa a ser aprendida, mas acabamos de construir uma base sólida para a futura aprendizagem.

Vamos recapitular brevemente o que aprendemos até agora. Você deve se sentir confortável trabalhando com:

-   Diferentes **tipos de dados**, como `str`, `float` e `int`. Você também aprendeu sobre as **estruturas de dados** de Python, como `list`.
-   Ciclos for e condições com `if` e `else`

No próximo curso, você vai aprender mais sobre outras estruturas de dados, como `dict`. Você também vai começar a escrever funções que vão aumentar significativamente sua eficiência. Por fim, vamos aprender o que é uma biblioteca e obter experiência em primeira mão trabalhando com a biblioteca chamada `pandas`.

Até a próxima!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-15-833Z.md
### Última modificação: 2025-05-28 18:59:16

# Introdução - TripleTen

Capítulo 1/7

Introdução

# Introdução

![](https://practicum-content.s3.amazonaws.com/resources/Artboard_1_copy_3-100_2_1697798521.jpg)

Olá, este é o segundo sprint do programa! Nas próximas duas semanas, você vai continuar ganhando proficiência em Python e aprendendo alguns materiais novos fundamentais para seu desenvolvimento como especialista de dados.

### Roteiro do sprint

Assim como o primeiro, este sprint também vai servir de base para muitas das tarefas que você vai concluir mais adiante no programa. A partir desta semana, você vai aprender alguns novos conceitos que incluem:

-   Trabalho com dicionários, que são uma estrutura de dados de Python nova para você usada para armazenar informações.
-   Funções que, uma vez criadas, podem ser usadas repetidamente para realizar operações similares várias vezes.
-   Você vai obter experiência de trabalho na biblioteca Pandas, usada amplamente por profissionais da área de dados em todo o mundo para processar e manipular dados.

Neste sprint, você irá continuar desenvolvendo estas habilidades:

![](https://practicum-content.s3.amazonaws.com/resources/Python_1713354596.png)

![](https://practicum-content.s3.amazonaws.com/resources/Pre-processamento_de_Dados_1713354606.png)

![](https://practicum-content.s3.amazonaws.com/resources/Analise_Estatistica_1713354691.png)

Pensando na sua conveniência, dividimos o sprint em 2 semanas. Para que o seu processo de aprendizado seja tranquilo, recomendamos seguir a seguinte programação:

![](https://practicum-content.s3.amazonaws.com/resources/1.1_pt_1697798531.png)

### Quanto tempo isso vai levar?

Deve levar cerca de 20 horas para concluir o sprint.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-17-142Z.md
### Última modificação: 2025-05-28 18:59:17

# Introdução aos dicionários - TripleTen

Capítulo 2/7

Dicionários

# Introdução aos dicionários

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_1698149929.png)

Neste capítulo, vamos apresentar um novo tipo de dados, o dicionário.

Você já aprendeu sobre vários tipos de dados:

-   strings (por exemplo, `'Python é ótimo'`)
-   números (`1` é um exemplo do tipo número inteiro e `2.4` é um exemplo de um número de ponto flutuante)
-   valores booleanos (`True` e `False`)
-   listas (por exemplo, `[1, 'Brian', True, 140]`)
-   e estruturas aninhadas criadas a partir de listas (por exemplo, `[[1, 'Brian', True, 140], [2, 'Alex', False, 380]]`)

Ter familiaridade com diferentes tipos de dados é uma base importante. No entanto, agora você já é capaz de pensar mais alto, comparando diferentes estruturas de dados e observando seus prós e contras. Então você pode combinar os melhores recursos de estruturas diferentes para equilibrar suas fraquezas.

Você verá que o mesmo problema pode ser resolvido de várias maneiras. Muitas vezes, para selecionar o melhor método, é necessário começar escolhendo a estrutura de dados correta.

Ao final deste capítulo, você será capaz de:

-   dizer o que são dicionários,
-   comparar dicionários com listas,
-   obter valores de dicionários,
-   adicionar elementos a um dicionário e excluí-los dele,
-   trabalhar com os resultados.

Concluir este capítulo deve levar cerca de 1-2,5 horas. Então vamos começar!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-18-431Z.md
### Última modificação: 2025-05-28 18:59:18

# Propriedades de dicionários - TripleTen

Capítulo 2/7

Dicionários

# Propriedades de dicionários

A última estrutura de dados que você ainda não aprendeu é o **dicionário**.

### 💡

Faça uma pesquisa e obtenha um resultado. Faça login em um aplicativo e veja suas mensagens. Busque uma palavra e receba seu significado.

O que essas frases têm em comum? Todas elas se resumem a duas coisas: "fornecer informações de identificação (chave), receber dados correspondentes (valor)". Tais informações são fornecidas de forma eficaz e eficiente pelos dicionários Python.

No final desta lição, você será capaz de explicar o que é um dicionário, como usar sua sintaxe e de comparar e contrastar dicionários e listas.

Em geral, o entendimento dos dicionários e do que os difere de listas é uma poderosa arma que você poderá usar em suas tarefas de programação.

<iframe class="base-markdown-iframe__iframe" id="player-1lkeNerjBr4" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Properties of Dictionaries" width="640" height="360" src="https://www.youtube.com/embed/1lkeNerjBr4?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fb5cdba42-1df7-4d9c-8004-b953d722b8e8%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

## Dicionários

Dicionários são uma ferramenta muito útil na programação, já que eles te permitem armazenar e organizar dados de forma que você os possa acessar facilmente quando quiser.

Um dicionário é uma coleção de elementos, similar a uma lista. No entanto, em um dicionário, cada elemento é um par chave-valor, em que cada chave serve como um identificador único para um valor específico. Essa é a diferença principal entre dicionários e listas. Em um dicionário, valores estão diretamente associados a suas chaves respectivas, e não a índices.

![](https://practicum-content.s3.amazonaws.com/resources/8.2_1PT_1690265245.png)

Listas são úteis para armazenar itens em uma ordem particular, enquanto dicionários são convenientes para descrever propriedades individuais de objetos.

## Sintaxe dos dicionários

Para usar um dicionário em Python, você precisa entender sua sintaxe. Ao contrário de listas, cujos elementos são colocados entre colchetes, os elementos de um dicionário são indicados entre chaves. Dentro delas, você pode definir seus pares chave-valor separando cada chave de seu valor respectivo por dois pontos.

Cada entrada de um dicionário Python consiste em duas partes: uma chave e um valor.

-   **Chave** — uma string ou número que identifica o valor
-   **Valor** — os dados associados à chave

Vamos criar um dicionário contendo informações de voos. A chave será o número do voo, e o valor será a data e o horário de partida:

```
schedule = {                    
    'SU2222': '12.06.18 12:30', # a chave fica à esquerda dos dois pontos,
    'SU1111': '12.06.18 14:05',    # o valor fica à direita
    'SU0777': '12.06.18 17:00' 
}
```

As três regras básicas da sintaxe dos dicionários são:

-   Elementos de dicionários são colocados entre chaves – `{}`.
-   Os itens em um dicionário (pares chave-valor) são separados por vírgulas.
-   Uma chave e um valor em um par são separados por dois pontos.

Vamos imprimir o dicionário que criamos. Execute o código abaixo e veja o resultado.

CódigoPYTHON

9

1

2

3

4

5

6

schedule \= {

'SU2222': '12.06.18 12:30', \# a chave fica à esquerda dos dois pontos,

'SU1111': '12.06.18 14:05', \# o valor fica à direita

'SU0777': '12.06.18 17:00'

}

print(schedule)

Mostrar a soluçãoExecutar

Se olharmos mais de perto para o resultado, veremos o seguinte:

![](https://practicum-content.s3.amazonaws.com/resources/8.2_2PT_1690265289.png)

Pergunta

### Questionário

Agora vamos praticar um pouco!

Selecione a opção que cria corretamente um dicionário que armazena a informação sobre os clientes e suas idades atuais.

Opção 1:

```
{'Anthony': 30, 'Kate': 31, 'Sam': 28} 
```

Isso está certo! Vírgulas são usadas para separar itens, uma chave e um valor em um par são separados por dois pontos e os elementos de um dicionário são escritos entre chaves.

Opção 2:

```
['Anthony': 30, 'Kate': 31, 'Sam': 28]
```

Opção 3:

```
{'Anthony': 30; 'Kate': 31; 'Sam': 28} 
```

Trabalho maravilhoso!

Teoria

### Dicionários e listas

Como você já sabe, todos os valores de um dicionário estão conectados a suas chaves, enquanto listas usam índices. Há mais algumas diferenças entre dicionários e listas:

-   A obtenção de dados em um dicionário é mais rápida, mas um dicionário ocupa mais espaço na memória.
-   Assim como listas, dicionários podem armazenar dados de qualquer tipo, mas as chaves só podem ser dados de tipos imutáveis, tais como strings e números (inteiros e de ponto flutuante).

No entanto, as nossas estruturas têm algo em comum! Podemos passar dicionários (assim como listas) para a função `len()` para obter o número de seus itens.

```
# chave - nome do jogador : valor - número de gols marcados em 2019
players_result = {
    'Cristiano Ronaldo': 39,
    'Robert Lewandowski': 54,
    'Lionel Messi': 50,
    'Harry Kane': 36,
    'Karim Benzema': 35
}
print(len(players_result))
```

```
5
```

### Tarefa

Chegou a hora de praticarmos mais um pouco.

Crie um dicionário chamado `movies` que inclui os seguintes quatro filmes. O nome do filme será a chave, e o ano de lançamento será o valor:

-   _Her_, 2013
-   _Big Eyes_, 2014
-   _Taxi Driver_, 1976
-   _The King of Comedy_, 1982

Após criar o dicionário, imprima-o.

CódigoPYTHON

9

1

2

3

4

5

6

movies \= {"Her":2013,

"Big Eyes":2014,

"Taxi Driver":1976,

"The King of Comedy":1982}\# crie um dicionário

  

print(movies)

Dica

Mostrar a soluçãoValidar

Na próxima lição, você vai explorar vários métodos usados para obter elementos de dicionários e adicionar novos elementos a eles.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-20-715Z.md
### Última modificação: 2025-05-28 18:59:21

# Como extrair valores de dicionários - TripleTen

Capítulo 2/7

Dicionários

# Como extrair valores de dicionários

Agora que você criou seu primeiro dicionário, vamos explorar como podemos obter valores dele e adicionar novos valores. Antes de continuarmos, vamos recapitular as três regras básicas da sintaxe dos dicionários:

-   Elementos de dicionários são colocados entre chaves – `{}`.
-   Os itens em um dicionário (pares chave-valor) são separados por vírgulas.
-   Uma chave e um valor em um par são separados por dois pontos.

Com isso em mente, vamos avançar e aprender como recuperar valores de um dicionário.

<iframe class="base-markdown-iframe__iframe" id="player-dvguOlEflJA" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Extracting Values from Dictionaries" width="640" height="360" src="https://www.youtube.com/embed/dvguOlEflJA?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fba3bd0e0-189a-47c9-a11b-b407a3dba86d%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

## Como obter valores de dicionários

Há duas maneiras de acessar um valor: usando a chave diretamente e usando o método `get()`.

Nós usaremos um dicionário, em que as chaves são nomes de empresas e os valores são os respectivos preços de suas ações.

```
financial_info = {
    'American Express': 93.23,
    'Boeing': 178.44,
    'Coca-Cola': 45.15,
    'Nike': 97.99,
    'JPMorgan':96.27,
    'Walmart': 130.68 
}
```

## Uso da chave

Você pode especificar uma chave do dicionário entre colchetes para extrair um valor para ela. Por exemplo, se quisermos recuperar um preço para Coca-Cola, faremos o seguinte:

```
# acessando o valor da chave 'Coca-Cola' do dicionário
coca_cola_price = financial_info['Coca-Cola']
print(coca_cola_price)
```

```
45.15
```

Esse procedimento é similar a como obtemos um item de uma lista utilizando seu índice, a única diferença é que você deve colocar a chave, e não o índice, entre colchetes.

A maior vantagem é que isso nos permite usar chaves mais descritivas e significativas, o que torna seu código mais legível e fácil de entender a longo prazo.

### Tarefa 1

Agora vamos praticar extraindo valores.

Obtenha o preço das ações de `'Walmart'` acessando o valor correspondente do dicionário pela sua chave e então o atribua à variável `walmart_price`. Imprima a variável `walmart_price` depois de extrair o preço.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

financial\_info \= {

'American Express': 93.23,

'Boeing': 178.44,

'Coca-Cola': 45.15,

'Walt Disney': 119.34,

'Nike': 97.99,

'JPMorgan':96.27,

'Walmart': 130.68

}

  

walmart\_price \= financial\_info.get("Walmart")\# Escreva seu código aqui

print(walmart\_price)

Dica

Mostrar a soluçãoValidar

Teoria

Extrair valores usando o nome de uma chave é um método simples, mas há um problema com isso. Se nos referirmos a uma chave que não esteja presente no dicionário, o código vai exibir um erro. Execute o código abaixo que tenta recuperar um preço para a empresa `'Pepsi'`, que não está no dicionário `financial_info`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

financial\_info \= {

'American Express': 93.23,

'Boeing': 178.44,

'Coca-Cola': 45.15,

'Nike': 97.99,

'JPMorgan':96.27,

'Walmart': 130.68

  

}

pepsi\_price \= financial\_info\['Pepsi'\]

print(pepsi\_price)

Mostrar a soluçãoExecutar

O Python vai notificar que a chave especificada não está no dicionário mostrando a exceção `KeyError`.

Na vida real, você pode precisar recuperar um valor de um dicionário sem saber se a chave necessária está presente nele. Por este motivo, Python nos fornece um método especial chamado **`get()`**.

## Como usar `get()`

O método `get()` recebe a chave como argumento. Se a chave está presente no dicionário, `get()` retorna o valor correspondente. Caso contrário, ele retorna o valor padrão, que é `None`.

```
# encontrar o valor da chave 'Pepsi':
pepsi_price = financial_info.get('Pepsi')
# encontrar o valor da chave 'Nike':
nike_price = financial_info.get('Nike')

print(pepsi_price)
print(nike_price)
```

```
None
97.99
```

O valor `None` é especial porque seu tipo é `NoneType`. Ele não é uma string, então `None` é escrito sem aspas, assim como os valores `True` e `False`. Normalmente, `None` indica a ausência de um valor.

`None` será útil mais tarde, quando precisarmos ver se uma chave está no dicionário. Podemos verificar se o método `get()` retorna `None` usando a condição `if`, da seguinte maneira:

```
if financial_info.get('Pepsi') == None:
    # fazemos algo
```

Vamos falar sobre isso nas próximas lições, mas, por enquanto, tenha em mente que `None` tem seu próprio tipo (`NoneType`), e o método `get()` retorna-o por padrão quando a chave não pode ser encontrada.

Se você não gosta de `None`, pode definir qualquer outro valor. O método `get()` também permite definir outro valor padrão. Basta passá-lo como o segundo argumento do método, e se a chave não for encontrada, o valor especificado será retornado. Veja o exemplo abaixo, em que passamos `-1` como um valor para ambos os métodos `get()`.

```
# Agora o método retorna -1 se a chave não estiver inserida no dicionário:
pepsi_price = financial_info.get('Pepsi', -1)
nike_price = financial_info.get('Nike', -1)
print(pepsi_price)
print(nike_price)
```

```
-1
97.99
```

`-1` é retornado para `'Pepsi'`, e `'Nike'` continua a retornar `97.99`, já que `'Nike'` está presente no dicionário.

### Tarefa 2

Por que a bolsa caiu? Porque ninguém conseguiu encontrar ações no dicionário! Agora é sério: vamos praticar!

Acesse o preço das ações da `'Nike'` do dicionário usando o método `get()`. Ao chamar `get()`, faça com que `None` seja o valor retornado se `'Nike'` não for encontrado entre as chaves.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

financial\_info \= {

'American Express': 93.23,

'Boeing': 178.44,

'Coca-Cola': 45.15,

'Walt Disney': 119.34,

'Nike': 97.99,

'JPMorgan':96.27,

'Walmart': 130.68

}

  

nike\_price \= financial\_info.get("Nike")\# escreva o código aqui

print(nike\_price)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-22-042Z.md
### Última modificação: 2025-05-28 18:59:22

# Adição e remoção de itens - TripleTen

Capítulo 2/7

Dicionários

# Adição e remoção de itens

Vimos anteriormente que podemos extrair os valores de um dicionário especificando o nome de uma chave (por exemplo, `financial_info['Coca-Cola']`) ou usando o método `get()` (por exemplo, `financial_info.get('Coca-Cola')`)

Agora vamos aprender como adicionar itens a dicionários e remover seus itens.

<iframe class="base-markdown-iframe__iframe" id="player-JeFC_IIi6rU" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Adding &amp; Removing Items" width="640" height="360" src="https://www.youtube.com/embed/JeFC_IIi6rU?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F56aa251c-39be-49a4-8638-af4f347c84f6%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

## Adição de itens

Lembre-se de que podemos adicionar itens ao final de uma lista usando o método `append()`. O método `insert()` pode ser usado para escolher uma posição para um novo item na lista.

Adicionar um novo elemento a um dicionário é ainda mais fácil. Escreva o nome do dicionário seguido pela chave entre colchetes e então o defina com o valor que você escolher. Por exemplo, se quisermos adicionar o preço das ações da Walt Disney ao nosso dicionário `financial_info`, faremos o seguinte:

```
financial_info['Walt Disney'] = 119.34
```

E assim, adicionamos um novo par contendo as ações da Disney ao final do dicionário `financial_info`. Vamos verificar o resultado imprimindo o dicionário `financial_info`:

```
print(financial_info)
```

```
{'American Express': 93.23, 'Boeing': 178.44, 'Coca-Cola': 45.15, 'Nike': 97.99, 'JPMorgan': 96.27, 'Microsoft': 213.67, 'Walmart': 130.68, 'Walt Disney': 119.34}
```

Às vezes, antes de adicionar um item a um dicionário, queremos verificar se a chave que queremos usar já existe no dicionário. Se a chave não existe, nós a adicionamos. Entretanto, se a chave já existe, podemos querer atualizar seu valor. Vamos fazer isso para `'Walt Disney'`:

```
if financial_info.get('Walt Disney') == None: # verificando se a chave Walt Disney existe
    financial_info['Walt Disney'] = 119.34 # se não, adicionamos a chave
else: # se ela existe
    financial_info['Walt Disney'] += 3.2 # atualizamos o valor 
```

O código acima verifica se `'Walt Disney'` está na variável `financial_info`. Para isso, ele compara o resultado do método `get()` com `None`. Se o resultado é igual a `None`, isso significa que `'Walt Disney'` não está em `financial_info`. Neste caso, adicionamos `'Walt Disney'` a `financial_info` e definimos um preço para ele. Se `'Walt Disney'` já está em `financial_info`, atualizamos o preço atual aumentando-o em 3.2.

### Tarefa

Adicione um novo item ao dicionário `financial_info`. Use `'Microsoft'` como chave e `208.35` como valor. Imprima `financial_info` quando terminar.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

financial\_info \= {

'American Express': 93.23,

'Boeing': 178.44,

'Coca-Cola': 45.15,

'Walt Disney': 119.34,

'Nike': 97.99,

'JPMorgan':96.27,

'Walmart': 130.68

}

  

financial\_info\["Microsoft"\] \= 208.35 \# adicione um novo par aqui

  

print(financial\_info)

Dica

Mostrar a soluçãoValidar

Teoria

### Removendo itens

Você pode usar o comando `del` para remover um item de um dicionário. Para fazer isso, especifique a chave do item que quer remover após a palavra-chave `del`. Aqui está um exemplo:

```
del financial_info['Nike']
print(financial_info)
```

```
{'American Express': 93.23, 'Boeing': 178.44, 'Coca-Cola': 45.15, 'Walt Disney': 119.34, 'JPMorgan': 96.27, 'Microsoft': 208.35, 'Walmart': 130.68}
```

Executar o código acima removerá o item com a chave `'Nike'` do dicionário `financial_info`.

Da mesma forma que em listas, você também pode usar o método `pop()` para remover um item de um dicionário acessando seu valor ao mesmo tempo. O método `pop()` aceita como argumento a chave do item que você quer remover.

```
walmart_price = financial_info.pop('Walmart')
print(walmart_price)
print(financial_info)
```

```
130.68
{'American Express': 93.23, 'Boeing': 178.44, 'Coca-Cola': 45.15, 'Walt Disney': 119.34, 'JPMorgan': 96.27, 'Microsoft': 208.35}
```

O método `pop()` retorna o valor do item removido, que neste caso é o preço das ações da Walmart.

E assim terminamos a nossa exploração dos conceitos básicos de dicionários em Python! Agora, com isso em mente, vamos praticar um pouco.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-23-361Z.md
### Última modificação: 2025-05-28 18:59:23

# Atividade prática do capítulo - TripleTen

Capítulo 2/7

Dicionários

# Atividade prática do capítulo

Esperamos que você esteja com tudo pronto para testar suas habilidades de Python! Neste capítulo, vamos revisar tudo o que você aprendeu sobre dicionários. Vamos recapitular brevemente o que aprendemos até agora:

-   Para obter um valor de uma lista, precisamos colocar a chave certa entre colchetes ou passá-la para o método `get()`:
    
    ```
      financial_info = {
          'American Express': 93.23,
          'Boeing': 178.44,
          'Coca-Cola': 45.15,
          'Nike': 97.99,
          'JPMorgan':96.27,
          'Walmart': 130.68 
      }
      
      walmart_price_1 = financial_info['Walmart']
      walmart_price_2 = financial_info.get('Walmart')
      
      print(walmart_price_1)
      print(walmart_price_2)
      
    ```
    
    ```
      130.68
      130.68
      
    ```
    
-   Para adicionar um novo par chave-valor a uma lista, basta fazer o seguinte:
    
    ```
      
      financial_info = {
          'American Express': 93.23,
          'Boeing': 178.44,
          'Coca-Cola': 45.15,
          'Nike': 97.99,
          'JPMorgan':96.27,
          'Walmart': 130.68 
      }
      
      financial_info['Walt Disney'] = 119.34
      print(financial_info)
      
    ```
    
    ```
      {'American Express': 93.23, 'Boeing': 178.44, 'Coca-Cola': 45.15, 'Nike': 97.99, 'JPMorgan': 96.27, 'Walmart': 130.68, 'Walt Disney': 119.34}
      
    ```
    
-   É uma boa prática verificar se uma chave já está em um dicionário. Se sim, geralmente é do nosso interesse modificar seu valor. Se não, adicionamos uma nova chave e o respectivo valor. Aqui está um exemplo:
    
    ```
      # verificando se a chave Walt Disney existe
      if financial_info.get('Walt Disney') == None: 
          financial_info['Walt Disney'] = 119.34 # se não, adicionamos a chave
      else: # se ela existe
          financial_info['Walt Disney'] += 3.2 # atualizamos o valor 
      
    ```
    
-   Por último, mas não menos importante, usamos `del` ou `pop()` para remover elementos de um dicionário:
    
    ```
       
      financial_info = {
          'American Express': 93.23,
          'Boeing': 178.44,
          'Coca-Cola': 45.15,
          'Nike': 97.99,
          'JPMorgan':96.27,
          'Walmart': 130.68 
      }
      
      del financial_info['Nike']
      boeing_price = financial_info.pop('Boeing')
      print(financial_info)
      
      
    ```
    
    ```
      {'American Express': 93.23, 'Coca-Cola': 45.15, 'JPMorgan': 96.27, 'Walmart': 130.68}
      
    ```
    

Agora vamos trazer o nosso projeto do Banco ABC de volta e refrescar a memória. Como você pode lembrar, a nossa tarefa foi criar um programa que armazena e gerencia dados dos clientes do banco. Nós usaremos dicionários para armazenar os dados de cada cliente; cada par chave-valor corresponderá a uma parte específica dos dados, como seu nome ou saldo da conta.

Agora vamos ver o quanto você aprendeu sobre dicionários. Nós preparamos alguns problemas que você precisa resolver. Boa sorte!

## Tarefas

### Tarefa 1

A equipe de gestão ABC pediu que você reorganize a lista dos clientes de modo que eles possam acessar os dados com maior facilidade.

Seu objetivo é transformar cada lista da lista `clients` em um dicionário e então imprimir esse dicionário, antes de converter a próxima lista em um novo dicionário.

Use as seguintes chaves para cada dicionário que criar:

"id"

"client\_name"

"age"

"yearly\_income"

"work\_field"

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

clients \= \[

\[32456, "Jack Wilson", 32, 150000, "Healthcare"\],

\[34591, "Nina Brown", 45, 250000, "Telecom"\],

\[37512, "Alex Smith", 39, 210000, "IT"\],

\[39591, "Brian Perez", 29, 340000, "Transportation"\],

\[45123, "Sarah Lee", 28, 120000, "Marketing"\],

\[47635, "David Kim", 36, 180000, "Finance"\],

\[49571, "Samantha Chen", 42, 220000, "Retail"\],

\[50391, "Juan Rodriguez", 31, 160000, "Architecture"\],

\[34556, "Lucas Hernandez", 37, 75000, "Education"\],

\[64291, "Jessica Li", 25, 125000, "IT"\],

\[74512, "Emma Davis", 47, 197000, "Finance"\],

\[83191, "Sophia Perez", 34, 225000, "Transportation"\],

\[91023, "Liam Kim", 29, 98000, "Retail"\],

\[96435, "Ava Chen", 31, 175000, "Marketing"\],

\[100571, "Noah Rodriguez", 28, 85000, "Architecture"\],

\[101321, "Olivia Wilson", 44, 310000, "Telecom"\],

\[104556, "William Brown", 38, 289000, "Finance"\],

\[105491, "Emily Smith", 29, 193000, "Healthcare"\],

\[107512, "Michael Perez", 53, 415000, "Transportation"\]

\]

  

for client in clients:

\# Escreva seu código aqui

dicionario \= {"id":client\[0\],

"client\_name":client\[1\],

"age":client\[2\],

"yearly\_income":client\[3\],

"work\_field":client\[4\]

}

print(dicionario)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Desta vez, o departamento de marketing do ABC está interessado em saber o nível de renda para cada área de trabalho de seus clientes. Ele pediu para você coletar dados de renda dos clientes para cada área.

No pré-código, assim como na tarefa anterior, você vai ver a lista `clients`. Se você quiser dar mais uma olhada na lista, ela está aqui:

```
clients = [
    [32456, "Jack Wilson", 32, 150000, "Healthcare"],
    [34591, "Nina Brown", 45, 250000, "Telecom"],
    [37512, "Alex Smith", 39, 210000, "IT"],
    [39591, "Brian Perez", 29, 340000, "Transportation"],
    [45123, "Sarah Lee", 28, 120000, "Marketing"],
    [47635, "David Kim", 36, 180000, "Finance"],
    [49571, "Samantha Chen", 42, 220000, "Retail"],
    [50391, "Juan Rodriguez", 31, 160000, "Architecture"],
    [34556, "Lucas Hernandez", 37, 75000, "Education"],
    [64291, "Jessica Li", 25, 125000, "IT"],
    [74512, "Emma Davis", 47, 197000, "Finance"],
    [83191, "Sophia Perez", 34, 225000, "Transportation"],
    [91023, "Liam Kim", 29, 98000, "Retail"],
    [96435, "Ava Chen", 31, 175000, "Marketing"],
    [100571, "Noah Rodriguez", 28, 85000, "Architecture"],
    [101321, "Olivia Wilson", 44, 310000, "Telecom"],
    [104556, "William Brown", 38, 289000, "Finance"],
    [105491, "Emily Smith", 29, 193000, "Healthcare"],
    [107512, "Michael Perez", 53, 415000, "Transportation"]
]
```

Além disso, no pré-código, inicializamos um dicionário chamado `incomes_per_field` (rendas por área). Seu objetivo é preenchê-lo. As chaves serão as áreas em que trabalham os clientes do ABC, e os valores serão listas com rendas de todos os clientes que trabalham em cada área correspondente. Aqui está um exemplo de um par chave-valor:

```
"Healthcare": [150000, 193000]
```

A chave é a área (`"Healthcare"`), ou seja, saúde), e os valores são duas rendas dos dois clientes da lista `clients` que trabalham em `"Healthcare"`.

Para preencher o dicionário `incomes_per_field`, percorra a lista `clients` e extraia o nome da área e a renda para cada cliente. Em seguida, verifique se o nome extraído da área existe no dicionário `incomes_per_field`:

-   Se não, adicione-o como uma chave e defina uma lista com uma única renda como um valor.
-   Se existir, adicione a renda à lista de rendas.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

clients \= \[

\[32456, "Jack Wilson", 32, 150000, "Healthcare"\],

\[34591, "Nina Brown", 45, 250000, "Telecom"\],

\[37512, "Alex Smith", 39, 210000, "IT"\],

\[39591, "Brian Perez", 29, 340000, "Transportation"\],

\[45123, "Sarah Lee", 28, 120000, "Marketing"\],

\[47635, "David Kim", 36, 180000, "Finance"\],

\[49571, "Samantha Chen", 42, 220000, "Retail"\],

\[50391, "Juan Rodriguez", 31, 160000, "Architecture"\],

\[34556, "Lucas Hernandez", 37, 75000, "Education"\],

\[64291, "Jessica Li", 25, 125000, "IT"\],

\[74512, "Emma Davis", 47, 197000, "Finance"\],

\[83191, "Sophia Perez", 34, 225000, "Transportation"\],

\[91023, "Liam Kim", 29, 98000, "Retail"\],

\[96435, "Ava Chen", 31, 175000, "Marketing"\],

\[100571, "Noah Rodriguez", 28, 85000, "Architecture"\],

\[101321, "Olivia Wilson", 44, 310000, "Telecom"\],

\[104556, "William Brown", 38, 289000, "Finance"\],

\[105491, "Emily Smith", 29, 193000, "Healthcare"\],

\[107512, "Michael Perez", 53, 415000, "Transportation"\]

\]

  

incomes\_per\_field \= {}

  

for client in clients:

field \= client\[4\]

income \= client\[3\]

if field not in incomes\_per\_field.keys():

incomes\_per\_field\[field\] \= \[income\]

else:

incomes\_per\_field\[field\].append(income)

\# não modifique o código abaixo. Ele imprime o resultado

print(incomes\_per\_field)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-25-161Z.md
### Última modificação: 2025-05-28 18:59:25

# Conclusão - TripleTen

Capítulo 2/7

Dicionários

# Conclusão

Uau! Você arrasou! Algumas partes deste capítulo podem ter sido complicadas, mas isso não te fez desistir. Temos muitas informações aqui, mas só a prática leva à perfeição.

Agora você é capaz de:

-   Dizer o que são dicionários Se ainda tiver dúvidas, revise a lição sobre [Propriedades de dicionários](https://tripleten.com/trainer/data-analyst/lesson/3c3e6174-41ee-4513-be86-b54a3e4c5c5e/).
-   Comparar e contrastar dicionários e listas Se ainda tiver dúvidas, revise a lição sobre [Propriedades de dicionários](https://tripleten.com/trainer/data-analyst/lesson/3c3e6174-41ee-4513-be86-b54a3e4c5c5e/).
-   Recuperar, adicionar e remover valores de dicionários Se ainda tiver dúvidas, revise as lições [Como extrair valores de dicionários](https://tripleten.com/trainer/data-analyst/lesson/8769bd35-46ad-4285-b23e-f1752d3cdd97/) e [Adição e remoção de itens](https://tripleten.com/trainer/data-analyst/lesson/3f45c02d-153f-4051-8ff7-43023928d2f4/).

### 🎉

Como você viu, os dicionários de Python são uma excelente maneira de organizar e obter informação. Os dicionários são uma implementação da chamada tabela hash e são muito utilizados na ciência da computação. Estruturas similares são utilizadas para tudo, desde buscas em bancos de dados até caches de discos. Como uma pessoa que trabalha com dados, você não precisa se aprofundar tanto no assunto, mas você pode se beneficiar do poder da computação!

## O que vem a seguir?

No próximo capítulo, você vai aprender a construir suas próprias funções, uma habilidade que permite que você personalize a maneira que você trabalha com dados para alcançar seus objetivos de forma mais eficiente.

Vamos lá!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-26-442Z.md
### Última modificação: 2025-05-28 18:59:26

# Introdução - TripleTen

Capítulo 3/7

Funções

# Introdução

Este capítulo será dedicado a funções, que são basicamente pequenos programas usados para automatizar operações de rotina. No capítulo anterior, ao trabalhar com dicionários, você escreveu um código para transformar uma lista de clientes em um dicionário. E se você quiser reutilizar esse código quando uma nova solicitação chegar? Seria legal ter um jeito de obter resultados com apenas um clique. É exatamente para isso que servem funções.

Ao final deste capítulo, você será capaz de:

-   criar suas próprias funções
-   otimizar o código, para que ele permita trabalhar com quantidades maiores de dados
-   entender qual é a diferença entre variáveis globais e locais
-   usar a palavra-chave `return` para definir o valor que uma função retorna após ser executada.

Você tem muito a aprender!

Serão necessárias de 1,5 a 3 horas para concluir este capítulo. Boa sorte!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-27-742Z.md
### Última modificação: 2025-05-28 18:59:28

# Sintaxe das funções - TripleTen

Capítulo 3/7

Funções

# Sintaxe das funções

Você já escreveu vários códigos e provavelmente já teve que escrever as mesmas coisas várias vezes.

Para criar um sistema eficiente, desenvolvedores de software utilizam uma abordagem modular, ou seja, eles escrevem pedaços de código que podem ser reutilizados para a mesma finalidade em diversas situações. Um elemento fundamental de código reutilizável é a função. Ainda mais do que na matemática, as funções em Python são uma forma de padronizar e compartilhar seus códigos com outras pessoas. É desta forma que analistas de dados e cientistas garantem que seu trabalho possa ser utilizado em um sistema maior.

<iframe class="base-markdown-iframe__iframe" id="player-Nne8ka1vT50" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Function Syntax" width="640" height="360" src="https://www.youtube.com/embed/Nne8ka1vT50?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fee2b853b-c794-4777-9755-6a880e2ac476%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

## Funções são como uma máquina

Imagine que você esteja preparando uma omelete pela primeira vez. Provavelmente você seguiria uma receita ao pé da letra, lendo e executando cada instrução em ordem. Mas depois de algum tempo, você, como alguém que trabalha com programação e come omeletes todos os dias, provavelmente pensaria: "Ah! Eu quero uma máquina para fazer omeletes!

![](https://practicum-content.s3.amazonaws.com/resources/9.2_1PT_1690265598.png)

Na programação, uma **função** funciona de maneira muito parecida a uma máquina que cozinha uma receita. A ideia básica é que escrevemos um conjunto de instruções e colocamos essas instruções em uma "caixa preta" com um nome. Quando a função é chamada, o Python vai executar as instruções e retornar o resultado.

Você precisa garantir que a função tenha toda a informação necessária para funcionar. Por exemplo, você precisa colocar ovos na máquina para que ela possa fazer uma omelete. Depois de construir a máquina, você não precisa saber o que está acontecendo dentro dela e pode simplesmente usá-la.

Agora vamos ver como criar uma função.

## Sintaxe das funções

Depois de discutirmos o que é uma função, é hora de tentarmos construir uma.

1.  O primeiro passo é criar o cabeçalho da função.
    
    -   A palavra-chave `def` é utilizada para especificar o nome da função.
    -   Os parâmetros são especificados entre parênteses.
    -   O cabeçalho termina com dois pontos.
2.  Em seguida, nós definimos o corpo, colocando cada instrução com uma indentação de quatro espaços. O corpo é onde as manipulações reais que pedimos para uma função fazer são realizadas.
    
3.  A palavra-chave `return` indica o valor que a função vai retornar quando concluir a solicitação recebida.
    

![](https://practicum-content.s3.amazonaws.com/resources/9.2_2PT_1702048077.png)

## Parâmetros das funções

Vamos voltar para a máquina de omeletes. Toda vez que quiser fazer uma omelete, você precisa seguir alguns passos básicos. Mas há algumas coisas que você pode mudar se quiser. Você pode fazer uma omelete com um número diferente de ovos, ou sem queijo. Ser capaz de modificar alguns aspectos das instruções é de grande ajuda.

O comportamento das funções em Python é controlado por **parâmetros.** Eles são variáveis que estão disponíveis dentro das funções. **Parâmetros** são como as configurações de uma máquina que, quando alteradas, vão influenciar seu comportamento.

Quando Python executa uma função, ele substitui os **parâmetros** por **argumentos** — valores concretos passados à função quando ela é chamada. Voltando para a nossa máquina de omeletes, ela pode ter muitos **parâmetros**, como o número de ovos a serem usados, adicionar queijo ou não, e assim por diante.

Por exemplo, se você disser à máquina: "Faça uma omelete com dois ovos e com queijo", os **parâmetros** da máquina serão substituídos pelos **argumentos:** **dois ovos** e **com queijo**.

![](https://practicum-content.s3.amazonaws.com/resources/9.2_3PT_1691056812.png)

O resultado de uma função pode ser atribuído a uma variável da mesma forma que qualquer outro valor. Para definir o resultado de uma função, use o comando `return` dentro da função. O valor à frente do comando `return` será o resultado retornado quando a função é chamada.

Vamos ver isso no nosso exemplo abaixo:

CódigoPYTHON

9

1

2

3

4

5

6

7

8

def omelet(eggs\_number):

result \= 'A omelete está pronta! Ovos usados: ' + str(eggs\_number)

return result

  

\# chamada de função com 2 ovos, o resultado é atribuído à variável omelet\_type

omelet\_type \= omelet(2)

print(omelet\_type) \# imprime o resultado da instrução anterior

print(omelet(3)) \# imprime o resultado de uma nova chamada com 3 ovos

Mostrar a soluçãoExecutar

Vamos resumir:

Os quatro componentes principais de uma função são:

-   O nome da função
-   Os parâmetros que modificam o comportamento da função
-   As instruções executadas quando a função é chamada (o **corpo** da função)
-   O resultado que a função retorna

Pergunta

### Questionário

Identifique as partes da função mostrada a seguir:

```
def square(x):
    return x ** 2

print(square(5))
```

`5`

Um argumento

`square`

O nome

`x ** 2`

O resultado

`x`

Um parâmetro

Fantástico!

Teoria

## A palavra-chave return

Se quisermos que a nossa função retorne um valor, em vez de apenas imprimir algo, usamos a palavra-chave `return`.

Quando usamos a palavra-chave `return` em uma função, especificamos o resultado que queremos que a função nos retorne. Por exemplo, podemos escrever uma função para calcular algo, e o resultado dos cálculos é o que queremos retornar. No questionário acima, vimos exatamente isso quando a palavra-chave `return` foi usada para retornar o valor ao quadrado. Vamos olhar novamente para essa função:

```
def square(x):
    return x ** 2
```

A palavra-chave `return` é especial porque ela faz com que a função termine e retorne imediatamente o valor especificado de volta para o ponto em que foi chamada. Isso significa que qualquer código após a execução de um comando `return` não será executado, já que a função termina sua execução. É importante ter isso em mente ao escrever funções porque qualquer código que você quiser executar deve ser colocado antes da instrução `return`.

Vamos avançar para a próxima lição em que você verá como funções podem ajudar a otimizar seu código e como projetar boas funções. Você também terá a oportunidade de praticar escrevendo suas próprias funções.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-29-598Z.md
### Última modificação: 2025-05-28 18:59:30

# Uso de funções para simplificar códigos - TripleTen

Capítulo 3/7

Funções

# Uso de funções para simplificar códigos

## Funções definidas pelo usuário, na prática

Agora que você sabe a teoria por trás do processo de criação de funções que permitem tornar seu código mais legível e eliminar repetições, é hora de colocá-la em prática. No final desta lição, você será capaz de usar funções para otimizar seu código.

<iframe class="base-markdown-iframe__iframe" id="player-qlcuM_cGG3A" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Using Functions to Simplify Code" width="640" height="360" src="https://www.youtube.com/embed/qlcuM_cGG3A?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F33a575eb-d4f9-4ef2-921a-1d2fb354f4b8%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Agora vamos considerar um caso que será usado ao longo desta lição. Uma loja virtual precisa criar um programa que automatize a concessão de descontos. Se um cliente comprar mais do que cem reais (100$) de um determinado produto, ele receberá um desconto de 5% sobre este produto.

Duas variáveis descrevem cada item do pedido:

-   `[number]_item_price` — o preço do item
-   `[number]_item_quantity` — a quantidade

Vamos supor que há três itens no carrinho do comprador.

```
first_item_price = 10.0
first_item_quantity = 3 

second_item_price = 51.0
second_item_quantity = 2 # o total é mais que 100$. O desconto será aplicado

third_item_price = 4.0
third_item_quantity = 10
```

Primeiro vamos resolver essa tarefa sem criar uma função.

## Sem função definida pelo usuário

Para cada item no carrinho:

-   Calcular o total
-   Com uma instrução condicional, comparar o total a 100
-   Se o total for maior do que 100, subtrair 5% do total

O programa obtido será o seguinte:

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

first\_item\_price \= 10.0

first\_item\_quantity \= 3

  

second\_item\_price \= 51.0

second\_item\_quantity \= 2

  

third\_item\_price \= 4.0

third\_item\_quantity \= 10

  

\# encontre o total do primeiro item

first\_item\_total \= first\_item\_quantity \* first\_item\_price

if first\_item\_total \> 100: \# se for maior do que 100,

first\_item\_total \-= first\_item\_total \* 0.05 \# subtraia 5%

print(first\_item\_total)

  

\# encontre o total do segundo item

second\_item\_total \= second\_item\_quantity \* second\_item\_price

if second\_item\_total \> 100: \# se for maior do que 100,

second\_item\_total \-= second\_item\_total \* 0.05 \# subtraia 5%

print(second\_item\_total)

  

\# encontre o total do terceiro item

third\_item\_total \= third\_item\_quantity \* third\_item\_price

if third\_item\_total \> 100: \# se for maior do que 100,

third\_item\_total \-= third\_item\_total \* 0.05 \# subtraia 5%

print(third\_item\_total)

Mostrar a soluçãoExecutar

Pergunta

### Questionário

A instrução condicional com `if` é repetida três vezes. O que pode ser melhorado nesse código?

Escolha quantas quiser

É difícil editá-lo. Se a loja quiser mudar o valor do desconto, será necessário modificá-lo três vezes em cada bloco repetido.

Isso mesmo!

É difícil escalá-lo. Se o pedido do cliente tiver mais de três itens, o código vai calcular o desconto apenas para os três primeiros itens. Além disso, se o pedido tiver menos de três itens diferentes, teremos variáveis não atribuídas, o que vai resultar em um erro.

Sua intuição está afiada!

É difícil lê-lo. É necessário comparar três blocos de código repetidos e procurar diferenças.

Exatamente!

Não há problemas no código. Ele resolve o problema.

Você conseguiu!

Teoria

Códigos perfeitos não apresentam repetições desnecessárias. Quando as mesmas instruções aparecem várias vezes no código, elas devem ser implementadas em suas próprias funções. Em vez de repetir um bloco de instruções inteiro, você vai simplesmente chamar aquela função.

![](https://practicum-content.s3.amazonaws.com/resources/9.3PT_1690265729.png) _Uso de funções para simplificar códigos_

No algoritmo do desconto de 5%, as variáveis mudam, mas os cálculos são os mesmos. Em uma função definida pelo usuário:

-   As variáveis vão se tornar seus parâmetros.
-   As instruções dos cálculos vão se tornar o corpo da função.

## Com uma função definida pelo usuário

Vamos criar uma função que faz o seguinte:

1.  Apresenta dois parâmetros: o preço do item e sua quantidade.
2.  No corpo:
    -   Ela calcula o preço total de cada item do carrinho.
    -   Compara o total a 100 em uma instrução condicional.
    -   Subtrai 5% se o total for superior a 100.
3.  Retorna o total.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

\# Defina a função com dois parâmetros: preço e quantidade

def calculate\_total\_price(price, quantity):

total \= price \* quantity \# calcule o total de cada item do carrinho

if total \> 100: \# compare o total a 100

total \-= total \* 0.05 \# se o total for maior do que 100, subtraia 5%

return total \# retorne o total

  

first\_item\_quantity \= 3

first\_item\_price \= 10.0

  

second\_item\_quantity \= 2

second\_item\_price \= 51.0

  

third\_item\_quantity \= 10

third\_item\_price \= 4.0

  

\# Chame a função três vezes.

\# Armazene o resultado de cada chamada em uma variável.

first\_item\_total \= calculate\_total\_price(first\_item\_price, first\_item\_quantity)

second\_item\_total \= calculate\_total\_price(second\_item\_price, second\_item\_quantity)

third\_item\_total \= calculate\_total\_price(third\_item\_price, third\_item\_quantity)

  

print(first\_item\_total)

print(second\_item\_total)

print(third\_item\_total)

Mostrar a soluçãoExecutar

Em vez de repetir longos pedaços de código, nós simplesmente chamamos a função e transmitimos os argumentos entre parênteses para ela. Agora o código está mais curto e claro.

### 💡

Assim como nas variáveis, recomenda-se escrever os nomes das funções seguindo o estilo snake case, com sublinhados no lugar de espaços (por exemplo, `calculate_total_price`).

### Tarefa

É hora de tentar por conta própria! Vamos praticar!

O trecho de código abaixo contém operações repetitivas. Especificamente, ele calcula o total, multiplicando o número de itens pelo seu preço. Podemos dar um desconto de 10% se a quantidade for maior do que 5. Fazemos isso multiplicando o total por 0,9. Seu objetivo é escrever uma função chamada `calculate_total_price` que elimine a redundância e que possa ser usada para todos os três casos apresentados abaixo.

```
item_price_1 = 20.0
item_quantity_1 = 20

item_price_2 = 30.0
item_quantity_2 = 1

item_price_3 = 10.0
item_quantity_3 = 6

# Calcule o total para o primeiro item
item_total_1 = item_price_1 * item_quantity_1

# Aplique um desconto de 10% se a quantidade for maior do que 5
if item_quantity_1 > 5:
    item_total_1 *= 0.9

# Calcule o total para o segundo item
item_total_2 = item_price_2 * item_quantity_2

# Aplique um desconto de 10% se a quantidade for maior do que 5
if item_quantity_2 > 5:
    item_total_2 *= 0.9

# Calcule o total para o terceiro item
item_total_3 = item_price_3 * item_quantity_3

# Aplique um desconto de 10% se a quantidade for maior do que 5
if item_quantity_3 > 5:
    item_total_3 *= 0.9
```

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

def calculate\_total\_price(price, quantidade):

x \= price \* quantidade

if quantidade \>= 5:

x \*= 0.9

return x

\# escreva seu código aqui para definir uma função

  

  

  

\# Defina os preços e as quantidades dos três itens

item\_price\_1 \= 20.0

item\_quantity\_1 \= 20

  

item\_price\_2 \= 30.0

item\_quantity\_2 \= 1

  

item\_price\_3 \= 10.0

item\_quantity\_3 \= 6

  

  

\# Chame a função para cada item e armazene o resultado em uma variável

item\_total\_1 \= calculate\_total\_price(item\_price\_1,item\_quantity\_1)\# Escreva seu código aqui

item\_total\_2 \= calculate\_total\_price(item\_price\_2,item\_quantity\_2)\# Escreva seu código aqui

item\_total\_3 \= calculate\_total\_price(item\_price\_3,item\_quantity\_3)\# Escreva seu código aqui

Dica

Mostrar a soluçãoValidar

Na próxima lição, você vai aprender como filtrar funções.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-30-907Z.md
### Última modificação: 2025-05-28 18:59:31

# Funções de filtragem - TripleTen

Capítulo 3/7

Funções

# Funções de filtragem

Agora que o seu código ficou bonitão, é hora de ver como você pode usar funções de formas novas.

<iframe class="base-markdown-iframe__iframe" id="player-mtHTxejE20M" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Filter Functions" width="640" height="360" src="https://www.youtube.com/embed/mtHTxejE20M?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F8b050975-9b08-4ea5-a18e-6966be674c27%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

## Função de filtragem

Anteriormente, no primeiro sprint, você usou o ciclo `for` e instruções condicionais para criar e filtrar sua tabela de filmes para retornar filmes que tinham mais de 180 minutos de duração:

```
movies_info = [
    ['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111],
    ['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730],
    ['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499],
    ["Schindler's List", 'USA', 1993, 'drama', 195, 8.818],
    ['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625],
    ['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619],
    ['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521],
    ['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644],
    ['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106],
    ['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077]
]

movies_filtered = [] # lista vazia para armazenar o resultado

for movie in movies_info: # iterando sobre as linhas da tabela original
    if movie[4] > 180: # se um filme dura mais do que 180 min
        movies_filtered.append(movie) # adicione a linha à lista movie_filtered

for movie in movies_filtered: # imprimindo o conteúdo da lista movies_filtered
    print(movie) 
```

**Resultado:**

```
["Schindler's List", 'USA', 1993, 'drama', 195, 8.818]
['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625]
```

Vamos melhorar a nossa solução usando funções. Para fazer isso, podemos pegar pedaços de código, como o mostrado acima, e criar uma função que vai funcionar para qualquer consulta, em vez de apenas para uma.

Tornaremos o nosso código mais harmonioso e flexível, para podermos trabalhar com muitas consultas ao longo do caminho sem ter que copiar, colar, ou reescrever nada.

Nós vamos separar o código em blocos lógicos e criar duas funções:

1.  Uma para filtrar os filmes
2.  Outra para imprimir os nomes dos filmes que obtivemos após a filtragem

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

\# função que retorna uma cópia filtrada de uma lista aninhada

def filter\_by\_timing(data, target\_duration):

filtered\_result \= \[\]

for row in data:

if row\[4\] \> target\_duration:

filtered\_result.append(row)

return filtered\_result

\# retorna os resultados filtrados

  

\# a função que aceita a lista aninhada como entrada e imprime-a sem retornar nada

def print\_movie\_info(data):

for movie in data:

print(movie)

  

\# vamos chamar a função para movies\_info:

  

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

Mostrar a soluçãoExecutar

As nossas funções funcionaram conforme planejado! Agora podemos reusar esse código para filtrar a tabela de filmes de qualquer forma considerada necessária e imprimi-la. As nossas possibilidades são infinitas com este versátil código!

Estas funções também tornaram o nosso código mais fácil de ler. Agora é hora de praticar!

## Tarefas

### Tarefa 1

No pré-código, você verá as funções que criamos nesta lição para filtrar os dados e imprimir o resultado. Use essas funções para filtrar filmes mais longos do que 140 minutos e imprima o resultado.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

def filter\_by\_timing(data, target\_duration):

filtered\_result \= \[\]

for row in data:

if row\[4\] \> target\_duration:

filtered\_result.append(row)

return filtered\_result

  

def print\_movie\_info(data):

for movie in data:

print(movie)

  

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

x \= filter\_by\_timing(movies\_info,140)\# chame a função de filtragem aqui

print\_movie\_info(x)\# chame a função para imprimir os resultados aqui

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Com base na função acima, escreva uma nova função `filter_by_year()` que vai filtrar os filmes por ano de lançamento. A nova função deve ter dois parâmetros:

-   `data=` — os dados dos filmes
-   `year=` — o critério de filtragem com base em um ano

A função deve retornar uma lista de listas que contenham apenas os filmes lançados após `year=`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

\# essa função imprime a tabela filtrada. Não a modifique

def print\_movie\_info(data):

for movie in data:

print(movie)

  

  

def filter\_by\_year(data, year):

filtered\_result \= \[\]

for row in data:

if row\[2\] \> year:

filtered\_result.append(row)

return filtered\_result \# defina sua função filter\_by\_year() aqui

  

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

\# abaixo há duas chamadas das funções: uma para filtrar os dados e outra para imprimir o resultado

movies\_filtered \= filter\_by\_year(movies\_info, 1990)

print\_movie\_info(movies\_filtered)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-32-190Z.md
### Última modificação: 2025-05-28 18:59:32

# Variáveis globais e locais - TripleTen

Capítulo 3/7

Funções

# Variáveis globais e locais

Ok, você acabou de aprender como criar uma função de filtragem e imprimir os resultados. Vamos dar mais uma olhada nas funções que você escreveu:

```
# função que extrai o ano e o compara
def filter_by_year(data, year):
    filtered_result = []
    for row in data:
        if row[2] > year:
            filtered_result.append(row)
    return filtered_result

# função para imprimir apenas o nome do filme
def print_movie_info(data):
    for movie in data:
        print(movie)
```

A função `filter_by_year` espera dois parâmetros: `data` e `year`. No corpo da função, criamos uma lista vazia chamada `filtered_result`. O ciclo `for` percorre as listas aninhadas de filmes, e se o ano de um filme for maior do que o parâmetro `year`, toda a lista aninhada é adicionada a `filtered_result`. Em seguida, usamos a palavra-chave `return` para retornar os resultados da filtragem.

Já sabemos sobre variáveis. Mais do que isso, costumamos usá-las muito agora. Uma coisa que ainda não abordamos é a diferença entre diferentes tipos de variáveis. Sim, semelhante a argumentos, variáveis também podem ser diferentes.

<iframe class="base-markdown-iframe__iframe" id="player-weP1wA0VFSA" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Global and Local Variables" width="640" height="360" src="https://www.youtube.com/embed/weP1wA0VFSA?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fc7680de7-147e-471c-a13a-62fb913df0eb%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Variáveis globais e locais são dois tipos de variáveis usadas em Python para armazenar valores.

Entender a diferença entre as variáveis locais e globais é importante quando escrevemos funções Python. Usando variáveis locais, podemos manter o nosso código organizado e evitar conflitos de nomenclatura. Variáveis globais, por outro lado, podem ser úteis quando precisamos partilhar dados entre diferentes partes do nosso código.

### Variáveis locais

Variáveis locais são definidas dentro de uma função e só podem ser acessadas dentro dessa função. Elas são criadas quando a função é chamada e destruídas quando a função termina. Variáveis locais podem ser acessadas apenas dentro do escopo da função em que elas são definidas. Isso significa que se uma variável é definida dentro de uma função, ela não pode ser acessada fora daquela função.

Aqui está a nossa função da omelete, que usa variáveis locais:

```
def omelet(cheese, eggs_number):
    result = 'A omelete está pronta! Ovos usados: ' + str(eggs_number)
    if cheese == True:
        result = result + ', com queijo'
    else:
        result = result + ', sem queijo'
    return result

print(omelet(True, 3))
```

Nesse exemplo, `result` é uma variável local. Ela é definida dentro da função `omelet` e só pode ser acessada dentro daquela função. Quando `omelet` é chamada, `result` é criada, e a string é atribuída a ela. E então a função retorna o valor de `result`.

Se você tentar imprimi-la do lado de fora da função, isso resultará em um erro `NameError: name 'result' is not defined`:

```
def omelet(cheese, eggs_number):
    result = 'A omelete está pronta! Ovos usados: ' + str(eggs_number)
    if cheese == True:
        result = result + ', com queijo'
    else:
        result = result + ', sem queijo'
    return result

omelet(True, 3)
print(result)
```

```
Traceback (most recent call last):
  File "main.py", line 11, in <module>

NameError: name 'result' is not defined
```

### Variáveis globais

Variáveis globais, por outro lado, podem ser acessadas de qualquer parte do código, inclusive de dentro das funções. Elas são definidas fora de qualquer função e podem ser usadas ao longo do programa. Variáveis globais podem ser acessadas e modificadas de qualquer parte do programa. No entanto, é importante usar variáveis globais com cautela, já que elas podem dificultar o entendimento e a manutenção do código.

Vamos fazer algumas omeletes globais com 50 ml de leite global:

```
milk = 50 #Declarada como uma variável global

def omelet(cheese, eggs_number):
    result = 'A omelete está pronta! Ovos usados: ' + str(eggs_number)
    if cheese == True:
        result = result + ', com queijo'
    else:
        result = result + ', sem queijo'
    result = result + ' e leite (ml): ' + str(milk) #acessamos a variável global
    print(result)

omelet(True, 3)
```

```
A omelete está pronta! Ovos usados: 3, com queijo e leite (ml): 50
```

Nesse exemplo, `milk` é uma variável global. Ela é definida fora da função `omelet` e pode ser acessada de dentro da função. A instrução `print` dentro de `omelet` retorna o valor de `result` que tem o valor de `milk`.

### Combinando variáveis locais e globais

Podemos usar variáveis locais e globais dentro da mesma função, mesmo que elas tenham o mesmo nome. Neste caso, a variável local tem prioridade sobre a variável global.

```
result = 'A omelete está pronta!' # Declarada como uma variável global

def omelet(cheese, eggs_number):
    result = 'A omelete está pronta! Ovos usados: ' + str(eggs_number)
    if cheese == True:
        result = result + ', com queijo'
    else:
        result = result + ', sem queijo'
    print(result)

omelet(True, 3)
print(result)
```

```
A omelete está pronta! Ovos usados: 3, com queijo
A omelete está pronta!
```

Neste exemplo, a função `omelet` define uma variável local `result` que tem prioridade sobre a variável global `result`. Quando `omelet` é chamada, ela exibe o valor da variável local `result`. A instrução `print` fora da função exibe o valor da variável global `result`.

Entender a diferença entre as variáveis locais e globais é importante quando escrevemos funções Python. Usando variáveis locais, podemos manter o nosso código organizado e evitar conflitos de nomenclatura. Variáveis globais, por outro lado, podem ser úteis quando precisamos partilhar dados entre diferentes partes do nosso código.

Pergunta

### Questionário

Agora vamos verificar se você compreendeu o material.

Conecte as descrições das variáveis com seus nomes correspondentes.

Podem ser acessadas apenas dentro do escopo da função em que elas são definidas.

Variáveis locais

Podem ser acessadas de qualquer parte do código, inclusive de dentro das funções.

Variáveis globais

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-34-160Z.md
### Última modificação: 2025-05-28 18:59:34

# Atividade prática do capítulo - TripleTen

Capítulo 3/7

Funções

# Atividade prática do capítulo

Nesse capítulo, você aprendeu muito sobre as funções. Agora você sabe como:

-   defini-las
-   usar tipos diferentes de argumentos e parâmetros
-   definir variáveis locais e globais
-   retornar variáveis usando a palavra-chave `return`

Vamos colocar isso em prática agora.

Você voltou ao seu trabalho no Banco ABC. Sua amiga da equipe de marketing te contou que o chefe dela não gostou muito da necessidade de ficar pedindo código novo toda vez que eles querem analisar dados de seus clientes. Eles pediram que você crie uma função que possa realizar várias tarefas de filtragem.

Vamos começar a trabalhar nisso e ver se podemos atender às suas necessidades de uma vez por todas!

## Tarefas

### Tarefa 1

Vamos começar escrevendo uma função chamada `filter_clients` para filtrar a lista de clientes por área de trabalho. A função vai receber dois parâmetros: `clients_list` (a lista de clientes) e `field` (o campo com a área de trabalho que a função precisa encontrar). A função vai iterar sobre cada cliente na lista, e se ela encontrar o cliente com a área de trabalho especificada na lista, a informação sobre o cliente será adicionada a uma nova lista. Essa lista será retornada como resultado da execução da função quando ela finalizar sua execução.

Chame a função para a área `"Transportation"`, para fazer um teste. Em seguida, imprima a lista filtrada (já no pré-código).

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

clients \= \[

\[32456, "Jack Wilson", 32, 150000, "Healthcare"\],

\[34591, "Nina Brown", 45, 250000, "Telecom"\],

\[37512, "Alex Smith", 39, 210000, "IT"\],

\[39591, "Brian Perez", 29, 340000, "Transportation"\],

\[45123, "Sarah Lee", 28, 120000, "Marketing"\],

\[47635, "David Kim", 36, 180000, "Finance"\],

\[49571, "Samantha Chen", 42, 220000, "Retail"\],

\[50391, "Juan Rodriguez", 31, 160000, "Architecture"\],

\[34556, "Lucas Hernandez", 37, 75000, "Education"\],

\[64291, "Jessica Li", 25, 125000, "IT"\],

\[74512, "Emma Davis", 47, 197000, "Finance"\],

\[83191, "Sophia Perez", 34, 225000, "Transportation"\],

\[91023, "Liam Kim", 29, 98000, "Retail"\],

\[96435, "Ava Chen", 31, 175000, "Marketing"\],

\[100571, "Noah Rodriguez", 28, 85000, "Architecture"\],

\[101321, "Olivia Wilson", 44, 310000, "Telecom"\],

\[104556, "William Brown", 38, 289000, "Finance"\],

\[105491, "Emily Smith", 29, 193000, "Healthcare"\],

\[107512, "Michael Perez", 53, 415000, "Transportation"\]

\]

  

  

def filter\_clients(clients\_list,field ):

filtered\_result \= \[\]

for row in clients\_list:

if row\[4\] \== field :

filtered\_result.append(row)

return filtered\_result \# crie sua função filter\_clients aqui

  

  

filtered\_list \= filter\_clients(clients,"Transportation")\# chame sua função aqui

  

\# imprimimos o resultado aqui

print(filtered\_list)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Vamos torná-la ainda mais flexível! Escreva mais duas funções similares à anterior que chamamos:

-   `filter_age()` e
-   `filter_income()`

Na primeira função, usaremos a idade como o filtro, enquanto na segunda, vamos usar a renda como o filtro.

A lista `clients` deve ser usada como argumento para o parâmetro `clients_list` em ambas as funções, de forma parecida ao que você fez na tarefa anterior.

Ambas as funções devem filtrar os clientes com valores maiores do que o valor passado como um argumento na chamada da função.

Chame a função `filter_age()` e filtre a lista `clients`, extraindo os clientes que tenham mais de 40 anos. Armazene os resultados na variável `filtered_age`.

Da mesma forma, chame a função `filter_income()` e filtre a lista `clients`, extraindo apenas os clientes que tenham uma renda maior que 250.000. Salve os resultados na variável `filtered_income`.

Imprima as variáveis `filtered_age` e `filtered_income` (que já estão no pré-código).

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

clients \= \[

\[32456, "Jack Wilson", 32, 150000, "Healthcare"\],

\[34591, "Nina Brown", 45, 250000, "Telecom"\],

\[37512, "Alex Smith", 39, 210000, "IT"\],

\[39591, "Brian Perez", 29, 340000, "Transportation"\],

\[45123, "Sarah Lee", 28, 120000, "Marketing"\],

\[47635, "David Kim", 36, 180000, "Finance"\],

\[49571, "Samantha Chen", 42, 220000, "Retail"\],

\[50391, "Juan Rodriguez", 31, 160000, "Architecture"\],

\[34556, "Lucas Hernandez", 37, 75000, "Education"\],

\[64291, "Jessica Li", 25, 125000, "IT"\],

\[74512, "Emma Davis", 47, 197000, "Finance"\],

\[83191, "Sophia Perez", 34, 225000, "Transportation"\],

\[91023, "Liam Kim", 29, 98000, "Retail"\],

\[96435, "Ava Chen", 31, 175000, "Marketing"\],

\[100571, "Noah Rodriguez", 28, 85000, "Architecture"\],

\[101321, "Olivia Wilson", 44, 310000, "Telecom"\],

\[104556, "William Brown", 38, 289000, "Finance"\],

\[105491, "Emily Smith", 29, 193000, "Healthcare"\],

\[107512, "Michael Perez", 53, 415000, "Transportation"\]

\]

  

def filter\_age(clients\_list, age):

filtered\_result \= \[\]

for row in clients\_list:

if row\[2\] \> age :

filtered\_result.append(row)

return filtered\_result \# escreva duas funções de filtragem aqui: filter\_age e filter\_income

def filter\_income(clients\_list,income):

filtered\_result \= \[\]

for row in clients\_list:

if row\[3\] \> income :

filtered\_result.append(row)

return filtered\_result

  

  

filtered\_age \= filter\_age(clients, 40)\# chame a função filter\_age aqui

  

filtered\_income \= filter\_income(clients, 250000)\# chame a função filter\_income aqui

  

\# imprime o resultado

print(filtered\_age)

print(filtered\_income)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-35-450Z.md
### Última modificação: 2025-05-28 18:59:35

# Conclusão - TripleTen

Capítulo 3/7

Funções

# Conclusão

Você conseguiu! Você oficialmente concluiu mais um capítulo do segundo sprint! No decorrer dos últimos 9 capítulos, você foi do básico desta linguagem de programação aos aspectos mais complexos dela. Agora você pode se considerar um programador júnior, e, na nossa opinião, isso merece uma salva de palmas. Parabéns!

Agora você é capaz de:

-   Escrever funções
    
    Se ainda tiver dúvidas, revise a lição [Sintaxe das funções](https://tripleten.com/trainer/data-analyst/lesson/16ca1e63-5d98-4191-9df4-d0cc9b46207b/).
    
-   Usar funções para simplificar o código Se ainda tiver dúvidas, revise a lição [Uso de funções para simplificar o código](https://tripleten.com/trainer/data-analyst/lesson/281f1109-31f4-46d4-bb8a-270ee17ea863/).
-   Usar funções de filtragem
    
    Se ainda tiver dúvidas, revise a lição [Funções de filtragem](https://tripleten.com/trainer/data-analyst/lesson/8810c317-0a14-46dc-9784-e1431b11aed1/).
    
-   Entender a diferença entre variáveis globais e locais e seu escopo dentro das funções
    
    Se ainda tiver dúvidas, revise a lição [Variáveis globais e locais](https://tripleten.com/trainer/data-analyst/lesson/64e410f2-e4a3-4d3c-9eea-8724a8a80b83/).
    

### 🎉

Agora que você já aprendeu sobre funções, você já pode começar a usá-las para tornar seu próprio código mais reutilizável. Mas você sabia que as funções também são uma parte essencial de como o processamento de dados se adapta aos macrodados? Gigantes da tecnologia utilizam abordagens de programação funcional (que consiste na criação de programas usando funções) como o MapReduce para dividir o cálculo em pequenos pedaços (solucionados pelas funções!) e depois juntá-los novamente. Aprender funções é o primeiro passo de um mundo completamente computacional.

## O que vem a seguir?

A porta para que você comece a tentar usar dados reais está finalmente aberta. Nos demais capítulos deste curso, você vai se familiarizar com a biblioteca pandas — a primeira e mais valiosa ferramenta que profissionais de dados usam para completar seus projetos. Se achar que é uma tarefa intimidante, lembre-se do quão longe você já chegou. Nada pode te parar!

Preparar… Apontar… Fogo!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-36-739Z.md
### Última modificação: 2025-05-28 18:59:37

# Introdução - TripleTen

Capítulo 4/7

A biblioteca Pandas

# Introdução

Se você já estava sentindo empolgação com funções, que permitem encapsular e reutilizar código, prepare-se para uma oportunidade incrível de elevar suas capacidades de manipulação de dados a um nível totalmente novo usando a potente e versátil biblioteca Pandas!

Nesta semana, você vai trabalhar com a biblioteca Pandas, uma das bibliotecas mais usadas na análise de dados.

A biblioteca Pandas, com seu potente objeto DataFrame, fornece uma maneira perfeita de manipular e analisar dados. Semelhante aos dicionários, que permitem organizar e recuperar informações de forma eficiente, a Pandas nos permite trabalhar com tabelas de dados sem esforço. A Pandas permite libertar a potência das funções e operações lógicas para filtrar e transformar seus dados, revelando insights valiosos.

Você vai aprender como usar seus métodos para pré-processamento de dados e análise inicial de seu conjunto de dados. E, como desafio final do sprint, você vai aplicar suas novas habilidades em um projeto em pequena escala de análise de dados.

### A biblioteca Pandas

### 💡

Como profissional de dados, você vai precisar realizar cálculos e analisar seus dados em linhas e colunas. A biblioteca Pandas, que você aprenderá neste capítulo, proporciona uma maneira fácil de manipular os seus dados quando eles são armazenados como objetos DataFrame. Um DataFrame é essencialmente uma tabela de dados em Pandas. Ele combina as ideias de dicionários e listas aninhadas, dando a você controle total sobre seus dados.

No final deste capítulo, você será capaz de:

-   Usar o pacote Pandas em Python para trabalhar com tabelas de dados, também conhecidas como DataFrames.
-   Indexar DataFrames para acessar as células da tabela.
-   Filtrar uma tabela por meio de expressões lógicas.
-   Usar métodos dos DataFrames.

Aprender tudo isso é o próximo passo para aumentar suas habilidades de manipulação de dados. Você está com tudo pronto para começar?

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-39-148Z.md
### Última modificação: 2025-05-28 18:59:39

# DataFrames na Biblioteca Pandas - TripleTen

Capítulo 4/7

A biblioteca Pandas

# DataFrames na Biblioteca Pandas

A biblioteca pandas torna o trabalho com dados fácil e intuitivo, utilizando estruturas de dados rápidas, flexíveis e simples como DataFrames. Como profissional de dados, você usará com frequência essas estruturas.

No final desta lição, você será capaz de definir o que é um DataFrame, identificar suas partes e obter uma breve visão geral dos dados que ele contém.

<iframe class="base-markdown-iframe__iframe" id="player-eB67aMa7JWo" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="DataFrames in Pandas Library" width="640" height="360" src="https://www.youtube.com/embed/eB67aMa7JWo?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F996663f1-f3de-47ac-987d-f4a29fec9fee%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

## Pandas como uma biblioteca

Você provavelmente já está familiarizado com o Excel ou o Google Sheets, os quais são ferramentas poderosas para armazenar e manipular dados de diferentes maneiras. No entanto, quando essas planilhas possuem grandes volumes de informações, o desempenho é significativamente reduzido, dificultando o trabalho.

Para grandes volumes de dados, podemos usar pandas, uma biblioteca Python.

Em geral, uma biblioteca é uma coleção de códigos que torna as tarefas diárias mais eficientes. Cada biblioteca possui pacotes de código que podem ser reutilizados em diferentes programas. Usaremos muitas bibliotecas ao longo deste curso, mas, por agora, vamos nos concentrar na Pandas.

Pandas é a biblioteca Python mais usada para processamento de dados. Possui vários recursos, incluindo:

-   Obter dados de várias fontes e uni-los.
-   Resolver problemas com dados, como remover duplicados ou preencher valores ausentes
-   Adicionar, remover ou modificar dados da tabela
-   Agrupamento de dados e cálculo de estatísticas para grupos

Além disso, pandas está bem [documentada](https://pandas.pydata.org/pandas-docs/stable/). Portanto, sempre que precisar, você poderá encontrar uma boa explicação para as funcionalidades que ela oferece.

Começaremos a conhecer pandas através de seu objeto básico, o **DataFrame**.

## Estrutura DataFrame

DataFrames são amplamente usados em ciência de dados, análise, computação científica e muitos outros campos de dados intensivos. Saber o que são é o primeiro passo para começar a trabalhar com pandas.

Um DataFrame é uma estrutura de dados bidimensional, sendo essencialmente uma tabela, na qual cada elemento possui duas coordenadas: uma linha e uma coluna. As linhas são acessadas por **índices** e as **colunas** por seus nomes.

Você pode ver um exemplo de um DataFrame contendo dados de músicas na imagem abaixo. Seus índices estão em verde e vão de 0 a 9, e os nomes das colunas são user\_id (id do usuário), total play (total de reproduções), Artist (artista), genre (gênero) e track (música).

![](https://practicum-content.s3.amazonaws.com/resources/11.2_1PT_1690265860.png)

As linhas normalmente representam uma única entidade, enquanto as colunas descrevem os atributos dessas entidades. No exemplo acima, que registra músicas tocadas em um aplicativo de música, cada música tocada seria gravada em uma linha. Cada música tocada possui informações associadas, como ID do usuário, duração, artista, gênero e nome da música, registradas como colunas.

Neste capítulo, usaremos os termos "DataFrame", "tabela" e "conjunto de dados" de forma intercambiável, pois todos se referem a uma tabela de dados na forma de um DataFrame Pandas.

## Documentação de dados

Como você pode ver, cada coluna tem um nome que nos ajuda a entender os dados. As colunas são semelhantes aos nomes de variáveis, pois dão uma pista sobre o significado dos dados contidos nelas. No entanto, os nomes das colunas sozinhos podem não fornecer uma compreensão completa de um conjunto de dados.

Por exemplo, a coluna `total play` sugere que armazena a duração total da música, que pode ser medida em segundos ou minutos. Pode até incluir a duração de um anúncio reproduzido antes de uma música.

Normalmente, esclarecemos esses detalhes procurando a documentação do conjunto de dados, que contém uma descrição de cada coluna ou perguntando ao proprietário do conjunto de dados.

A documentação costuma ser a opção preferida para entender a estrutura dos dados. Uma documentação bem preparada pode fornecer descrições da estrutura dos dados, incluindo descrições de colunas. Um exemplo de tal descrição pode ser assim:

-   `user_id` — identificação única do usuário
-   `total play` — a quantidade de tempo (em segundos) que o usuário tocou a música
-   `Artist` — o nome do artista
-   `genre` — o gênero da música (rock, pop, eletrônico, clássico, etc.)
-   `track` — o nome da música

Agora você pode ter certeza de que `user_id` identifica um usuário de forma única, o total reproduzido está armazenado em segundos, em vez de minutos, e ter uma ideia do conteúdo de todas as colunas no seu DataFrame.

### Atributos da estrutura DataFrame

Se você usar pandas para ler um conjunto de dados e armazenar como um objeto DataFrame, terá acesso fácil e conveniente às informações gerais do conjunto de dados, além das oportunidades de manipulação de dados fornecidas pela pandas. Essas informações podem ser visualizadas usando **atributos**.

Um atributo é uma característica ou uma propriedade de um objeto. Ele define o estado do objeto e pode ser acessado ou modificado usando a notação de ponto. Por exemplo, uma classe Gato que define o objeto gato poderia ter os atributos raça, número de patas, tamanho, peso, etc.

No contexto da Pandas, atributos são propriedades de um objeto DataFrame que fornecem informações sobre os dados que ele contém.

Os atributos da Pandas permitem acessar informações gerais sobre o conjunto de dados sem realizar qualquer manipulação dos dados. Eles fornecem uma maneira conveniente de obter uma visão geral da estrutura e do conteúdo do DataFrame.

Aqui estão alguns exemplos:

-   O atributo `dtypes` nos permite visualizar os tipos de dados dos valores em cada coluna.
-   O atributo `columns` pode ser usado para exibir os nomes das colunas.
-   O atributo `shape` é usado para entender o tamanho da tabela.

Para ilustrar como usar atributos, vamos verificar o tamanho de um DataFrame. Se a variável `df` contiver um DataFrame, podemos verificar seu tamanho usando o atributo `shape` da seguinte maneira:

![](https://practicum-content.s3.amazonaws.com/resources/11.2_2PT_1690265888.png)

O DataFrame `df` tem 67963 linhas e 5 colunas. É por isso que a saída do atributo `shape` é `(67963, 5)`. Como você vê, chamar um atributo é semelhante a chamar um método, exceto que os atributos não são seguidos por parênteses.

### Tipos de dados

Uma coluna pode conter valores de diferentes tipos. O atributo `dtypes` vai dizer quais são eles.

```
print(df.dtypes)
```

Como saída, obteremos um tipo de dados de cada coluna em um DataFrame. É dessa forma que vai ficar:

![](https://practicum-content.s3.amazonaws.com/resources/2pt_1694774261.png)

A biblioteca pandas tem seus próprios tipos de dados. Cada um deles corresponde a um determinado tipo de dados na linguagem Python:

Python type

Pandas type

string

str

object

integer

int

int64

floating-point number

float

float64

logical data type

bool

bool

Nota: na Pandas, os dados que não se enquadram em nenhum outro tipo, como strings, são indicados como `object`.

### Nomes das colunas

O atributo `columns` armazena uma lista contendo os nomes das colunas. E já que os nomes das colunas são strings, cada nome de coluna tem um tipo `object` que você pode ver indicado no resultado na chamada abaixo.

```
print(df.columns)
```

Aqui está o resultado:

![](https://practicum-content.s3.amazonaws.com/resources/image_1699962287.png)

Pergunta

### Questionário 1

O que o atributo `columns` diz sobre um conjunto de dados?

Nos mostra o número de colunas em um conjunto de dados.

Contém os tipos de dados para cada coluna em um DataFrame.

Armazena os nomes das colunas.

Está correto!

Excelente!

Teoria

### Tamanho da tabela

Discutimos anteriormente o atributo `shape`, que exibe o tamanho de uma tabela. Esse atributo armazena uma **tupla**, com o primeiro valor representando o número de linhas e o segundo valor representando o número de colunas.

Então, o que é uma tupla? Elas são semelhantes às listas, mas:

-   Enquanto os elementos de uma lista podem ser modificados, adicionados ou removidos, os elementos de uma tupla são imutáveis, ou seja, inalteráveis.
-   Em vez de colchetes, as tuplas usam parênteses (por exemplo, `(67963, 5)`).

Essas diferenças tornam uma tupla necessária para o atributo shape, caso contrário, você poderia modificar algo por engano, e a informação exibida não seria correta.

Obter um valor de uma tupla é tão fácil quanto obter um valor de uma lista:

```
# obter o número de linhas da tupla
rows_number = df.shape[0]

# obter o número de colunas da tupla
columns_number = df.shape[1]
```

Usando o exemplo acima, teríamos `rows_number = 67963` e `columns_number = 5`!

## Obtendo todos os atributos com o método `info()`

Você pode acessar cada atributo da tabela separadamente ou pode solicitá-los de uma só vez chamando o método `info()`. Esse método é o equivalente a um canivete suíço para o acesso de atributos.

```
print(df.info())
```

Isso é o que esse método exibe:

![](https://practicum-content.s3.amazonaws.com/resources/11.2_5PT_1690265969.png)

Para se familiarizar com seus dados, você pode começar com uma chamada `info()`. Após estudar os resultados, você pode escolher uma estratégia para processar a tabela.

Por exemplo, no nosso caso, diferentes colunas têm diferentes números de elementos não-nulos, o que significa que elas têm números diferentes de células preenchidas com dados, e não vazias.

-   `Artist` — `65157 non-null`
-   `genre` — `65223 non-null`
-   `track` — `65368 non-null`

Cada uma dessas colunas contém valores nulos ou ausentes. Você pode descobrir isso comparando o número total de linhas em sua tabela com o número de células não nulas nessas colunas. Por exemplo, a coluna `genre` tem `67963 - 65223 = 2740` valores nulos ou ausentes.

Esses valores nulos ou ausentes devem ser processados antes de você poder avançar para a análise dos dados. Lidar com valores ausentes é um aspecto vital do pré-processamento de dados.

Pergunta

### Questionário 2

Quais são as vantagens de usar o método `info()`?

Escolha quantas quiser

É mais rápido do que avaliar cada atributo individual separadamente.

Ah, sim! Em vez de chamar cada atributo um por um, você pode usar uma única linha de código.

Isso exibe a informação comumente solicitada sobre um conjunto de dados.

Exatamente. Há muito a aprender com os dados que temos em mãos.

Usar um método é melhor do que usar atributos.

Você conseguiu!

Esses são os conceitos básicos da Pandas e DataFrame. Agora vamos avançar e nos aprofundar neles, aprendendo teoria e concluindo algumas tarefas práticas!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-40-456Z.md
### Última modificação: 2025-05-28 18:59:40

# Importação e uso de pacotes - TripleTen

Capítulo 4/7

A biblioteca Pandas

# Importação e uso de pacotes

Agora que você conhece os conceitos básicos da biblioteca Pandas, vamos tentar trabalhar com ela.

No final desta lição, você será capaz de importar pandas em Python e criar seu primeiro DataFrame.

<iframe class="base-markdown-iframe__iframe" id="player-apE8PC2Tcmg" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Importing and Using Packages" width="640" height="360" src="https://www.youtube.com/embed/apE8PC2Tcmg?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fbdd14d63-e1e6-4853-b5fd-065be128be61%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

### Importando a biblioteca

O primeiro passo para aproveitar todos os benefícios da Pandas é importar a biblioteca:

```
import pandas
```

Após importar a biblioteca, você pode chamar seus métodos e funções internas. Normalmente, a Pandas é importada com o alias `pd`, facilitando a referência posterior no código. Na verdade, você pode usar qualquer alias, mas esse é o mais comum para a Pandas, e sugerimos que você use essa notação em seu trabalho futuro. Assim:

```
import pandas as pd
```

### Fazendo um DataFrame

Para usar Pandas, você precisa transformar seus dados em um DataFrame. Para fazer isso, você pode usar a classe **DataFrame()**.

Uma classe é como um modelo para criar objetos, e a Pandas tem muitas classes. A classe `DataFrame()` é a mais usada.

A classe `DataFrame()` recebe dois argumentos: `data` e `columns`.

-   `data` espera uma lista contendo os dados.
-   `columns` espera uma lista de nomes de colunas que correspondem à lista de dados.

![](https://practicum-content.s3.amazonaws.com/resources/11.3PT_1690266007.png)

Suponha que você tenha um conjunto de dados simples: uma lista aninhada contendo os nomes dos países e suas capitais. Então, temos uma variável chamada `atlas` que armazena a seguinte informação:

```
atlas = [  
        ['France', 'Paris'],  
        ['Russia', 'Moscow'],  
        ['China', 'Beijing'],  
        ['Mexico', 'Mexico City'],  
        ['Egypt', 'Cairo']  
]
```

Podemos usá-la como valor para o argumento `data`.

```
pd.DataFrame(data = atlas, columns = )
```

Como você pode ver, ainda precisamos fornecer um valor para o argumento `columns`. Vamos criar uma lista contendo os nomes das colunas `'country'` e `'capital'`:

```
geography = ['country', 'capital']
```

Agora temos dois argumentos e podemos transformar nossa lista aninhada em um DataFrame e armazená-lo na variável `world_map`:

```
world_map = pd.DataFrame(data=atlas, columns=geography)
```

Como a `DataFrame()` é uma classe na biblioteca Pandas, ela é precedida por `pd` quando chamada.

O código completo ficará assim:

```
import pandas as pd

# preparando os dados e os nomes das colunas
atlas = [
      ['France', 'Paris'],  
        ['Russia', 'Moscow'],  
        ['China', 'Beijing'],  
        ['Mexico', 'Mexico City'],  
        ['Egypt', 'Cairo'],
]
geography = ['country', 'capital']

# fazendo um DataFrame
world_map = pd.DataFrame(data=atlas , columns=geography)

# imprimindo o DataFrame
print(world_map)
```

```
  country      capital
0  France        Paris
1  Russia       Moscow
2   China      Beijing
3  Mexico  Mexico City
4   Egypt        Cairo
```

Transformamos dados brutos de uma lista aninhada em um DataFrame com colunas nomeadas.

Agora é sua vez!

## Tarefas

### Tarefa 1

Importe pandas de forma que a biblioteca possa ser acessada utilizando `pd`. Crie uma lista aninhada, `music`, com quatro elementos. Cada sublista deve armazenar dois valores string — artista e nome da música:

-   `'Bob Dylan'` — `'Like A Rolling Stone'`
-   `'John Lennon'` — `'Imagine'`
-   `'The Beatles'` — `'Hey Jude'`
-   `'Nirvana'` — `'Smells Like Teen Spirit'`

CódigoPYTHON

9

1

2

3

4

5

6

7

8

9

import pandas as pd\# Escreva seu código aqui

  

music \= \[

\['Bob Dylan' , 'Like A Rolling Stone'\],

\['John Lennon' , 'Imagine'\],

\['The Beatles' , 'Hey Jude'\],

\['Nirvana' , 'Smells Like Teen Spirit'\]

\]

print(pd.DataFrame(music))

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Crie uma lista chamada `entries` com dois elementos: os nomes das colunas `'artist'` e `'track'`. Em seguida, use a classe `DataFrame()` para criar uma tabela a partir das listas `music` e `entries`. Armazene o resultado na variável `playlist` e o imprima.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

import pandas as pd

  

music \= \[

\['Bob Dylan', 'Like A Rolling Stone'\],

\['John Lennon', 'Imagine'\],

\['The Beatles', 'Hey Jude'\],

\['Nirvana', 'Smells Like Teen Spirit'\],

\]

entries \= \["artist", "track"\]

playlist \= pd.DataFrame(data\=music, columns\=entries)

print(playlist)\# Escreva seu código aqui

Dica

Mostrar a soluçãoValidar

E é isso! Seu primeiro DataFrame! Agora vamos ver como podemos criar DataFrames a partir de grandes volumes de dados sem suar!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-41-775Z.md
### Última modificação: 2025-05-28 18:59:42

# Pandas para Excel e arquivos CSV - TripleTen

Capítulo 4/7

A biblioteca Pandas

# Pandas para Excel e arquivos CSV

Na lição anterior, você criou seu próprio DataFrame a partir de um conjunto de dados armazenado em uma lista aninhada. Você também já sabe como acessar os atributos de um DataFrame separadamente ou usando o método `info()`.

Antes de mergulhar em qualquer análise, é uma boa ideia obter uma visão geral rápida de seus dados. Já aprendemos sobre os atributos e sobre o método `info()`, que nos permite fazer isso. Além dos acima mencionados, a Pandas oferece vários outros métodos.

Além disso, em cenários da vida real, geralmente não criamos nossos próprios dados, mas recebemos conjuntos de dados na forma de arquivos ou obtemos esses arquivos de um banco de dados. Quando obtemos um arquivo com dados, tudo o que precisamos fazer é ler esses conjuntos de dados em DataFrames.

<iframe class="base-markdown-iframe__iframe" id="player-c-xbY7gC_K0" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Pandas for Excel and CSV files" width="640" height="360" src="https://www.youtube.com/embed/c-xbY7gC_K0?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F38c9705e-d675-4cf9-8c63-9853f410fa6c%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Nesta lição, você vai aprender como ler arquivos CSV e Excel com a Pandas e recuperar as primeiras e as últimas linhas do seu conjunto de dados usando os métodos `head()` e `tail()`.

## Leitura de arquivos CSV

Na lição anterior, usamos um conjunto de dados simples com países e suas capitais, com uma pequena quantidade de dados, já armazenados como uma lista aninhada. No entanto, se recebermos um conjunto de dados na forma de um arquivo, ele será armazenado com uma estrutura específica que a pandas consegue converter em um objeto DataFrame.

Você estará lidando principalmente com arquivos CSV (valores separados por vírgula), embora também apareçam alguns arquivos Excel.

Em arquivos CSV, vírgulas separam os valores de diferentes colunas. O Pandas lê esses arquivos e transfere os dados para um DataFrame.

Conforme discutido anteriormente, um DataFrame requer dados e nomes de colunas, ambos fornecidos em arquivos CSV:

-   A primeira linha contém os nomes das colunas.
-   Cada linha subsequente contém os dados correspondentes a uma linha na tabela.

Abaixo, ilustramos o arquivo CSV à esquerda e como ele é convertido em um DataFrame Pandas à direita.

![](https://practicum-content.s3.amazonaws.com/resources/11.4_1PT_1690266076.png)

Para ler um arquivo CSV usando pandas, precisamos chamar a função `read_csv()`. Esta função requer apenas um argumento: o caminho do arquivo:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv') # o argumento é o caminho do arquivo
```

Um caminho de arquivo é uma string que mostra onde um arquivo está armazenado em um computador. O caminho difere dependendo de dois fatores:

1.  O **sistema operacional** determina seu formato.
    -   No Windows, o caminho é geralmente escrito com barras invertidas: `'C:\catalog\file.csv'`
    -   Nos sistemas operacionais macOS e Linux, ele é escrito com barras: `'/catalog/files.csv'`
2.  Se o caminho é **relativo** ou **absoluto**.
    
    -   Um caminho absoluto começa no diretório raiz do sistema. No Windows, esse diretório é o disco ("C"): `'C:\catalog\file.csv'`. No Linux e no macOS, o caminho começa com uma "raiz" indicada por uma barra antes de seu nome: `'/catalog/file.csv'`.
    -   Um caminho relativo começa em um local que é relativo ao diretório atual. Por exemplo, se um programa e um arquivo são executados a partir do mesmo diretório, o caminho será apenas o nome do arquivo. Se um programa for executado a partir de um diretório que ocupa uma posição mais alta na hierarquia de diretórios, o caminho relativo irá incluir o nome do subdiretório onde o arquivo se encontra:
    
    ![](https://practicum-content.s3.amazonaws.com/resources/11.4_2PT_1690266101.png)
    

Nesse caso, lemos o arquivo `music_log_chpt_11.csv` localizado na pasta `datasets`, no diretório raiz. Se você quiser ver o código novamente, aqui está:

```
df = pd.read_csv('/datasets/music_log_chpt_11.csv') # o argumento é um caminho de arquivo ABSOLUTO
```

Pergunta

### Questionário 1

Qual é o código correto para ler o arquivo `data.csv` localizado na pasta `datasets`? A pasta está localizada no diretório raiz.

Escolha quantas quiser

`df = pd.read_csv("/datasets/data.xlsx")`

`df = pd.read_csv("data.csv")`

Sim, funcionará se estivermos no diretório `datasets` e lermos o arquivo diretamente daqui.

`df = pd.read_csv("/datasets/data.csv")`

Isso está correto. Você passou um caminho absoluto para um arquivo.

Fantástico!

Teoria

## Leitura de arquivos Excel

Como mencionado anteriormente, o Excel sofre com arquivos grandes, enquanto pandas lida com eles com facilidade. Portanto, além de ler arquivos CSV, muitas vezes precisamos importar arquivos Excel para trabalhar com eles.

Isso também ocorre porque Excel é uma maneira de armazenar dados muito popular entre empresas. E um profissional de dados precisa usar esses dados para alcançar seus objetivos.

O processo de criação de um DataFrame a partir de um arquivo Excel é semelhante ao que fizemos com arquivos CSV. A Pandas fornece uma função interna `read_excel()` para ler arquivos Excel, que também recebe um caminho como argumento. É assim que a usamos:

```
import pandas as pd

df = pd.read_excel('/datasets/music_log.xlsx') # o argumento é o caminho do arquivo
```

O trecho de código acima lê o arquivo `music_log.xlsx`, o qual é um arquivo Excel. Podemos identificar um arquivo Excel por sua extensão. Extensões comuns para arquivos Excel são `.xlsx`, `.xls`, `.xlsm`, `.xlsb` e `.xltx`.

Como resultado, obtemos um DataFrame. Para concluir, existem várias maneiras de criar um DataFrame:

-   Criar o DataFrame a partir de uma lista, como fizemos anteriormente para os países e suas capitais.
-   Ler um arquivo CSV usando a função integrada da pandas.
-   Ler um arquivo do Excel.

Você deve estar se perguntando, por que é importante armazenar nossos dados na forma de um DataFrame? A resposta está nos recursos de análise integrados que estão disponíveis apenas para DataFrames. Até agora, você aprendeu sobre atributos e o método `info()`. Agora, vamos ver o que mais podemos fazer.

## head() e tail()

Você já conhece nosso conjunto de dados `music_log`, que armazena informações sobre músicas tocadas por usuários de uma plataforma de música online. Nós o lemos de um arquivo CSV e o armazenamos como um DataFrame na variável `df` da seguinte forma:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')
```

Agora temos os nossos dados carregados em um novo conjunto de dados. Qual é o próximo passo lógico? Para nós, a resposta é simples. Devemos dar uma olhada no conjunto de dados, ou em uma parte dele, para ter uma noção do seu conteúdo e assim conseguir avaliá-lo.

A Pandas fornece dois métodos para exibir o início ou o fim de um conjunto de dados: `head()` e `tail()`. O método `head()` exibe as primeiras linhas de uma tabela, enquanto `tail()` exibe as últimas linhas.

Vamos agora ver esses métodos em ação. Como você sabe, um método é algo que podemos aplicar a um objeto, no nosso caso um DataFrame. Vamos começar aplicando o método `head()` e imprimir o resultado.

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

print(df.head())
```

```
      user_id  total play        Artist    genre                   track
0  BF6EA5AF   92.851388    Marina Rei      pop                  Musica
1  FB1E568E  282.981000  Stive Morgan  ambient             Love Planet
2  FB1E568E  282.981000  Stive Morgan  ambient             Love Planet
3  EF15C7BA    8.966000           NaN    dance     Loving Every Minute
4  82F52E69  193.776327        Rixton      pop  Me And My Broken Heart
```

Como resultado, obtivemos os nomes das colunas e as cinco primeiras linhas de um conjunto de dados, juntamente com seus respectivos índices, variando de 0 a 4. A função `head()` exibe as primeiras 5 linhas por padrão, mas você pode verificar qualquer número de linhas passando um argumento que especifica o número desejado. Por exemplo, para exibir as 3 primeiras linhas, use o seguinte código:

```
 import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

print(df.head(3))
```

Observe que passamos `3` como argumento e aqui está nosso resultado:

```
      user_id  total play        Artist    genre        track
0  BF6EA5AF   92.851388    Marina Rei      pop       Musica
1  FB1E568E  282.981000  Stive Morgan  ambient  Love Planet
2  FB1E568E  282.981000  Stive Morgan  ambient  Love Planet
```

O método `tail` funciona de maneira semelhante, mas exibe as últimas linhas. E dessa forma podemos verificar as últimas 3 linhas de um conjunto de dados:

```
 import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

print(df.tail(3))
```

```
                user_id  total play        Artist  genre      track
67960  26B7058C  292.455000       Red God  metal  Действуй!
67961  DB0038A8   11.529112  Less Chapell    pop       Home
67962  FE8684F6    0.100000           NaN    NaN        NaN
```

Pergunta

### Questionário 2

Vamos recapitular o que aprendemos até agora.

Selecione todas as declarações corretas sobre os métodos `head()` e `tail()`

Escolha quantas quiser

Ambos exibem 5 linhas por padrão.

Exatamente! Por padrão, eles mostram as primeiras ou últimas 5 linhas, respectivamente.

Eles esperam um argumento que especifica o número exato de linhas que queremos ver.

Isso aí! Ao fornecer um número como argumento, informamos a pandas o número exato de linhas que queremos ver.

Ambos `head()` e `tail()` só podem ser aplicados a um objeto DataFrame.

Como esses são métodos da pandas, eles são aplicáveis apenas a DataFrames.

Você não pode alterar o número de linhas exibidas pelos `head()` e `tail()`

Fantástico!

Agora você pode construir DataFrames de qualquer tamanho para resolver quaisquer tarefas! A seguir, vamos dar uma olhada em como trabalhar com os dados dentro dos DataFrames.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-43-865Z.md
### Última modificação: 2025-05-28 18:59:44

# Indexação do DataFrame - TripleTen

Capítulo 4/7

A biblioteca Pandas

# Indexação do DataFrame

Agora que você sabe como obter informações gerais sobre tabelas, conheceu o método info() e os atributos que ele exibe, como columns e dtypes, e aprendeu como criar seus próprios DataFrames a partir de listas e arquivos externos, é hora de aprender sobre indexação com DataFrames.

Ao final dessa lição, você será capaz de usar a indexação para extrair apenas os dados de seu interesse.

<iframe class="base-markdown-iframe__iframe" id="player-Jv85ez5WkBY" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="DataFrame Indexing" width="640" height="360" src="https://www.youtube.com/embed/Jv85ez5WkBY?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Ff23bc326-d043-4658-87cf-99405da00ce2%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

## Indexação por coordenadas

A indexação nos permite acessar uma célula da tabela usando duas coordenadas: o índice da linha e o nome da coluna.

Há um atributo para acessar valores de uma tabela em um DataFrame, o atributo `loc[]`: `df.loc[row, column]`. Os colchetes após `loc` devem receber o número de `row` (linha) e o nome de `column` (coluna) do valor desejado.

Por exemplo, para obter os conteúdos da célula na quinta linha e na coluna `'genre'`, use `df.loc[4, 'genre']`. Observe que a indexação começa em 0, como de costume.

![](https://practicum-content.s3.amazonaws.com/resources/11.5PT_1690266171.png)

Execute o código abaixo e veja por conta própria:

CódigoPYTHON

9

1

2

3

4

5

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

result \= df.loc\[4, 'genre'\]

print(result)

Mostrar a soluçãoExecutar

Funcionou! Agora é sua vez.

## Tarefas

### Tarefa 1

Escreva o código que recupera o nome da música da 8ª linha do conjunto de dados. Os nomes das músicas estão localizados na coluna `'track'`. Atribua o valor obtido à variável `result` e imprima-o.

CódigoPYTHON

9

1

2

3

4

5

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

result \= df.loc\[7, 'track'\]

print(result)

Dica

Mostrar a soluçãoValidar

Teoria

Você pode solicitar células individuais, bem como grupos de células usando a indexação. Por exemplo, você pode acessar:

-   Todas as células de uma determinada linha.
-   Todas as células de várias linhas.
-   Todas as células de um intervalo de linhas.

Semelhante ao fatiamento de listas, você pode obter um intervalo de valores de uma tabela especificando o início e o fim de uma fatia, separados por dois pontos `:`. Essa sintaxe funciona para ambas as linhas e colunas de uma tabela.

Observação importante! Ao contrário das listas, os limites do intervalo **também** incluem a fatia de linhas. O comando `df.loc[2:4]` vai retornar as linhas com índices 2, 3 e 4.

Aqui estão os exemplos de indexação:

Tipo

Amostra

Resultado esperado

Uma célula

`.loc[7, 'genre']`

Extrai o valor da célula localizada na coluna `'genre'` e na 8ª linha (que possui o índice `7`).

Uma coluna

`.loc[:, 'genre']`

Extrai os valores de todas as células na coluna `'genre'`.

Múltiplas colunas

`.loc[:, ['Artist', 'genre']]`

Extrai todos os valores de todas as células nas colunas `'Artist'` e `'genre'`.

Múltiplas colunas consecutivas (fatia)

`.loc[:, 'user_id': 'genre']`

Extrai todos os valores de todas as células nas colunas começando na coluna `'user_id'` e terminando com a coluna `'genre'`.

Uma linha

`.loc[1]`

Extrai os valores da 2ª linha (com índice `1`).

Todas as linhas, começando pela linha especificada

`.loc[1:]`

Extrai todos os valores da segunda linha e vai até a última linha.

Todas as linhas, até a linha especificada

`.loc[:3]`

Extrai todos os valores das quatro primeiras linhas (da linha com índice `0` até a linha com índice `3`).

Múltiplas linhas consecutivas (fatia)

`.loc[2:5]`

Extrai todos os valores de células entre a terceira linha e a linha com índice `5`.

Veja como a indexação funciona na Pandas com o widget interativo abaixo. À medida que você altera os valores dentro de `.loc[]`, as células apropriadas serão realçadas. A indexação pode levar de 10 a 15 segundos para ser executada.

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Widgets/Data/loc_in_pandas/pt_br/loc.html" style="min-width: 112%; min-height: 565px;"></iframe>

### Tarefa 2

Agora vamos praticar a indexação.

Fatie o DataFrame `df`, extraindo todos os valores para a coluna `'genre'` entre a terceira e a décima primeira linha. Armazene os valores extraídos na variável `index_res` e imprima-os.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

  

index\_res \= df.loc\[2:10, 'genre'\]\# Escreva seu código aqui

  

print(index\_res)

Dica

Mostrar a soluçãoValidar

### Tarefa 3

Divida o DataFrame `df`, extraindo as células da 6ª linha para as colunas entre `'total play'` e `'genre'`. Armazene os valores extraídos na variável `index_res` e imprima-os.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

  

index\_res \= df.loc\[5, 'total play': 'genre'\]

  

print(index\_res)

Dica

Mostrar a soluçãoValidar

Teoria

## Notação abreviada para indexação

Na aplicação prática, um método de indexação bastante comum é usando a notação abreviada. Em vez de chamar explicitamente o atributo `.loc[]`, podemos passar de forma conveniente os índices ou os nomes das colunas que queremos entre os colchetes de uma variável de DataFrame. E, por exemplo, se você quiser duas colunas, use dois pares de colchetes. Essa abordagem simplificada fornece uma maneira mais concisa e eficiente de acessar os elementos de dados desejados dentro do conjunto de dados. Veja como fica na prática:

Tipo

Amostra

Notação abreviada

Uma célula

`.loc[7, 'genre']`

\-

Uma coluna

`.loc[:, 'genre']`

`df['genre']`

Várias colunas

`.loc[:, ['genre', 'Artist']]`

`df[['genre', 'Artist']]`

Várias colunas consecutivas (fatia)

`.loc[:,'total play': 'genre']`

\-

Uma linha

`.loc[1]`

\-

Todas as linhas, começando pela linha determinada

`.loc[1:]`

`df[1:]`

Todas as linhas, até a linha especificada

`.loc[:3]` incluindo 3

`df[:3]` não incluindo 3

Múltiplas linhas consecutivas (fatia)

`.loc[2:5]` incluindo 5

`df[2:5]` não incluindo 5

Conforme mostrado na tabela, a indexação em notação abreviada funciona de uma forma diferente:

-   Fatias **excluem** o final do intervalo.
-   Você não pode lidar com uma única célula ou linha, nem uma fatia de uma coluna.

### Tarefa 4

É hora de ver como a notação abreviada funciona na prática.

Selecione a coluna `'user_id'` do DataFrame `df` usando a notação abreviada e armazene-a na variável `index_res`. Imprima `index_res` no final.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

  

index\_res \= df\['user\_id'\]\# Escreva seu código aqui

  

print(index\_res)

  

Dica

Mostrar a soluçãoValidar

### Tarefa 5

Obtenha as colunas `'user_id'` e `'track'` do DataFrame `df` usando a notação abreviada e armazene-as na variável `index_res`. Imprima `index_res` no final.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

  

index\_res \= df\[\['user\_id', 'track'\]\]\# Escreva seu código aqui

  

print(index\_res)

Dica

Mostrar a soluçãoValidar

### Tarefa 6

Extraia da linha 10 à linha 20 do DataFrame `df` usando notação abreviada e armazene o resultado na variável `index_res`. Imprima `index_res` no final.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

  

index\_res \= df\[9:20\]\# Escreva seu código aqui

  

print(index\_res)

Dica

Mostrar a soluçãoValidar

Muito bem! Agora você sabe como obter de forma rápida um valor específico de um DataFrame ou fatiar um subconjunto necessário para análise. Na próxima lição, você aprenderá como a indexação é usada para filtrar dados com base em condições específicas.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-45-198Z.md
### Última modificação: 2025-05-28 18:59:45

# Filtragem com indexação lógica - TripleTen

Capítulo 4/7

A biblioteca Pandas

# Filtragem com indexação lógica

Na lição anterior, exploramos a indexação de DataFrames, que envolve acessar e selecionar linhas e colunas específicas em um DataFrame. Aprendemos como indexar um DataFrame usando os índices de suas linhas e os nomes das colunas. Conseguimos extrair e manipular subconjuntos de dados específicos especificando os índices de linhas e os nomes de colunas desejados. Você também aprendeu como fatiar os dados de qualquer maneira que possa precisar usando `loc[]` e a notação abreviada.

<iframe class="base-markdown-iframe__iframe" id="player-z2OVfCrSAgY" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Filtering with logical indexing" width="640" height="360" src="https://www.youtube.com/embed/z2OVfCrSAgY?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F576e1df0-fe32-4c1c-bb7e-7ca4c3d797c5%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Nesta lição, usaremos esse conhecimento de base para explorar ferramentas adicionais que podem ser usadas para indexação.

## Indexação lógica (booleana)

A indexação pode ser ainda mais poderosa se você incluir uma expressão lógica. No capítulo "Instruções condicionais", você criou alguns filtros sem usar a Pandas. A sua abordagem anterior era essa:

-   Você iterava por uma lista aninhada.
-   Verificava se a condição era atendida usando `if`.
-   Executava a ação que você queria.

Na biblioteca pandas, esse algoritmo complexo pode ser realizado em uma linha de código. O procedimento é chamado de **indexação lógica** ou **indexação booleana** e funciona assim:

1.  Primeiro, encontraremos todas as linhas em que uma determinada condição é atendida. Por exemplo, usando o conjunto de dados `music_log_chpt_11.csv` com o qual trabalhamos antes, podemos verificar se a coluna `'genre'` é igual a `'pop'`. Podemos usar a notação abreviada da seguinte forma:
    
    ```
     print(df['genre'] == 'pop')
     
    ```
    
    Como alternativa, podemos usar o atributo `loc`:
    
    ```
     print(df.loc[:, 'genre'] == 'pop')
     
    ```
    
    A saída resultante é:
    
    ```
     0         True
     1        False
     2        False
     3        False
     4         True
              ...
     67958     True
     67959    False
     67960    False
     67961     True
     67962    False
     Name: genre, Length: 67963, dtype: bool
     
    ```
    
    Como você pode ver, para cada linha (ou índice) na coluna `'genre'`, obtemos um valor booleano. `True` indica que a condição foi atendida naquela linha específica, o que significa que o `'genre'` naquela linha é de fato `'pop'`.
    
2.  Agora podemos usar o resultado obtido no passo anterior para filtrar a tabela original. Devemos obter uma tabela filtrada apenas com músicas `'pop'`. Para isso, utilizamos o atributo `.loc` mais uma vez, passando os valores booleanos obtidos na etapa anterior:
    
    ```
     print(df.loc[df.loc[:, 'genre'] == 'pop'])
     
     
    ```
    
    Como alternativa, podemos usar:
    
    ```
     print(df.loc[df['genre'] == 'pop'])
     
     
    ```
    
    Ambas as linhas de código obtêm todas as linhas de `df` onde a coluna `'genre'` é igual a `'pop'`.
    
    ```
     user_id  total play                 Artist genre
     0      BF6EA5AF   92.851388             Marina Rei   pop  \\
     4      82F52E69  193.776327                 Rixton   pop
     8      A5E0D927    3.161000  Andrew Paul Woodworth   pop
     11     596A4517    0.000000           David Civera   pop
     13     79D2876C    2.000000        Henning Wehland   pop
     ...         ...         ...                    ...   ...
     67921  2D758485   18.554472               Yuri May   pop
     67937  8B6704A2   68.495839               Dom Blvd   pop
     67952  B0DF0750   44.200000   Bierstrassen Cowboys   pop
     67958  2E27DF51  220.551837           Nadine Coyle   pop
     67961  DB0038A8   11.529112           Less Chapell   pop
     
                                           track
     0                                    Musica
     4                    Me And My Broken Heart
     8      The Name of This Next Song Is Called
     11                                  Bye Bye
     13                       Räuber und Gendarm
     ...                                     ...
     67921                         Вера Вероника
     67937                             Dear Love
     67952  Du hast den schönsten Arsch der Welt
     67958                         Girls On Fire
     67961                                  Home
     
     [8663 rows x 5 columns]
     
     
    ```
    
    Também podemos usar a notação abreviada, descartando o método `loc`. Desta forma:
    
    ```
     print(df[df['genre'] == 'pop'])
     
    ```
    
    Como esperado, a saída será idêntica.
    

Essa forma de indexação lógica é muito comum. Você pode usar expressões lógicas com quaisquer operações lógicas:

-   Para verificar a igualdade ou desigualdade, respectivamente, use `==` e `!=`.
-   Para verificar se um valor é maior ou menor, use `>` ou `<`, respectivamente.
-   Use `>=` para verificar se um valor é maior ou igual a, e `<=` para verificar se um valor é menor ou igual a.

Todas as expressões lógicas retornarão os valores lógicos: `True` e `False`.

## Tarefas

### Tarefa 1

Vamos praticar um pouco agora.

Use a indexação lógica para filtrar o DataFrame armazenado na variável `df`. A tabela resultante deve conter apenas linhas com `'genre'` igual a `'jazz'`. Armazene a tabela filtrada na variável `jazz_df` e imprima-a.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

  

jazz\_df \= df\[df\['genre'\] \== 'jazz'\]\# Escreva seu código aqui

  

print(jazz\_df)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Agora vamos usar a indexação lógica para filtrar o DataFrame novamente. Filtre a tabela original para incluir apenas as músicas com `'total play'` superior a 90 segundos. Armazene a tabela filtrada na variável `high_total_play_df` e imprima-a.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

  

high\_total\_play\_df \= df\[df\['total play'\] \> 90\]\# Escreva seu código aqui

  

print(high\_total\_play\_df)

Dica

Mostrar a soluçãoValidar

Teoria

A propósito, podemos aplicar várias condições primeiro aplicando a primeira condição e armazenando o resultado. Então, podemos chamar a indexação lógica com a segunda condição. Veja o exemplo abaixo e execute o código para verificar a saída.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

  

\# selecionando linhas em que o gênero é jazz e total play varia entre 80 e 130

df \= df\[df\['total play'\] \>= 80\]

df \= df\[df\['total play'\] <= 130\]

df \= df\[df\['genre'\] \== 'jazz'\]

  

print(df)

Mostrar a soluçãoExecutar

A seguir, aprenderemos como extrair um valor ainda maior da tabela filtrada. Até a próxima!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-46-511Z.md
### Última modificação: 2025-05-28 18:59:46

# Vamos contar e somar - TripleTen

Capítulo 4/7

A biblioteca Pandas

# Vamos contar e somar

Agora que você consegue acessar com facilidade os dados em suas tabelas usando as estratégias de filtragem que aprendeu nas lições anteriores, é hora de ver o quão prático isso pode ser. Como profissionais de dados, usamos dados para responder a perguntas. Ao filtrar uma tabela, podemos encontrar respostas para uma variedade de perguntas, desde preferências de usuários até a compreensão de tendências. Vamos responder a algumas perguntas sobre nossos dados de música.

<iframe class="base-markdown-iframe__iframe" id="player-4d0JDBNyrSo" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Let’s Count and Sum" width="640" height="360" src="https://www.youtube.com/embed/4d0JDBNyrSo?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F0d31715c-3668-47f7-957f-c7fc4f5ec41c%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Aqui está um exemplo de uma análise que podemos fazer. Por exemplo, se você quiser saber quanto tempo, em média, um usuário toca uma música pop, comece obtendo uma tabela filtrada apenas para músicas `'pop'`:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

pop_df = df[df['genre'] == 'pop']
```

Agora temos a variável `pop_df`, que contém uma tabela filtrada de músicas pop. No entanto, ter esta tabela é apenas metade da batalha. Nosso objetivo é determinar a duração média. Para conseguir isso, precisamos extrair a coluna `'total play'`. Esta coluna contém o tempo (em segundos) que o usuário tocou a música.

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

pop_df = df[df['genre'] == 'pop']

# extraindo a coluna de reprodução total do DataFrame filtrado
pop_duration = pop_df['total play']

print(pop_duration)
```

Se imprimirmos a variável `pop_duration`, veremos que é igual à coluna `'total_play'` do DataFrame `pop_df`:

```
0         92.851388
4        193.776327
8          3.161000
11         0.000000
13         2.000000
            ...    
67921     18.554472
67937     68.495839
67952     44.200000
67958    220.551837
67961     11.529112
Name: total play, Length: 8663, dtype: float64
```

# O método `mean()`

Imagine que você quer calcular uma média – basta aplicar o método `mean()`, e ele fará todo o trabalho pesado por você. Desta forma:

```
mean_duration = pop_duration.mean()
print(mean_duration)
```

```
90.37219914513818
```

Aqui está o código completo que escrevemos para calcular e imprimir a média:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

pop_df = df[df['genre'] == 'pop']

pop_duration = pop_df['total play']

mean_duration = pop_duration.mean()

print(mean_duration)
```

```
90.37219914513818
```

A duração média de uma música pop é de cerca de 90 segundos. Fizemos uma quantidade considerável de código para calcular esse valor. Existe um jeito de resolver essa tarefa de uma maneira mais direta, sem declarar todas essas variáveis? Sim, existe. Veja como você pode fazer isso:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

mean_duration = df[df['genre'] == 'pop']['total play'].mean()

print(mean_duration)
```

```
90.37219914513818
```

Agora, vamos dar uma olhada em uma linha de código que calcula a média para nós.

1.  `df[df['genre'] == 'pop']` filtra a tabela original e extrai apenas as linhas com músicas pop.
2.  Ao adicionar `['total play']`, nós extraímos a coluna `'total play'` da tabela filtrada.
3.  Aplicar o método `mean()` calcula a média para nós.

Tudo isso é realizado em uma única linha de código. Quão legal é isso?

Hora de tentar fazer isso por conta própria.

## Tarefas

### Tarefa 1

Em cenários da vida real, as empresas costumam ter perguntas específicas que exigem respostas. Por exemplo, uma empresa talvez precise analisar os dados de um usuário específico com o `'user_id'` igual a `'5D9AAD37'`.

Vamos fazer isso agora. Para conseguir isso, você precisa filtrar a tabela para extrair apenas as linhas relevantes para o usuário (`'5D9AAD37'`). Em seguida, pediremos que você calcule a duração média das músicas que esse usuário reproduziu. Essas informações são armazenadas na coluna `'total play'`. Após calcular, armazene os resultados na variável `user_mean_dur` e a imprima.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

  

user\_mean\_dur \= df\[

df\['user\_id'\] \== '5D9AAD37'\]\['total play'\].mean()\# escreva seu código aqui.

  

print(user\_mean\_dur)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-56-835Z.md
### Última modificação: 2025-05-28 18:59:57

# Atividade prática do capítulo - TripleTen

Capítulo 4/7

A biblioteca Pandas

# Atividade prática do capítulo

Você aprendeu muita coisa nessas lições. Antes de você continuar aprendendo, vamos recapitular.

Agora você sabe como importar pandas, criar DataFrames e verificar seus atributos `dtypes`, `columns` e `shape` diretamente ou usando o método `info()`:

```
import pandas as pd

# preparando os dados e os nomes das colunas
atlas = [
      ['France', 'Paris'],  
        ['Russia', 'Moscow'],  
        ['China', 'Beijing'],  
        ['Mexico', 'Mexico City'],  
        ['Egypt', 'Cairo'],
]
geography = ['country', 'capital']

# fazendo um DataFrame
world_map = pd.DataFrame(data=atlas , columns=geography)

print(world_map.dtypes) #imprimindo o atributo dtypes
print(world_map.columns) #imprimindo o atributo columns
print(world_map.shape) #imprimindo o atributo shape
print(world_map.info()) #imprimindo todos os atributos de uma só vez
```

Você também pode criar DataFrames a partir de listas e arquivos CSV e Excel e usar os métodos `head()` e `tail()`:

```
df = pd.read_csv('/datasets/music_log_chpt_11.csv') # ler um arquivo CSV

print(df.head()) # imprimir as primeiras 5 linhas do DataFrame

df2 = pd.read_excel('/datasets/music_log.xlsx')# ler um arquivo Excel

print(df2.tail()) # imprimir as últimas 5 linhas do DataFrame
```

Você pode indexar seu DataFrame usando a notação completa e abreviada e também aplicar expressões lógicas e métodos `count()`, `sum()` e `mean()` a ele:

```
# encontrando a média usando a notação completa e a expressão ==
mean_duration = df[df.loc[:, 'genre'] == 'pop']['total play'].mean()
# encontrando o número de músicas usando a notação abreviada e a expressão <=
count_duration = df[df['total play'] <= 130]['total play'].count()
# encontrando a média usando a notação abreviada e a expressão ==
sum_duration = df[df['genre'] == 'pop']['total play'].sum()
```

Agora que terminamos a recapitulação, vamos pular para a prática!

Pergunta

Qual é o objeto principal em Pandas?

Listas

DataFrames

Exatamente! Neste capítulo, você aprendeu exatamente o que é o quê.

As colunas são um elemento básico de uma tabela.

Índices

Você conseguiu!

Pergunta

Como importamos pandas? Selecione tudo que se aplica

Escolha quantas quiser

`import pd as pd`

`import pandas as pd`

Exatamente. Esta é provavelmente a maneira mais usada para importar pandas.

`import pandas`

Esse método funcionará, mas lembre-se de que você terá que se referir à pandas usando o nome completo da biblioteca em vez do alias `pd`. Por exemplo, use `pandas.DataFrame()` em vez de `pd.DataFrame()`.

`import pandas as np`

Para ser franco, não usamos dessa maneira, mas funcionará. Apenas lembre-se de que você precisará se referir a pandas usando seu alias `np` posteriormente em seu código. A razão pela qual não recomendamos essa forma de importação é porque existe outra biblioteca popular, chamada numpy, que é geralmente importada como `np`.

Excelente!

Crie uma lista de listas intituladas `state_animals`. Cada lista contém dois valores string: o nome de um estado e seu animal correspondente. A lista `state_animals` deve ter dez elementos.

Em seguida, crie uma lista chamada `col_names` com dois elementos: os nomes das colunas `'state'` e `'animal'`.

A etapa final é criar um DataFrame com `state_animals` como valores e `col_names` como nomes das colunas. Armazene este DataFrame na variável `df` e imprima-o.

-   `'Alabama'` — `'black bear'`
-   `'Alaska'` — `'moose'`
-   `'Arizona'` — `'ringtail'`
-   `'Arkansas'` — `'white-tailed deer'`
-   `'California'` — `'grizzly bear'`
-   `'Colorado'` — `'rocky Mt. bighorn sheep'`
-   `'Connecticut'` — `'sperm whale'`
-   `'Delaware'` — `'gray fox'`
-   `'Florida'` — `'manatee'`
-   `'Georgia'` — `'white-tailed deer'`

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

import pandas as pd

  

state\_animals\= \[

\['Alabama' , 'black bear'\],

\['Alaska' , 'moose'\],

\['Arizona' , 'ringtail'\],

\['Arkansas' , 'white-tailed deer'\],

\['California' , 'grizzly bear'\],

\['Colorado' , 'rocky Mt. bighorn sheep'\],

\['Connecticut' , 'sperm whale'\],

\['Delaware' , 'gray fox'\],

\['Florida' , 'manatee'\],

\['Georgia' , 'white-tailed deer'\]

\]

  

df \= pd.DataFrame(state\_animals, columns\=\['state', 'animal'\])\# Escreva seu código aqui

  

  

print(df)

Dica

Mostrar a soluçãoValidar

Pergunta

Quais são as funções válidas em pandas que nos permitem ler diferentes tipos de arquivos de dados?

Escolha quantas quiser

Use `pd.read_csv()` para ler arquivos CSV.

Sim, frequentemente trabalhamos com arquivos CSV.

Use `pd.read_excel()` para ler em arquivos Excel.

Exatamente. Sempre que precisamos trabalhar com um arquivo Excel, utilizamos essa função.

Use `pd.read_file()` para ler qualquer arquivo de texto.

Use `pd.read_txt()` para ler arquivos `.txt`.

Trabalho maravilhoso!

Vamos praticar a indexação do DataFrame usando o conjunto de dados `music_log_chpt_11.csv` que vimos antes. Seu objetivo é usar a notação `.loc[]` para extrair as linhas 2 a 600 para as colunas do DataFrame `Artist` e `track`. Armazene a fatia extraída na variável `sliced` e imprima-a.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

  

sliced \= df.loc\[1:599,\["Artist","track"\]\] \# Escreva seu código aqui

  

print(sliced)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T21-59-58-183Z.md
### Última modificação: 2025-05-28 18:59:58

# Conclusão - TripleTen

Capítulo 4/7

A biblioteca Pandas

# Conclusão

Ótimo trabalho! Você aprendeu muito sobre pandas e suas funcionalidades básicas.

Agora você é capaz de:

-   Importar pandas para o Python Se ainda tiver dúvidas, revise a lição [Importação e uso de pacotes](https://tripleten.com/trainer/data-analyst/lesson/e31a6305-1564-4766-a195-d14e4febc41f/).
-   Explicar como preparar os dados para pandas Se ainda tiver dúvidas, revise a lição [Importação e uso de pacotes](https://tripleten.com/trainer/data-analyst/lesson/e31a6305-1564-4766-a195-d14e4febc41f/).
-   Identificar os atributos e métodos mais comuns para DataFrames Se ainda tiver dúvidas, revise a lição [DataFrames na biblioteca Pandas](https://tripleten.com/trainer/data-analyst/lesson/3cb4dbba-0007-4286-a4b6-6a2d23893204/).
-   Usar a indexação para DataFrames Se ainda tiver dúvidas, revise a lição [Indexação de DataFrames](https://tripleten.com/trainer/data-analyst/lesson/b78a6821-4460-4e59-aadd-0f9733dcf2ae/).
-   Contar e somar os valores em um DataFrame após filtrá-lo Se ainda tiver dúvidas, revise a lição [Vamos contar e somar](https://tripleten.com/trainer/data-analyst/lesson/110f66b6-dbdd-4698-90a1-24dd6ef54cf0/).

### 🎉

Com `pandas`, você aprendeu uma ferramenta que provavelmente você vai usar nos próximos anos. Analistas e cientistas de dados frequentemente puxam `pandas` para fora da sua caixa de ferramentas e a usam tanto para fazer análise inicial de dados como para construir sistemas que ajudam a processá-los e fazer pipeline para outros. Como você vai aprender, `pandas` roda tranquilamente com muitos outros pacotes no ecossistema da ciência de dados — mas embora você nem sempre necessite de todas as bibliotecas de predição e de plotagem, você sempre precisa de algo para te ajudar a ler e dar sentido aos dados.

### O que vem aí

Antes de começar a trabalhar no projeto, você precisará resolver quaisquer valores ausentes, duplicados e outros problemas. Seu trabalho afetará importantes decisões de negócios; por isso, é crucial garantir que os dados com os quais você está trabalhando não sejam problemáticos.

Se prepare para o pré-processamento.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-02-13-271Z.md
### Última modificação: 2025-05-28 19:02:13

# Introdução - TripleTen

Capítulo 5/7

Introdução ao pré-processamento de dados e análise inicial

# Introdução

No capítulo anterior, você aprendeu todos os princípios básicos da biblioteca Pandas, como os atributos de um DataFrame e como obter informações iniciais dele usando o método `info()`. Você também é capaz de recuperar quaisquer dados de seus DataFrames para manipulá-los conforme necessário. Neste capítulo, você vai fazer exatamente isso!

Como profissional de dados, você provavelmente passará a maior parte do tempo de trabalho nas fases de pré-processamento e exploração de dados. A limpeza adequada dos dados é crucial para tirar conclusões confiáveis a partir deles.

Após ler este capítulo, você será capaz de usar métodos para:

-   Renomear colunas
-   Processar valores ausentes
-   Trabalhar com valores duplicados
-   Entender agrupamentos
-   Identificar as etapas do agrupamento
-   Agrupar dados em Pandas
-   Ordenar dados para encontrar valores atípicos

Serão necessárias de 1 a 2,5 horas para concluir este capítulo.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-02-14-588Z.md
### Última modificação: 2025-05-28 19:02:14

# Problemas com dados: garbage in, garbage out - TripleTen

Capítulo 5/7

Introdução ao pré-processamento de dados e análise inicial

# Problemas com dados: garbage in, garbage out

Agora que você sabe como acessar os dados e trabalhar com eles, é hora de começar a analisá-los. Para fazer isso, precisamos primeiro garantir a qualidade desses dados. E não podemos fazer isso se não soubermos o que exatamente devemos procurar. Nesta lição, veremos quais são os problemas mais comuns com dados que normalmente precisamos resolver antes de começar a análise.

A razão para isso é simples: se os dados usados para a análise tiverem problemas, as conclusões tiradas deles não serão confiáveis. Isso é conhecido como "garbage in, garbage out" (GIGO). Ou seja, se entra lixo no processo de análise, a chance é que sairá lixo no resultado. Nosso objetivo é te ajudar a identificar valores ausentes, dados duplicados e erros de apresentação em seus dados para evitar GIGO.

<iframe class="base-markdown-iframe__iframe" id="player-yym27JAr3BY" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Problems With Data: Garbage In, Garbage Out" width="640" height="360" src="https://www.youtube.com/embed/yym27JAr3BY?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F4aa58b61-e290-4c70-bd4f-35c76ecbceb7%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

### Valores ausentes e duplicados

Valores ausentes são aqueles que, por alguma razão, não estão disponíveis. Isso pode acontecer quando uma pessoa não respondeu a uma das perguntas de um formulário, por alguns problemas técnicos ou por alguma outra razão. Nos DataFrames da Pandas, esses valores costumam ser indicados como valores `NaN`. `NaN` significa "não é um número", e é uma maneira comum de marcar valores ausentes.

Valores duplicados indicam que uma das linhas é exatamente igual a alguma outra linha na tabela. Na maioria das vezes, esses valores são gerados por um problema técnico no processamento dos dados, mas, às vezes, e dependendo dos dados, eles podem não ser duplicados reais. Por exemplo, você pode ouvir a mesma música muitas vezes, e se você coletar dados apenas sobre os nomes das músicas que ouviu, esses não serão valores duplicados. O número de entidades iguais vai indicar quantas vezes você ouviu essa música. E se você precisar de uma lista de músicas que ouviu, sem levar em conta quantas vezes elas foram reproduzidas? Nesse caso, elas vão se tornar valores duplicados, e você vai precisar resolver esse problema.

Vamos ver como ficam valores ausentes e duplicados na prática.

Continuaremos trabalhando com o conjunto de dados `music_log`, mas desta vez usaremos sua versão bruta: `music_log_raw` (do inglês "raw" - bruto, cru). É a versão original desse conjunto de dados, que possui alguns problemas de qualidade. Como profissional de dados, você frequentemente receberá conjuntos de dados brutos como este.

Para garantir sua qualidade, vamos dar uma olhada rápida nas primeiras 10 linhas.

user\_id

total play

Artist

genre

track

0

BF6EA5AF

92.851388

Marina Rei

pop

Musica

1

FB1E568E

282.981000

Stive Morgan

ambient

Love Planet

2

FB1E568E

282.981000

Stive Morgan

ambient

Love Planet

3

EF15C7BA

8.966000

NaN

dance

Loving Every Minute

4

82F52E69

193.776327

Rixton

pop

Me And My Broken Heart

5

4166D680

3.007000

Henry Hall & His Gleneagles Hotel Band

jazz

Home

6

F4F5677

0.100000

NaN

classicmetal

NaN

7

386FE1ED

211.880000

NaN

electronic

Riviera

8

A5E0D927

3.161000

Andrew Paul Woodworth

pop

The Name of This Next Song Is Called

9

E9E8A0CA

274.390000

Pillar Point

indie

Dove

Então, o que temos aqui?

-   As linhas 3, 6 e 7 contêm valores ausentes, indicados por `NaN`. `NaN` significa "não é um número" e é uma maneira comum de marcar valores ausentes.
-   Observe as linhas 1 e 2. A mesma música está repetida duas vezes, ou seja, temos um **valor duplicado**. Se isso passar despercebido ao analista, a conclusão pode ser que a música é mais popular do que realmente é.

Esses não são todos os problemas que temos em nossos dados. Eles são problemas de conteúdo. Mas também podemos ter problemas relacionados à estrutura.

## Erros de apresentação

Agora vamos examinar a estrutura da própria tabela. Os nomes das colunas também podem estar incorretos. Vamos extrair os dados da coluna `'user_id'`:

```
print(df['user_id'])
```

```
KeyError: 'user_id'
```

Como resultado, recebemos uma mensagem de erro indicando que a coluna especificada não existe. Mas por quê? À primeira vista, o nome da coluna parece estar correto. Entretanto, na verdade há um espaço antes do nome da coluna: `' user_id'`.

É difícil determinar o número de espaços visualmente, portanto, geralmente é melhor evitar completamente o uso de espaços nos nomes das colunas. Se o nome de uma coluna for formado por várias palavras, é melhor usar snake\_case.

Snake case (ou `snake_case`) refere-se ao estilo de escrita em que cada espaço é substituído por um caractere de sublinhado (`_`) e a primeira letra de cada palavra é escrita em minúscula.

Caracteres não desejados, como espaços, podem ser introduzidos imprevistamente nos dados durante processos de importação ou exportação. Isso é exatamente o que aconteceu com o nome da nossa coluna `' user_id'`.

Todas as questões discutidas nesta lição requerem nossa atenção. Devemos abordá-las antes de prosseguir com a análise real. Ensinaremos como fazer isso nas próximas lições!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-02-15-912Z.md
### Última modificação: 2025-05-28 19:02:16

# Renomear colunas - TripleTen

Capítulo 5/7

Introdução ao pré-processamento de dados e análise inicial

# Renomear colunas

Na lição anterior, você aprendeu sobre os problemas mais comuns que pode encontrar em seus dados. Eles foram divididos em problemas de estrutura e de conteúdo.

Os problemas de conteúdo que analisamos são:

-   Valores ausentes, quando uma célula não está preenchida por alguma razão
-   Valores duplicados, quando duas ou mais linhas são exatamente iguais

E o problema estrutural que vimos são nomes de colunas incorretos. Nesta lição, vamos aprender como resolver esse problema estrutural.

Alguns problemas típicos com nomes de coluna são:

-   Nomes de colunas contendo espaços. Estes podem ser difíceis de perceber e podem criar problemas quando você tenta acessar uma coluna. É melhor usar snake\_case ao nomear colunas e variáveis.
-   Falta de clareza nos nomes das colunas. Pode ser difícil dizer o que os dados em uma coluna representam.

Ao final dessa lição, você será capaz de identificar erros comuns nos nomes das colunas e usar o método `rename()` para corrigi-los. Saber como alterar os nomes das colunas para descrever melhor os dados facilitará muito o trabalho.

<iframe class="base-markdown-iframe__iframe" id="player-RdNIiS6v_o4" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Renaming Columns" width="640" height="360" src="https://www.youtube.com/embed/RdNIiS6v_o4?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F6d7ad331-eff7-4a6a-84d3-1720667b4cbd%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

### Por onde começar

A primeira etapa é verificar se você de fato tem um problema de nomes incorretos nas colunas. Recomendamos começar com o método `info()` para obter uma ideia geral sobre o conjunto de dados.

Vamos lembrar como usá-lo:

```
print(df.info())
```

Aqui está o resultado:

![](https://practicum-content.s3.amazonaws.com/resources/11.2_5PT_1698311430.png)

Este método exibe não apenas os nomes das colunas, mas também informação sobre os tipos de dados na tabela e o número de objetos não nulos em cada coluna. É um ótimo ponto de partida.

Como alternativa, você pode usar o atributo `.columns` que só imprime os nomes das colunas e nada mais.

Para ilustrar como funciona o atributo `.columns`, vamos criar uma tabela contendo as distâncias entre a Terra e vários corpos celestes. Vamos criar um DataFrame a partir desses dados.

```
import pandas as pd

# as distâncias são armazenadas em uma lista de listas
measurements = [['Sun', 146, 152],
                                ['Moon', 0.36, 0.41], 
                                ['Mercury', 82, 217], 
                                ['Venus', 38, 261],
                                ['Mars', 56, 401],
                                ['Jupiter', 588, 968],
                                ['Saturn', 1195, 1660],
                                ['Uranus', 2750, 3150],
                                ['Neptune', 4300, 4700],
                                ['Halley\'s comet', 6, 5400]]

# os nomes das colunas são armazenados na variável de cabeçalho
header = ['Celestial bodies ','MIN', 'MAX'] 

# armazenando o DataFrame na variável celestial
celestial = pd.DataFrame(data=measurements, columns=header)
```

Para verificar os nomes das colunas, vamos imprimir o atributo `columns` do DataFrame:

```
print(celestial.columns)
```

```
Index(['Celestial bodies ','MIN', 'MAX'], dtype='object')
```

Nós temos três problemas aqui:

1.  `'Celestial bodies '` contém dois espaços: um entre as palavras e outro no final.
2.  `'MIN'` e `'MAX'` estão escritos com letra maiúscula, enquanto em `'Celestial bodies '` apenas o primeiro caractere de cada palavra é maiúsculo. Esse tipo de inconsistência pode causar problemas.
3.  Os nomes `'MIN'` e `'MAX'` não são muito descritivos. Precisamos de nomes mais explicativos, que transmitam claramente seu significado.

### Correção de problemas

Temos que corrigir esses problemas, então vamos renomear as colunas:

-   Vamos alterar `'Celestial bodies '` para `'celestial_bodies'`, ou seja, usaremos o estilo snake\_case e corrigiremos os problemas com esse nome da coluna.
-   Substituiremos `'MIN'` e `'MAX'` por `'min_distance'` e `'max_distance'`, dessa forma, usaremos o estilo snake\_case e deixaremos os nomes das colunas mais significativos, o que vai facilitar o entendimento do conteúdo.

Para renomear colunas, chame o método `rename()` com um dicionário como argumento `columns`. As chaves no dicionário devem ser os nomes das colunas antigas e os valores correspondentes devem ser os novos nomes. Assim:

```
# Declarando um dicionário com os nomes antigos das colunas como as chaves
# e os novos nomes de colunas como os valores
columns_new ={
    "Celestial bodies ": "celestial_bodies",
    "MIN": "min_distance",
    "MAX": "max_distance",
    }

# Chamando o método rename e passando
# o dicionário como um argumento para o parâmetro columns
celestial = celestial.rename(columns = columns_new)
print(celestial.columns)
```

```
Index(['celestial_bodies', 'min_distance', 'max_distance'], dtype='object')
```

Acima, demonstramos como renomear colunas e reatribuir a variável `celestial` para refletir as alterações. Se você não reatribuir a variável, os nomes das colunas não serão alterados.

No entanto, existe uma maneira mais elegante de renomear colunas que não requer reatribuição como fizemos acima. Precisamos apenas especificar o parâmetro `inplace` e defini-lo como `True`.

```
# Declarando um dicionário com os nomes antigos das colunas como as chaves
# e os novos nomes de colunas como os valores
columns_new ={
    "Celestial bodies ": "celestial_bodies",
    "MIN": "min_distance",
    "MAX": "max_distance",
    }

# Chamando o método rename e passando
# o dicionário como um argumento para o parâmetro columns
# e True como um argumento para o parâmetro inplace
celestial.rename(columns = columns_new, inplace = True)
print(celestial.columns)
```

```
Index(['celestial_bodies', 'min_distance', 'max_distance'], dtype='object')
```

Como você pode ver, alcançamos o mesmo resultado sem fazer atribuições diretas.

## Tarefas

### Tarefa 1

Agora é sua vez de praticar!

Primeiro, você precisa saber se algo está errado com os nomes das colunas e o que exatamente está errado. Então comece imprimindo os nomes das colunas da tabela `df`.

CódigoPYTHON

9

1

2

3

4

5

6

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_raw.csv')

  

colunas \= df.columns\# Escreva seu código aqui

print(colunas)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Você deve ter identificado três problemas nos nomes das colunas `' user_id'`, `'total play'` e `'Artist'`. Então vá em frente e corrija-os.

Renomeie as três colunas a seguir em `df`:

-   `'  user_id'` → `'user_id'`
-   `'total play'` → `'total_play'`
-   `'Artist'` → `'artist'`

Crie um dicionário com os nomes antigos e novos das colunas e então chame o método `rename()` em `df` e passe seu dicionário para ele.

No dicionário, use os nomes das colunas antigas como chaves e as novas como valores.

Em seguida, imprima o atributo `columns` de `df` para confirmar que as alterações foram aplicadas.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_raw.csv')

  

teste \= {

' user\_id' : 'user\_id',

'total play' : 'total\_play',

'Artist' : 'artist'}

  

df \= df.rename(columns \= teste)\# Escreva seu código aqui

  

  

print(df.columns)

Dica

Mostrar a soluçãoValidar

Teoria

### Forma automatizada de renomear colunas

Às vezes, o número de colunas em um conjunto de dados pode ser alto, tornando impraticável atribuir manualmente novos valores aos nomes das colunas. E também pode ser difícil encontrar problemas em um nome de coluna. Nesses casos, ciclos e métodos de string podem ser muito úteis.

Veja um trecho de código abaixo que itera sobre os nomes antigos das colunas, os altera e salva os resultados na lista `new_col_names`, sendo posteriormente atribuída como os novos nomes das colunas:

```
new_col_names = []

for old_name in celestial.columns:
    # Primeiro, remova os espaços no início e no final
    name_stripped = old_name.strip()
    # Em seguida, coloque todas as letras em minúsculas
    name_lowered = name_stripped.lower()
    # Por fim, substitua os espaços entre as palavras por sublinhados
    name_no_spaces = name_lowered.replace(' ', '_')
    # Adicione o novo nome à lista de novos nomes das colunas
    new_col_names.append(name_no_spaces)

# Substitua os nomes antigos pelos novos
celestial.columns = new_col_names
```

Como resultado, obtemos um DataFrame no qual todos os nomes das colunas estão conforme o formato desejado.

Uma coisa importante a ser observada é como substituímos os nomes antigos da coluna por novos nomes usando esta linha:

```
celestial.columns = new_col_names
```

Vamos analizá-la:

-   `new_col_names` é uma lista com os novos nomes da coluna que queremos definir
-   ao configurar `celestial.columns` como igual a `new_col_names`, fazemos a troca, atualizando os nomes antigos da coluna pelos novos armazenados em `new_col_names`.

Esse método de troca é útil quando você tem nomes de coluna que quer definir no formato de lista.

É muito simples! Percorra os nomes das colunas, remova todos os espaços iniciais e finais nos nomes usando o método `strip()`, transforme tudo em letra minúscula usando o método `lower()`, substitua todos os espaços entre palavras por sublinhados aplicando o método `replace()` e então adicione os novos nomes arrumados a uma nova lista.

Agora é sua vez de novo. Vamos voltar ao nosso conjunto de dados `music_log_raw.csv`.

### Tarefa 3

Agora queremos que você faça a mesma renomeação, mas usando 3 métodos de string: `strip()`, `lower()` e `replace()`. Coloque os novos nomes das colunas na lista `new_col_names`.

Em seguida, imprima o atributo `columns` de `df` para confirmar que as alterações foram aplicadas.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

9

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_raw.csv')

  

new\_col\_names \= \[old\_name.strip().lower().replace(' ', '\_') for old\_name in df.columns\]

df.columns \= new\_col\_names

  

  

print(df.columns)

Dica

Mostrar a soluçãoValidar

E aí está! Um simples ciclo pode deixar tudo limpo e brilhante! Na próxima lição, você vai começar a aprender como resolver problemas relacionados a conteúdo lidando com valores ausentes.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-02-25-552Z.md
### Última modificação: 2025-05-28 19:02:26

# Processamento de valores ausentes - TripleTen

Capítulo 5/7

Introdução ao pré-processamento de dados e análise inicial

# Processamento de valores ausentes

Na lição anterior, você aprendeu como corrigir problemas estruturais nos nomes das colunas manual e automaticamente, programando seu próprio ciclo para lidar com esses processos. Vamos refrescar a memória:

```
new_col_names = []

for old_name in celestial.columns:
    # Primeiro, remova os espaços no início e no final
    name_stripped = old_name.strip()
    # Em seguida, coloque todas as letras em minúsculas
    name_lowered = name_stripped.lower()
    # Por fim, substitua os espaços entre as palavras por sublinhados
    name_no_spaces = name_lowered.replace(' ', '_')
    # Adicione o novo nome à lista de novos nomes das colunas
    new_col_names.append(name_no_spaces)

# Substitua os nomes antigos pelos novos
celestial.columns = new_col_names
```

Agora que corrigimos os nomes das colunas, podemos mostrar como pré-processar os valores ausentes nos próprios dados. Ao final dessa lição, você será capaz de verificar rapidamente quais colunas possuem valores ausentes usando o método `isna()`, preencher os valores ausentes com o método `fillna()` e remover linhas ou colunas com valores ausentes usando o método `dropna()`.

<iframe class="base-markdown-iframe__iframe" id="player-xO3sW7Uwd4Q" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Processing Missing Values" width="640" height="360" src="https://www.youtube.com/embed/xO3sW7Uwd4Q?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F3104a7d9-30bd-41fb-8283-525c3cfc310c%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

### Por que é importante processar valores ausentes

O processamento de valores ausentes é essencial. Os algoritmos de aprendizado de máquina não funcionam bem com dados que possuem valores ausentes. Além disso, os algoritmos estatísticos que auxiliam as empresas a tomar decisões informadas também sofrem quando recebem informações incompletas. Em regra geral, devemos sempre tratar os valores ausentes antes de prosseguir.

Para começar, vamos dar uma olhada nas primeiras 10 linhas do nosso conjunto de dados. Não se preocupe, esses dados ainda não refletem as alterações que fizemos nos nomes das colunas. Idealmente, essas alterações já seriam visíveis aqui, mas decidimos deixar os nomes originais como estão para focar em cada etapa do pré-processamento separadamente.

```

import pandas as pd

df = pd.read_csv('/datasets/music_log_raw.csv')

print(df.head(10))
```

user\_id

total play

Artist

genre

track

0

BF6EA5AF

92.851388

Marina Rei

pop

Musica

1

FB1E568E

282.981000

Stive Morgan

ambient

Love Planet

2

FB1E568E

282.981000

Stive Morgan

ambient

Love Planet

3

EF15C7BA

8.966000

NaN

dance

Loving Every Minute

4

82F52E69

193.776327

Rixton

pop

Me And My Broken Heart

5

4166D680

3.007000

Henry Hall & His Gleneagles Hotel Band

jazz

Home

6

F4F5677

0.100000

NaN

classicmetal

NaN

7

386FE1ED

211.880000

NaN

electronic

Riviera

8

A5E0D927

3.161000

Andrew Paul Woodworth

pop

The Name of This Next Song Is Called

9

E9E8A0CA

274.390000

Pillar Point

indie

Dove

Como vimos antes, há alguns valores ausentes (`NaN`) na coluna `'Artist'`.

Você se lembra de que valores ausentes podem aparecer por diferentes razões? Por exemplo, um usuário decidiu não fornecer alguma informação pessoal. Ou talvez houve algum problema com o sistema automático de coleta de dados. Às vezes valores ausentes são inseridos intencionalmente, para depois serem automaticamente substituídos por caracteres especiais.

Valores ausentes podem ter várias formas, mas aqui estão duas das mais comuns:

1.  **Formas esperadas:** `None` ou `NaN`. Esses são dois valores especiais que indicam valores ausentes:
    -   `None` é uma instância do objeto `NoneType`. Ou seja, é usado para representar um valor nulo ou a ausência de qualquer valor.
    -   `NaN` significa "Not A Number" (Não é um número), sendo usado para representar valores ausentes nos dados. Ele é tratado como um número do tipo `float`. `NaN` representa qualquer valor indefinido ou não apresentável.
2.  **Formas inesperadas.** As pessoas podem optar por indicar valores ausentes de maneiras alternativas, como `0`, `'?'`, `'NN'` ou `'n/a'`. Idealmente, eles também devem fornecer documentação para os dados. Caso contrário, você terá que adivinhar o significado dos valores ausentes. Se você encontrar frequentemente um caractere especial em uma tabela sem qualquer explicação, então ele provavelmente representa valores ausentes. E você provavelmente vai querer lidar com eles da mesma forma que faz com `None` ou `NaN`, mas lembre-se de que Python só reconhece `None` e `NaN` como valores ausentes, então você precisa identificá-los manualmente.

Pergunta

### Questionário

Vamos verificar sua intuição no seguinte questionário:

Veja a tabela abaixo. Você pode confirmar que o `0` na segunda linha indica valores ausentes?

Video

Watched

0

Music Video Backstage

10

1

New Music Video

0

2

Tips that will help you improve your photography

123

Sim

Não

Isso mesmo! Nesse caso, não sabemos realmente se `0` significa que falta informação ou se ninguém assistiu ao vídeo ainda.

Excelente!

Teoria

Como acabamos de ver, nem sempre está claro que zeros são apenas zeros.

## Processando valores ausentes

Então, agora sabemos como identificar valores ausentes em nossos dados, mas como podemos processá-los?

Aqui, demonstraremos algumas maneiras de tratar valores ausentes. Usaremos um conjunto de dados da Organização Mundial da Saúde. [O registro epidemiológico da OMS](https://apps.who.int/iris/bitstream/handle/10665/274654/WER9338.pdf?ua=1) contém dados estatísticos sobre a cólera a partir de 2017:

```
import pandas as pd

cholera = pd.read_csv('/datasets/cholera.csv')
print(cholera)
```

**Resultado:**

region

Property

country

total\_cases

imported\_cases

deaths

case\_fatality\_rate

notes

Asia

0

Afghanistan

33

0

1

3

NaN

Asia

1

India

385

NaN

3

0.7

NaN

Asia

2

Iran

634

625

4

0.6

NaN

Asia

3

Yemen

1032481

0

2261

0.2

NaN

Asia

4

China

14

NaN

0

0

NaN

Asia

5

Qatar

5

5

0

0

NaN

Asia

6

Malaysia

2

0

0

0

NaN

Asia

7

Nepal

7

NaN

0

0

NaN

Asia

8

UAE

12

12

0

0

NaN

Asia

9

Saudi Arabia

5

5

0

0

NaN

Asia

10

Singapore

3

3

0

0

NaN

Asia

11

Thailand

8

0

0

0

NaN

Asia

12

Philippines

134

NaN

2

1.5

NaN

Asia

13

South Korea

5

5

0

0

NaN

Asia

14

Japan

7

5

0

0

NaN

Americas

15

Haiti

13681

0

159

1.2

NaN

Americas

16

Dominican Republic

122

0

4

3.3

NaN

Americas

17

Canada

4

3

0

0

NaN

Americas

18

USA

11

9

0

0

NaN

Africa

19

All countries

179835

NaN

3220

1.8

no information on imported cases

Europe

20

All countries

NaN

NaN

no reports in 2017

Oceania

21

Australia

3

3

0

0

NaN

World

22

All countries

1227391

675

5654

0.5

NaN

Recebemos uma descrição das colunas do conjunto de dados. Isso é o que ele nos diz:

`'region'` e `'country'` indicam a localização geográfica

`'total_cases'` número total de casos de cólera

`'imported_cases'` quantos casos foram importados de outros países

`'deaths'` - o número de casos letais

`'case_fatality_rate'` - armazena a taxa de mortalidade

`'notes'` contém strings com certos comentários

### Procurando por valores ausentes

Para encontrar todos os valores ausentes em uma tabela, você pode usar o método `isna()`. Funciona de maneira bastante simples: se um valor ausente for encontrado, ele retornará `True`, caso contrário retornará `False`.

`isna()`não é tão útil sem outros métodos. Normalmente usamos o método `isna()` com o método `sum()`. A função `sum()` conta todos os valores `True` e retorna sua soma:

```
print(cholera.isna().sum())
```

**Resultado:**

column

0

country

0

total\_cases

1

imported\_cases

6

deaths

1

case\_fatality\_rate

1

notes

21

dtype: int64

0

Portanto, para cada coluna no conjunto de dados, obtemos um valor que indica o número de valores ausentes nessa coluna. Por exemplo, a coluna `'imported_cases'` possui 6 valores ausentes e a coluna `'notes'` possui 21 valores ausentes.

Você também pode encontrar o comando `isnull()` no lugar de `isna()`. Na verdade, eles fazem a mesma coisa, então `cholera.isna().sum()` e `cholera.isnull().sum()` produzirão o mesmo resultado.

Recomendamos usar `isna()`, já que ele é o mais utilizado no mercado. Lembre-se de que `isna()` e `isnull()` trabalham apenas com valores ausentes nas formas comuns. Em outras palavras, eles tratam apenas `NaN` e `None`.

Dessa forma vemos que quase todas as colunas contêm valores ausentes. Vamos descobrir o que fazer com eles.

## Maneiras de processar valores ausentes

Dependendo dos objetivos do seu trabalho, você pode querer processar valores ausentes de maneiras diferentes.

-   **Linhas da tabela podem ser completamente removidas se elas perderem o seu significado devido aos valores ausentes**. Às vezes, um segmento de dados torna-se inútil devido a valores ausentes e precisa ser removido.

Observe que não há dados sobre a Europa em nossa tabela da OMS; a linha está cheia de valores ausentes. Segundo a coluna `'notes'`, não houve casos de cólera na Europa em 2017. Portanto, podemos remover a linha sem nenhum impacto nos resultados.

-   **Às vezes valores ausentes são substituídos por outros valores**. Isso pode ser feito quando os valores ausentes não são muito relevantes para a nossa análise, mas as linhas ou colunas contêm outros dados valiosos.

Por exemplo, a África teve 179.835 casos de cólera, mas nenhum deles foi importado. Observe que `'imported_cases'` contém `NaN`. No entanto, se removermos a linha inteira, perderemos dados importantes.

### Substituição de valores

Para preservar todas as linhas com dados valiosos, substituiremos os valores `NaN` na coluna `'imported_cases'` por zeros.

Podemos fazer isso usando o método `fillna()`, que retorna uma cópia da coluna original com todos os valores `NaN` substituídos pelo valor especificado.

![](https://practicum-content.s3.amazonaws.com/resources/12.4_1PT_1690266236.png)

Vejamos como funciona na prática:

```
cholera['imported_cases'] = cholera['imported_cases'].fillna(0)

print(cholera)
```

**Resultado:**

region

index

country

total\_cases

imported\_cases

deaths

case\_fatality\_rate

notes

Asia

0

Afghanistan

33

0

1

3

NaN

Asia

1

India

385

0

3

0.7

NaN

Asia

2

Iran

634

625

4

0.6

NaN

Asia

3

Yemen

1032481

0

2261

0.2

NaN

Asia

4

China

14

0

0

0

NaN

Asia

5

Qatar

5

5

0

0

NaN

Asia

6

Malaysia

2.0

0

0

0

NaN

Asia

7

Nepal

7

0

0

0

NaN

Asia

8

UAE

12

12

0

0

NaN

Asia

9

Saudi Arabia

5

5

0

0

NaN

Asia

10

Singapore

3

3

0

0

NaN

Asia

11

Thailand

8

0

0

0

NaN

Asia

12

Philippines

134

0

2

1.5

NaN

Asia

13

South Korea

5

5

0

0

NaN

Asia

14

Japan

7

5

0

0

NaN

Americas

15

Haiti

13681

0

159

1.2

NaN

Americas

16

Dominican Republic

122

0

4

3.3

NaN

Americas

17

Canada

4

3

0

0

NaN

Americas

18

USA

11

9

0

0

NaN

Africa

19

All countries

179835

0

3220

1.8

no information on imported cases

Europe

20

All countries

NaN

0

no reports in 2017

Oceania

21

Australia

3

3

0

0

NaN

World

22

All countries

1227391

675

5654

0.5

NaN

A coluna `'imported_cases'` agora tem todos os seus valores ausentes preenchidos com zeros. Como alternativa, você poderia definir o argumento `inplace=True` para não precisar atribuir uma nova coluna no lugar da antiga:

```
cholera['imported_cases'].fillna(0, inplace=True)
```

O resultado será idêntico: todos os valores ausentes na coluna `'imported_cases'` serão substituídos por `0`.

A propósito, você pode até usar o ciclo `for` para substituir os valores ausentes. Tudo o que você precisa é criar uma lista que vai conter todas as colunas nas quais você deseja fazer a substituição e, em seguida, iterar sobre esses nomes para fazer a substituição:

```
# percorrendo os nomes das colunas e substituindo valores ausentes com 0s
columns_to_replace = ['imported_cases']

for col in columns_to_replace:
    cholera[col].fillna(0, inplace=True)
```

Os ciclos são úteis quando você tem várias colunas e deseja substituir os valores ausentes por um valor específico. Nesse caso, precisamos apenas substituir os valores ausentes na coluna `'imported_cases'`. Portanto, `cholera['imported_cases'].fillna(0, inplace=True)` é o método preferido.

Agora tente fazer por conta própria.

## Tarefas

### Tarefa 1

Escreva um código que some o número de valores ausentes em todas as colunas do conjunto de dados. Armazene o resultado na variável `mis_val` e imprima-o.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_raw.csv')

  

mis\_val \= df.isna().sum()\# Escreva seu código aqui

  

print(mis\_val)

Dica

Mostrar a soluçãoValidar

Teoria

### Remoção de linhas

Para remover linhas com valores ausentes em um DataFrame pandas, use o método `dropna()`. Esse método remove linhas com pelo menos um valor ausente. Você também pode especificar uma lista de colunas para o parâmetro `subset=` e ele vai remover as linhas com valores nulos apenas nessas colunas.

![](https://practicum-content.s3.amazonaws.com/resources/12.4_2PT_1690266265.png)

Funciona assim:

```
cholera = cholera.dropna(subset=['total_cases', 'deaths', 'case_fatality_rate'])
print(cholera)
```

region

index

country

total\_cases

imported\_cases

deaths

case\_fatality\_rate

notes

Asia

0

Afghanistan

33

0

1

3

NaN

Asia

1

India

385

0

3

0.7

NaN

Asia

2

Iran

634

625

4

0.6

NaN

Asia

3

Yemen

1032481

0

2261

0.2

NaN

Asia

4

China

14

0

0

0

NaN

Asia

5

Qatar

5

5

0

0

NaN

Asia

6

Malaysia

2

0

0

0

NaN

Asia

7

Nepal

7

0

0

0

NaN

Asia

8

UAE

12

12

0

0

NaN

Asia

9

Saudi Arabia

5

5

0

0

NaN

Asia

10

Singapore

3

3

0

0

NaN

Asia

11

Thailand

8

0

0

0

NaN

Asia

12

Philippines

134

0

2

1.5

NaN

Asia

13

South Korea

5

5

0

0

NaN

Asia

14

Japan

7

5

0

0

NaN

Americas

15

Haiti

13681

0

159

1.2

NaN

Americas

16

Dominican Republic

122

0

4

3.3

NaN

Americas

17

Canada

4

3

0

0

NaN

Americas

18

USA

11

9

0

0

NaN

Africa

19

All countries

179835

0

3220

1.8

no information on imported cases

Oceania

21

Australia

3

3

0

0

NaN

World

22

All countries

1227391

675

5654

0.5

NaN

E a linha com os dados sobre a Europa desapareceu.

Agora, removeremos toda a coluna `'notes'`, que consiste quase inteiramente em valores ausentes.

Usaremos o método `dropna()` novamente, mas desta vez adicionaremos outro argumento: `axis=`. Este argumento nos permite especificar se queremos remover linhas ou colunas. Se passarmos a string `'columns'` para `axis=`, ele removerá todas as colunas que tenham valores ausentes. Como `'notes'` é a única coluna com valores ausentes, podemos usar essa opção com segurança para removê-la.

```
cholera = cholera.dropna(axis='columns')
print(cholera)
```

region

index

country

total\_cases

imported\_cases

deaths

case\_fatality\_rate

Asia

0

Afghanistan

33

0

1

3

Asia

1

India

385

0

3

0.7

Asia

2

Iran

634

625

4

0.6

Asia

3

Yemen

1032481

0

2261

0.2

Asia

4

China

14

0

0

0

Asia

5

Qatar

5

5

0

0

Asia

6

Malaysia

2

0

0

0

Asia

7

Nepal

7

0

0

0

Asia

8

UAE

12

12

0

0

Asia

9

Saudi Arabia

5

5

0

0

Asia

10

Singapore

3

3

0

0

Asia

11

Thailand

8

0

0

0

Asia

12

Philippines

134

0

2

1.5

Asia

13

South Korea

5

5

0

0

Asia

14

Japan

7

5

0

0

Americas

15

Haiti

13681

0

159

1.2

Americas

16

Dominican Republic

122

0

4

3.3

Americas

17

Canada

4

3

0

0

Americas

18

USA

11

9

0

0

Africa

19

All countries

179835

0

3220

1.8

Oceania

21

Australia

3

3

0

0

World

22

All countries

1227391

675

5654

0.5

Esteja ciente de que se você tiver várias colunas com valores ausentes, `cholera.dropna(axis='columns')` eliminará todas elas. Isso normalmente não é o que queremos. Em vez disso, você pode usar o método `drop()` para controlar quais colunas deseja descartar. Isso é o que você deve fazer se quiser descartar apenas a coluna `'notes'` usando o método `drop()`:

```
cholera = cholera.drop(labels=['notes'], axis='columns')
```

Ambos `drop()` e `dropna()` suportam o argumento `inplace=`, que permite realizar a operação no local sem reatribuição. Aqui está um exemplo de como usar `drop()` para realizar uma substituição sem reatribuição:

```
cholera.drop(labels=['notes'], axis='columns', inplace=True)
```

Agora que limpamos os dados, eles serão mais confiáveis para a nossa análise.

Se você detectar valores ausentes é sempre uma boa ideia discuti-los com aqueles que fornecem seus dados. É melhor fazer uma pausa, descobrir porque temos dados ausentes, e depois tratar deles. E então você poderá continuar a sua análise.

E agora é sua vez de excluir linhas desnecessárias. Temos mais duas tarefas para você praticar.

### Tarefa 2

Escreva código para percorrer as colunas `genre`, `Artist` e `track` do DataFrame `df` e substitua todos os valores ausentes pela string `'no_info'`. A lista de colunas a serem substituídas está armazenada na variável `columns_to_replace`.

Após realizar as substituições, verifique novamente o número de valores ausentes usando `isna().sum()`

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_raw.csv')

  

columns\_to\_replace \= \['genre', 'Artist', 'track'\]

  

for col in columns\_to\_replace:

df\[col\].fillna('no\_info',inplace\=True)

  

print(df.top)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-02-26-889Z.md
### Última modificação: 2025-05-28 19:02:27

# Processamento de valores duplicados - TripleTen

Capítulo 5/7

Introdução ao pré-processamento de dados e análise inicial

# Processamento de valores duplicados

Nas lições anteriores, você aprendeu como renomear colunas e lidar com valores ausentes em seus dados.

Usamos três métodos para fazer isso: `isna()` para encontrar os valores ausentes, `fillna()` para preenchê-los com dados que podemos usar e `dropna()` para remover linhas com dados desnecessários.

<iframe class="base-markdown-iframe__iframe" id="player-aKgv9s8eJb4" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Processing Duplicate Values" width="640" height="360" src="https://www.youtube.com/embed/aKgv9s8eJb4?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fa22c72dd-eb1c-4860-857e-c5b0f3aaac27%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Nessa lição, vamos nos concentrar em como tratar duplicados.

Vamos usar o conjunto de dados `music_log_raw.csv` em sua versão bruta, sem nenhuma alteração nos nomes das colunas ou valores ausentes. Na vida real, não começaríamos tentando eliminar valores duplicados. O processo seria o seguinte: Primeiro renomeamos as colunas, depois procedemos à eliminação de NaNs e, finalmente, polimos nosso conjunto de dados ao remover os valores duplicados. Você aprenderá a remover duplicados nesta lição.

Ao final desta lição, você será capaz de usar os métodos `duplicated()`, `drop_duplicates()`, `reset_index()`**,** `unique()` e `nunique()` para trabalhar com dados duplicados.

## Procurando por casos óbvios

Lembra-se daqueles duplicados que vimos no começo do capítulo:

![](https://practicum-content.s3.amazonaws.com/resources/12.5_1PT_1690266314.png)

Esse conjunto de dados fornece um exemplo de uma linha totalmente duplicada, que pode ser facilmente detectada usando o método `duplicated()`. Esse método funciona de forma semelhante ao método `isna()`: ele retorna `True` se um valor for duplicado e `False` caso contrário.

Para contar esses valores, pode-se combinar com o método `sum()`:

```
import pandas as pd
df = pd.read_csv('/datasets/music_log_raw.csv')

print(df.duplicated().sum())
```

```
5313
```

Há muitos valores duplicados. Vamos descobrir como removê-los. Mas antes disso, tente encontrar alguns valores duplicados por conta própria na próxima tarefa.

## Tarefas

### Tarefa 1

No trecho de código abaixo, você encontrará a variável `pop`, que armazena um DataFrame filtrado contendo apenas músicas pop. Seu objetivo é determinar o número de duplicados neste DataFrame e armazenar este valor na variável `duplicates`. E, finalmente, imprima esta variável.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

9

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_raw.csv')

  

pop \= df\[df\['genre'\] \== 'pop'\]

  

duplicates \= pop.duplicated().sum()\# Escreva seu código aqui

  

print(duplicates)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-02-28-216Z.md
### Última modificação: 2025-05-28 19:02:28

# Agrupamento de dados - TripleTen

Capítulo 5/7

Introdução ao pré-processamento de dados e análise inicial

# Agrupamento de dados

Nas lições anteriores, você viu como lidar com problemas estruturais e de conteúdo em seus dados e como prepará-los para a próxima etapa de análise resolvendo esses problemas. Usamos os métodos `isna()`, `fillna()` e `dropna()` para lidar com dados ausentes e `duplicated()`, `drop_duplicates()`, `reset_index()`, `unique()` e `nunique()` para processar dados duplicados.

<iframe class="base-markdown-iframe__iframe" id="player-Sk_laFp4CKM" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Grouping Data" width="640" height="360" src="https://www.youtube.com/embed/Sk_laFp4CKM?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F00ac9cac-64b2-4586-bf0f-8a36b9df02ef%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

Agora estamos com tudo pronto para começar a obter insights do conjunto de dados.

Vamos supor que você tenha um conjunto de dados de vendas de telefones celulares, e queira determinar os modelos de telefone mais populares, como também a loja com as vendas mais altas. Você pode fazer isso facilmente usando **agrupamento**. Os agrupamentos permitem dividir os dados em grupos com base em determinados critérios (no nosso caso, modelos de telefone e lojas).

No final dessa lição, você será capaz de agrupar dados por critérios específicos, calcular o número de itens e o seu total em cada grupo. Primeiro assista ao vídeo abaixo e depois continue lendo para aprender tudo sobre isto!

### Quando agrupar dados?

Devemos utilizar o agrupamento quando os dados se enquadrarem, de maneira lógica, em grupos baseados em determinadas características e quando esses grupos forem relevantes para a tarefa em questão.

Por exemplo, se tivermos dados sobre todos os itens comprados em uma loja específica, podemos agrupar os dados por hora do dia para identificar os picos de tráfego. Ou podemos agrupar todas as compras por ID do cliente para calcular o tamanho médio da compra, sendo uma métrica importante no varejo.

### Etapas do agrupamento

1.  **Dividir**: primeiro, divida os dados em grupos, respeitando critérios determinados.
2.  **Aplicar**: em seguida, aplique os métodos de cálculo a cada grupo para, por exemplo, encontrar o número de elementos em um grupo com o método `count()` ou a soma de seus valores com `sum()`.
3.  **Combinar**: por fim, os resultados são armazenados em uma nova estrutura de dados

Essas são as etapas padrão do agrupamento, e, para nossa sorte, a biblioteca pandas tem métodos prontos para cada uma delas.

### Exemplo

Vamos analisar alguns dados sobre exoplanetas para ver como o agrupamento funciona na prática.

Cientistas já encontraram milhares de planetas fora do nosso sistema solar, utilizando telescópios espaciais que nos enviam imagens. Essas imagens são depois estudadas por analistas de dados. Iremos te mostrar como eles encontram planetas semelhantes à Terra.

A tabela `exoplanet` armazena dados de milhares de exoplanetas. Dê uma olhada nas primeiras 30 linhas:

```
import pandas as pd

exoplanet = pd.read_csv('/datasets/exoplanets.csv')

print(exoplanet.head(30))
```

```
            name      mass  radius  discovered
0    1RXS 1609 b  14.00000  1.7000        2008
1   2M 0122-24 b  20.00000  1.0000        2013
2   2M 2140+16 b  20.00000  0.9200        2010
3   2M 2206-20 b  30.00000  1.3000        2010
4       51 Peg b   0.47000  1.9000        1995
5       55 Cnc e   0.02703  0.1737        2004
6       CT Cha b  17.00000  2.2000        2008
7      CoRoT-1 b   1.03000  1.4900        2007
8     CoRoT-10 b   2.75000  0.9700        2010
9     CoRoT-11 b   2.33000  1.4300        2010
10    CoRoT-12 b   0.91700  1.4400        2010
11    CoRoT-13 b   1.30800  0.8850        2010
12    CoRoT-14 b   7.60000  1.0900        2010
13    CoRoT-15 b  63.40000  1.1200        2010
14    CoRoT-16 b   0.53500  1.1700        2010
15    CoRoT-17 b   2.43000  1.0200        2010
16    CoRoT-18 b   3.47000  1.3100        2011
17    CoRoT-19 b   1.11000  1.2900        2011
18     CoRoT-2 b   3.31000  1.4650        2007
19    CoRoT-20 b   4.24000  0.8400        2011
20    CoRoT-21 b   2.26000  1.3000        2011
21    CoRoT-22 b   0.06000  0.4354        2011
22    CoRoT-23 b   2.80000  1.0800        2011
23    CoRoT-24 b   0.01800  0.3300        2011
24    CoRoT-24 c   0.08800  0.4400        2011
25    CoRoT-25 b   0.27000  1.0800        2012
26    CoRoT-26 b   0.52000  1.2600        2012
27    CoRoT-27 b  10.39000  1.0070        2012
28    CoRoT-29 b   0.85000  0.9000        2012
29     CoRoT-3 b  21.66000  1.1900        2008
```

**Documentação (Fonte — o catálogo de exoplanetas em [exoplanet.eu](http://exoplanet.eu/catalog/)):**

-   `'name'` — o nome do exoplaneta
-   `'mass'` — a massa do exoplaneta em massas de Júpiter (uma unidade de massa muito utilizada na astronomia)
-   `'radius'` — seu raio em unidades de raio da Terra
-   `'discovered'` — o ano em que o exoplaneta foi descoberto

Primeiro, vamos ver como a descoberta de novos planetas mudou ao longo do tempo. Agruparemos os dados por ano de descoberta usando a coluna `'discovered'`. O conceito dividir-aplicar-combinar para esta tarefa é mostrado no diagrama a seguir:

![](https://practicum-content.s3.amazonaws.com/resources/12.6PT_1690267172.png)

-   Primeiro, dividimos os dados em grupos seguindo o critério ano de descoberta.
-   Em seguida, aplicamos o método `count()` para encontrar o número de elementos em cada grupo.
-   Por fim, salvamos o resultado como uma nova tabela na qual cada linha contém o ano e o número de exoplanetas descobertos.

A seguir, você verá como isso fica no código.

### Agrupamento em pandas

Na Pandas, agrupamos os dados usando o método `groupby()`, que faz o seguinte:

-   Assume o nome de uma coluna na qual os dados devem ser agrupados como um argumento. Este parâmetro é chamado `by=`. No nosso caso, vamos agrupar os dados pelo ano de descoberta.
-   Retorna um objeto de um tipo especial: **DataFrameGroupBy**. Este objeto corresponde a dados agrupados. Se você aplicar um método da biblioteca pandas nele, ele vai se transformar em uma nova estrutura de dados.

Vamos encontrar o número de exoplanetas agrupados por ano utilizando o método `count()`:

```
print(exoplanet.groupby(by='discovered'))
print() # vai aparecer como uma linha vazia entre duas impressões
print(exoplanet.groupby(by='discovered').count())
```

```
 <pandas.core.groupby.DataFrameGroupBy object at 0x7fc1e1ca3400>

           name  mass  radius
discovered                    
1995           1     1       1
1996           1     1       1
1999           1     1       1
2000           2     2       2
2001           1     1       1
2002           1     1       1
2004           7     7       7
2005           4     4       4
2006          10    10      10
2007          19    19      19
2008          23    23      23
2009          15    15      15
2010          57    57      57
2011          95    95      95
2012          73    73      73
2013          96    96      96
2014         105   105     105
```

Nem sempre é necessário especificar o argumento `by=`. Passar somente o nome da coluna funcionará exatamente da mesma maneira:

```
print(exoplanet.groupby('discovered'))
print() # vai aparecer como uma linha vazia entre duas impressões
print(exoplanet.groupby('discovered').count())
```

```
 <pandas.core.groupby.DataFrameGroupBy object at 0x7fc1e1ca3400>

           name  mass  radius
discovered                    
1995           1     1       1
1996           1     1       1
1999           1     1       1
2000           2     2       2
2001           1     1       1
2002           1     1       1
2004           7     7       7
2005           4     4       4
2006          10    10      10
2007          19    19      19
2008          23    23      23
2009          15    15      15
2010          57    57      57
2011          95    95      95
2012          73    73      73
2013          96    96      96
2014         105   105     105
```

O resultado do nosso código é um novo DataFrame. Podemos começar imediatamente a ver uma tendência: ao longo do período em questão, o número de exoplanetas descobertos tende a aumentar.

Caso precise comparar observações usando apenas um parâmetro, use o método no objeto DataFrameGroupBy e indique a coluna desejada, por exemplo, `'radius'` .

```
exo_number = exoplanet.groupby('discovered')['radius'].count()
print(exo_number)
```

```
discovered
1995      1
1996      1
1999      1
2000      2
2001      1
2002      1
2004      7
2005      4
2006     10
2007     19
2008     23
2009     15
2010     57
2011     95
2012     73
2013     96
2014    105
Name: radius, dtype: int64
```

Temos o número de exoplanetas com raios conhecidos, descobertos em cada ano.

Agora que agrupamos nossos dados, podemos verificar como, por exemplo, o raio médio dos exoplanetas descobertos mudou ao longo do tempo.

Para calcular a média, vamos encontrar a soma dos raios dos exoplanetas descobertos em um determinado ano e dividir pelo número dos planetas, ou seja, pelo valor que encontramos na etapa anterior.

Para encontrar a soma dos raios, use o método `sum()`:

```
exo_radius_sum = exoplanet.groupby('discovered')['radius'].sum()
print(exo_radius_sum)
```

```
discovered
1995     1.900000
1996     1.060000
1999     1.380000
2000     2.007000
2001     0.921000
2002     1.200000
2004     6.789700
2005     4.789000
2006    20.355000
2007    24.334600
2008    31.329000
2009    15.366794
2010    56.828660
2011    77.738974
2012    50.074507
2013    69.372100
2014    55.268000
Name: radius, dtype: float64
```

Em seguida, precisamos dividi-lo pelo número de exoplanetas descobertos cada ano.

```
exo_radius_mean = exo_radius_sum / exo_number
print(exo_radius_mean)
```

```
discovered
1995    1.900000
1996    1.060000
1999    1.380000
2000    1.003500
2001    0.921000
2002    1.200000
2004    0.969957
2005    1.197250
2006    2.035500
2007    1.280768
2008    1.362130
2009    1.024453
2010    0.996994
2011    0.818305
2012    0.685952
2013    0.722626
2014    0.526362
Name: radius, dtype: float64
```

A precisão da tecnologia está melhorando e exoplanetas cada vez menores estão sendo descobertos. Realizar análises com o método `groupby()` nos ajuda a desvendar vários tipos de padrões!

## Tarefas

### Tarefa 1

Vamos dar mais uma olhada no nosso conjunto de dados de música e agrupá-lo de maneira semelhante à que fizemos com os exoplanetas. É importante observar que o agrupamento é geralmente executado em um conjunto de dados tratado, que não possui NaNs, duplicados ou nomes de coluna não formatados. Portanto, não usaremos o conjunto de dados `music_log_raw.csv` original e, em vez disso, usaremos o conjunto de dados pré-processado com todos os problemas eliminados.

A primeira etapa é agrupar o conjunto de dados por `'genre'`. Quando o agrupamento for aplicado, armazene o resultado na variável `genre_groups` e imprima seu tipo.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_processed.csv')

  

genre\_groups \= df.groupby(by\='genre')

  

print(type(genre\_groups))

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Agora vamos passar para o ambiente de aplicação e aplicar métodos computacionais a cada grupo. Lembre-se de que, eventualmente, queremos calcular o tempo total. Quando queremos encontrar o tempo total, o método que precisamos aplicar deve nos dar uma soma como resultado. Aplique ao pré-código abaixo o método apropriado (quando for adicionada, a variável `genre_groups` vai armazenar um DataFrame com o resultado). Quando terminar, imprima a variável `genre_groups`.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_processed.csv')

  

genre\_groups \= df.groupby('genre').sum() \# aplique o método apropriado aqui

  

print(genre\_groups)

Dica

Mostrar a soluçãoValidar

### Tarefa 3

Nosso passo final é combinar os resultados. Não se esqueça, queremos calcular o tempo total que nossos ouvintes passaram ouvindo cada gênero. Temos uma coluna `'total_play'` em nosso conjunto de dados que contém exatamente o que precisamos. Precisamos passar isso para o nosso agrupamento: primeiro, selecione a coluna e então aplique um método que calcule o tempo total.

Faça isso e imprima o resultado final.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_processed.csv')

  

genre\_groups \= df.groupby('genre')\["total\_play"\].sum() \# Escreva seu código aqui

  

print(genre\_groups)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-02-31-230Z.md
### Última modificação: 2025-05-28 19:02:31

# Ordenamento de dados - TripleTen

Capítulo 5/7

Introdução ao pré-processamento de dados e análise inicial

# Ordenamento de dados

A primeira habilidade de manipulação de dados que você aprendeu foi o agrupamento de dados. Você conseguiu fazer isso usando a técnica dividir-aplicar-combinar, juntamente com os métodos `groupby()`, `count()` e `sum()`.

No entanto, para ser proficiente em manipulação de dados, você precisa aprender outra habilidade valiosa: ordenação. A ordenação é útil para muitas coisas. No final dessa lição, você será capaz de ordenar seus dados em ordem crescente ou decrescente usando o método `sort_values()`.

<iframe class="base-markdown-iframe__iframe" id="player--ke763c1ffw" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Sorting Data" width="640" height="360" src="https://www.youtube.com/embed/-ke763c1ffw?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fc8ad47f3-7846-41ef-9163-0ee536b9816f%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Teoria

O método `sort_values()` em pandas é uma ferramenta poderosa para ordenar valores. Ele consegue ordenar a tabela inteira ou grupos de linhas da tabela.

Este método é aplicado a um DataFrame e possui dois parâmetros:

-   `by=` — o nome ou nomes das colunas cujos valores são usados para ordenar as linhas do DataFrame.
-   `ascending=` — a ordem da ordenação. Seu valor padrão é `True`. Para ordenar os dados em ordem decrescente, defina esse parâmetro como `False`.

![](https://practicum-content.s3.amazonaws.com/resources/12.7PT_1690267678.png)

Se você aplicar o método `sort_values()` a uma única coluna, não haverá necessidade de definir o parâmetro `by=` (já que temos apenas uma coluna).

### Ordenando os dados sobre exoplanetas

Vamos supor que estamos especialmente interessados em exoplanetas de tamanho semelhante ao da Terra. Vamos ordenar os dados por raio em ordem crescente. Os planetas menores vão aparecer primeiro:

```
print(exoplanet.sort_values(by='radius').head(10))
```

```
             name     mass    radius  discovered
253   Kepler-37 b  0.01000  0.291431        2013
137  Kepler-102 b  0.01353  0.470774        2014
175  Kepler-138 b  0.00021  0.526818        2014
327   Kepler-62 c  0.01300  0.538027        2013
281   Kepler-42 d  0.00300  0.571654        2011
138  Kepler-102 c  0.00944  0.582863        2014
280   Kepler-42 c  0.00600  0.728579        2011
254   Kepler-37 c  0.03776  0.750996        2013
128      KOI-55 b  0.00140  0.762205        2011
279   Kepler-42 b  0.00900  0.784623        2011
```

Aqui está o código que ordena apenas a coluna radius (raio) e imprime os 10 primeiros resultados:

```
print(exoplanet['radius'].sort_values().head(10))
```

```
253    0.291431
137    0.470774
175    0.526818
327    0.538027
281    0.571654
138    0.582863
280    0.728579
254    0.750996
128    0.762205
279    0.784623
Name: radius, dtype: float64
```

Lembre-se de que um valor igual a 1 indica que o raio é o mesmo que o da Terra. Parece que temos muitos exoplanetas menores.

Podemos obter todos os exoplanetas com raios menores que o da Terra usando indexação lógica:

```
print(exoplanet[exoplanet['radius'] < 1])
```

```
             name     mass    radius  discovered
128      KOI-55 b  0.00140  0.762205        2011
129      KOI-55 c  0.00210  0.863085        2011
137  Kepler-102 b  0.01353  0.470774        2014
138  Kepler-102 c  0.00944  0.582863        2014
141  Kepler-102 f  0.01636  0.885503        2014
146  Kepler-106 b  0.01668  0.818250        2014
148  Kepler-106 d  0.02549  0.952757        2014
152  Kepler-107 d  0.01196  0.863085        2014
174  Kepler-131 c  0.02600  0.840668        2014
175  Kepler-138 b  0.00021  0.526818        2014
194   Kepler-20 e  0.00970  0.863085        2011
195   Kepler-20 f  0.04500  0.997592        2011
253   Kepler-37 b  0.01000  0.291431        2013
254   Kepler-37 c  0.03776  0.750996        2013
264  Kepler-406 c  0.00900  0.851876        2014
266  Kepler-408 b  0.01573  0.818250        2014
279   Kepler-42 b  0.00900  0.784623        2011
280   Kepler-42 c  0.00600  0.728579        2011
281   Kepler-42 d  0.00300  0.571654        2011
327   Kepler-62 c  0.01300  0.538027        2013
336   Kepler-68 c  0.00642  0.926976        2013
```

Agora digamos que nosso interesse está apenas nos valores para 2014, podemos usar a indexação lógica mais uma vez para extrair o seguinte:

```
print(exoplanet[exoplanet['discovered'] == 2014])
```

```
            name    mass     radius  discovered
42      GU Psc b  11.000  15.132015        2014
84    HAT-P-49 b   1.730  15.838176        2014
86    HAT-P-54 b   0.760  10.581202        2014
92     HATS-15 b   2.170  12.385834        2014
95      HATS-4 b   1.323  11.433078        2014
..           ...     ...        ...         ...
478    WASP-74 b   0.826  13.988707        2014
487    WASP-83 b   0.300  11.657256        2014
489  WASP-87 A b   2.210  15.524326        2014
491    WASP-89 b   5.900  11.657256        2014
493  WASP-94 A b   0.452  19.279308        2014

[105 rows x 4 columns]
```

Agora vamos combinar as duas condições para encontrar os planetas:

-   Planetas descobertos em 2014
-   Planetas menores que a Terra

```
# Exoplanetas menores que a Terra e descobertos em 2014
exo_small_14 = exoplanet[exoplanet['radius'] < 1]
exo_small_14 = exo_small_14[exo_small_14['discovered'] == 2014]
print(exo_small_14)
```

```
             name     mass    radius  discovered
137  Kepler-102 b  0.01353  0.470774        2014
138  Kepler-102 c  0.00944  0.582863        2014
141  Kepler-102 f  0.01636  0.885503        2014
146  Kepler-106 b  0.01668  0.818250        2014
148  Kepler-106 d  0.02549  0.952757        2014
152  Kepler-107 d  0.01196  0.863085        2014
174  Kepler-131 c  0.02600  0.840668        2014
175  Kepler-138 b  0.00021  0.526818        2014
264  Kepler-406 c  0.00900  0.851876        2014
266  Kepler-408 b  0.01573  0.818250        2014
```

Vamos ordenar o resultado por raio em ordem decrescente.

```
print(exo_small_14.sort_values(by='radius', ascending=False))
```

```
             name     mass    radius  discovered
148  Kepler-106 d  0.02549  0.952757        2014
141  Kepler-102 f  0.01636  0.885503        2014
152  Kepler-107 d  0.01196  0.863085        2014
264  Kepler-406 c  0.00900  0.851876        2014
174  Kepler-131 c  0.02600  0.840668        2014
146  Kepler-106 b  0.01668  0.818250        2014
266  Kepler-408 b  0.01573  0.818250        2014
138  Kepler-102 c  0.00944  0.582863        2014
175  Kepler-138 b  0.00021  0.526818        2014
137  Kepler-102 b  0.01353  0.470774        2014
```

O método `sort_values()` retorna um novo objeto, em vez de modificar o objeto atual. Portanto, se você quiser continuar trabalhando com o DataFrame ordenado, será necessário armazenar o resultado em uma variável. Você pode salvar o resultado no mesmo objeto:

```
exo_small_14 = exo_small_14.sort_values(by='radius', ascending=False)
```

O maior destes planetas, o Kepler-106 d, é quase tão grande quanto a Terra. Ele orbita a estrela Kepler-106, que faz parte da constelação de Cisne. Seria necessário fazer uma longa viagem para visitá-lo, uma vez que ele está a mais de 1.000 anos-luz de distância.

O interessante é que a ordenação pode ser feita para strings também, não só para números. Vamos conferir outro exemplo para ilustrar isso. Vamos criar um DataFrame simples com uma única coluna chamada `alphabet`:

```
import pandas as pd

df = pd.DataFrame(['b','a','c'], columns=['alphabet'])
```

Confira como é esse DataFrame se o imprimirmos:

```
print(df)
```

```
 alphabet
0       b
1       a
2       c
```

Como podemos ver, aqui estão as três primeiras letras do alfabeto, mas a ordem está errada. O que acontece se ordenarmos esse DataFrame pela ordem da coluna `alphabet`? Vamos descobrir:

```
df = df.sort_values(by='alphabet')
print(df)
```

```
 alphabet
1       a
0       b
2       c
```

Por causa da ordenação, nosso alfabeto está na ordem certa! A conclusão aqui é que a ordenação pode ser aplicada não apenas em colunas com valores numéricos, mas também com strings. Ao ordenar strings, podemos escolher a ordem alfabética ou uma ordem inversa se definirmos `ascending=` como `False`. Vamos praticar ordenações agora!

## Tarefas

### Tarefa 1

Na lição anterior, você agrupou nossos dados `music_log_processed.csv` por `'genre'` e calculou o tempo total que nossos ouvintes passaram ouvindo cada gênero. O resultado para cada `'genre'`, temos o tempo total ouvido. Ele está armazenado na variável `time_by_genre` no pré-código.

Agora, vamos ordenar os resultados por ordem decrescente e ver os 10 principais gêneros que nossos ouvintes mais ouviram. Faça isso e salve os resultados na variável `time_by_genre_sort`.

Observe que para esta tarefa, você não precisa especificar a coluna pela qual os dados precisam ser ordenados, já que há apenas uma coluna na variável `time_by_genre`.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

9

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_processed.csv')

  

time\_by\_genre \= df.groupby('genre')\['total\_play'\].sum()

  

time\_by\_genre\_sort \= time\_by\_genre.sort\_values( ascending\=False)\# Escreva seu código aqui

  

print(time\_by\_genre\_sort.head(10))

Dica

Mostrar a soluçãoValidar

Se você tiver algumas perguntas complicadas, como, por exemplo, qual planeta tem um raio mais parecido ao da Terra, a ordenação trará a resposta bem rápido! Com uma ajudinha de nossos amigos da indexação lógica!

E agora que resolvemos todos os problemas e temos os dados agrupados e ordenados conforme necessário, vamos praticar mais um pouco! Isso é o que vamos fazer na próxima lição!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-02-32-548Z.md
### Última modificação: 2025-05-28 19:02:32

# Quiz do capítulo - TripleTen

Capítulo 5/7

Introdução ao pré-processamento de dados e análise inicial

# Quiz do capítulo

Você aprendeu muitas técnicas bem úteis neste sprint e, em breve, vai colocar em prática esse conhecimento no projeto. Estamos muito entusiasmados com o seu progresso. Agora, refresque sua memória respondendo a algumas perguntas.

Pergunta

Como o método `rename()` pode ser usado para renomear os nomes das colunas? Selecione todas as opções que se aplicam:

Escolha quantas quiser

`df = df.rename(columns={'old name':'new name'})`

Sim, este é um dos caminhos!

`df.rename(columns={'old name':'new name'}, inplace=True)`

Isso é definitivamente algo que fará o trabalho.

`df = df.rename(columns=['old name', 'new name'])`

Usando um loop `for` e métodos de string.

Isso também está correto!

Excelente!

Pergunta

Como você verificaria o número de valores ausentes em um conjunto de dados? Selecione todas as opções que se aplicam:

Escolha quantas quiser

`df.isna().sum()`

Sim, este é um dos caminhos!

`df.isnull().sum()`

Exatamente!

Rolando a tabela e procurando por NaNs

Trabalho maravilhoso!

Pergunta

Para eliminar todos os NaNs, existem várias abordagens que podem ser adotadas. Aqui estão algumas opções:

Escolha quantas quiser

`df['my column'].fillna(0, inplace=True)`

Essa abordagem substitui todos os NaNs na `'my column'` por 0s.

`df['my column'] = df['my column'].fillna(0)`

Essa abordagem substitui todos os NaNs na `'my column'` por 0s e substitui a coluna antiga pela nova.

`df['my column'] = df['my column'].drop_duplicates()`

`df['my column'] = df['my column'].dropna()`

Essa abordagem elimina todas as linhas com NaNs. É uma opção em alguns casos.

Fantástico!

Pergunta

Como podemos verificar se há duplicados em um dataframe e ver quantas delas existem?

`df.duplicated().sum()`

Este método nos dá o número de duplicados no dataframe. Para descartar duplicados, você precisa usar o método `drop_duplicates()`.

`df.duplicated().count()`

`df.duplicated()`

Fantástico!

Pergunta

O que é verdade sobre os métodos `unique()` e `nunique()`?

Escolha quantas quiser

Eles não retornam o mesmo.

Esta afirmação está correta. Há uma diferença entre os dois métodos.

`unique()` retorna os valores únicos, enquanto `nunique()` retorna o número de valores únicos.

Esta afirmação está correta.

`unique()` retorna os valores únicos, enquanto `nunique()` retorna os valores não únicos.

Trabalho maravilhoso!

Pergunta

Quais são os estágios de agrupamento que existem?

Escolha quantas quiser

Dividir:

A primeira etapa onde aplicamos o método `groupby()` passando na coluna que queremos usar para o agrupamento.

Analisar:

Aplicar:

A segunda etapa. Aqui aplicamos métodos aos grupos como `count()` ou `sum()`, por exemplo.

Separar:

Combinar:

A última etapa onde especificamos a coluna de nosso interesse e armazenamos um resultado em uma nova estrutura de dados, como um DataFrame.

Seu entendimento sobre o material é impressionante!

"Treine a si mesmo a deixar partir tudo que teme perder." – Yoda.

Você está com tudo pronto para o projeto!

"Faça. Ou não faça. Tentativa não há." - Yoda também.

Temos certeza de que você vai conseguir e passar com distinção!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-02-33-832Z.md
### Última modificação: 2025-05-28 19:02:34

# Conclusão - TripleTen

Capítulo 5/7

Introdução ao pré-processamento de dados e análise inicial

# Conclusão

Você lutou e venceu sua primeira batalha com um dos problemas mais comuns dos dados - valores ausentes e duplicados. Muito bem! Saber lidar com valores ausentes e duplicados, bem como agrupar e ordenar os dados, vai ajudar muito em suas análises futuras.

Agora você é capaz de:

-   Usar o método `rename()` para renomear colunas Se ainda tiver dúvidas, revise a lição [Renomear colunas](https://tripleten.com/trainer/data-analyst/lesson/568102ce-d5eb-40ca-92eb-cae0600ad898/).
-   Usar `isnull()`, `fillna()` e `dropna()` para identificar, preencher e descartar valores ausentes Se ainda tiver dúvidas, revise a lição sobre [Processamento de valores ausentes](https://tripleten.com/trainer/data-analyst/lesson/f2d7d9e9-1000-4245-98c5-63201cc13515/).
-   Usar os métodos `duplicated()`, `reset_index()`, `unique()` e `nunique()` para trabalhar com valores duplicados Se ainda tiver dúvidas, revise a lição [Processamento de valores duplicados](https://tripleten.com/trainer/data-analyst/lesson/5559b5f0-0f79-4e99-8179-81aa5f0ebd66/).
-   Explicar o que é agrupamento, os estágios do agrupamento e como agrupar em pandas Se ainda tiver dúvidas, revise a lição [Agrupamento de dados](https://tripleten.com/trainer/data-analyst/lesson/791a6d40-0eea-4582-a3c4-f2ba5b68d7e2/).
-   Ordenar dados usando o método `sort_values()` Se ainda tiver dúvidas, revise a lição [Ordenamento de dados](https://tripleten.com/trainer/data-analyst/lesson/47e40878-6f0c-404c-868e-3f1986faa5ef/). ![](https://practicum-content.s3.amazonaws.com/resources/12.10PT_1690269615.png)

### 💡

A utilidade dos estudos de caso e a estatística descritiva vão muito além da análise de dados musicais. Eles podem ser utilizados para entender toda a sociedade. [Veja como 200 países e 200 anos de história podem ser resumidos em 4 minutos](https://www.youtube.com/watch?v=jbkSRLYSojo) utilizando abordagens parecidas com as quais você já aprendeu e ampliadas com visualizações de dados, os quais você irá estudar em materiais futuros.

### O que vem a seguir?

Você completou o trabalho deste sprint! Muito bem! Agora você já é capaz de começar seu primeiro projeto independente. Ele será mais difícil do que os que você fez até agora, mas não se preocupe — só assim você vai aprender! Estaremos aqui para te ajudar.

Você vai concluir seu projeto em um **Jupyter notebook** na nossa plataforma. Você sabe o que é isso? Não se preocupe! Vamos te mostrar.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-02-35-970Z.md
### Última modificação: 2025-05-28 19:02:36

# Seu segundo projeto - TripleTen

Capítulo 6/7

Projeto Final

# Seu segundo projeto

Você passou por um longo caminho desde o início da sua jornada no Python. Agora você está pronto para aplicar suas novas habilidades em um pequeno projeto de análise de dados.

Aqui, como em todos os projetos da TripleTen, você receberá dados e terá que analisá-los para resolver um problema de negócios real. Seus projetos serão feitos em notebooks Jupyter.

Nos últimos sprints o trabalho será mais independente. Desta vez, para ajudar, forneceremos um caderno Jupyter contendo:

-   A estrutura do projeto
-   Diretrizes para análises
-   Conclusões

### 💡

Ao finalizar o projeto, envie-o para revisão. Você vai receber comentários em até 48 horas. Faça mudanças conforme o indicado pelo revisor e entregue uma versão atualizada. Você pode receber novos comentários. É normal ter várias versões e fazer mudanças antes que o projeto seja aceito.

Quando o revisor aceitar o seu projeto, ele será considerado concluído.

## Diretrizes

Você vai encontrar uma descrição detalhada do projeto na próxima lição. Aqui nós apenas discutiremos os pontos principais:

-   **Isso é a sua pesquisa**. Se você encontrou algo interessante nos dados, adicione suas observações no notebook. Apenas se certifique de seguir as instruções, testar suas hipóteses (isso faz parte de seu projeto, falaremos sobre isso daqui a pouco) e cumprir o prazo final.
-   **O formato importa**. O estilo de escrita é fundamental. Preste atenção à formatação de cabeçalhos e conclusões e mantenha a consistência em todo o seu documento. Lembre-se que o trabalho de um analista não é apenas programar; também é preciso comunicar suas descobertas a outras pessoas.
-   **Comentários ajudam na revisão do projeto**. Não remova os comentários que você vê nas células de código. Lembre-se de que todos os comentários começam com o sinal `#` e incluem algum comentário textual após o sinal. Em vez disso, apenas adicione os seus comentários. Isso vai ajudar o revisor a ler rapidamente e entender a sua solução e sugerir melhorias.
-   **Você já é um sucesso**. Você já concluiu muitas tarefas e demonstrou que está com tudo pronto para esse trabalho. É normal achar alguns detalhes difíceis de entender, já que você está apenas começando a explorar sua nova profissão. Se você tiver alguma dúvida, entre em contato com seu gerente de sucesso de carreira.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-02-37-270Z.md
### Última modificação: 2025-05-28 19:02:37

# Sprint 2 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 6/7

Projeto Final

# Descrição do projeto

O trabalho de um analista é analisar dados para obter percepções valiosas dos dados e tomar decisões fundamentadas neles. Esse processo consiste em várias etapas: visão geral de dados, pré-processamento de dados e testes de hipóteses.

Hipóteses são suposições feitas sobre a população com base em uma amostra de dados. Essas suposições são testadas usando métodos estatísticos para determinar se elas são verdadeiras ou falsas. Testar uma hipótese envolve fazer uma declaração sobre a população, coletar dados e usar métodos estatísticos para testar essa declaração.

Sempre que fazemos análises, nós temos que formular hipóteses que depois poderemos testar. Às vezes nós aceitamos essas hipóteses; outras vezes, nós as rejeitamos. Para tomar as decisões certas, um negócio deve ser capaz de entender se está fazendo as suposições certas ou não.

## Descrição do projeto

No contexto deste projeto, você vai testar uma hipótese relacionada às preferências musicais de duas cidades. Para isso, você vai analisar os dados de um serviço de streaming de música online para testar a hipótese apresentada abaixo e comparar o comportamento dos usuários nessas duas cidades.

Isso envolverá analisar os dados de um serviço de streaming real para comparar o comportamento dos usuários em Springfield e Shelbyville. O projeto é dividido em três etapas, cada uma das quais tem seus objetivos específicos.

Na Etapa 1, você fornecerá uma visão geral dos dados e escreverá suas observações. Na Etapa 2, você fará o pré-processamento dos dados, limpando-os. Finalmente, na Etapa 3, você testará a hipótese dando os passos de programação necessários para testar cada declaração e comentar seus resultados nos blocos apropriados.

Depois de concluir essas etapas, você será capaz de obter percepções valiosas dos dados e tomar decisões baseadas neles.

## Hipótese

Para este projeto, reunimos os requisitos e preparamos uma hipótese que precisamos confirmar ou rejeitar.

Ao testar hipóteses, é importante perceber que elas podem ser totalmente aceitas, parcialmente aceitas, parcialmente rejeitadas ou totalmente rejeitadas.

Quando uma hipótese é totalmente aceita, isso significa que os resultados do teste confirmam a declaração feita sobre a população sem quaisquer dúvidas.

Se ela for parcialmente aceita, isso significa que os resultados confirmam a declaração até certo ponto, mas não suficientemente para aceitá-la totalmente.

Por outro lado, se uma hipótese for totalmente rejeitada, isso significa que os resultados do teste não confirmam a declaração feita sobre a população.

Finalmente, uma hipótese também pode ser parcialmente rejeitada se os dados indicarem que é falsa, mas você não puder rejeitá-la totalmente. Quando interpretamos os resultados de um teste de hipótese, é importante considerar todas essas diferentes possibilidades.

Aqui está a hipótese que precisamos aceitar ou rejeitar:

1.  A atividade dos usuários é diferente dependendo do dia da semana e da cidade.

## Dicionário de dados

Os dados estão armazenados no arquivo `/datasets/music_project_en.csv`. Alternativamente, você pode fazer o download [aqui](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/music_project_en.csv).

**Descrição das colunas:**

-   `'userID'` — identifica cada usuário univocamente
-   `'Track'` — título da música
-   `'artist'` — nome do artista
-   `'genre'` — gênero da música
-   `'City'` — cidade do usuário
-   `'time'` — a hora do dia em que uma música foi tocada (HH:MM:SS)
-   `'Day'` — dia da semana

## Instruções para completar o projeto

Preparamos para você um modelo de notebook, em que você pode escrever seu código e descrever suas análises. Para completar o projeto, preencha cada célula de código no modelo e edite as células Markdown nos casos em que o modelo solicitar que você explique seus resultados.

É uma boa prática sempre incluir uma introdução que descreve brevemente seus objetivos e uma conclusão que resume seus resultados na forma de células Markdown.

Antes de você começar, vamos revisar as três etapas do projeto novamente:

**Etapa 1:** visão geral dos dados. O notebook tem células prontas com instruções sobre que tipo de código escrever, bem como blocos de texto onde você pode escrever suas observações.

**Etapa 2:** pré-processamento de dados. Nesta etapa, você arrumará nomes de colunas e removerá valores ausentes e duplicados. Siga o esquema fornecido no notebook e certifique-se de escrever suas observações no final desta seção.

**Etapa 3:** teste da hipótese. Esta é a parte principal do seu projeto. Dê os passos de codificação necessários para testar cada declaração e comentar seus resultados nos blocos apropriados. Finalmente, resuma todo o projeto na seção "Conclusões".

No vídeo abaixo, abordamos alguns pontos essenciais nos quais você deve prestar atenção enquanto trabalha no seu projeto.

<iframe class="base-markdown-iframe__iframe" id="player-Zu3x9YhsgXg" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Your First Project NM" width="640" height="360" src="https://www.youtube.com/embed/Zu3x9YhsgXg?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F20c53b4a-81d8-4d18-bc38-d36d30ae6fba%2Ftask%2F923aaa78-4ba6-48ca-ae53-18bff281b55e%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

### Checklist

Antes de enviar o projeto, certifique-se que:

-   você concluiu todas as etapas do projeto e preencheu todas as células
-   a estrutura lógica do projeto segue a estrutura do modelo.

![](https://practicum-content.s3.amazonaws.com/resources/14.3PT_1690218323.png)

Quando a revisão do projeto for concluída, você vai receber um e-mail no seu endereço de e-mail cadastrado na plataforma. Você vai precisar abrir o projeto e fazer correções, se necessário.

![](https://practicum-content.s3.amazonaws.com/resources/Bildschirmfoto_2024-01-16_um_16.25.26_pt_1706170870.png)

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-94290213-9a8a-4f2b-83b8-2a200493f6b9.containerhub.tripleten-services.com/doc/tree/20c53b4a-81d8-4d18-bc38-d36d30ae6fba.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-02-42-241Z.md
### Última modificação: 2025-05-28 19:02:42

# Feedback do Sprint 2 - TripleTen

Capítulo 7/7

Conclusão

# Compartilhe sua opinião

**Olá!**

Você acabou de concluir um sprint e enviar seu projeto. Parabéns por alcançar este marco! 🚀

Agora é o momento perfeito para dar seu feedback sobre o sprint. Sua opinião vai nos ajudar a aprimorar ainda mais o processo de aprendizagem.

Este questionário curto vai levar apenas 2 minutos. Todas as respostas vão ser confidenciais: não vamos compartilhá-las de forma que identifique você.

Agradecemos por nos ajudar a evoluir com você!

Como você avalia a experiência com os materiais didáticos, as tarefas e os projetos neste sprint?

1

2

3

4

5

Respostas: 1 – Muito insatisfatória; 5 – Muito satisfatória.

Avançar

1 — 9

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-02-43-575Z.md
### Última modificação: 2025-05-28 19:02:43

# Conclusão - TripleTen

Capítulo 7/7

Conclusão

# Conclusão

Parabéns! Você chegou ao final da segunda parte do curso de Python básico do programa. Você aprendeu muita coisa! Agora você deve se sentir confortável trabalhando com Python, a ferramenta principal que usaremos no resto do curso.

![](https://practicum-content.s3.amazonaws.com/resources/Artboard_1-100_1_1698402733.jpg)

O conhecimento prático de Python é essencial para alguém que planeja construir sua carreira na área de dados. É claro que ainda há muita coisa a ser aprendida, mas acabamos de construir uma base sólida para a futura aprendizagem.

Vamos recapitular brevemente o que aprendemos até agora. Você deve se sentir confortável trabalhando com:

-   Diferentes **tipos de dados**, como `str`, `float` e `int`. Você também está familiarizado com as **estruturas de dados** de Python, como `list` e `dict`
-   **Variáveis** locais e globais, bem como **funções** e seus parâmetros
-   `pandas`, a biblioteca principal para a manipulação e tratamento de dados

No próximo curso, você vai aprender mais sobre a manipulação de dados, uma ferramenta muito importante para examinar seus dados visualmente e encontrar padrões neles.

Até a próxima!

Além disso, é importante mencionar que vamos mudar a abordagem nos próximos sprints: em vez do formato de vídeos, teremos um formato de texto. Vídeos são ótimos, mas, como profissional de dados, você vai trabalhar muito mais com informações textuais, então é importante se familiarizar com esse formato.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-04-52-737Z.md
### Última modificação: 2025-05-28 19:04:53

# Introdução - TripleTen

Capítulo 1/11

Introdução

# Introdução

Boas-vindas ao nosso sprint sobre manipulação de dados! Ao longo deste sprint, você vai trabalhar com vários conjuntos de dados novos para desenvolver suas habilidades neste campo.

A **manipulação de dados**, também conhecida como "data wrangling" ou "data munging", é a preparação de dados para fins de processos, como análises.Esse processo geralmente envolve eliminar defeitos, agregar e visualizar os dados. Em geral, é tudo o que é necessário para preparar os dados para outros possíveis usos.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_22_1690870846.png)

Além disso, a manipulação de dados costuma ser acompanhada pela **análise exploratória de dados (AED)**. A AED envolve a realização de investigações iniciais em dados para encontrar padrões, entender tendências ou verificar suposições. Normalmente, isso envolve explorar e resumir dados visualmente e de forma agregada para tirar conclusões.

No sprint anterior, você aprendeu os fundamentos da preparação de dados, que são essenciais tanto para a manipulação de dados quanto para a AED. Neste sprint, abordaremos habilidades essenciais para que você continue sua jornada com confiança. Especificamente, você vai aprender:

-   Métodos mais avançados de leitura e processamento de dados;
-   Como identificar e lidar com dados ausentes ou duplicados;
-   Estratégias mais avançadas para filtragem de dados;
-   Técnicas de visualização de dados para seu próprio entendimento e para apresentações para uma audiência;
-   Como criar novas colunas processando colunas de dados brutos;
-   Como agrupar e combinar dados de tabelas diferentes de diversas maneiras.

No final do sprint, você vai concluir um projeto independente no qual usará todas as suas novas habilidades para responder a algumas perguntas interessantes sobre um extenso conjunto de dados sobre compras no Instacart.

Este sprint abrange muito material, que você usará no restante do programa e em toda sua carreira como profissional de dados, e serão necessárias de 25 a 35 horas para concluí-lo. Portanto, não se apresse e estude com calma. Se você sentir que está ficando para trás ou tendo dificuldades com um tópico, entre em contato com nossa equipe de orientação.

Neste sprint, você irá continuar desenvolvendo estas habilidades:

![](https://practicum-content.s3.amazonaws.com/resources/Pre-processamento_de_Dados_1713354714.png)

![](https://practicum-content.s3.amazonaws.com/resources/Analise_Exploratoria_1713354747.png)

![](https://practicum-content.s3.amazonaws.com/resources/Narrativa_de_Dados_1713354784.png)

![](https://practicum-content.s3.amazonaws.com/resources/Analise_Estatistica_1713354794.png)

Boa sorte e esperamos que goste do programa!

![](https://practicum-content.s3.amazonaws.com/resources/1.2_PT_1707138752.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-04-54-054Z.md
### Última modificação: 2025-05-28 19:04:54

# Introdução - TripleTen

Capítulo 2/11

Leitura e visualização de dados

# Introdução

Boas-vindas ao primeiro capítulo do sprint sobre manipulação de dados!

Neste capítulo, você vai aprender mais sobre a leitura de dados usando a biblioteca Pandas e como ter uma ideia geral de como são seus dados. Essas coisas soam familiares para você? Felizmente, você já teve a chance de aprender sobre elas durante o primeiro sprint. Agora é hora de avançar nosso conhecimento!

No final do capítulo, você será capaz de:

-   ler arquivos CSV com formatações diferentes;
-   ler dados armazenados em diferentes planilhas de arquivos Excel;
-   visualizar a estrutura e as propriedades gerais dos dados;
-   obter estatísticas resumidas básicas sobre as características numéricas dos dados.

É sempre importante dar uma olhada inicial na estrutura de um novo conjunto de dados, porque isso ajuda a desenvolver boas perguntas que servirão como ponto de partida quando você começar a explorar os dados.

Você vai precisar de 1,5 a 2 horas para concluir este capítulo.

Então, para começar, vamos mergulhar na leitura de dados de diferentes fontes.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_23_1690870921.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-04-55-408Z.md
### Última modificação: 2025-05-28 19:04:55

# Como resolver problemas com arquivos CSV - TripleTen

Capítulo 2/11

Leitura e visualização de dados

# Como resolver problemas com arquivos CSV

A primeira etapa para trabalhar com dados é acessá-los. No sprint sobre Python básico, você aprendeu a ler arquivos CSV com formatação padrão em DataFrames da pandas usando o método `read_csv()`. Nesta lição, você vai aprender a ler dados de arquivos CSV que não estão em conformidade com o formato padrão que vimos no Python Básico.

Video

Assista ao vídeo abaixo e depois continue lendo para aprender mais.

<iframe class="base-markdown-iframe__iframe" id="player-pOiYV-0SW7g" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Fixing Issues with CSV Files" width="640" height="360" src="https://www.youtube.com/embed/pOiYV-0SW7g?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F12a6a7e3-0629-4d25-b7a7-a5f64bc6323a%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Lembre-se de que CSV é uma sigla do inglês que significa **valores separados por vírgula**. No entanto, os arquivos CSV não precisam usar apenas vírgulas como **delimitadores**; qualquer caractere pode ser usado. Por exemplo, valores separados por tab são outro formato comum. Esses são arquivos com os valores delimitados por tabulações. Às vezes, eles podem ser encontrados como arquivos `.tsv` ou `.tab` (também conhecidos como arquivos TSV), além de `.csv`.

Vamos dar uma olhada nos dados em um arquivo CSV chamado `gpp_modified.csv`. Esse conjunto de dados é um subconjunto sobre [usinas elétricas ao redor do globo](https://datasets.wri.org/dataset/globalpowerplantdatabase) _(os materiais estão em inglês)_ disponível publicamente no site da instituição de pesquisa [World Resources Institute](https://www.wri.org/). Os dados em `gpp_modified.csv` foram modificados do conjunto de dados original para reduzir a quantidade de informações e incorporar formatação fora do padrão para você poder praticar nesta lição.

Vamos tentar ler o arquivo usando `read_csv()` sem argumentos adicionais e imprimir as primeiras 5 linhas:

```
import pandas as pd

data = pd.read_csv('/datasets/gpp_modified.csv')

print(data.head())
```

```
  Afghanistan|Kajaki Hydroelectric Power Plant Afghanistan|33   0|32   322|65   119|Hydro|
0                                 Afghanistan|Kandahar DOG|10   0|31    67|65   795|Solar|
1                                 Afghanistan|Kandahar JOL|10   0|31   623|65   792|Solar|
2           Afghanistan|Mahipar Hydroelectric Power Plant ...   0|34   556|69  4787|Hydro|
3           Afghanistan|Naghlu Dam Hydroelectric Power Pla...   0|34   641|69   717|Hydro|
4           Afghanistan|Nangarhar (Darunta) Hydroelectric ...  55|34  4847|70  3633|Hydro|
```

Opa! O que diabos está acontecendo aqui? Não recebemos nenhum erro do Python, mas o DataFrame está uma bagunça. Os nomes das colunas parecem com os próprios dados, e é difícil de entender quais são os dados. Essa é uma ótima pista de que devemos abrir o arquivo em um editor de texto e dar uma olhada mais de perto.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled-5_1690978419.png)

Pergunta

Qual caractere parece ser o delimitador desse arquivo CSV?

`,`

`-`

`;`

`|`

Sim, parece que cada coluna é separada por uma barra vertical.

Fantástico!

Você acabou de ver como os valores em um arquivo de texto podem ser delimitados com outros caracteres além de vírgulas. Você se lembra dos arquivos `.tsv` que mencionamos mais cedo? Os valores nesses arquivos são separados por tabulações ou caracteres `\\t` na terminologia da ciência da computação.

## O parâmetro `sep=`

O arquivo definitivamente tem vírgulas, mas parece que o delimitador é, na verdade, o caractere `|`. Podemos tratar disso quando lemos os dados usando o parâmetro `sep=` em `read_csv()`. O argumento padrão para `sep=` é `','`, o que explica a estrutura de colunas que observamos antes. Vamos usar `'|'` como argumento e ver como isso afeta o DataFrame:

```
import pandas as pd

data = pd.read_csv('/datasets/gpp_modified.csv', sep='|')

print(data.head())
```

```
   Afghanistan       Kajaki Hydroelectric Power Plant Afghanistan   33,0   32,322   65,119  Hydro  Unnamed: 6
0  Afghanistan                                       Kandahar DOG   10,0    31,67   65,795  Solar         NaN
1  Afghanistan                                       Kandahar JOL   10,0   31,623   65,792  Solar         NaN
2  Afghanistan      Mahipar Hydroelectric Power Plant Afghanistan   66,0   34,556  69,4787  Hydro         NaN
3  Afghanistan   Naghlu Dam Hydroelectric Power Plant Afghanistan  100,0   34,641   69,717  Hydro         NaN
4  Afghanistan  Nangarhar (Darunta) Hydroelectric Power Plant ...  11,55  34,4847  70,3633  Hydro         NaN
```

Outro exemplo envolve a leitura de um arquivo `.tsv`. Para fazer isso, precisamos definir o parâmetro `sep=` como `'\\t'`. Vamos retornar ao conjunto de dados sobre usinas e continuar trabalhando com ele. Agora, sim, isso está começando a se parecer com um DataFrame adequado! No entanto, ainda temos algumas questões a resolver.

## Os parâmetros `header=` e `names=`

Você deve ter notado que os nomes das colunas não parecem corretos. Parece que a primeira linha de dados foi usada para os nomes das colunas. Isso não é bom, pois não queremos perder esses dados na primeira linha e precisamos de bons nomes para as colunas para ter maior clareza no nosso trabalho.

Se você conferir de novo as linhas que mostramos do arquivo CSV, verá que a primeira linha do arquivo tem apenas dados. Em outras palavras, o arquivo não possui um **cabeçalho**, a linha no início do arquivo que contém os nomes das colunas. Felizmente, `read_csv()` pode manipular arquivos sem cabeçalhos se usarmos o argumento correto para o parâmetro `header=`.

O argumento padrão é `header='infer'`, que diz à biblioteca pandas para tentar adivinhar quais são os nomes das colunas. Nesse caso, a pandas achou que a primeira linha de dados tinha os nomes das colunas. Podemos corrigir isso passando `header=None` assim:

```
import pandas as pd

data = pd.read_csv('/datasets/gpp_modified.csv', sep='|', header=None)

print(data.head())
```

```
             0                                                  1      2        3        4      5    6    
0  Afghanistan       Kajaki Hydroelectric Power Plant Afghanistan   33,0   32,322   65,119  Hydro  NaN
1  Afghanistan                                       Kandahar DOG   10,0    31,67   65,795  Solar  NaN
2  Afghanistan                                       Kandahar JOL   10,0   31,623   65,792  Solar  NaN
3  Afghanistan      Mahipar Hydroelectric Power Plant Afghanistan   66,0   34,556  69,4787  Hydro  NaN
4  Afghanistan   Naghlu Dam Hydroelectric Power Plant Afghanistan  100,0   34,641   69,717  Hydro  NaN
```

Agora todos os dados estão nos devidos lugares. Como o arquivo não tem nomes de coluna e não especificamos nenhum, a pandas usa números inteiros consecutivos (`0`, `1`, `2` ...) para nomear as colunas. Seria uma má prática continuarmos com o trabalho sem dar nomes descritivos às nossas colunas. Para fazer isso, você poderia examinar a [documentação](https://datasets.wri.org/dataset/globalpowerplantdatabase) (a documentação está em inglês) do conjunto de dados para descobrir o que as colunas representam. Nesse caso, vamos fornecer uma lista de bons nomes para as colunas.

Poderíamos renomear as colunas depois de ler os dados usando o atributo `columns` do DataFrame ou o método `rename()`, mas também podemos definir os nomes enquanto lemos os dados usando o parâmetro opcional `names=` em `read_csv()`. Se você passar uma lista de nomes de colunas para o parâmetro `names=`, a pandas atribuirá esses nomes às colunas na ordem em que eles aparecem na lista.

```
import pandas as pd

column_names = [
    'country',
    'name',
    'capacity_mw',
    'latitude',
    'longitude',
    'primary_fuel',
    'owner'
]
data = pd.read_csv('/datasets/gpp_modified.csv', sep='|', header=None, names=column_names)

print(data.head())
```

```
       country                                              name capacity_mw latitude longitude primary_fuel owner     
0  Afghanistan      Kajaki Hydroelectric Power Plant Afghanistan        33,0   32,322    65,119        Hydro   NaN
1  Afghanistan                                      Kandahar DOG        10,0    31,67    65,795        Solar   NaN
2  Afghanistan                                      Kandahar JOL        10,0   31,623    65,792        Solar   NaN
3  Afghanistan     Mahipar Hydroelectric Power Plant Afghanistan        66,0   34,556   69,4787        Hydro   NaN
4  Afghanistan  Naghlu Dam Hydroelectric Power Plant Afghanistan       100,0   34,641    69,717        Hydro   NaN
```

Agora temos um DataFrame com o qual podemos trabalhar de fato. No entanto, agora que você sabe o que as colunas numéricas representam, você deve ter notado mais um problema que precisamos corrigir.

## O parâmetro `decimal=`

Os valores para capacidade em megawatt, latitude e longitude usam vírgulas em vez de pontos para o [separador decimal](https://pt.wikipedia.org/wiki/Separador_decimal). É a convenção padrão no Brasil usar a vírgula como separador de decimal, mas o Python exige que você use o ponto como separador de decimal em números de ponto flutuante. Se você verificar os tipos de dados dessas colunas, verá que a Pandas lê os dados como linhas.

Em `read_csv()` podemos usar o parâmetro `decimal=` e passar `','` como argumento. Por padrão, `read_csv()` usa `decimal='.'`, razão pela qual não reconheceu os valores numéricos em nosso conjunto de dados como números de ponto flutuante.

Resumindo e concluindo, vamos ler o conjunto de dados uma última vez:

```
import pandas as pd

column_names = [
    'country',
    'name',
    'capacity_mw',
    'latitude',
    'longitude',
    'primary_fuel',
    'owner'
]
data = pd.read_csv(
    '/datasets/gpp_modified.csv',
    sep='|',
    header=None,
    names=column_names,
    decimal=',',
)

print(data.head())
```

```
       country                                              name capacity_mw latitude longitude primary_fuel owner      
0  Afghanistan      Kajaki Hydroelectric Power Plant Afghanistan        33.0   32.322    65.119        Hydro   NaN
1  Afghanistan                                      Kandahar DOG        10.0    31.67    65.795        Solar   NaN
2  Afghanistan                                      Kandahar JOL        10.0   31.623    65.792        Solar   NaN
3  Afghanistan     Mahipar Hydroelectric Power Plant Afghanistan        66.0   34.556   69.4787        Hydro   NaN
4  Afghanistan  Naghlu Dam Hydroelectric Power Plant Afghanistan       100.0   34.641    69.717        Hydro   NaN
```

Com apenas uma lista de nomes de colunas e uma linha de código, conseguimos ler nosso arquivo CSV fora do padrão em um DataFrame pronto para análises futuras!

### Recapitulação

![](https://practicum-content.s3.amazonaws.com/resources/2.2_PT_1692083999.png)

Como sempre, você pode aprender sobre todos esses parâmetros de `read_csv()` e mais na [documentação](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) (a documentação está em inglês). oficial. Seu ambiente de trabalho também pode fornecer essas informações. Por exemplo, no Jupyter notebook, você pode digitar o nome de um método e pressionar Shift+Tab\\mathtt {Shift+Tab}Shift+Tab para exibir a documentação. Aparece desta forma:

![](https://practicum-content.s3.amazonaws.com/resources/Screenshot_from_2023-05-19_11-16-53_1690871047.png)

Isso é o que chamamos de docstring. Quando aberto, você pode rolar para baixo e conferir todos os parâmetros, valores padrão e descrições breves.

Agora vamos praticar um pouco usando suas novas habilidades!

Pergunta

Temos o seguinte arquivo CSV chamado `letter_colors_decimals.csv`. Confira o conteúdo dele:

```
letters;colors;decimals
a;yellow;1a2
b;red;1a3
c;cyan;1a4
```

Qual parâmetro devemos passar para `read_csv()` para que os valores sejam separados corretamente?

`sep=','`

`decimal=','`

`sep=';'`

Isso mesmo, os valores estão separados por ponto e vírgula.

`decimal='a'`

Fantástico!

Pergunta

Qual parâmetro devemos passar para read\_csv() para que a primeira linha não vazia se torne o cabeçalho?

Não passar o parâmetro, usar o padrão.

Isso mesmo! A primeira linha se torna o cabeçalho por padrão.

`header='None'`

`header=None`

`header=1`

Fantástico!

Pergunta

Qual parâmetro devemos passar para `read_csv()` para tratar corretamente os decimais na coluna `decimals`?

Este quiz é um pouco desafiador, então preste muita atenção na coluna `decimals` e na forma como os números são separados lá.

```
letters;colors;decimals
a;yellow;1a2
b;red;1a3
c;cyan;1a4
```

Qual parâmetro devemos passar para `read_csv()` para que os valores sejam separados corretamente?

`sep=','`

`decimal=','`

`sep=';'`

`decimal='a'`

Isso mesmo. O caractere `a` é usado para separar os números. Embora esse não seja um caso muito comum, ainda vale a pena praticar.

Excelente!

## Tarefas

### Tarefa 1

Um conjunto de dados está disponível em `/datasets/letters_colors_decimals.csv`. Os valores nesse conjunto estão separados por `$`, e `a` é usado como ponto decimal.

Leia-o de forma que:

-   A primeira linha se torne o cabeçalho;
-   As colunas sejam separadas corretamente;
-   Os decimais sejam lidos corretamente.

Imprima o DataFrame.

CódigoPYTHON

9

1

2

3

4

import pandas as pd

  

df \= pd.read\_csv("/datasets/letters\_colors\_decimals.csv", decimal\="a", sep\="$")\# escreva o código aqui

print(df)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-04-56-954Z.md
### Última modificação: 2025-05-28 19:04:57

# Como ler arquivos do Excel - TripleTen

Capítulo 2/11

Leitura e visualização de dados

# Como ler arquivos do Excel

Não é incomum receber dados de um colega ou cliente no formato de planilhas do Excel em vez de arquivos CSV. Nesta lição, continuaremos a aprender como ler arquivos do Excel em Python. Felizmente, a biblioteca pandas facilita isso.

Video

Assista ao vídeo abaixo e depois continue lendo para aprender mais.

<iframe class="base-markdown-iframe__iframe" id="player-OrnduFFW9C8" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="How to Read Excel Files" width="640" height="360" src="https://www.youtube.com/embed/OrnduFFW9C8?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F8ee6a83a-9f1e-41ea-bed5-c041e9d0533f%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

### O Excel ainda é relevante?

O Microsoft Excel é usado por companhias e organizações pelo mundo todo desde o final da década de 80. Isso significa que existem muitos dados históricos nas planilhas Excel. Hoje em dia, as maiores empresas têm os próprios centros de dados ou contratos com provedores de armazenamento na nuvem, mas ainda assim uma boa parte do trabalho é feita no Excel.

## Como trabalhar com arquivos Excel

Como você já sabe, a pandas fornece a função `read_excel()` para ler arquivos do Excel. Por padrão, essa função carrega a primeira planilha, mas um arquivo do Excel pode conter várias delas.

Como você pode ver na imagem abaixo, temos um conjunto de dados de resenhas de produtos que contém quatro planilhas: `reviews`, `reviewers`, `products` e `product_categories`.

![](https://practicum-content.s3.amazonaws.com/resources/multiple_sheets_1688649605.png)

Vamos ler o arquivo Excel usando apenas o caminho do arquivo `/datasets/product_reviews.xlsx`:

```
import pandas as pd

df = pd.read_excel('/datasets/product_reviews.xlsx')

print(df.head())
```

```
           id reviewer_id  product_id  review
0  2546305677    cG441617  5003186430       3
1  2603422798    cH443811  7130698135       1
2  2598103631    bF100137  4023404310       4
3  2632674394    cF786880  7130698135       4
4  2594782880    aF649317  5003186430       5
```

A planilha `reviews` foi lida em `df` por padrão porque é a primeira planilha do arquivo Excel. Para cada resenha, essa planilha contém um ID de resenha (coluna `'id'`), um ID para o cliente que deixou a resenha (coluna `'reviewer_id'`), um ID para o produto que está sendo avaliado (coluna `'product_id'`) e uma pontuação entre 1 e 5 (coluna `'review'`).

## O parâmetro `sheet_name=`

Se você quer acessar uma planilha diferente do padrão, precisa passar dois argumentos: o caminho do arquivo (string) e `sheet_name=` com o nome da planilha (também uma string). Vamos tentar ler a planilha `reviewers`.

```
import pandas as pd

df = pd.read_excel('/datasets/product_reviews.xlsx', sheet_name='reviewers')

print(df.head())
```

```

         id date_joined  zipcode
0  aF195825  2012-06-21    91914
1  aF249047  2019-03-26    91915
2  aF362092  2012-01-05    91941
3  aF484180  2019-06-11    91941
4  aF539111  2011-03-04    92003
```

Também podemos usar o índice da planilha em vez do nome como segundo argumento: `sheet_name=0` mostra a primeira planilha, `sheet_name=1` a segunda, e assim por diante.

Pergunta

Qual linha de código vai ler a planilha `reviewers`? Se você precisar ver o arquivo novamente, aqui está:

![](https://practicum-content.s3.amazonaws.com/resources/multiple_sheets_1_1688649684.png)

`df = pd.read_excel('/datasets/product_reviews.xlsx')`

`df = pd.read_excel('/datasets/product_reviews.xlsx', sheet_name=0)`

`df = pd.read_excel('/datasets/product_reviews.xlsx', sheet_name=1)`

Isso mesmo! Isso lerá a segunda planilha, que é a mesma que `sheet_name='reviewers'`.

`df = pd.read_excel('/datasets/product_reviews.xlsx', sheet_name=2)`

Muito bem!

E isso é tudo que você precisa para ler planilhas de arquivos do Excel em DataFrames da pandas. O método `read_excel()` também tem muitos outros parâmetros que você pode usar para ajustar o processo de acordo com a estrutura dos dados no arquivo Excel, como o parâmetro `header=`. Como sempre, você pode consultar a [documentação](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html) (a documentação está em inglês) oficial para solucionar quaisquer problemas que possa encontrar.

Agora vamos praticar suas novas habilidades explorando mais o conjunto de dados de resenhas de produtos nas tarefas!

## Tarefas

### Tarefa 1

Calcule a pontuação média das avaliações seguindo estas etapas:

1.  Leia a primeira planilha do arquivo Excel localizado em `/datasets/product_reviews.xlsx` e armazene-a em uma variável chamada `df_reviews`.
2.  Selecione a coluna `'review'` em `df_reviews`.
3.  Aplique o método apropriado para calcular o valor médio.
4.  Imprima o resultado na tela.

CódigoPYTHON

9

1

2

3

4

5

6

import pandas as pd

  

df\_reviews \= pd.read\_excel("/datasets/product\_reviews.xlsx" )\# escreva seu código aqui

df\_reviews \= df\_reviews\["review"\].mean()

  

print(df\_reviews)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Para obter mais informações sobre os produtos nesse conjunto de dados, leia a planilha `products` do arquivo Excel em uma variável chamada `df_products`.

Em seguida, classifique o DataFrame pela coluna `'id'` em ordem crescente usando o método apropriado e armazene-o na variável `sorted_df_products`. Se você não conseguir lembrar o nome do método, não se preocupe: temos uma dica que vai ajudar.

Por fim, não se esqueça de imprimir o DataFrame classificado.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df\_products \= pd.read\_excel('/datasets/product\_reviews.xlsx', sheet\_name\='products')

  

sorted\_df\_products \= df\_products.sort\_values('id')

  

print(sorted\_df\_products)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-04-58-276Z.md
### Última modificação: 2025-05-28 19:04:58

# Olhando para os nossos dados - TripleTen

Capítulo 2/11

Leitura e visualização de dados

# Olhando para os nossos dados

Lembra quando aprendemos sobre alguns atributos e métodos de DataFrame que fornecem uma visão geral dos seus dados no sprint sobre Python básico? Eles são ótimos para quando você começa a trabalhar com um novo conjunto de dados, pois te ajudam a ter uma ideia do conteúdo e a pensar nas perguntas iniciais a serem exploradas.

Por serem tão importantes, vamos revisar essas ferramentas nesta lição, bem como apresentar um novo método para dar uma olhada inicial em seus dados. Continuaremos usando o [banco de dados sobre usinas elétricas ao redor do globo](https://datasets.wri.org/dataset/globalpowerplantdatabase) (os materiais estão em inglês) modificado nesta lição.

## Revisão do método `info()`

Nunca é uma má ideia chamar o método `info()` quando você começar a trabalhar com um novo DataFrame. O método `info()` não retorna nada; em vez disso, ele imprime informações gerais sobre o DataFrame. Isso significa que você não precisa usar `print()` com `info()`.

Vamos ler os dados sobre as usinas novamente e então chamar `info()` para ver quais informações ele mostra:

```
import pandas as pd

column_names = [
    'country',
    'name',
    'capacity_mw',
    'latitude',
    'longitude',
    'primary_fuel',
    'owner'
]
data = pd.read_csv(
    '/datasets/gpp_modified.csv',
    sep='|',
    header=None,
    names=column_names,
    decimal=',',
)

data.info()
```

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 34936 entries, 0 to 34935
Data columns (total 7 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   country       34936 non-null  object 
 1   name          34936 non-null  object 
 2   capacity_mw   34936 non-null  float64
 3   latitude      34936 non-null  float64
 4   longitude     34936 non-null  float64
 5   primary_fuel  34936 non-null  object 
 6   owner         20868 non-null  object 
dtypes: float64(3), object(4)
memory usage: 1.9+ MB
```

A partir de apenas um método, obtemos as seguintes informações:

-   Número de linhas (`RangeIndex: 34936 entries`)
-   Número de colunas (`total 7 columns`)
-   Nome de cada coluna (`Column`)
-   Número de valores em cada coluna que _não_ estão ausentes (`Non-Null Count`)
-   Tipo de dados de cada coluna (`Dtype`)

Observação: você não pode acessar diretamente as informações exibidas por `info()` para armazenar em variáveis ou usar em cálculos porque `info()` não retorna nada, apenas imprime na tela. Se você precisar atribuir um atributo de DataFrame em uma variável para usar depois, será necessário acessar esse atributo diretamente.

Se você deseja armazenar o número de linhas e colunas como variáveis, pode usar o atributo `shape`. Esse atributo retorna o número de linhas e colunas no conjunto de dados. Confira como ele pode ser usado:

```
import pandas as pd

column_names = [
    'country',
    'name',
    'capacity_mw',
    'latitude',
    'longitude',
    'primary_fuel',
    'owner'
]
data = pd.read_csv(
    '/datasets/gpp_modified.csv',
    sep='|',
    header=None,
    names=column_names,
    decimal=',',
)

n_rows, n_cols = data.shape

print(f"O DataFrame tem {n_rows} linhas e {n_cols} colunas")
```

```
O DataFrame tem 34936 linhas e 7 colunas
```

Observe como usamos uma string f para imprimir informações sobre as linhas e colunas no conjunto de dados.

A função `data.shape` retorna uma **tupla** como resultado. Uma tupla é um tipo de dados em Python, assim como listas e outros. De modo simples, uma tupla é uma coleção de objetos separados por vírgulas.

Uma tupla é semelhante a uma lista do Python em termos de indexação, objetos aninhados e repetição. No entanto, a principal diferença entre os dois é que uma tupla do Python é **`imutável`** (não pode ser alterada), e uma lista do Python é mutável.

![](https://practicum-content.s3.amazonaws.com/resources/2.4_PT_1692083344.png)

No código acima, atribuímos os valores na tupla a diferentes variáveis, separando os nomes das variáveis com uma vírgula: `n_rows, n_cols = data.shape`.

Em geral, você pode "descompactar" elementos de tuplas ou listas em variáveis dessa maneira.

Pergunta

Sabemos que o método `info()` gera o número de valores não ausentes. Sabendo disso, podemos calcular facilmente o número de valores faltantes.

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 34936 entries, 0 to 34935
Data columns (total 7 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   country       34936 non-null  object 
 1   name          34936 non-null  object 
 2   capacity_mw   34936 non-null  float64
 3   latitude      34936 non-null  float64
 4   longitude     34936 non-null  float64
 5   primary_fuel  34936 non-null  object 
 6   owner         20868 non-null  object 
dtypes: float64(3), object(4)
memory usage: 1.9+ MB
```

Por exemplo, podemos determinar o número de valores ausentes na coluna `'owner'` usando informações do método `info()` ao subtrair o número de valores não nulos na coluna `'owner'` do número total de linhas no DataFrame.

Em vez de fazer isso manualmente, vamos revisar como calcular o número de valores ausentes em uma determinada coluna usando a pandas.

Qual linha de código retorna o valor correto para podermos armazenar ele em uma variável e usá-lo posteriormente em nosso código?

`data['owner'].isna().count()`

`data['owner'].isna().sum()`

Correto!

`data.dtypes['owner']`

`data.columns[6]`

Você conseguiu!

## O método `sample()`

O método `info()` é ótimo, mas você pode ter uma noção ainda melhor dos dados ao usar `info()` com uma impressão de algumas linhas do DataFrame. Lembre-se de que podemos fazer isso com os métodos `head()` ou `tail()`. Por enquanto, vamos começar usando `head()`:

```
import pandas as pd

column_names = [
    'country',
    'name',
    'capacity_mw',
    'latitude',
    'longitude',
    'primary_fuel',
    'owner'
]
data = pd.read_csv(
    '/datasets/gpp_modified.csv',
    sep='|',
    header=None,
    names=column_names,
    decimal=',',
)

print(data.head())
```

```
       country                                              name capacity_mw latitude longitude primary_fuel owner   
0  Afghanistan      Kajaki Hydroelectric Power Plant Afghanistan        33.0   32.322    65.119        Hydro   NaN
1  Afghanistan                                      Kandahar DOG        10.0    31.67    65.795        Solar   NaN
2  Afghanistan                                      Kandahar JOL        10.0   31.623    65.792        Solar   NaN
3  Afghanistan     Mahipar Hydroelectric Power Plant Afghanistan        66.0   34.556   69.4787        Hydro   NaN
4  Afghanistan  Naghlu Dam Hydroelectric Power Plant Afghanistan       100.0   34.641    69.717        Hydro   NaN
```

Agora podemos ver os valores reais nos dados junto com a sinopse geral fornecida por `info()`. Por padrão, `head()` imprime as primeiras 5 linhas. Nesse caso, essas linhas são todas para as usinas elétricas no Afeganistão. Como esse é um conjunto de dados global, esperamos que outros países também sejam representados, mas não podemos ter certeza de que esse é o caso ao visualizar apenas as primeiras 5 linhas. Vamos tentar 10 linhas:

```
import pandas as pd

column_names = [
    'country',
    'name',
    'capacity_mw',
    'latitude',
    'longitude',
    'primary_fuel',
    'owner'
]
data = pd.read_csv(
    '/datasets/gpp_modified.csv',
    sep='|',
    header=None,
    names=column_names,
    decimal=',',
)

print(data.head(10))
```

```
       country                                               name capacity_mw  latitude  longitude primary_fuel owner
0  Afghanistan       Kajaki Hydroelectric Power Plant Afghanistan       33.00   32.3220    65.1190        Hydro   NaN
1  Afghanistan                                       Kandahar DOG       10.00   31.6700    65.7950        Solar   NaN
2  Afghanistan                                       Kandahar JOL       10.00   31.6230    65.7920        Solar   NaN
3  Afghanistan      Mahipar Hydroelectric Power Plant Afghanistan       66.00   34.5560    69.4787        Hydro   NaN
4  Afghanistan   Naghlu Dam Hydroelectric Power Plant Afghanistan      100.00   34.6410    69.7170        Hydro   NaN
5  Afghanistan  Nangarhar (Darunta) Hydroelectric Power Plant ...       11.55   34.4847    70.3633        Hydro   NaN
6  Afghanistan            Northwest Kabul Power Plant Afghanistan       42.00   34.5638    69.1134          Gas   NaN
7  Afghanistan  Pul-e-Khumri Hydroelectric Power Plant Afghani...        6.00   35.9416    68.7100        Hydro   NaN
8  Afghanistan   Sarobi Dam Hydroelectric Power Plant Afghanistan       22.00   34.5865    69.7757        Hydro   NaN
9      Albania                                         Bistrica 1       27.00   39.9116    20.1047        Hydro   NaN
```

Aha! Agora também podemos ver dados de uma usina elétrica na Albânia. Os países provavelmente estão em ordem alfabética. Você pode verificar isso usando `unique()`, que retorna valores na ordem em que aparecem.

Não é incomum que conjuntos de dados tenham o mesmo valor em uma coluna para muitas linhas consecutivas, mesmo para centenas ou milhares de linhas! Nessas situações, pode ser mais útil usar `sample()` em vez de `head()` ou `tail()`.

O método `sample()` seleciona linhas aleatórias do DataFrame em vez de linhas consecutivas do início ou do final dele. Vamos usá-lo para imprimir 5 linhas dos nossos dados:

```
import pandas as pd

column_names = [
    'country',
    'name',
    'capacity_mw',
    'latitude',
    'longitude',
    'primary_fuel',
    'owner'
]
data = pd.read_csv(
    '/datasets/gpp_modified.csv',
    sep='|',
    header=None,
    names=column_names,
    decimal=',',
)

print(data.sample(5))
```

```
                        country                         name  capacity_mw  latitude  longitude primary_fuel                   owner 
5208                      China  Bayanur Wuliji Wind Phase 2         49.0   41.5333   106.4167         Wind                     NaN  
29010  United States of America          J Robert Massengale         84.0   33.6039  -101.8408          Gas  City of Lubbock - (TX) 
611                   Australia        Mugga Lane Solar Park         13.0  -35.3989   149.1460        Solar       Maoneng Australia
5350                      China           Changli Datan Wind         48.0   39.5050   119.1797         Wind                     NaN
4419                     Canada                       Rawdon          2.5   46.0458   -73.7294        Hydro         Algonquin Power
```

Agora temos uma visualização de linhas mais diversificada do que tínhamos apenas usando `head()` ou `tail()`. Ao contrário de `head()` e `tail()`, `sample()` seleciona 1 linha por padrão, então tivemos que especificar querermos 5 linhas. Observe que `sample()` também preserva os valores dos índices do DataFrame original para que você possa ver de onde vieram as linhas. O resultado do trecho de código acima indica que a amostra tem as linhas com índices 5.208, 29.010, 611, 5.350 e 4.419.

Se você executasse o código acima no sandbox da plataforma ou localmente em sua própria máquina, é quase certo que obteria um resultado diferente do que mostramos acima. E o resultado mudará cada vez que você executar o código. Isso porque `sample()` seleciona as linhas "aleatoriamente".

Se você quiser usar `sample()` e sempre obter o mesmo resultado ao executar o código, precisará usar o parâmetro `random_state=` e configurá-lo para algum valor inteiro de sua escolha (qualquer número inteiro entre 0 e 4294967295). Aqui está um exemplo:

```
import pandas as pd

column_names = [
    'country',
    'name',
    'capacity_mw',
    'latitude',
    'longitude',
    'primary_fuel',
    'owner'
]
data = pd.read_csv(
    '/datasets/gpp_modified.csv',
    sep='|',
    header=None,
    names=column_names,
    decimal=',',
)

print(data.sample(5, random_state=1369))
```

```
                        country                 name  capacity_mw  latitude  longitude primary_fuel                         owner 
10818                    France             Curières      1.13499   44.6495     2.8924        Solar                           NaN
1513                     Brazil  Cachoeira Caldeirão    219.00000    0.8533   -51.2917        Hydro                           NaN
1030                    Belgium              AWIRS 4     95.00000   50.5851     5.4180      Biomass                           NaN
31250  United States of America    Paragould Turbine     13.90000   36.0243   -90.5092          Gas  Paragould Light & Water Comm
20126               South Korea             Yeongnam    400.00000   35.5141   129.3835         Coal          Korea Southern Power
```

Se você executar esse código com `random_state=1369`, obterá o mesmo resultado todas as vezes. Pode ser útil definir um estado aleatório específico para a reprodutibilidade do código, para comparações ou para testar bugs.

## Tarefas

### Tarefa 1

O pré-código já inclui código para imprimir as primeiras 5 linhas do conjunto de dados de usinas usando o método `head()`. Escreva um código que:

1.  tenha uma amostra de 5 linhas aleatórias do conjunto de dados e as armazene na variável `sample`. Use `random_state=543210` para a amostragem.
2.  imprima a variável `sample`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

import pandas as pd

  

column\_names \= \[

'country',

'name',

'capacity\_mw',

'latitude',

'longitude',

'primary\_fuel',

'owner'

\]

data \= pd.read\_csv(

'/datasets/gpp\_modified.csv',

sep\='|',

header\=None,

names\=column\_names,

decimal\=',',

)

  

print(data.head())

print()

  

sample \= data.sample(5,random\_state\=543210)\# escreva seu código aqui

print(sample)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Agora que temos uma noção de como são os dados em cada coluna, confira a informação geral sobre esse conjunto de dados chamando o método `info()`. Incluímos o código da última tarefa para que você possa comparar lado a lado as linhas da amostra com o resultado de `info()`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

import pandas as pd

  

column\_names \= \[

'country',

'name',

'capacity\_mw',

'latitude',

'longitude',

'primary\_fuel',

'owner'

\]

data \= pd.read\_csv(

'/datasets/gpp\_modified.csv',

sep\='|',

header\=None,

names\=column\_names,

decimal\=',',

)

  

sample \= data.sample(5, random\_state\=543210)

print(sample)

print()

  

data.info()\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-04-59-589Z.md
### Última modificação: 2025-05-28 19:04:59

# Descrições numéricas e describe() - TripleTen

Teoria

# Descrições numéricas e describe()

Nesta lição, você vai aprender sobre o método `describe()`, uma maneira útil de obter informações sobre as colunas numéricas nos seus dados. Você pode chamar `describe()` em um DataFrame ou Series; ele retorna o mesmo tipo de objeto em que foi chamado.

Vamos usá-lo no DataFrame de usinas e ver o que acontece:

```
import pandas as pd

column_names = [
    'country',
    'name',
    'capacity_mw',
    'latitude',
    'longitude',
    'primary_fuel',
    'owner'
]
data = pd.read_csv(
    '/datasets/gpp_modified.csv',
    sep='|',
    header=None,
    names=column_names,
    decimal=',',
)

print(data.describe())
```

```
        capacity_mw      latitude     longitude
count  34936.000000  34936.000000  34936.000000
mean     163.355148     32.816637     -6.972803
std      489.636072     22.638603     78.405850
min        1.000000    -77.847000   -179.977700
25%        4.900000     29.256475    -77.641550
50%       16.745000     39.727750     -2.127100
75%       75.344250     46.263125     49.502675
max    22500.000000     71.292000    179.388700
```

![](https://practicum-content.s3.amazonaws.com/resources/2.5_PT_1692083245.png)

![](https://practicum-content.s3.amazonaws.com/resources/2.5.2_PT_1690871327.png)

É muita informação útil vinda de apenas uma pequena linha de código. Juntos, o desvio padrão e os quartis nos dão uma visão da dispersão e variação nos valores de uma coluna. Você aprenderá muito mais sobre essas quantidades no sprint de estatísticas mais adiante no programa.

Embora descrições numéricas, também conhecidas como estatísticas descritivas, possam nos dizer muito sobre nossos dados, precisaremos combinar elas com visualizações de dados para ter uma ideia geral. Isso se aplica principalmente ao comparar diferentes conjuntos de dados, porque é possível que as estruturas sejam bem distintas, mesmo que tenham sínteses estatísticas semelhantes. O [Quarteto de Anscombe](https://pt.wikipedia.org/wiki/Quarteto_de_Anscombe) é um conjunto clássico de quatro conjuntos de dados com descrições numéricas parecidas, mas gráficos bem diferentes. E se você quiser se divertir com isso, confira o [Datasaurus Dozen](https://cran.r-project.org/web/packages/datasauRus/vignettes/Datasaurus.html) (os materiais estão em inglês) abaixo!

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/image.gif"></iframe>

Na animação acima, podemos ver como gráficos completamente diferentes podem ter valores estatísticos quase idênticos.

Observe também que, só porque uma coluna é numérica, não significa necessariamente que todas as informações de `describe()` sejam úteis ou façam sentido para ela. Por exemplo, a média da latitude e longitude de todas as usinas do mundo provavelmente não é uma informação útil para nenhum tipo de análise. Afinal, são apenas coordenadas geográficas. Determinar valores mínimos, médios ou máximos para elas não faria sentido. No entanto, a contagem de valores não nulos talvez seja. O principal é que você precisa estar ciente de quais informações são relevantes e fazem sentido.

## Chamando `describe()` em colunas não numéricas

Você também pode chamar `describe()` em um objeto Series, que vai retornar outro objeto Series. Vamos testá-lo na coluna numérica `'capacity_mw'`:

```
import pandas as pd

column_names = [
    'country',
    'name',
    'capacity_mw',
    'latitude',
    'longitude',
    'primary_fuel',
    'owner'
]
data = pd.read_csv(
    '/datasets/gpp_modified.csv',
    sep='|',
    header=None,
    names=column_names,
    decimal=',',
)

print(data['capacity_mw'].describe())
```

```
count    34936.000000
mean       163.355148
std        489.636072
min          1.000000
25%          4.900000
50%         16.745000
75%         75.344250
max      22500.000000
Name: capacity_mw, dtype: float64
```

Obtemos a mesma informação para `'capacity_mw'` que obtivemos quando chamamos `describe()` em todo o DataFrame. Mas e se tentarmos chamar `describe()` em um objeto Series que não seja numérico? Vamos experimentar isso na coluna `'country'` e ver o que acontece:

```
import pandas as pd

column_names = [
    'country',
    'name',
    'capacity_mw',
    'latitude',
    'longitude',
    'primary_fuel',
    'owner'
]
data = pd.read_csv(
    '/datasets/gpp_modified.csv',
    sep='|',
    header=None,
    names=column_names,
    decimal=',',
)

print(data['country'].describe())
```

```
count                        34936
unique                         167
top       United States of America
freq                          9833
Name: country, dtype: object
```

Não temos erro! Em vez disso, obtemos as seguintes informações úteis:

-   `'count'`: O número de valores não nulos
-   `'unique'`: O número de valores unívocos
-   `'top'`: O valor que ocorre com mais frequência
-   `'freq'`: O número de vezes que o valor mais frequente ocorre

Assim, a partir do resultado acima, podemos descobrir imediatamente algumas características interessantes sobre nossos dados. Por exemplo, existem 167 países representados no conjunto de dados, e os Estados Unidos têm mais usinas elétricas do que qualquer outro país: 9.833 no total.

Para as colunas numéricas e não numéricas, `describe()` retorna uma contagem do número de valores não nulos, mas as outras quantidades (`'unique'`, `'top'` e `'freq'`) são retornadas apenas para as colunas não numéricas.

## O parâmetro `include=`

E se você quiser as informações de `describe()` para cada coluna em um DataFrame, mesmo que tenha colunas numéricas e não numéricas? Ou talvez você queira informações apenas para as colunas não numéricas? Como costuma ser o caso em Python, há um parâmetro para isso!

Podemos usar o parâmetro `include=` para dizer ao método `describe()` quais colunas queremos incluir no resultado. No entanto, não passamos uma lista de nomes de colunas como argumento. Em vez disso, passamos o tipo de dados das colunas que queremos como uma string (ou uma lista de strings se quisermos mais de um tipo de dados).

Por exemplo, vamos obter apenas as colunas não numéricas do conjunto de dados de usinas:

```
import pandas as pd

column_names = [
    'country',
    'name',
    'capacity_mw',
    'latitude',
    'longitude',
    'primary_fuel',
    'owner'
]
data = pd.read_csv(
    '/datasets/gpp_modified.csv',
    sep='|',
    header=None,
    names=column_names,
    decimal=',',
)

print(data.describe(include='object'))
```

```
                         country           name primary_fuel                    owner
count                      34936          34936        34936                    20868
unique                       167          34528           15                    10144
top     United States of America  Santo Antônio        Solar  Cypress Creek Renewable
freq                        9833              6        10665                      185
```

Muito bom! Lembre-se de que na pandas o tipo de dados do `object` geralmente se refere a strings (ou colunas com tipos de dados mistos).

Por fim, podemos passar `'all'` para obter as informações sobre cada coluna, seja qual for o tipo de dados:

```
import pandas as pd

column_names = [
    'country',
    'name',
    'capacity_mw',
    'latitude',
    'longitude',
    'primary_fuel',
    'owner'
]
data = pd.read_csv(
    '/datasets/gpp_modified.csv',
    sep='|',
    header=None,
    names=column_names,
    decimal=',',
)

print(data.describe(include='all'))
```

```
                         country           name   capacity_mw      latitude     longitude primary_fuel                     owner
count                      34936          34936  34936.000000  34936.000000  34936.000000        34936                     20868 
unique                       167          34528           NaN           NaN           NaN           15                     10144
top     United States of America  Santo Antônio           NaN           NaN           NaN        Solar  Cypress Creek Renewables
freq                        9833              6           NaN           NaN           NaN        10665                       185
mean                         NaN            NaN    163.355148     32.816637     -6.972803          NaN                       NaN
std                          NaN            NaN    489.636072     22.638603     78.405850          NaN                       NaN
min                          NaN            NaN      1.000000    -77.847000   -179.977700          NaN                       NaN
25%                          NaN            NaN      4.900000     29.256475    -77.641550          NaN                       NaN
50%                          NaN            NaN     16.745000     39.727750     -2.127100          NaN                       NaN
75%                          NaN            NaN     75.344250     46.263125     49.502675          NaN                       NaN
max                          NaN            NaN  22500.000000     71.292000    179.388700          NaN                       NaN
```

Essa saída contém 11 linhas que cobrem todas as informações que `describe()` pode fornecer. Se a coluna for numérica, há valores ausentes (`NaN`) nas linhas que correspondem às informações de variáveis não numéricas (por exemplo, `'capacity_mw'`). E se a coluna não for numérica, há valores ausentes nas linhas que correspondem às informações de variáveis numéricas (por exemplo, `'country'`).

Agora vamos continuar explorando `describe()` nas tarefas.

Descrições numéricas e describe()

Tarefa3 / 3

1.

Obtenha uma visão geral apenas da coluna `'primary_fuel'` chamando `describe()` nela e imprimindo o resultado. O conjunto de dados já foi lido corretamente para você no pré-código.

2.

No resultado da tarefa anterior, descobrimos que existem 15 valores únicos na coluna `'primary_fuel'`. Vamos verificar isso agora. Para fazer isso, chame o método `nunique()` nessa coluna. Atribua o resultado a uma variável chamada `unique` e imprima-a na tela.

3.

Agora verifique se o valor mais comum na coluna `'primary_fuel'` realmente é `'Solar'`. Para fazer isso:

1.  Filtre o DataFrame original, extraindo apenas as linhas em que `'primary_fuel'` é igual a `'Solar'` e armazene-o na variável `solar_data`.
2.  Verifique a forma do DataFrame obtido e o armazene na variável `solar_shape`.
3.  Imprima a variável.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

import pandas as pd

  

column\_names \= \[

'country',

'name',

'capacity\_mw',

'latitude',

'longitude',

'primary\_fuel',

'owner'

\]

data \= pd.read\_csv(

'/datasets/gpp\_modified.csv',

sep\='|',

header\=None,

names\=column\_names,

decimal\=',',

)

  

solar\_data \= data\[data\['primary\_fuel'\] \== "Solar"\]

solar\_shape \= solar\_data.shape\# escreva seu código aqui

print(solar\_shape)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-05-02-620Z.md
### Última modificação: 2025-05-28 19:05:03

# Quiz do capítulo - TripleTen

Capítulo 2/11

Leitura e visualização de dados

# Quiz do capítulo

Pergunta

Um arquivo CSV pode usar `;` como delimitador?

Não, CSV é uma sigla do inglês que significa "valores separados por vírgulas" e, como tal, usa apenas `,` como delimitador.

Sim, um arquivo CSV não precisa usar vírgulas como delimitador.

Qualquer caractere pode ser usado como delimitador. Por exemplo, o delimitador de outro formato comum separa os valores por tab (valores separados por tab). Às vezes, eles podem ser encontrados como arquivos `.tsv` ou `.tab`, além de `.csv`.

Sim, mas não seria um arquivo CSV compatível.

Excelente!

Pergunta

Você tem um arquivo CSV chamado `clients.csv` com o delimitador tab. Como você o leria em um objeto DataFrame?

`pd.read_tsv('clients.csv')`

`pd.read_csv('clients.csv', sep='\t')`

O parâmetro `sep` pode ser usado para especificar qualquer caractere ou uma sequência de caracteres como delimitador de campo.

`pd.read_csv('clients.csv')`

`pd.read_csv('clients.csv', delim='\t')`

Excelente!

Pergunta

Você recebeu um arquivo CSV. Ao examinar as primeiras linhas do arquivo em um editor de texto, você notou que um delimitador incomum está sendo usado. Como você o leria em um objeto DataFrame?

```
Year<DEL>"Score"<DEL>"Title"
1968<DEL>86<DEL>"Greetings"
1970<DEL>17<DEL>"Bloody Mama"
1970<DEL>73<DEL>"Hi, Mom!"
1971<DEL>40<DEL>"Born to Win"
1973<DEL>98<DEL>"Mean Streets"
1973<DEL>88<DEL>"Bang the Drum Slowly"
1974<DEL>97<DEL>"The Godfather, Part II"
```

Esse não é um arquivo CSV padrão. Vou pedir uma versão consertada dele.

Posso lê-lo com `pd.read_csv()` fornecendo apenas o nome do arquivo.

Posso especificar o delimitador como `<DEL>` usando o parâmetro `sep='<DEL>'`.

Não é incomum ter que especificar um parâmetro adicional (usando o parâmetro `sep`) para ler arquivos CSV corretamente. Em vez de estudar e memorizar todos eles, é mais fácil verificar na [documentação](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) (a documentação está em inglês) os parâmetros que ajudam a ler corretamente cada arquivo CSV.

Fantástico!

Pergunta

Você tem um arquivo chamado `clients.tsv`. Supostamente, é um CSV. Como você o leria em um objeto DataFrame?

`pd.read_tsv('clients.tsv')`

`pd.read_csv('clients.tsv', sep='\t')`

A extensão do arquivo, `.tsv`, sugere que é um arquivo separado por tab, então o parâmetro `sep` pode ser usado para especificar esse delimitador.

`pd.read_csv('clients.tsv')`

Excelente!

Pergunta

Você recebeu um arquivo Excel `month_stats.xlsx` com duas planilhas chamadas `Clients` e `Cities`. Você precisa de dados de ambas. Como você carregaria todos esses dados em objetos DataFrame?

Escolha quantas quiser

Não é possível, porque a pandas só permite a leitura de dados da primeira planilha.

`df = pd.read_excel('month_stats.xlsx')`. Isso carrega dados de todas as planilhas em um objeto DataFrame.

`df = pd.read_excel('month_stats.xlsx', sheet_name=['Clients', 'Cities])`. Isso carregará os dados de todas as planilhas em um objeto DataFrame.

Isso vai funcionar, porque carrega os dados de todas as planilhas especificadas em um dicionário, com os nomes das planilhas como chaves e objetos DataFrame como valores.

```
df_clients = pd.read_excel('month_stats.xlsx', sheet_name='Clients')
df_citites = pd.read_excel('month_stats.xlsx', sheet_name='Cities')
```

Isso carrega os dados de ambas as planilhas especificadas nos objetos DataFrame correspondentes.

Esse é provavelmente o método mais simples e transparente. Alternativamente, podemos especificar `sheet_name=['Clients', 'Cities]`, e os dados de todas as planilhas especificadas serão carregados em um dicionário com os nomes das planilhas como chaves e objetos DataFrame como valores.

Fantástico!

Pergunta

Quando `sample()` é preferível em vez de `head()`?

Escolha quantas quiser

Se suspeitarmos que os pontos de dados no início de um conjunto de dados não são tão representativos do total quanto uma amostra aleatória seria.

O método `sample()` seleciona linhas aleatórias do DataFrame em vez de linhas consecutivas desde o início. Ele aumenta a probabilidade de ter uma ideia "mais ampla" dos dados.

Se quisermos obter as primeiras linhas do conjunto de dados em ordem aleatória.

Se quisermos obter uma amostra para um conjunto de dados muito grande, seria mais rápido usar `sample()` do que `head()`.

Se quisermos executar `sample()` várias vezes para examinar dados por meio de várias amostras.

Cada execução de `sample()`, a menos que fixemos o estado aleatório, retornará uma nova amostra. Isso nos permite ver diferentes pontos de diferentes partes dos dados. `head()` continuaria retornando os mesmos poucos pontos do início do conjunto.

Você conseguiu!

Pergunta

Você quer obter o número total de valores não nulos para cada coluna. Qual método seria o mais adequado?

`info()`

Isso retorna o número total de valores não ausentes para cada coluna.

`describe()`

`info()`, `describe()`

`isna()`

Fantástico!

Pergunta

Você quer uma síntese estatística de um DataFrame para todas as colunas e, portanto, chama a função `describe()` para obter isso. Infelizmente, algumas colunas estão faltando no resultado. O que você pode fazer para obter estatísticas básicas para todas as colunas?

Escolha quantas quiser

Listar todos os nomes das colunas no parâmetro `include`.

Passar uma lista de tipos de dados que abrange todas as colunas no parâmetro `include`.

É uma opção viável, mas talvez não seja o ideal ter que descobrir e listar todos os tipos de dados relevantes.

Passar `'all'` no parâmetro `include`.

As colunas que foram perdidas provavelmente não são numéricas. A opção `include='all'` faz com que `describe()` forneça estatísticas para todos os tipos de dados dentro do conjunto de dados, ou seja, para todas as colunas ([mais detalhes](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html)) (os materias estão em inglês) .

Trabalho maravilhoso!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-05-03-916Z.md
### Última modificação: 2025-05-28 19:05:04

# Conclusão - TripleTen

Capítulo 2/11

Leitura e visualização de dados

# Conclusão

Parabéns! Mais um capítulo foi concluído, e você está um passo mais perto de se tornar um superprofissional de dados! Vamos resumir o que você aprendeu.

Pergunta

Agora você é capaz de:

Escolha quantas quiser

Ler arquivos CSV com vários tipos de formatação

Se ainda não tiver certeza, você pode revisar a lição em [Como resolver problemas com arquivos CSV](https://tripleten.com/trainer/data-analyst/lesson/c3521ea4-037a-4b8c-b208-dea4319d34be/)

Ler arquivos do Excel e suas planilhas

Se ainda não tiver certeza, você pode revisar a lição em [Como ler arquivos do Excel](https://tripleten.com/trainer/data-analyst/lesson/25c630c9-eee7-450c-8095-ec16ceb061b6/)

Exibir informações gerais sobre um DataFrame com `info()` e `sample()`

Se ainda não tiver certeza, você pode revisar a lição em [Olhando para os nossos dados](https://tripleten.com/trainer/data-analyst/lesson/f06cb9ae-c6af-4f62-b872-3e3fb089e47e/)

Obter resumos de colunas numéricas e categóricas com `describe()`

Se ainda não tiver certeza, você pode revisar a lição em [Descrições numéricas e describe](https://tripleten.com/trainer/data-analyst/lesson/e93b5d34-f8db-412a-a3d2-e0532a4f7dad/task/16999cd5-5307-4d5f-9bb7-e4b6bd5f0f02/)

Você conseguiu!

Essas técnicas são cruciais para iniciar seus processos de manipulação de dados e AED. Após ler os dados e obter uma visão geral, você pode começar a fazer perguntas que vão orientar o restante da análise.

No próximo capítulo, vamos nos aprofundar nos valores ausentes e duplicados.

### Leve isso com você

Faça download do sumário do capítulo para que você possa consultá-los quando necessário.

-   [Resumo do capítulo: Leitura e visualização de dados](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/PT/2_Resumo_do_captulo_Leitura_e_visualizao_de_dados.pdf)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-05-05-214Z.md
### Última modificação: 2025-05-28 19:05:05

# Introdução - TripleTen

Capítulo 3/11

Tratamento de valores ausentes e duplicados

# Introdução

Boas-vindas ao segundo capítulo do sprint! Aqui você vai explorar métodos alternativos para trabalhar com valores ausentes e duplicados em uma série de conjuntos de dados de marketing e vendas de uma empresa de comércio eletrônico.

Neste capítulo, você vai aprender a:

-   distinguir entre tipos de dados categóricos (qualitativos) e quantitativos;
-   abordar valores ausentes para cada tipo;
-   substituir valores ausentes;
-   lidar com dados duplicados do tipo string distinguindo entre maiúsculas e minúsculas.

Serão necessárias de 2 a 3 horas para concluir este capítulo. Portanto, mantenha o foco e prepare-se para dominar novas habilidades de trabalho com valores ausentes e duplicados.

![](https://practicum-content.s3.amazonaws.com/resources/3.1_PT_1690871419.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-05-08-116Z.md
### Última modificação: 2025-05-28 19:05:08

# Contagem de valores ausentes - TripleTen

Capítulo 3/11

Tratamento de valores ausentes e duplicados

# Contagem de valores ausentes

Nas próximas lições, vamos trabalhar com um conjunto de dados de marketing de uma empresa de comércio eletrônico. Os dados estão em um arquivo CSV chamado `visit_log.csv`, e cada linha representa uma visita ao site da empresa. Existem 4 colunas:

-   `'user_id'` — identificador unívoco para cada pessoa que visitou o site.
-   `'source'` — fonte de tráfego de visitas do site. Nossa atenção está em três categorias da fonte:
    -   Visitas a partir dos links dos e-mails de marketing — `'email`
    -   Visitas a partir de publicidades contextuais online — `'context'`
    -   Visitas de qualquer outra fonte — `'other'`
-   `'email'` — endereço de e-mail criptografado associado ao visitante.
-   `'purchase'` — indica se o visitante comprou algo durante a visita (1 se sim, 0 se não).

Seu objetivo é determinar a **taxa de conversão** para cada fonte, que é a proporção de visitas em que uma compra foi realizada em relação ao número total de visitas. Comparar a taxa de conversão de cada fonte permite que você determine qual delas está rendendo mais vendas.

No entanto, antes de iniciar os cálculos, precisamos verificar o conjunto de dados em busca de coisas como valores ausentes e decidir o que fazer com eles.

## Como verificar se há valores ausentes

Uma boa maneira de começar a verificar se há valores ausentes é chamar o método `info()` no DataFrame, assim como fizemos no último capítulo.

O método `info()` imprime o número de valores que _não_ estão ausentes em cada coluna. Valores nulos são valores ausentes, enquanto valores não-nulos são valores não ausentes. Esteja ciente de que `info()` reconhece apenas valores ausentes "de verdade" (`None` ou `NaN`) como nulos, enquanto valores ausentes representados por qualquer outro valor não serão considerados como ausentes.

Exemplos de espaços reservados para valores ausentes são o número `0`, a string vazia `"` ou outras strings como `'None'` (que não é o mesmo que o tipo `None` do Python).

Nunca é uma má ideia chamar `info()` em um novo conjunto de dados. Vamos verificar isso no DataFrame dos logs dos visitantes, que associamos a uma variável chamada `df_logs`:

CódigoPYTHON

9

1

2

3

4

5

6

import pandas as pd

  

df\_logs \= pd.read\_csv('/datasets/visit\_log.csv')

  

df\_logs.info()\# escreva seu código aqui

  

Dica

Mostrar a soluçãoValidar

O resultado de `info()` mostra que nosso conjunto de dados tem 200.000 linhas, mas as colunas `'source'` e `'email'` têm menos de 200.000 valores não nulos. Isso significa que ambas as colunas têm valores ausentes. `info()` aponta que _temos_ valores ausentes, mas se nosso objetivo for _contar_ esses valores, então há uma maneira melhor de fazer isso.

Pergunta

Qual método aprendemos no sprint de Python básico para encontrar e contar valores ausentes?

Escolha quantas quiser

`df.isna().sum()`

Sim, `df.isna().sum()` retorna o número total de valores ausentes.

`df.isnull().sum()`

Sim, `df.isnull().sum()` retorna o número total de valores ausentes.

df.NaNs

Excelente!

Agora vamos tentar isso no nosso conjunto de dados:

CódigoPYTHON

9

1

2

3

4

import pandas as pd

  

df\_logs \= pd.read\_csv('/datasets/visit\_log.csv')

print(df\_logs.isna().sum())

Dica

Mostrar a soluçãoValidar

Isso é bastante! Mas ainda há outra maneira de contar valores ausentes.

## Contagem de valores ausentes com `value_counts()`

Em vez somar os valores retornados por `isna()`, podemos contar os valores ausentes diretamente usando o método `value_counts()`. Quando o chamamos em uma única coluna (ou seja, um objeto Series), ele retorna o número de vezes que cada valor único ocorre nessa coluna.

Esse método tem um parâmetro chamado `dropna=`, definido como `True` por padrão. Isso significa que `value_counts()` exclui os valores `None` ou `NaN` a não ser que você configure `dropna=False`.

Vamos chamá-lo na coluna `'source'` do DataFrame, incluindo os valores ausentes:

CódigoPYTHON

9

1

2

3

4

import pandas as pd

  

df\_logs \= pd.read\_csv('/datasets/visit\_log.csv')

print(df\_logs\["source"\].value\_counts(dropna\=False))

Dica

Mostrar a soluçãoValidar

Podemos ver que a coluna `'source'` contém os valores das fontes de tráfego do nosso interesse, que são `'context'` para publicidades contextuais, `'email'` para e-mails de marketing e `'other'` para o resto. Também vemos que existem 1.674 valores ausentes na coluna `'source'`, o que representa aproximadamente 1% dos dados. Existe ainda o valor `'undef'`, mas não temos certeza ao que isso se refere ainda.

Como você pode ver, quando imprimimos `df_logs['source'].value_counts(dropna=False)`, o resultado é armazenado em ordem decrescente com base na contagem de cada valor. Como alternativa, podemos ordenar o resultado em ordem alfabética com base nos nomes dos valores. Para fazer isso, podemos usar o método `sort_index()`.

```
import pandas as pd

df_logs = pd.read_csv('/datasets/visit_log.csv')
print(df_logs['source'].value_counts(dropna=False).sort_index())
```

```
source
context     52032
email       12279
other      133834
undef         181
NaN          1674
Name: count, dtype: int64
```

Você acabou de usar o método `value_counts()` seguido pelo método `sort_index()`. Aplicar vários métodos em sequência é uma prática comum na área dos dados. Mais adiante neste sprint, você verá muitas outras aplicações dessa técnica.

### Recapitulação

Existem muitas formas de encontrar e contar valores ausentes na pandas. Você aprendeu três maneiras nesta lição:

-   Chamando `info()` em um DataFrame
-   Chamando `isna().sum()` em um DataFrame ou objeto Series
-   Chamando `value_counts(dropna=False)` em um objeto Series

Antes de avançarmos para como lidar com valores ausentes, vamos investigar os valores ausentes na coluna `'email'` nas tarefas.

## Tarefas

### Tarefa 1

Aplique o método `value_counts()` à coluna `'email'` e armazene o resultado na variável `email_values`. Dessa vez, não inclua os valores ausentes no resultado. Imprima o resultado.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df\_logs \= pd.read\_csv('/datasets/visit\_log.csv')

  

email\_values \= df\_logs\["email"\].value\_counts()\# escreva seu código aqui

  

print(email\_values)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Agora vamos tentar ordenar os resultados por índice, e não por valor, para ver se isso acrescenta algum significado aos valores da coluna `'email'`. Reescreva a variável `email_values` usando a ordenação e imprima o resultado.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

import pandas as pd

  

df\_logs \= pd.read\_csv('/datasets/visit\_log.csv')

  

email\_values \= df\_logs\['email'\].value\_counts()

email\_values \= email\_values.sort\_index()\# escreva seu código aqui

  

print(email\_values)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-05-09-428Z.md
### Última modificação: 2025-05-28 19:05:09

# Filtragem de DataFrames com NaNs - TripleTen

Teoria

# Filtragem de DataFrames com NaNs

## Filtragem de DataFrames para lidar com valores ausentes

Às vezes, é necessário examinar as linhas do nosso conjunto de dados em que há valores ausentes. Há várias maneiras de filtrar um DataFrame para extrair linhas com esses valores. Uma delas é usar o método `isna()`, que produz um objeto Series para cada linha. Nesse objeto Series, `True` indica um valor ausente, e `False` indica um valor não ausente.

Depois, podemos usar esse objeto Series para filtrar o DataFrame. Por exemplo, se quisermos extrair linhas em que a coluna `'source'` tem valores ausentes, podemos fazer isso usando o seguinte código:

```
print(df_logs[df_logs['source'].isna()])
```

```
           user_id source       email  purchase
22      1397217221    NaN  79ac569f0b         0
49      5062457902    NaN  9ddce3a861         0
171     6724868284    NaN  c0e48c7cf8         0
258     3221384063    NaN  7fe8da1823         0
379     7515782311    NaN  462462af10         0
...            ...    ...         ...       ...
199342  3439213943    NaN  7edda4e2a4         0
199661  9473123762    NaN  3535509f51         0
199689   722485056    NaN  470ffa3800         0
199709  5950023506    NaN  0fb749d485         0
199758  3747926428    NaN  604850216f         0

[1674 rows x 4 columns]
```

Vamos analisar essa linha de código:

1.  Extraímos a coluna `'source'` usando `df_logs['source']`.
2.  Em seguida, aplicamos o método `isna()` a ela para obter um objeto Series de valores booleanos que indica a presença de valores ausentes: `df_logs['source'].isna()`.
3.  Usamos esse Séries de booleanos para filtrar o DataFrame original, extraindo apenas as linhas em que `'source'` tem valores ausentes.
4.  Por fim, imprimimos a tabela resultante.

Como alternativa, podemos filtrar o DataFrame e extrair apenas as linhas em que `'source'` não está ausente. A abordagem que usamos anteriormente funciona com apenas uma pequena modificação:

```
print(df_logs[~df_logs['source'].isna()])
```

```
           user_id   source       email  purchase
0       7141786820    other         NaN         0
1       5644686960    email  c129aa540a         0
2       1914055396  context         NaN         0
3       4099355752    other         NaN         0
4       6032477554  context         NaN         1
...            ...      ...         ...       ...
199995  8714621942    other         NaN         0
199996  6064948744  context         NaN         1
199997  9210683879  context         NaN         0
199998  1629959686    other         NaN         1
199999  2089329795    other         NaN         0

[198326 rows x 4 columns]
```

A única diferença entre esse caso e aquele em que procurávamos as linhas com valores ausentes é a adição do caractere til (`~`), que inverte o resultado. Vamos analisar o código:

1.  Extraímos a coluna 'source' usando `df_logs['source']`.
2.  Em seguida, aplicamos o método `isna()` a ela para obter um objeto Séries de valores booleanos que indica a presença de valores ausentes: `df_logs['source'].isna()`.
3.  Invertemos o objeto Series usando `~`. Isso inverte todos os valores `True` para `False` e vice-versa.
4.  Usamos esse Séries de booleanos para filtrar o DataFrame original, extraindo apenas as linhas em que `'source'` não tem valores ausentes.
5.  Por fim, imprimimos a tabela resultante.

## Múltiplas condições de filtragem

Podemos filtrar um DataFrame com base em várias condições. Por exemplo, para criar um DataFrame que não tem valores ausentes na coluna `'email'` e contém apenas o valor `'email'` na coluna `'source'`, podemos usar o seguinte código:

```
 print(df_logs[(~df_logs['email'].isna()) & (df_logs['source'] == 'email')])
```

```
           user_id source       email  purchase
1       5644686960  email  c129aa540a         0
11      8623045648  email  d6d19c571c         0
18      5739438900  email  19379ee49c         0
19      7486955288  email  09c27794fa         0
33      7298923004  email  1fe184ed73         0
...            ...    ...         ...       ...
199922  4075894991  email  2c9a202435         0
199958  9794381984  email  85712b433a         0
199970  3396355438  email  4bba3fde78         0
199979  5008169696  email  e5128e15fd         0
199989  9470921783  email  3977de6aaa         0

[12279 rows x 4 columns]
```

O código de filtragem acima tem duas partes:

1.  `(~df_logs['email'].isna())` retorna um objeto Series de valores booleanos em que `True` indica que não há valores ausentes na coluna `'email'`.
2.  `(df_logs['source'] == 'email')` retorna um objeto Series de valores booleanos em que `True` indica que `'source'` tem `'email'` como valor e `False` caso contrário.
3.  Verificamos dois objetos Series de valores booleanos para descobrir onde ambas as condições retornam `True`. Usamos o símbolo `&` para representar a operação lógica **e**. Apenas as linhas que atendem a ambas as condições (ou seja, satisfazem a primeira **e** a segunda condição) são incluídas no resultado.

Agora vamos praticar!

Filtragem de DataFrames com NaNs

Tarefa2 / 2

1.

Determinamos anteriormente que a coluna `'email'` tem 13.953 valores não ausentes. Isso significa que mais de 90% dos dados estão ausentes! Filtre o DataFrame `df_logs` para que ele contenha apenas linhas onde _não_ existam valores ausentes na coluna `'email'`. Atribua os resultados filtrados a uma variável chamada `df_emails`, depois imprima as primeiras 10 linhas.

Para verificar se uma condição _não_ é verdadeira ao filtrar um DataFrame, use o caractere `~` antes da condição (por exemplo, `df[~df.method()]`).

2.

A coluna `'source'` mostra que muitas dessas visitas têm origem dos links de e-mails de marketing. No entanto, existem alguns valores `NaN`. Pode ser que visitas com endereços de e-mail, mas sem o valor de `'source'`, também sejam de links dos e-mails de marketing, porém a fonte não foi salva.

Verifique se há alguma linha em que ambas as colunas `'source'` e `'email'` têm valores ausentes. Se não houver nenhuma linha em que ambas as condições sejam verdadeiras, isso é um sinal de que os valores ausentes na coluna `'source'` deveriam ser `'email'`.

Para isso, filtre `df_logs` com a condição de que ambas as colunas `'email'` e `'source'` tenham valores ausentes. Atribua o resultado para uma variável chamada `df_emails` e a imprima.

99

1

2

3

4

5

6

7

8

9

10

import pandas as pd

  

df\_logs \= pd.read\_csv('/datasets/visit\_log.csv')

  

df\_emails \= df\_logs\[(df\_logs\['email'\].isna()) & (df\_logs\['source'\].isna())\]

  

  

print(df\_emails)

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-05-10-742Z.md
### Última modificação: 2025-05-28 19:05:11

# Preenchimento de valores categóricos ausentes - TripleTen

Capítulo 3/11

Tratamento de valores ausentes e duplicados

# Preenchimento de valores categóricos ausentes

Antes de nos aprofundarmos no preenchimento de valores ausentes, primeiro precisamos discutir os dois tipos de variáveis com que você vai trabalhar como profissional de dados: quantitativas e categóricas.

## Variáveis quantitativas e categóricas

As variáveis quantitativas têm valores numéricos que podemos usar para cálculos aritméticos. Coisas como altura, peso, idade e receita são exemplos de variáveis quantitativas. Em Python, essas grandezas costumam ser armazenadas como números inteiros ou de ponto flutuante.

Já as variáveis categóricas representam um conjunto de valores possíveis que uma observação específica pode ter. Coisas como cor, marca e modelo de um carro são exemplos de variáveis categóricas. Em Python, elas costumam ser armazenadas como strings, mas também podem ser valores booleanos ou até mesmo números inteiros.

Exemplos de valores inteiros categóricos são códigos postais ou rótulos numéricos que representam outros valores (por exemplo, 1 = vermelho, 2 = azul, etc.) De qualquer forma, não faz sentido realizar operações aritméticas com valores categóricos.

A maneira usada para preencher valores ausentes depende se eles são quantitativos ou categóricos.

![](https://practicum-content.s3.amazonaws.com/resources/3.4_PT_1692086532.png)

Pergunta

Determinamos na última lição que existem valores `NaN` nas colunas de endereços de e-mail e fontes de tráfego. Qual o tipo de variáveis dessas colunas?

Tanto `'source'` quanto `'email'` são variáveis quantitativas.

`'source'` é categórica, e `'email'` é quantitativa.

`'source'` é quantitativa, e `'email'` é categórica.

Tanto `'source'` quanto `'email'` são variáveis categóricas.

Isso mesmo, ambas são categóricas.

Excelente!

Tanto `'source'` quanto `'email'` são colunas categóricas. Aliás, todas as colunas de fontes de tráfego do conjunto de dados são categóricas. Nesta lição, vamos aprender algumas formas eficientes de preencher valores ausentes categóricos.

Pergunta

Relembre o sprint de Python básico. Lá nós aprendemos um método para substituir NaN por valores à nossa escolha. Você lembra qual era? Escreva sua resposta sem um ponto no início, mas com parênteses no final.

fillna()

O método fillna() substitui NaN no conjunto de dados pelo valor que incluímos nos parênteses.

Você conseguiu!

Os valores `NaN` na coluna `'email'` são substitutos para os endereços de e-mail de usuários que não assinaram a newsletter da loja. Já que não há nenhuma maneira de descobrir o endereço de e-mail deles, não podemos preencher manualmente os valores ausentes com dados significativos.

No entanto, podemos preenchê-los com um **valor padrão** para representar os e-mails que estão faltando. Vamos substituir os valores ausentes na coluna `'email'` pela string vazia `''` como nosso valor padrão.

1.  Use o método `fillna()` para substituir os valores ausentes em `'email'` por strings vazias.
    
2.  Imprima as primeiras cinco linhas do DataFrame.
    

CódigoPYTHON

9

1

2

3

4

5

import pandas as pd

df\_logs \= pd.read\_csv('/datasets/visit\_log.csv')

  

df\_logs\['email'\] \= df\_logs\['email'\].fillna(value\='')

print(df\_logs.head())

Dica

Mostrar a soluçãoValidar

Agora, ao imprimir o DataFrame não vemos nada com valores ausentes em `'email'`, porque com a string vazia, não há nada para imprimir.

Pergunta

Lembre-se de que o conjunto de dados tem 200.000 linhas e 186.047 valores ausentes na coluna `'email'`. Após preencher os valores ausentes com a string vazia `''`, quantos valores não nulos `info()` vai encontrar na coluna `'email'`?

200,000

Isso mesmo, `info()` reconhece apenas valores `NaN` e `None` como valores nulos.

0

186,047

13,953

Fantástico!

Usar `fillna()` não é a única maneira de preencher valores ausentes com strings vazias. Por falar nisso, também é possível fazer isso diretamente ao ler os dados usando `read_csv()`.

## O parâmetro `keep_default_na=`

Se você der uma olhada nos dados de texto brutos do arquivo `visit_log.csv`, vai descobrir que valores ausentes são representados pela falta de texto. Em outras palavras, a ausência de texto é interpretada como `NaN`.

Existem duas vírgulas adjacentes onde o valor de `'email'` está ausente em quatro dessas cinco linhas. Por padrão, essa falta de texto é lida por `read_csv()` como `NaN`.

Mas podemos fazer com que `read_csv()` leia valores ausentes como strings vazias em vez de `NaN` ao definir o parâmetro `keep_default_na=` como `False`. Vamos tentar isso no nosso conjunto de dados:

```
import pandas as pd

df_logs = pd.read_csv('/datasets/visit_log.csv', keep_default_na=False)

print(df_logs.head())
print()
df_logs.info()
```

```
      user_id   source       email  purchase
0  7141786820    other                     0
1  5644686960    email  c129aa540a         0
2  1914055396  context                     0
3  4099355752    other                     0
4  6032477554  context                     1

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 200000 entries, 0 to 199999
Data columns (total 4 columns):
 #   Column    Non-Null Count   Dtype 
---  ------    --------------   ----- 
 0   user_id   200000 non-null  int64 
 1   source    200000 non-null  object
 2   email     200000 non-null  object
 3   purchase  200000 non-null  int64 
dtypes: int64(2), object(2)
memory usage: 6.1+ MB
```

Podemos ver na impressão que os valores ausentes são strings vazias. Agora `info()` não vai mais detectar valores nulos, o que significa que nós é que teremos que reconhecer que `''` representa valores ausentes durante a análise.

Esteja ciente de que definir `keep_default_na=False` converte todos os valores ausentes em strings vazias, mesmo nas colunas numéricas. Isso faz com que elas sejam lidas como strings se tiverem valores ausentes. Então, use `keep_default_na=False` apenas quando quiser que todos os valores ausentes em todas as colunas sejam lidos como strings vazias.

No caso de `visit_log.csv`, é conveniente para nós fazermos isso, porque todas as colunas são categóricas. Assim, conseguimos ler nos dados _e_ substituir todos os valores ausentes com apenas uma linha de código.

Como não podemos descobrir os endereços de e-mail dos visitantes que nunca os forneceram, faz sentido usar uma string vazia para esses valores ausentes. Mas e os valores ausentes de fontes de tráfego?

Estamos confiantes de que os valores ausentes na coluna `'source'` são, na verdade, visitas vindas dos links de e-mail. É possível que algum erro tenha acontecido no rastreamento dessas visitas. De qualquer forma, precisamos fazer a mudança manualmente.

Leia os dados usando `keep_default_na=False` e imprima todos os valores unívocos na coluna `'source'`.

CódigoPYTHON

9

1

2

3

4

5

6

import pandas as pd

  

df\_logs \= pd.read\_csv('/datasets/visit\_log.csv', keep\_default\_na\=False)

  

df\_sources \= df\_logs\['source'\].unique()

print(df\_sources)

Mostrar a soluçãoExecutar

Temos 5 valores distintos na categoria fonte de tráfego, com valores ausentes agora representados por `''`. O valor para fontes de e-mail de marketing é a string `'email'`. Com base na nossa análise, queremos converter todos os valores `''` para `'email'`.

Pergunta

Relembre o sprint de Python básico. Aprendemos um método para substituir um valor com outro em uma coluna de um DataFrame. Você lembra qual era? Escreva sua resposta sem um ponto no início, mas com parênteses no final.

replace()

O método `replace()`, quando chamado em uma coluna do DataFrame, permite trocar um valor por outro. Lembre-se de que a sintaxe é `replace(valor_antigo, valor_novo)`.

Trabalho maravilhoso!

Ótimo! Agora vamos consertar nossos valores ausentes:

1.  Use `replace()` para substituir os valores ausentes na coluna `'source'` pela string `'email'`.
2.  Verifique o seu trabalho chamando o método `unique()` na coluna `'source'` e imprimindo os resultados.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df\_logs \= pd.read\_csv('/datasets/visit\_log.csv', keep\_default\_na\=False)

  

df\_logs\['source'\] \= df\_logs\['source'\].replace('',"email")\# escreva seu código abaixo

  

print(df\_logs\['source'\].unique())

Dica

Mostrar a soluçãoValidar

Agora que preenchemos os valores ausentes, podemos calcular a taxa de conversão para cada fonte. Para fazer isso, precisaremos usar a função `groupby()` para agregação. Vamos revisar como usá-la.

Pergunta

Você tem um DataFrame armazenado na variável `df_logs`. Você quer agrupar os dados pela coluna `'source'` e extrair os valores na coluna `'user_id'`. Como você faria isso?

`df_logs.groupby('user_id')['source']`

`df_logs.groupby('source')['user_id']`

Isso mesmo. É isso mesmo que queremos.

`df.groupby('source')['user_id']`

Excelente!

Pergunta

Agora que você refrescou a sua memória sobre agrupamentos, vamos recapitular os métodos que podem ser aplicados aos dados agregados. No quiz anterior, você agregou os dados pela coluna `'source'` e extraiu a coluna `'user_id'`. Agora queremos contar o número de ocorrências de cada valor naquela coluna. Como você completaria o código para resolver essa tarefa?

`df_logs.groupby('source')['user_id'].sum()`

`df_logs.groupby('source')['user_id'].count()`

Isso mesmo. É isso mesmo que queremos.

`df_logs.groupby('source')['user_id'].max()`

Trabalho maravilhoso!

## Tarefas

### Tarefa 1

Para calcular a taxa de conversão de cada fonte de tráfego, primeiro determine quantas visitas de cada fonte existem.

Para encontrar o número total de visitas de cada fonte de tráfego, use `groupby()` para agrupar dados pela coluna `'source'`, depois conte o número de valores na coluna `'user_id'` do DataFrame agrupado. Atribua o resultado à variável `visits` e a imprima.

O pré-código já contém o que você fez para preencher os valores ausentes.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df\_logs \= pd.read\_csv('/datasets/visit\_log.csv', keep\_default\_na\=False)

df\_logs\['source'\] \= df\_logs\['source'\].replace('', 'email')

  

visits \= df\_logs.groupby('source')\['user\_id'\].count()\# escreva seu código aqui

print(visits)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

A seguir, determine o número de visitas em que uma compra foi feita para cada fonte, calculando a soma da coluna `'purchase'` para cada agrupamento de fonte. Depois disso, atribua os resultados à variável `purchases` e a imprima.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df\_logs \= pd.read\_csv('/datasets/visit\_log.csv', keep\_default\_na\=False)

df\_logs\['source'\] \= df\_logs\['source'\].replace('', 'email')

  

purchases \= df\_logs.groupby('source')\['purchase'\].sum()

print(purchases)

Dica

Mostrar a soluçãoValidar

### Tarefa 3

Calcule a taxa de conversão de cada fonte de tráfego, salve os resultados em `conversion` e a imprima. A taxa de conversão é a proporção de visitas em que uma compra foi feita. O pré-código tem as visitas e compras do exercício anterior.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

import pandas as pd

  

df\_logs \= pd.read\_csv('/datasets/visit\_log.csv', keep\_default\_na\=False)

df\_logs\['source'\] \= df\_logs\['source'\].replace('', 'email')

  

visits \= df\_logs.groupby('source')\['user\_id'\].count()

purchases \= df\_logs.groupby('source')\['purchase'\].sum()

  

conversion \= purchases / visits\# divida o número de compras pelo número de visitas

print(conversion)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-05-13-385Z.md
### Última modificação: 2025-05-28 19:05:13

# Preenchimento de valores quantitativos ausentes - TripleTen

Capítulo 3/11

Tratamento de valores ausentes e duplicados

# Preenchimento de valores quantitativos ausentes

Todo nosso trabalho duro valeu a pena! Ao trabalhar com valores ausentes no conjunto de dados da fonte de tráfego, determinamos que e-mails têm a maior taxa de conversão, enquanto anúncios contextuais não estão fazendo tanto sucesso. Poderíamos terminar a análise aqui, mas ainda não chegamos ao fundo da questão.

Além de dados sobre fonte de tráfego, o Google Analytics fornece informações resumidas sobre usuários que visitaram o site de comércio eletrônico. Felizmente, a empresa nos mandou outro conjunto de dados que contém essas informações de análise da web.

Nesta lição, você vai usar esse conjunto de dados para aprender a trabalhar com dados _quantitativos_ ausentes, visando comparar a média de tempo gasto no site para usuários de dispositivos móveis e computadores desktop.

Salvamos o arquivo na pasta `/datasets` com o nome `web_analytics_data.csv`. Carregue os dados na variável `analytics_data` e imprima as 10 primeiras linhas.

CódigoPYTHON

9

1

2

3

import pandas as pd

df \= pd.read\_csv("/datasets/web\_analytics\_data.csv")

print(df.head(10))

Dica

Mostrar a soluçãoValidar

Assim como no conjunto de dados da fonte de tráfego em que você trabalhou mais cedo, cada linha nesse conjunto de dados corresponde a uma visita ao site. Para cada visita, temos:

-   `user_id`: Identifica univocamente cada visitante do site
-   `device_type'`: Informa o tipo de dispositivo usado para acessar o site
-   `age'`: Diz a idade do visitante, em anos
-   `'time'`: Aponta o tempo gasto no site, em segundos

Pergunta

Quais colunas nesse conjunto de dados de análise da web são variáveis quantitativas?

Escolha quantas quiser

`'user_id'`

`'device_type'`

`'age'`

`'age'` é definitivamente uma coluna quantitativa!

`'time'`

Nós concordamos! Podemos dizer que a coluna `'time'` é uma variável quantitativa.

Muito bem!

É isso, `'age'` e `'time'` são quantitativas, enquanto `'user_id'` e `'device_type'` são categóricas.

Você pode estar pensando, "mas a coluna `'user_id'` contém números!". Isso é verdade, e se você chamou `info()` no DataFrame, viu que `'user_id'` tem o tipo de dados integer. Mesmo quando chamar `info()` não é pedido na tarefa, te incentivamos a fazer isso na sandbox. Mas lembre-se de que números identificadores são variáveis categóricas, não quantitativas. É importante levar em consideração essas propriedades dos dados conforme você realiza a análise.

Além disso, perceba que especificamos unidades para variáveis quantitativas (anos e segundos). É importante que você sempre esteja ciente das unidades quando trabalhar com dados quantitativos. Claro, talvez seja óbvio que a coluna `'age'` está em anos, mas pode não ser tão óbvio que a coluna `'time'` está em segundos. De qualquer jeito, é melhor não fazer suposições.

Agora vamos voltar aos valores ausentes. A partir das primeiras 10 linhas, vemos que tanto `'age'` quanto `'time'` têm valores `NaN`. Já que queremos fazer cálculos numéricos com essas colunas, não podemos preencher esses valores com strings como `'Unknown'` (Desconhecido) ou `''`. Em vez disso, precisamos preenchê-los com os **valores representativos** apropriados. A **média** ou **mediana** desses dados costuma ser usada como esses valores.

Como lembrete, a média é a soma dos valores dividida pelo total deles, e a mediana é o valor no meio quando os valores são ordenados.

### Médio

![](https://practicum-content.s3.amazonaws.com/resources/3.5.2_PT_1690871712.png)

### Mediana

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/ES/3.5_PT.png)

## Quando usar a média ou a mediana

Se soubermos tanto a média quanto a mediana, como devemos escolher qual usar como valor representativo? Bem, depende de qual delas é a melhor representante de um valor "típico" para o conjunto de dados.

A média não é um bom valor típico quando os dados com os quais você está trabalhando têm vários valores atípicos. Por exemplo, digamos que cinco funcionários em uma empresa têm salários de $30.000. Tanto a média como a mediana são iguais a $30.000.

Então um diretor de marketing é contratado com um salário de $90.000. A média agora aumentou para $40.000, enquanto a mediana permanece $30.000.

Esse valor atípico torna a mediana um indicador melhor do salário típico do que a média.

## Resumindo

Para decidir qual é o melhor valor representativo entre a média e a mediana, podemos seguir os passos abaixo:

1.  Determine se os dados têm valores atípicos significativos.
2.  Se não há valores atípicos significativos, calcule a média usando o método `mean()`.
3.  Se seus dados têm valores atípicos significativos, calcule a mediana usando o método `median()`.
4.  Substitua valores ausentes pela média ou pela mediana usando o método `fillna()`.

Também é importante notar que às vezes não precisamos preencher os valores ausentes. Por exemplo, se apenas uma pequena porção dos dados estiver ausente e esses dados forem aleatórios, pode ser melhor deixar os valores como `NaN`, e eles não serão incluídos em cálculos numéricos.

Conseguiremos analisar os dados dos usuários web para descobrir fatores além de fontes de tráfego que influenciam as decisões de compras. Mas para fazer isso, primeiro precisamos preencher os valores ausentes nas colunas `'age'` e `'time'`.

## Verificando se há valores atípicos

Antes de começar a preencher valores ausentes quantitativos, sempre verifique se há valores atípicos. Por enquanto, vamos te poupar de algumas etapas extras e dizer que não há grandes valores atípicos no conjunto de dados de marketing.

Pergunta

Dada a informação acima, qual seria o valor representativo dos dados?

A média

Usar a média só é apropriado quando não há valores atípicos nos dados.

A mediana

Excelente!

Já que não existem grandes valores atípicos, podemos usar a média como valor representativo tanto para `'age'` quanto para `'time'`. No código seguinte, calculamos a idade média e a salvamos em uma variável chamada `age_avg`, depois a usamos para preencher os valores ausentes em `'age'`:

```
import pandas as pd

analytics_data = pd.read_csv('/datasets/web_analytics_data.csv')

age_avg = analytics_data['age'].mean()
print("Idade média:", age_avg)

analytics_data['age'] = analytics_data['age'].fillna(age_avg)
```

```
Mean age: 32.48966336969903
```

Perceba que, por padrão, as linhas com valores ausentes em `'age'` não são incluídas no cálculo pelo método `mean()`.

E desse jeito, lidamos com os valores ausentes de `'age`. Agora é sua vez de preencher os valores ausentes de `'time'` nas tarefas.

## Tarefas

### Tarefa 1

Lembre-se de que queremos comparar o tempo médio gasto no site por usuários de computadores desktop e de dispositivos móveis, depois usar esses tempos médios para preencher os valores ausentes.

Comece dividindo os dados em dois DataFrames: um para visitas vindas de desktops e outro para visitas de dispositivos móveis. Atribua as visitas de desktop a uma variável chamada `desktop_data` e as visitas de aparelhos móveis a outra chamada `mobile_data`.

O pré-código já lê nos dados e preenche valores ausentes de `'age'`. Ele também chama `info()` após a criação dos DataFrames filtrados para que você possa ver quantos valores ausentes existem para cada dispositivo.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

import pandas as pd

  

analytics\_data \= pd.read\_csv('/datasets/web\_analytics\_data.csv')

  

age\_avg \= analytics\_data\['age'\].mean()

analytics\_data\['age'\] \= analytics\_data\['age'\].fillna(age\_avg)

  

desktop\_data \= analytics\_data\[analytics\_data\["device\_type"\] \== 'desktop'\]\# termine esta linha

mobile\_data \= analytics\_data\[analytics\_data\["device\_type"\] \== 'mobile'\]\# termine esta linha

  

desktop\_data.info()

print()

mobile\_data.info()

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Agora que os dados de desktop e dispositivos móveis estão separados, calcule o tempo médio de visita de cada dispositivo.

Atribua o tempo médio de visita de desktop à variável `desktop_avg` e a média de usuários de dispositivos móveis à `mobile_avg`.

O pré-código já contém código para imprimir os resultados, então não o modifique, apenas inclua o necessário.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

import pandas as pd

  

analytics\_data \= pd.read\_csv('/datasets/web\_analytics\_data.csv')

  

age\_avg \= analytics\_data\['age'\].mean()

analytics\_data\['age'\] \= analytics\_data\['age'\].fillna(age\_avg)

  

desktop\_data \= analytics\_data\[analytics\_data\['device\_type'\] \== 'desktop'\]

mobile\_data \= analytics\_data\[analytics\_data\['device\_type'\] \== 'mobile'\]

  

desktop\_avg \= desktop\_data\['time'\].mean()\# termine esta linha

mobile\_avg \= mobile\_data\['time'\].mean()\# termine esta linha

  

print(f"Tempo médio de desktop: {desktop\_avg:.2f} segundos")

print(f"Tempo médio dos dispositivos móveis: {mobile\_avg:.2f} segundos")

Dica

Mostrar a soluçãoValidar

### Tarefa 3

Use o tempo médio das visitas de desktop para preencher os valores ausentes da coluna `'time'` de `desktop_data` e a média de tempo de visita de dispositivos móveis para preenchê-los em `mobile_data`.

O pré-código contém seu trabalho das tarefas anteriores e chamadas de `info()` para você verificar quais valores ausentes foram preenchidos.

Você talvez tenha notado um `SettingWithCopyWarning` (Configuração com aviso de cópia) quando executou o código. Não é nada para se preocupar nesse caso, mas verifique a [documentação](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy) (a documentação está em inglês) se quiser aprender mais sobre isso.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

import pandas as pd

  

analytics\_data \= pd.read\_csv('/datasets/web\_analytics\_data.csv')

  

age\_avg \= analytics\_data\['age'\].mean()

analytics\_data\['age'\] \= analytics\_data\['age'\].fillna(age\_avg)

  

desktop\_data \= analytics\_data\[analytics\_data\['device\_type'\] \== 'desktop'\]

mobile\_data \= analytics\_data\[analytics\_data\['device\_type'\] \== 'mobile'\]

  

desktop\_avg \= desktop\_data\['time'\].mean()

mobile\_avg \= mobile\_data\['time'\].mean()

  

desktop\_data \= desktop\_data.fillna(desktop\_avg)

mobile\_data \= mobile\_data.fillna(mobile\_avg)

  

  

\# isso vai verificar se você tem algum valor ausente

desktop\_data.info()

print()

mobile\_data.info()

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-05-14-679Z.md
### Última modificação: 2025-05-28 19:05:15

# Como lidar com duplicados - TripleTen

Capítulo 3/11

Tratamento de valores ausentes e duplicados

# Como lidar com duplicados

Agora que você sabe como trabalhar com valores ausentes, é hora de aprender algumas técnicas para lidar com outro problema comum na etapa do pré-processamento de dados: valores duplicados.

No sprint de Python básico, você aprendeu sobre os métodos `duplicated()` e `drop_duplicates()`. Nesta lição, vamos revisar esses conceitos e apresentar outras formas de tratar dados duplicados.

## Procurar por duplicados à mão

A análise que você fez antes sobre as fontes de tráfego do site de comércio eletrônico deixou a equipe responsável por atrair novos clientes com mais perguntas. Ela quer saber se existe uma marca de celular popular que esteja pouco representada no site, assim como números que mostram os modelos de celular nos quais os consumidores têm mais interesse.

Para começar, ela quer que você descubra quantas lojas já estão vendendo certos aparelhos em um marketplace online.

Para completar essa análise, chame seu velho amigo `read_csv()` para ler dados localizados em `/datasets/phone_stock.csv`, salvando-os na variável `df_stock`. Imprima as primeiras cinco linhas da tabela.

CódigoPYTHON

9

1

2

3

4

import pandas as pd

  

df \= pd.read\_csv("/datasets/phone\_stock.csv")

print(df.head(5))

Mostrar a soluçãoExecutar

A tabela tem:

-   Uma coluna `'id'` com identificadores de produto
-   Uma coluna `'item'` com nomes de modelo
-   Uma coluna `'count'` com o número total de celulares em estoque de alguns modelos

Nas primeiras duas linhas já podemos ver dados duplicados.

Para Apple iPhone Xr 64GB, deve haver apenas uma linha de dados, e a contagem nessa linha deve ser 29. E se os dados de outros celulares também estiverem duplicados em outros pontos da tabela? Precisamos analisar o conjunto de dados de uma só vez, em vez de torcer que vamos encontrar os duplicados lendo uma impressão dos dados.

## Revisão: encontrar dados duplicados

Existem duas técnicas boas para encontrar dados duplicados:

### Técnica 1

Podemos usar o método `duplicated()` com `sum()` para obter o número de valores duplicados em uma única coluna ou linhas duplicadas em um DataFrame. Lembre-se de que se você chamar `duplicated()` sem chamar `sum()`, um objeto Series booleano com o mesmo comprimento do DataFrame vai ser impresso, com `True` onde existe um duplicado e `False` onde não existe. Execute o código abaixo para ver um exemplo:

```
import pandas as pd
df = pd.DataFrame({'col_1': ['A', 'B', 'A', 'A'], 'col_2': [1, 2, 2, 1]})

print('É assim que fica o conjunto de dados:')
print(df)
print('É assim que fica um objeto Séries booleano:')
print(df.duplicated())
print('É assim que fica um resultado do método duplicated() seguido por sum():')
print(df.duplicated().sum())
```

```
É assim que fica o conjunto de dados:
    col_1  col_2
0     A      1
1     B      2
2     A      2
3     A      1

É assim que fica um objeto Séries booleano:
0    False
1    False
2    False
3     True
dtype: bool

É assim que fica um resultado do método duplicated() seguido por sum():
1
```

Então, para verificar duplicadas, use `duplicated()` seguido pelo método `sum()`. A propósito, você pode ver os duplicados usando uma filtragem simples:

```
import pandas as pd
df = pd.DataFrame({'col_1': ['A', 'B', 'A', 'A'], 'col_2': [1, 2, 2, 1]})

print(df[df.duplicated()])
```

```
    col_1  col_2
3     A      1
```

Obtivemos a quarta linha (a linha com o índice 3) como resultado. Isso ocorre porque essa linha é um duplicado da primeira linha (com o índice 0).

### Técnica 2

Chame o método `value_counts()`. Esse método identifica todos os valores unívocos em uma coluna e calcula quantas vezes cada um aparece. Você pode aplicar esse método a um objeto Series para obter pares valor-frequência em ordem decrescente. As entradas mais frequentemente duplicadas podem ser encontradas no topo da lista. Aqui está um exemplo:

```
import pandas as pd
df = pd.DataFrame({'col_1': ['A', 'B', 'A', 'A'], 'col_2': [1, 2, 2, 1]})

print('É assim que fica o conjunto de dados:')
print(df)
print()
print('Aqui está o resultado da chamada do método value_counts() para col_1:')
print(df['col_1'].value_counts())
```

```
É assim que fica o conjunto de dados:
  col_1  col_2
0     A      1
1     B      2
2     A      2
3     A      1

Aqui está o resultado da chamada do método value_counts() para col_1:
A    3
B    1
Name: col_1, dtype: int64
```

A letra `A` aparece três vezes, enquanto `B` aparece apenas uma vez. Não podemos concluir que há três linhas duplicadas onde `'col_1'` é `A` sem mais análises. No entanto, agora temos uma hipótese de que esse pode ser o caso. Por exemplo, há duas linhas com `Apple iPhone Xr 64gb` no caso de celulares que precisamos mesclar para ter uma ideia clara sobre o estoque.

## Como lidar com duplicados

Como aprendemos anteriormente no sprint de Python básico, podemos lidar com linhas completamente duplicadas, como a primeira e a última no DataFrame do exemplo acima, usando o método `drop_duplicates()`:

```
import pandas as pd
df = pd.DataFrame({'col_1': ['A', 'B', 'A', 'A'], 'col_2': [1, 2, 2, 1]})

print('Conjunto de dados original:')
print(df)
print()
print('Conjunto de dados após a remoção dos duplicados:')
print(df.drop_duplicates())
```

```
Original dataset:
  col_1  col_2
0     A      1
1     B      2
2     A      2
3     A      1

Conjunto de dados após a remoção dos duplicados:
  col_1  col_2
0     A      1
1     B      2
2     A      2
```

Se você só quiser considerar duplicados em uma (ou algumas) das colunas em vez de linhas totalmente duplicadas, use o parâmetro `subset=`. Passe o nome da coluna (ou uma lista dos nomes das colunas) onde você quer procurar duplicados:

```
import pandas as pd
df = pd.DataFrame({'col_1': ['A', 'B', 'A', 'A'], 'col_2': [1, 2, 2, 1]})

print(df)
print()
print(df.drop_duplicates(subset='col_1'))
```

```
  col_1  col_2
0     A      1
1     B      2
2     A      2
3     A      1

  col_1  col_2
0     A      1
1     B      2
```

Mas a natureza dos dados duplicados de celular é um pouco diferente do que esse DataFrame do exemplo. Vamos dar outra olhada:

```
import pandas as pd

stock = pd.read_csv('/datasets/phone_stock.csv')
print(stock['item'].value_counts())
```

```
Apple iPhone Xr 64gb       1
Apple iPhone Xr 64GB       1
Xiaomi Redmi 6A 16GB       1
HUAWEI P30 lite            1
Samsung Galaxy A30 32GB    1
Samsung Galaxy A30 32gb    1
Honor 8X 64GB              1
Name: item, dtype: int64
```

Existem duas entradas tanto para Samsung Galaxy quanto para Apple iPhone. A única diferença entre as duas entradas para ambos os celulares é `gb` e `GB`. Esses também são duplicados, mas o Python não reconhece isso porque as strings não são idênticas.

A maneira mais simples de trabalhar com entradas duplicadas como essas é tornar todas as letras nas strings minúsculas chamando o método `lower()`. Aqui está um exemplo:

```
import pandas as pd
df = pd.DataFrame({'col_1': ['A', 'B', 'A', 'A'], 'col_2': [1, 2, 2, 1]})

print('col_1 original no conjunto de dados:')
print(df['col_1'])
print()
print('Conjunto de dados com valores em minúsculas em col_1')
print(df['col_1'].str.lower())
```

Observe que temos `.str` após `df['col_1']`, o que nos permite aplicar métodos de strings diretamente à coluna. Isso é exatamente o que precisamos fazer para aplicar o método `lower()` em seguida.

Observe que se você quiser substituir a coluna original pela nova com todas as letras minúsculas, precisa fazer a nova atribuição.

```
import pandas as pd
df = pd.DataFrame({'col_1': ['A', 'B', 'A', 'A'], 'col_2': [1, 2, 2, 1]})

# substituindo a coluna col_1
df['col_1'] = df['col_1'].str.lower()
print(df)
```

```
    col_1  col_2
0     a      1
1     b      2
2     a      2
3     a      1
```

Você deve ter certeza de que a caixa alta das letras não é importante antes de transformar todas as strings em letras minúsculas.

Vamos verificar o número de duplicados verdadeiros na coluna `'col_1'` após transformar os valores em minúsculas:

```
import pandas as pd
df = pd.DataFrame({'col_1': ['A', 'B', 'A', 'A'], 'col_2': [1, 2, 2, 1]})

df['col_1'] = df['col_1'].str.lower()
print(df['col_1'].duplicated().sum())
```

```
2
```

Como esperado, obtivemos `2` como resultado. Isso porque a terceira e a quarta linhas são iguais à primeira.

Se você quiser mudar as letras entre maiúsculo ou minúsculo em apenas uma parte da string, use o método `replace()` que aprendeu no curso de Python básico. Por exemplo, você pode mudar todas as ocorrências de `'GB'` para `'gb'`na coluna `'item'` dos dados sobre celulares:

```
import pandas as pd

stock = pd.read_csv('/datasets/phone_stock.csv')
stock['item'] = stock['item'].str.replace('GB', 'gb')

print(stock.head())
```

```
                    id                     item  count
0  100480924     Apple iPhone Xr 64gb     10
1  100480924     Apple iPhone Xr 64gb     19
2  100480959     Xiaomi Redmi 6A 16gb     44
3  100480975          HUAWEI P30 lite     38
4  100480988  Samsung Galaxy A30 32gb     49
```

Se você não tem certeza sobre qual abordagem é melhor, pode manter a coluna original e criar uma coluna adicional onde as strings foram modificadas. Por exemplo, você pode salvar o resultado da substituição feita na coluna `'item'` em uma nova coluna chamada `'item_modified'`:

```
import pandas as pd

stock = pd.read_csv('/datasets/phone_stock.csv')
stock['item_modified'] = stock['item'].str.replace('GB', 'gb')

print(stock.head())
```

```
                    id                     item  count            item_modified
0  100480924     Apple iPhone Xr 64gb     10     Apple iPhone Xr 64gb
1  100480924     Apple iPhone Xr 64gb     19     Apple iPhone Xr 64gb
2  100480959     Xiaomi Redmi 6A 16gb     44     Xiaomi Redmi 6A 16gb
3  100480975          HUAWEI P30 lite     38          HUAWEI P30 lite
4  100480988  Samsung Galaxy A30 32gb     49  Samsung Galaxy A30 32gb
```

Vamos usar essa técnica para consertar o nosso conjunto de dados do estoque de celulares.

## Tarefas

### Tarefa 1

1.  Transforme os nomes dos modelos de celulares em letras minúsculas usando o método `str.lower()` e salve-os em uma nova coluna chamada `'item_lowercase'`, mas mantenha a coluna original `'item'`.
    
2.  Imprima as primeiras linhas da tabela atualizada e veja o que acontece.
    

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df\_stock \= pd.read\_csv('/datasets/phone\_stock.csv')

  

df\_stock\['item\_lowercase'\] \= df\_stock\['item'\].str.lower() \# escreva seu código aqui

  

print(df\_stock.head())

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Usando sua coluna recém-criada `'item_lowercase'` e o método `sum()`, calcule o número total dos dois modelos de celulares:

1.  `'apple iphone xr 64gb'`
2.  `'samsung galaxy a30 32gb'`

Para contar o número de modelos de celulares Apple, filtre o DataFrame com base na coluna `'item_lowercase'` para incluir apenas as linhas com `'apple iphone xr 64gb'` como o valor. Em seguida, extraia a coluna `'count'` do DataFrame filtrado e aplique o método `sum()` a ela. Armazene o número total de celulares Apple na variável `apple`.

Para celulares Samsung, siga o mesmo procedimento, com a única diferença sendo que o número total de celulares Samsung deve ser salvo na variável `samsung`.

O pré-código já contém código para imprimir os resultados, então não o modifique.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

import pandas as pd

  

df\_stock \= pd.read\_csv('/datasets/phone\_stock.csv')

df\_stock\['item\_lowercase'\] \= df\_stock\['item'\].str.lower()

  

apple \= df\_stock\[df\_stock\['item\_lowercase'\] \== "apple iphone xr 64gb"\]\['count'\].sum()

  

samsung \= df\_stock\[df\_stock\['item\_lowercase'\] \== "samsung galaxy a30 32gb"\]\['count'\].sum()

  

  

  

print("Número total de celulares Apple:", apple)

print("Número total de celulares Samsung:", samsung)

Dica

Mostrar a soluçãoValidar

### Tarefa 3

Agora exclua as linhas com celulares duplicados chamando `drop_duplicates()` em `df_stock`. Precisamos excluir linhas com base apenas na coluna `item_lowercase`, então certifique-se de usar esse nome como valor para o parâmetro `subset=`.

Lembre-se de que, depois de remover duplicados, precisamos chamar o método `reset_index()` com o parâmetro `drop=True`. Isso nos permite corrigir a indexação e remover o índice antigo.

A propósito, você pode fazer tudo isso em apenas uma linha de código! Pode ser um pouco desafiador, mas pense em como pode fazer isso.

Seu resultado final deve ser atribuído de volta a `df_stock`. Imprima as primeiras linhas de `df_stock` quando terminar.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df\_stock \= pd.read\_csv('/datasets/phone\_stock.csv')

df\_stock\['item\_lowercase'\] \= df\_stock\['item'\].str.lower()

  

df\_stock \= df\_stock.drop\_duplicates(subset\="item\_lowercase").reset\_index(drop\=True) \# escreva seu código aqui

print(df\_stock.head())

Dica

Mostrar a soluçãoValidar

### Tarefa 4

Vamos fazer algo muito legal! Não se preocupe, você tem tudo que precisa para isso, e nós vamos ajudar.

O pré-código inclui uma linha que você escreveu anteriormente para se livrar de duplicados, então agora temos o DataFrame `df_stock` sem duplicados. Seu objetivo é definir os valores corretos na coluna `'count'` para as linhas em que `'item'` é `'Apple iPhone XR 64GB'` e `'Samsung Galaxy A30 32GB'`.

Os valores que você vai definir já foram calculados anteriormente e estão armazenados nas variáveis `apple` e `samsung` no pré-código.

A melhor maneira de atualizar os valores na coluna `'count'` é usar o atributo `loc[]`, que pode substituir os valores em um determinado lugar.

Vamos dar uma olhada em `df_stock` após a remoção de duplicados para ilustrar como `loc[]` pode nos ajudar a atualizar valores:

```
                    id                     item  count           item_lowercase
0  100480924     Apple iPhone Xr 64gb     10     apple iphone xr 64gb
1  100480959     Xiaomi Redmi 6A 16GB     44     xiaomi redmi 6a 16gb
2  100480975          HUAWEI P30 lite     38          huawei p30 lite
3  100480988  Samsung Galaxy A30 32GB     49  samsung galaxy a30 32gb
4  100481020            Honor 8X 64GB     64            honor 8x 64gb
```

Para atualizar o valor na primeira linha (índice 0) e a coluna `count'` para o modelo Apple iPhone, podemos usar `loc[]`. Passamos dois valores para `loc[]` para especificar o índice da linha e o nome da coluna e, em seguida, usamos o sinal `=` para definir o valor desejado:

```
df_stock.loc[0,'count'] = 33
```

No exemplo anterior, o valor `33` foi definido, mas nós queremos definir o valor da variável `apple` que anteriormente calculamos e salvamos.

Esse foi um exemplo para Apple iPhone. Para Samsung, o procedimento é o mesmo, exceto que passamos diferentes valores para o atributo `loc[]`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

import pandas as pd

  

df\_stock \= pd.read\_csv('/datasets/phone\_stock.csv')

df\_stock\['item\_lowercase'\] \= df\_stock\['item'\].str.lower()

  

apple \= df\_stock\[df\_stock\['item\_lowercase'\] \== 'apple iphone xr 64gb'\]\['count'\].sum()

samsung \= df\_stock\[df\_stock\['item\_lowercase'\] \== 'samsung galaxy a30 32gb'\]\['count'\].sum()

  

df\_stock \= df\_stock.drop\_duplicates(subset\='item\_lowercase').reset\_index(drop\=True)

  

df\_stock.loc\[0,'count'\] \= apple

df\_stock.loc\[3,'count'\] \= samsung

print(df\_stock)\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-05-16-042Z.md
### Última modificação: 2025-05-28 19:05:16

# Quiz do capítulo - TripleTen

Capítulo 3/11

Tratamento de valores ausentes e duplicados

# Quiz do capítulo

Pergunta

O que são dados ausentes?

Escolha quantas quiser

São valores desconhecidos para uma variável.

Dados ausentes ou valores ausentes significam que não sabemos nada sobre os valores reais, que não temos esses valores. Dados ausentes costumam ser indicados como `None` em um objeto DataFrame.

São strings vazias como `''`.

São valores booleanos `False`.

São valores que não têm significado para uma variável.

Dados ausentes podem ser codificados de forma diferente de `None` com valores de aparência regular, por exemplo, `1`, `0`, `''`, `NA` que não têm nenhum significado no contexto da variável específica.

Seu entendimento sobre o material é impressionante!

Pergunta

Você leu os dados de `visit_log.csv` em um objeto DataFrame e verificou se há dados ausentes com `info()`.

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 200000 entries, 0 to 199999
Data columns (total 4 columns):
 #   Column    Non-Null Count   Dtype 
---  ------    --------------   ----- 
 0   user_id   200000 non-null  int64 
 1   source    198326 non-null  object
 2   email     13953 non-null   object
 3   purchase  200000 non-null  int64 
dtypes: int64(2), object(2)
memory usage: 6.1+ MB
```

O método `sample(10)` imprimiu os seguintes dez registros aleatórios:

```
       user_id source       email  purchase
 4  5644686960  email  c129aa540a         0
11  8623045648  email  d6d19c571c         0
18  5739438900         19379ee49c         0
19  7486955288  email  09c27794fa         0
22  1397217221    NaN  79ac569f0b         0
33  7298923004  email  1fe184ed73         0
43  6034222291  email  fb58a27f03         0
49  5062457902    NaN  9ddce3a861         0
56  5690036640  email  a088a48182         0
66  9963049355         9cc43ebd15         0
```

Você sabe que a visita de cada usuário deve estar associada a um dos três tipos de fonte: `email`, `android_push` ou `ios_push`.

Quantos valores ausentes provavelmente existem na coluna `source` (considerando que 200000-198326=1674)?

Exatamente 1.674 valores

Pelo menos 1.674 valores

Mais de 1.674 valores

Existem strings vazias em `source`. Esses espaços vazios são espaços reservados para os dados ausentes. Podemos adicionar esses registros aos 1.674 com `NaN` que `info()` já reconheceu.

Muito bem!

Pergunta

Você agora considera as strings vazias no campo `source` como valores ausentes e deseja preenchê-las com o valor `email`. Após conversar com o departamento de marketing, isso parece a substituição óbvia. Qual método você usa para fazer isso?

`fillna()`

`replace()`

A função `replace()`, quando chamada em uma coluna do DataFrame, permite trocar um valor por outro. Lembre-se de que a sintaxe é `replace(valor_antigo, valor_novo)`.

`fillna()` ou `replace()`

Nem `fillna()` nem `replace()`

Excelente!

Pergunta

Você tem dados sobre clientes de uma operadora de celular que contêm uma coluna numérica chamada `churned` com 1.0 para os clientes que rescindiram os contratos e `NaN` para os demais. Qual valor você usa para substituir `NaN`?

Escolha quantas quiser

`''`

`False`

0

Preencher `churned` com `0` para clientes que não rescindiram o contrato parece sensato. Isso pode ser tratado como um análogo de `False`.

Qualquer número inteiro que não seja 1.

Preencher `churned` com qualquer outro valor numérico parece válido, pois nos permite diferenciar os dois tipos de cliente. O clássico seria definir o valor como 0.

Fantástico!

Pergunta

Você tem dados sobre os clientes de um banco, e há valores ausentes na coluna `income`. Qual estatística você usa para preencher os valores ausentes com um valor representativo?

`mean()`

`median()`

A mediana é uma boa estimativa de valor esperado quando os dados são altamente dispersos. Isso ocorre porque ela não é tão distorcida por valores atípicos quanto a média.

Excelente!

Pergunta

Você tem 15 milhões de registros sobre visitas online de clientes a um site e gostaria de estudar a duração típica de uma visita. Devido a problemas técnicos anteriores, existem 3.348 registros nos dados com valores desconhecidos para `client_id`. O que você faz?

Escolha quantas quiser

Vou preencher com o valor médio (a média).

Vou preencher com o valor mediano (a mediana) de `client_id`.

Vou descartar os registros.

Dada a pequena porcentagem de registros, essa opção parece bastante segura.

Não vou descartar os registros, mas excluí-los da análise.

Dada a pequena porcentagem de registros, essa opção parece bastante segura. Também podemos descartá-los completamente para evitar ter que filtrá-los de alguma forma em todas as etapas posteriores da análise.

Fantástico!

Vamos fornecer um pequeno conjunto de dados sobre produtos, categorias e preços. No código abaixo, lemos o conjunto e o armazenamos na variável `df`. Veja como ficam as primeiras linhas do conjunto de dados:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_24_1690872297.png)

Esse conjunto de dados tem alguns problemas, incluindo valores duplicados e ausentes. Vamos tentar encontrar esses valores e removê-los.

Na primeira etapa, vamos dar uma olhada nas linhas com os valores ausentes. Já identificamos a presença deles na coluna `'price'` criando um objeto Series de valores booleanos e armazenando-o na variável `mis_booleans`.

Agora use essa variável para filtrar o DataFrame original, extraindo as linhas que contêm valores ausentes, e salve o resultado da filtragem na variável `mis_rows`. Imprima o resultado.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/products\_data.csv')

mis\_booleans \= df\['price'\].isna()

  

mis\_rows \= df\[mis\_booleans\]\# escreva seu código aqui

print(mis\_rows)

Dica

Mostrar a soluçãoValidar

Em seguida, vamos preencher o único valor ausente que temos. Para fazer isso:

1.  Agrupe os dados pela coluna `'category'` e extraia a coluna `'price'`.
2.  Aplique um método apropriado para calcular o valor médio de preço de cada categoria e salve-o na variável `avg_per_category`.

Na próxima tarefa, você vai usar esse valor para substituir o valor ausente.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/products\_data.csv')

  

avg\_per\_category \= df.groupby('category')\['price'\].mean()\# escreva seu código aqui

  

print(avg\_per\_category)

Dica

Mostrar a soluçãoValidar

Está quase tudo pronto para preencher o valor ausente. Para fazer isso:

1.  Extraia o preço médio da categoria que nos interessa (aquela a que pertence uma linha com o valor ausente, ou seja, `'hair care'`). Para fazer isso, faça a indexação de um objeto Series usando um número de índice e salve na variável `mean_val`. Observação: se você precisa encontrar o índice necessário, procure-o no resultado da tarefa anterior.
2.  Imprima `mean_val`.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

9

import pandas as pd

  

df \= pd.read\_csv('/datasets/products\_data.csv')

avg\_per\_category \= df.groupby('category')\['price'\].mean()

  

mean\_val \= avg\_per\_category\[2\]\# escreva seu código aqui

  

  

print(mean\_val)

Dica

Mostrar a soluçãoValidar

Agora é hora de substituir o valor ausente por uma média na sua categoria. Em Python, você pode usar o atributo `loc[]` para fazer isso. Comece localizando a linha que contém o valor que você quer substituir.

```
    category      product  price
7  hair care  conditioner    NaN
```

Em seguida, passe o índice apropriado e o nome da coluna para `loc[]` e use o caractere `=` para definir o valor desejado.

Você pode determinar o índice e o nome da coluna a serem usados em `loc[]` examinando as linhas que contêm o valor ausente.

Quando terminar, imprima a variável `df`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

import pandas as pd

  

df \= pd.read\_csv('/datasets/products\_data.csv')

avg\_per\_category \= df.groupby('category')\['price'\].mean()

  

mean\_val \= avg\_per\_category\[2\]

  

df.loc\[7,'price'\] \= mean\_val\# escreva seu código aqui

  

print(df)

Dica

Mostrar a soluçãoValidar

Pergunta

Você está verificando dados em busca de registros duplicados. Você deseja obter a lista de valores unívocos e suas contagens. Qual função você usa?

`duplicated()`

`value_counts()`

Esse método identifica todos os valores unívocos em uma coluna e calcula quantas vezes cada um deles aparece. As entradas mais duplicadas aparecem no topo da lista.

`unique()`

`unique_counts()`

Você conseguiu!

Mencionamos anteriormente que o nosso conjunto de dados tem problemas, incluindo duplicados. Para identificar os duplicados, aplicamos a função `value_counts()` à coluna `'product'`, o que gerou o seguinte resultado:

```
product
lipstic        1
LIPSTIC        1
pacifier       1
shampoo        1
diapers        1
lotion         1
hair gel       1
conditioner    1
Name: count, dtype: int64
```

Imediatamente percebemos que `'lipstic'` e `'LIPSTIC'` aparecem na coluna como valores separados. Para lidar com isso, recomendamos criar uma nova coluna em que todos os valores estejam em minúsculas, para garantir a consistência. Substitua a coluna `'product'` por uma versão em letras minúsculas e imprima novamente.

CódigoPYTHON

9

1

2

3

4

5

6

import pandas as pd

  

df \= pd.read\_csv('/datasets/products\_data\_no\_nans.csv')

  

df\['product'\] \= df\['product'\].str.lower()

print(df\['product'\])

Dica

Mostrar a soluçãoValidar

Por fim, vamos lidar com os duplicados. Depois de converter os valores da coluna `'product'` para minúsculas, o conjunto de dados está assim:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_26_1690872322.png)

As linhas com os índices `0` e `1` são duplicados óbvios. Vamos descartá-las. Não se esqueça de redefinir os índices depois de descartar as linhas.

Assim que terminar, imprima o DataFrame inteiro.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/products\_data\_no\_nans.csv')

df\['product'\] \= df\['product'\].str.lower()

  

df \= df.drop\_duplicates().reset\_index(drop\=True)\# escreva seu código aqui

print(df)

Dica

Mostrar a soluçãoValidar

Pergunta

Vamos praticar como trabalhar com duplicados mais um pouco usando outro exemplo. Agora você está analisando dados sobre as vendas de carros Bentley para clientes diferentes e quer categorizá-los por cidade. O `value_counts()` na coluna de `city` retorna o seguinte:

```
New Your    1034
London      543
Paris       345
LONDON      32
 London     1
```

Como você trata os valores de `city`?

`drop_duplicates()`

Removo o espaço de `London` (aquele com um espaço na frente), converto todos os "Londons" para que a primeira letra fique maiúscula usando `title()` e executo `drop_duplicates()`.

Removo o espaço de `London` (com um espaço na frente) e converto todos os "Londons" para que a primeira letra fique maiúscula.

Converter `London`, `LONDON` e `London` (com um espaço na frente) para que a primeira letra fique maiúscula é a ideia correta. Nesse caso, ter o mesmo nome de `city` faz sentido sem ter duplicados.

Não faço nada.

Trabalho maravilhoso!

Por fim, observamos anteriormente que há um valor `'tbc'` na coluna `'category'`. Confira como é esta linha:

```
   category    product     price
1       tbc    pacifier     9.0
```

Vamos usar o método `replace()` para definir um valor apropriado aqui. Está claro que o produto `'pacifier'` (chupeta) pertence à categoria `'baby care'` (produtos para bebês) que temos no conjunto de dados. Depois de substituir o valor, imprima o DataFrame.

CódigoPYTHON

9

1

2

3

4

5

6

import pandas as pd

  

df \= pd.read\_csv('/datasets/products\_data\_no\_nans\_and\_dupl.csv')

  

df\['category'\] \= df\['category'\].str.replace('tbc', 'baby care')

print(df)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-05-21-585Z.md
### Última modificação: 2025-05-28 19:05:21

# Conclusão - TripleTen

Capítulo 3/11

Tratamento de valores ausentes e duplicados

# Conclusão

Muito bem! Você fez um ótimo trabalho lidando com todos esses valores ausentes e duplicados, e agora é hora de resumir tudo o que você aprendeu.

Pergunta

Agora você é capaz de:

Escolha quantas quiser

Identificar e contar valores ausentes usando várias técnicas

Se ainda não tiver certeza, você pode revisar a lição em [Contagem de valores ausentes](https://tripleten.com/trainer/data-analyst/lesson/d9e212a4-d061-47de-b4e5-6823edec103c/){target=”blank”}

Filtrar conjuntos de dados para extrair linhas com valores ausentes

Se ainda não tiver certeza, você pode revisar a lição em [Filtragem de DataFrames com NaNs](https://tripleten.com/trainer/data-analyst/lesson/de0994e4-5d33-42f8-95f3-48b6e45d517b/task/518289f1-69bb-46f8-8493-9ff00c049817/){target=”blank”}

Distinguir entre valores categóricos e quantitativos

Se ainda não tiver certeza, você pode revisar a lição em [Preenchimento de valores categóricos ausentes](https://tripleten.com/trainer/data-analyst/lesson/58e1f8d5-f742-4b67-96c4-2c0cf14d1b0c/){target=”blank”}

Preencher valores categóricos e quantitativos ausentes

Se ainda não tiver certeza, você pode revisar as lições em [Preenchimento de valores categóricos ausentes](https://tripleten.com/trainer/data-analyst/lesson/58e1f8d5-f742-4b67-96c4-2c0cf14d1b0c/){target=”blank”} e [Preenchimento de valores quantitativos ausentes](https://tripleten.com/trainer/data-analyst/lesson/a6074f59-efc0-48fd-89f8-3bf2216ec487/){target=”blank”}

Identificar, eliminar e até mesmo mesclar valores duplicados

Se ainda não tiver certeza, você pode revisar as lições em [Como lidar com duplicados](https://tripleten.com/trainer/data-analyst/lesson/4566dc3d-796c-4a22-bf5f-1fc6beea1e94/){target=”blank”}

Muito bem!

### 🎉

Você aprendeu como trabalhar com valores ausentes e duplicados nos dados e viu alguns exemplos de como isso pode acontecer no mundo real. Da próxima vez que você responder a uma pesquisa e tiver a opção de pular algumas perguntas, pense em como a pessoa responsável pela análise desses resultados vai lidar com eles. Ela vai remover os valores ausentes? Ou será que vai preenchê-los? Qual é o tipo de variável e como ela vai impactar a análise? Muito em breve talvez você esteja no outro lado da pesquisa (ou com dados semelhantes), analisando e descobrindo como trabalhar com valores ausentes.

### Leve isso com você

Faça download do sumário do capítulo e folha de conclusões para que você possa consultá-los quando necessário.

-   [Resumo do capítulo: Tratamento de valores ausentes e duplicados](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/PT/3_Resumo_do_captulo_Tratamento_de_valores_ausentes_e_duplicados.pdf)
-   [Folha de conclusões: Tratamento de valores ausentes e duplicados](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/PT/3_Folha_de_concluses_Tratamento_de_valores_ausentes_e_duplicados.pdf)
-   [Um guia rápido sobre como lidar com valores ausentes](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/PT/3_Um_guia_rpido_sobre_como_lidar_com_valores_ausentes.pdf)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-34-231Z.md
### Última modificação: 2025-05-28 19:07:34

# Introdução - TripleTen

Capítulo 4/11

Filtragem de dados

# Introdução

Como especialista de dados, você vai trabalhar com grandes conjuntos: às vezes, eles terão milhões ou até bilhões de linhas. No entanto, geralmente será necessário trabalhar apenas com uma pequena porção do conjunto de dados completo.

Para encontrar as informações relevantes, você terá que selecionar os critérios apropriados para filtrar o conjunto. Depois, você poderá processar e analisar esses subconjuntos menores para explorar as questões de interesse de sua pesquisa.

Neste capítulo, vamos usar conjuntos de dados reais para te ensinar habilidades de filtragem mais avançadas. Ao final deste capítulo, você vai:

-   entender como índices funcionam em DataFrames e objetos Series;
-   aprender a indexar DataFrames usando `loc[]` e `iloc[]`;
-   ser capaz de filtrar DataFrames escrevendo strings de consulta com o método `query()` e com base em condições lógicas complexas;
-   ser capaz de filtrar e substituir valores ao mesmo tempo usando o método `where()`.

Serão necessárias cerca de 2 horas para concluir este capítulo. Você está com tudo pronto para se tornar um mestre da filtragem? Vamos lá!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-35-531Z.md
### Última modificação: 2025-05-28 19:07:35

# Índices em DataFrames e Series - TripleTen

Capítulo 4/11

Filtragem de dados

# Índices em DataFrames e Series

Para se tornar mestre na filtragem de dados, é essencial entender o conceito de índices nos DataFrames e Series da Pandas, porque os índices afetam a forma como um DataFrame é filtrado quando você usa outro DataFrame ou Series para fazer a filtragem.

Você já aprendeu um pouco sobre DataFrames e indexação nos objetos Series no sprint de Python básico. Nesta lição, vamos aprender mais informações técnicas sobre o que é um índice. Aprenderemos sobre diferentes tipos de índices, revisaremos o atributo `loc[]`, apresentaremos o atributo `iloc[]` e veremos como definir seus próprios valores de índices nos DataFrames e Series.

## O atributo `index`

Objetos Series e DataFrames na Pandas sempre têm índices, armazenados no atributo `index`. Toda vez que você criar um objeto Series ou DataFrame, o atributo `index` correspondente será criado automaticamente com valores padrão se você não especificar os valores de índice.

Vamos criar um objeto Series para demonstrar:

```
import pandas as pd

oceans = pd.Series(['Pacific', 'Atlantic', 'Indian', 'Southern', 'Arctic'])

print(oceans)
```

```
0     Pacific
1    Atlantic
2      Indian
3    Southern
4      Arctic
dtype: object
```

Aqui usamos a classe `Series()` da `pandas` para criar um objeto Series armazenado na variável `oceans` usando uma lista dos nomes dos oceanos. A impressão do objeto Series tem três componentes:

1.  Os valores do Series à direita (nomes dos oceanos)
2.  Os valores de índice à esquerda (números inteiros de 0 a 4)
3.  O tipo de dados da pandas dos elementos do objeto Series na parte inferior (`dtype: object`)

Não especificamos um índice quando criamos o Series, então temos o índice padrão: números inteiros consecutivos que começam em zero.

Vamos dar uma olhada mais de perto no índice acessando o atributo `index` e seu tipo:

```
import pandas as pd

oceans = pd.Series(['Pacific', 'Atlantic', 'Indian', 'Southern', 'Arctic'])

print(oceans.index)
print(type(oceans.index))
```

```
RangeIndex(start=0, stop=5, step=1)
<class 'pandas.core.indexes.range.RangeIndex'>
```

Esse índice é do tipo `RangeIndex`. Esta é a primeira vez que encontramos esse tipo. Basicamente, é um intervalo de índices usado em um objeto Series ou um DataFrame. Esse é o tipo padrão de índices usados por DataFrames e Series, mas ele não é o único tipo de dados de índices na pandas.

O tipo `RangeIndex` tem três parâmetros: `start=`, `stop=` e `step=`.

-   `start=` é o primeiro índice de um objeto Series ou um DataFrame.
-   `stop=` é o último índice. Como você pode ver, ele não está incluído em um objeto Series ou DataFrame.
-   `step=` é o passo dado ao mover do primeiro índice ao último. Por padrão, é `1`.

Como uma opção, podemos definir índices como quisermos. Por exemplo, vamos definir os valores de índice usando uma lista de números inteiros de 1 a 5. Para isso, atribuímos a nossa lista de números ao atributo `index` de `oceans`:

```
import pandas as pd

oceans = pd.Series(['Pacific', 'Atlantic', 'Indian', 'Southern', 'Arctic'])

oceans.index = [1, 2, 3, 4, 5]

print(oceans.index)
print(type(oceans.index))
```

```
Int64Index([1, 2, 3, 4, 5], dtype='int64')
<class 'pandas.core.indexes.numeric.Int64Index'>
```

Nesse caso, nosso índice é do tipo de dados `Int64Index`, que é um tipo geral para um índice de valores inteiros que não são gerados a partir de um objeto range. Não se preocupe sobre a variedade de novos tipos de dados. Esses são apenas tipos de índices que você pode manter como estão ou definir por conta própria.

Também podemos definir o atributo `index` usando o parâmetro `index=` na chamada de `Series()`, o que é preferível quando não queremos o índice padrão e não planejamos modificar o índice mais tarde:

```
import pandas as pd

oceans = pd.Series(['Pacific', 'Atlantic', 'Indian', 'Southern', 'Arctic'],
                   index=[1, 2, 3, 4, 5])

print(oceans.index)
print(type(oceans.index))
```

```
Int64Index([1, 2, 3, 4, 5], dtype='int64')
<class 'pandas.core.indexes.numeric.Int64Index'>
```

Por fim, vamos definir os valores de índice como strings e ver qual tipo de dados teremos no índice:

```
import pandas as pd

oceans = pd.Series(['Pacific', 'Atlantic', 'Indian', 'Southern', 'Arctic'],
                   index=['A', 'B', 'C', 'D', 'E'])

print(oceans)
print()
print(oceans.index)
print(type(oceans.index))
```

```
A     Pacific
B    Atlantic
C      Indian
D    Southern
E      Arctic
dtype: object

Index(['A', 'B', 'C', 'D', 'E'], dtype='object')
<class 'pandas.core.indexes.base.Index'>
```

Para os valores de índice de strings (ou de uma combinação de strings e outros tipos), temos o tipo de dados geral de índice, `Index`.

## Indexação usando `loc[]`

Agora que você tem um entendimento mais profundo do que é um índice na Pandas, vamos falar sobre a **indexação**. A terminologia pode ser confusa, então tenha em mente as seguintes definições:

-   **Índice**: é um componente de um objeto Series ou DataFrame que pode ser acessado usando o atributo `index`.
-   **Indexação**: é o processo de acessar valores em um objeto Series ou DataFrame usando os respectivos índices.

Você já sabe como usar o atributo `loc[]` para acessar elementos de um DataFrame usando valores de índice e nomes de colunas, então faremos apenas uma revisão breve.

Vamos começar criando um DataFrame:

```
import pandas as pd

states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas']
flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom']
insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee']
index   = ['state 1', 'state 2', 'state 3', 'state 4']

df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index)

print(df)
```

```
            state                  flower                          insect
state 1   Alabama                Camellia               Monarch butterfly
state 2    Alaska           Forget-me-not  Four-spotted skimmer dragonfly
state 3   Arizona  Saguaro cactus blossom          Two-tailed swallowtail
state 4  Arkansas           Apple blossom              European honey bee
```

Criamos um DataFrame pequeno que contém a flor e o inseto oficial de quatro estados dos EUA e definimos os valores de índice como strings usando o parâmetro `index=`.

Lembre-se de que podemos acessar elementos do DataFrame com `loc[]` passando valores de índice e nomes de colunas, como `df.loc[index_value, col_name]`. Assim, veja como podemos obter o inseto do estado de Arkansas:

```
import pandas as pd

states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas']
flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom']
insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee']
index   = ['state 1', 'state 2', 'state 3', 'state 4']

df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index)

print(df.loc['state 4', 'insect'])
```

```
European honey bee
```

Isso retornou um único elemento. Também podemos usar `loc[]` para obter vários elementos como um objeto Series ou outro DataFrame. Para obter vários elementos, tudo o que você precisa fazer é passar para `loc[]` uma lista de índices e uma lista de colunas da seguinte maneira:

```
df.loc[[index_value_1, index_value_1], [col_name_1, col_name_2]]
```

Agora pratique usando `loc[]` dessa maneira para obter apenas as colunas `'flower'` e `'insect'` do Alabama e do Arizona. Salve o resultado na variável `filtered_df`. Para entender quais índices o Alabama e o Arizona têm, procure no DataFrame:

```
            state                  flower                          insect
state 1   Alabama                Camellia               Monarch butterfly
state 2    Alaska           Forget-me-not  Four-spotted skimmer dragonfly
state 3   Arizona  Saguaro cactus blossom          Two-tailed swallowtail
state 4  Arkansas           Apple blossom              European honey bee
```

Seu código deve retornar outro DataFrame. Imprima `filtered_df`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

import pandas as pd

  

states \= \['Alabama', 'Alaska', 'Arizona', 'Arkansas'\]

flowers \= \['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom'\]

insects \= \['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee'\]

index \= \['state 1', 'state 2', 'state 3', 'state 4'\]

  

df \= pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index\=index)

  

filtered\_df \= df.loc\[\['state 1', 'state 3'\], \['flower', 'insect'\]\]

print(filtered\_df)

Dica

Mostrar a soluçãoValidar

Em seguida, vamos aprender como usar indexação para obter um intervalo de índices para uma única coluna. Para fazer isso, precisamos apenas especificar o primeiro e o último índice separados por um caractere `:`. Aqui está um exemplo que retorna as flores dos três primeiros índices de um DataFrame:

```
import pandas as pd

states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas']
flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom']
insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee']
index   = ['state 1', 'state 2', 'state 3', 'state 4']

df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index)

print(df.loc['state 1': 'state 3', 'flower'])
```

```
state 1                  Camellia
state 2             Forget-me-not
state 3    Saguaro cactus blossom
Name: flower, dtype: object
```

Como podemos ver, obtivemos os valores na coluna `'flower'` dos três primeiros estados selecionados indexando `'state 1': 'state 3'`.

De forma parecida, você pode selecionar várias colunas, bem como vários índices:

```
import pandas as pd

states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas']
flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom']
insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee']
index   = ['state 1', 'state 2', 'state 3', 'state 4']

df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index)

print(df.loc['state 1': 'state 3', 'flower': 'insect'])
```

```
                        flower                          insect
state 1                Camellia               Monarch butterfly
state 2           Forget-me-not  Four-spotted skimmer dragonfly
state 3  Saguaro cactus blossom          Two-tailed swallowtail
```

O código acima retorna uma tabela filtrada com valores das colunas de `'flower'` até `'insect'`, para o intervalo dos índices de `'state 1'` até e incluindo `'state 3'`.

Agora vamos praticar. Desta vez, use `loc[]` para obter apenas a coluna `'insect` de todos os estados, _exceto_ Alabama. Seu código deve retornar um objeto Series. Imprima o resultado.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

import pandas as pd

  

states \= \['Alabama', 'Alaska', 'Arizona', 'Arkansas'\]

flowers \= \['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom'\]

insects \= \['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee'\]

index \= \['state 1', 'state 2', 'state 3', 'state 4'\]

  

df \= pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index\=index)

  

dfk \= df.loc\['state 2':\]\['insect'\]

print(dfk)

Dica

Mostrar a soluçãoValidar

Bom trabalho usando `loc[]` para indexar o DataFrame!

Para fins de referência, aqui está uma tabela de comandos de sintaxe de `loc[]`:

![](https://practicum-content.s3.amazonaws.com/resources/4.2_PT_1690872439.png)

## Indexação usando `iloc[]`

Também podemos indexar um DataFrame usando as posições numéricas de suas linhas e colunas com `iloc[]`.

Enquanto `loc[]` usa _rótulos_ de índices e colunas para acessar elementos, `iloc[]` usa _números inteiros_ para designar as posições dos elementos que você deseja obter.

```
import pandas as pd

states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas']
flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom']
insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee']
index   = ['state 1', 'state 2', 'state 3', 'state 4']

df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index)

print(df)
print()
print(df.iloc[3, 2])
```

```
            state                  flower                          insect
state 1   Alabama                Camellia               Monarch butterfly
state 2    Alaska           Forget-me-not  Four-spotted skimmer dragonfly
state 3   Arizona  Saguaro cactus blossom          Two-tailed swallowtail
state 4  Arkansas           Apple blossom              European honey bee

European honey bee
```

Aqui, selecionamos o elemento na 4ª linha e na 3ª coluna que contém o inseto estadual do Arkansas. Esse é o mesmo resultado que obtivemos usando `df.loc['state 4', 'insect']`. Ao contrário de `loc[]`, `iloc[]` usa a indexação normal de Python a partir de 0 com a qual você se acostumou.

Assim como em `loc[]`, podemos acessar várias linhas e/ou colunas com `iloc[]` ao passar listas das posições ou usar fatiamento. Confira como podemos alcançar o mesmo resultado que alcançamos com

`df.loc[['state 1', 'state 3'], ['flower', 'insect']]` usando `iloc[]`:

```
import pandas as pd

states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas']
flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom']
insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee']
index   = ['state 1', 'state 2', 'state 3', 'state 4']

df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index)

print(df)
print()
print(df.iloc[[0, 2], 1:])
```

```
            state                  flower                          insect
state 1   Alabama                Camellia               Monarch butterfly
state 2    Alaska           Forget-me-not  Four-spotted skimmer dragonfly
state 3   Arizona  Saguaro cactus blossom          Two-tailed swallowtail
state 4  Arkansas           Apple blossom              European honey bee

                         flower                  insect
state 1                Camellia       Monarch butterfly
state 3  Saguaro cactus blossom  Two-tailed swallowtail
```

O código acima seleciona a primeira linha com o índice Python `0` (ou o índice `'state 1'` no nosso DataFrame) e a terceira linha com o índice Python `2` (ou o índice `'state 3'` no nosso DataFrame). Em termos de colunas, ele seleciona todas as colunas a partir da segunda (que tem o índice Python `1` ou o nome `'flower'`) até e incluindo a última.

A propósito, você também pode usar indexação negativa. Aqui está um exemplo que seleciona a última coluna (que tem o índice Python de `-1`) e a primeira e a terceira linhas (os índices Python `0` e `2`):

```
import pandas as pd

states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas']
flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom']
insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee']
index   = ['state 1', 'state 2', 'state 3', 'state 4']

df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index)

print(df.iloc[[0, 2], -1])
```

```
state 1         Monarch butterfly
state 3    Two-tailed swallowtail
Name: insect, dtype: object
```

Pergunta

Qual linha de código produz o mesmo resultado que `df.loc['state 2':'state 4', 'insect']`? Para refrescar a memória, aqui está uma impressão do DataFrame:

```
            state                  flower                          insect
state 1   Alabama                Camellia               Monarch butterfly
state 2    Alaska           Forget-me-not  Four-spotted skimmer dragonfly
state 3   Arizona  Saguaro cactus blossom          Two-tailed swallowtail
state 4  Arkansas           Apple blossom              European honey bee
```

E aqui está o resultado esperado:

```
state 2    Four-spotted skimmer dragonfly
state 3            Two-tailed swallowtail
state 4                European honey bee
Name: insect, dtype: object
```

`df.iloc[:, 2]`

`df.iloc[1:, :]`

`df.iloc[1:, -1]`

Isso mesmo! Podemos usar -1 para selecionar a última coluna, assim como fazemos com listas de Python.

`df.iloc[1:3, :2]`

Você conseguiu!

Agora que você se familiarizou com `loc[]` e `iloc[]`, como escolher qual método usar?

Usar `iloc[]` pode ser mais conveniente se você quiser apenas visualizar as primeiras linhas ou colunas ou quando você sabe o número exato da linha ou coluna que precisa. `iloc[]` também pode servir para economizar tempo que seria gasto na digitação de nomes de colunas ou rótulos de índices.

Usar `loc[]` pode ser melhor para gerar um código mais legível e de fácil compreensão, tanto para seus colegas quanto para você no futuro.

Em última análise, não importa se você usa `loc[]` ou `iloc[]` para indexar um DataFrame, contanto que você obtenha os elementos corretos. Cabe a você decidir qual prefere.

## Alteração do índice de um DataFrame usando o método `set_index()`

Anteriormente, aprendemos duas maneiras de definir valores de índice:

1.  Passar os valores de índice para o parâmetro `index=` criando um DataFrame ou Series
2.  Atribuir os valores de índice ao atributo `index` de um DataFrame ou Series já existente

Para DataFrames, existe outra forma de definir valores de índice usando o método `set_index()`. Este método pega uma coluna existente de um DataFrame e substitui o índice pelos valores dessa coluna:

```
import pandas as pd

states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas']
flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom']
insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee']
index   = ['state 1', 'state 2', 'state 3', 'state 4']

df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index)
df = df.set_index('state') # substitui o índice

print(df)
print()
print(df.index)
```

```
                          flower                          insect
state                                                           
Alabama                 Camellia               Monarch butterfly
Alaska             Forget-me-not  Four-spotted skimmer dragonfly
Arizona   Saguaro cactus blossom          Two-tailed swallowtail
Arkansas           Apple blossom              European honey bee

Index(['Alabama', 'Alaska', 'Arizona', 'Arkansas'], dtype='object', name='state')
```

Inicialmente, os índices eram os números dos estados: `'state 1'`, `'state 2'` etc. Depois, nós os substituímos pelos valores da coluna `'state'`.

No resultado, podemos ver que `df` só tem duas colunas: `'flower'` e `'insect'`. O índice foi substituído pelos valores contidos em `'state'`. O novo índice também tem um componente de nome que não vimos antes. O nome do índice é `'state'`, porque `set_index()` automaticamente dá o nome antigo da coluna ao novo índice.

Se você não quiser que o índice tenha um nome, pode removê-lo definindo o atributo `index_name` de um DataFrame como `None`. Aqui está como você pode fazer isso:

```
import pandas as pd

states  = ['Alabama', 'Alaska', 'Arizona', 'Arkansas']
flowers = ['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom']
insects = ['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee']
index   = ['state 1', 'state 2', 'state 3', 'state 4']

df = pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index=index)
df = df.set_index('state')

df.index.name = None
print(df)
print()
print(df.index)
```

```
                          flower                          insect
Alabama                 Camellia               Monarch butterfly
Alaska             Forget-me-not  Four-spotted skimmer dragonfly
Arizona   Saguaro cactus blossom          Two-tailed swallowtail
Arkansas           Apple blossom              European honey bee

Index(['Alabama', 'Alaska', 'Arizona', 'Arkansas'], dtype='object')
```

Na próxima lição, você vai aprender mais formas de filtrar dados. Depois, vamos juntar suas habilidades de indexação e suas novas habilidades de filtragem para aprender técnicas de filtragem ainda mais avançadas. Por enquanto, é hora de praticar mais nas tarefas.

## Tarefas

### Tarefa 1

Use `loc[]` para extrair as flores do Alabama, Alaska e Arizona e salve o resultado na variável `flowers`. Depois, imprima a variável.

O pre-código já cria o DataFrame para você e define a coluna `'state'` como índice, então certifique-se de usar os nomes dos estados como os valores de índice em `loc[]`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

import pandas as pd

  

states \= \['Alabama', 'Alaska', 'Arizona', 'Arkansas'\]

flowers \= \['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom'\]

insects \= \['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee'\]

index \= \['state 1', 'state 2', 'state 3', 'state 4'\]

  

df \= pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index\=index)

df \= df.set\_index('state')

  

flowers \= df.loc\[:'Arizona'\]\['flower'\]

print(flowers)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Agora use `iloc[]` para indexar a mesma parte do DataFrame que você indexou na tarefa anterior. Assim como antes, salve o resultado na variável `flowers` e imprima.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

import pandas as pd

  

states \= \['Alabama', 'Alaska', 'Arizona', 'Arkansas'\]

flowers \= \['Camellia', 'Forget-me-not', 'Saguaro cactus blossom', 'Apple blossom'\]

insects \= \['Monarch butterfly', 'Four-spotted skimmer dragonfly', 'Two-tailed swallowtail', 'European honey bee'\]

index \= \['state 1', 'state 2', 'state 3', 'state 4'\]

  

df \= pd.DataFrame({'state': states, 'flower': flowers, 'insect': insects}, index\=index)

df \= df.set\_index('state')

  

flowers \= df.iloc\[:3,0\]

print(flowers)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-36-854Z.md
### Última modificação: 2025-05-28 19:07:37

# Filtragem personalizada usando query() - TripleTen

Capítulo 4/11

Filtragem de dados

# Filtragem personalizada usando query()

Você já aprendeu técnicas básicas de filtragem que usam operadores lógicos como `>`, `<`, `>=`, `<=` e `==`. Nesta lição, você vai aprender como filtrar dados usando dois novos métodos da pandas:

1.  `isin()`, que verifica a existência de valores
2.  `query()`, que permite filtrar dados usando strings personalizadas de consulta

## Explorando o conjunto de dados

Vamos trabalhar com um conjunto de dados que contém informações históricas sobre vendas de consoles de videogames. Os dados originais são de [VGChartz](https://www.vgchartz.com/gamedb/) _(os materiais estão em inglês)_, e foram modificados para os nossos propósitos aqui.

Vamos dar uma olhada nos dados:

```
df = pd.read_csv('/datasets/vg_sales.csv')
print(df.head())
print()
print(df.info())
```

```
                       name platform  year_of_release         genre publisher  \
0                Wii Sports      Wii           2006.0        Sports  Nintendo   
1         Super Mario Bros.      NES           1985.0      Platform  Nintendo   
2            Mario Kart Wii      Wii           2008.0        Racing  Nintendo   
3         Wii Sports Resort      Wii           2009.0        Sports  Nintendo   
4  Pokemon Red/Pokemon Blue       GB           1996.0  Role-Playing  Nintendo   

  developer  na_sales  eu_sales  jp_sales  critic_score  user_score  
0  Nintendo     41.36     28.96      3.77          76.0         8.0  
1       NaN     29.08      3.58      6.81           NaN         NaN  
2  Nintendo     15.68     12.76      3.79          82.0         8.3  
3  Nintendo     15.61     10.93      3.28          80.0         8.0  
4       NaN     11.27      8.89     10.22           NaN         NaN  

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 16717 entries, 0 to 16716
Data columns (total 11 columns):
 #   Column           Non-Null Count  Dtype  
---  ------           --------------  -----  
 0   name             16717 non-null  object 
 1   platform         16717 non-null  object 
 2   year_of_release  16448 non-null  float64
 3   genre            16717 non-null  object 
 4   publisher        16663 non-null  object 
 5   developer        10096 non-null  object 
 6   na_sales         16717 non-null  float64
 7   eu_sales         16717 non-null  float64
 8   jp_sales         16717 non-null  float64
 9   critic_score     8137 non-null   float64
 10  user_score       7590 non-null   float64
dtypes: float64(6), object(5)
memory usage: 1.4+ MB
```

Há muitas colunas no conjunto de dados. Muitas delas são autoexplicativas, mas vamos detalhar algumas daquelas que talvez não sejam:

-   `'platform'`: é o console para o qual o jogo foi lançado
-   `'xx_sales'`: são as vendas para a América do Norte (NA), Europa (EU) e Japão (JP) em milhões de dólares
-   `'critic_score'`: é a nota de 0 a 100 dada ao jogo pelos críticos
-   `'user_score'`: é a nota de 0 a 100 dada ao jogo pelos consumidores

Observe também que muitas colunas têm valores ausentes. Por enquanto, vamos deixá-las assim.

## Filtragem com strings de consulta e o método `query()`

Agora que você se familiarizou com os dados, vamos aprender sobre a filtragem com strings de consulta. Você já sabe como filtrar DataFrames usando operadores lógicos para criar um objeto Series de valores booleanos, que vamos chamar de máscara booleana de agora em diante.

Vamos filtrar os dados de forma que apenas selecionemos os jogos cujas vendas no Japão foram de pelo menos um milhão de dólares:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

mask = df['jp_sales'] >= 1
print(df[mask][['name', 'jp_sales']])
```

```
                                             name  jp_sales
0                                      Wii Sports      3.77
1                               Super Mario Bros.      6.81
2                                  Mario Kart Wii      3.79
3                               Wii Sports Resort      3.28
4                        Pokemon Red/Pokemon Blue     10.22
...                                           ...       ...
1970                  Tag Team Match M.U.S.C.L.E.      1.05
1971                            Derby Stallion 96      1.04
1972                             Adventure Island      1.05
2051    Oshare Majo Love and Berry: DS Collection      1.01
2065  Jissen Pachi-Slot Hisshouhou: Hokuto no Ken      1.00

[243 rows x 2 columns]
```

No código acima, a variável `mask` contém um objeto Series com valores `True` e `False`. `True` indica que um valor na coluna `'jp_sales'` tem vendas iguais ou maiores que um milhão de dólares, enquanto `False` indica vendas abaixo desse valor.

Em seguida, usamos essa máscara para filtrar o DataFrame original com `df[mask]` e selecionar duas colunas de interesse: `['name', 'jp_sales']`.

Como resultado, há mais de 16.000 jogos no conjunto de dados, e apenas 243 excederam um milhão nas vendas no Japão. Para simplificar, estamos visualizando apenas as colunas `'name'` e `'jp_sales'`. É claro, poderíamos ter feito isso sem criar a variável `mask` e simplesmente colocado a expressão de máscara na nossa linha de filtragem no código.

Podemos fazer a mesma filtragem usando o método `query()`.

Esse método é chamado em um DataFrame e espera uma string como parâmetro. A string representa a **consulta** que você quer fazer no DataFrame, o que significa que ela diz ao Python quais linhas precisam ser filtradas:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

print(df.query("jp_sales > 1")[['name', 'jp_sales']])
```

```
                                           name  jp_sales
0                                    Wii Sports      3.77
1                             Super Mario Bros.      6.81
2                                Mario Kart Wii      3.79
3                             Wii Sports Resort      3.28
4                      Pokemon Red/Pokemon Blue     10.22
...                                         ...       ...
1885                              Densha De Go!      1.02
1970                Tag Team Match M.U.S.C.L.E.      1.05
1971                          Derby Stallion 96      1.04
1972                           Adventure Island      1.05
2051  Oshare Majo Love and Berry: DS Collection      1.01

[239 rows x 2 columns]
```

Tudo o que precisamos fazer foi usar o nome da coluna em nossa string de consulta com a condição em que queríamos filtrar os dados. A escolha da técnica que você usa para a filtragem é uma questão de preferência, mas strings de consulta em geral são mais legíveis.

Para filtrar usando `query()` com base nas comparações de strings, você precisa colocar a string entre aspas. Por exemplo, vamos selecionar apenas os jogos publicados pela Nintendo:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

print(df.query("publisher == 'Nintendo'")[['name', 'publisher']].head())
```

```
                       name publisher
0                Wii Sports  Nintendo
1         Super Mario Bros.  Nintendo
2            Mario Kart Wii  Nintendo
3         Wii Sports Resort  Nintendo
4  Pokemon Red/Pokemon Blue  Nintendo
```

Usamos aspas simples para denotar `'Nintendo'`, porque a nossa string de consulta completa já estava entre aspas duplas.

Agora use `query()` para filtrar os dados. Mantenha apenas as linhas em que as colunas `'publisher'` e `'developer'` têm os mesmos valores. Seu objetivo é verificar a igualdade entre as duas colunas. Para isso, selecione o operador lógico que faz isso.

A variável `cols`, que já está presente no pré-código, especifica as colunas que queremos selecionar do resultado da consulta. Para selecionar apenas as colunas de interesse, use a variável `cols` imediatamente após o método `query()`. Aqui está a sintaxe: `df.query(...)[cols]`.

Por fim, atribua o resultado a uma variável chamada `df_filtered` e imprima as 5 primeiras linhas.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

  

cols \= \['name', 'publisher', 'developer'\]

  

df\_filtered \= df.query("publisher == developer")\[cols\]

print(df\_filtered.head())

Dica

Mostrar a soluçãoValidar

## Filtragem usando o método `isin()`

Há outro método que podemos usar para filtrar dados, chamado `isin()`. Em vez de usar os operadores lógicos que você conhece, `isin()` verifica se os valores em uma coluna correspondem a algum dos valores em outro vetor, como uma lista ou um dicionário.

Podemos usar uma lista de consoles de videogames portáteis para obter apenas as linhas de jogos em um desses consoles:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

handhelds = ['3DS', 'DS', 'GB', 'GBA', 'PSP']
print(df[df['platform'].isin(handhelds)][['name', 'platform']])
```

```
                                                    name platform
4                               Pokemon Red/Pokemon Blue       GB
5                                                 Tetris       GB
6                                  New Super Mario Bros.       DS
10                                            Nintendogs       DS
11                                         Mario Kart DS       DS
...                                                  ...      ...
16702                           Mezase!! Tsuri Master DS       DS
16703  Eiyuu Densetsu: Sora no Kiseki Material Collec...      PSP
16706                                           Plushees       DS
16710                 Woody Woodpecker in Crazy Castle 5      GBA
16715                                   Spirits & Spells      GBA

[4801 rows x 2 columns]
```

Vamos analisar o código acima:

-   `df['platform'].isin(handhelds)` verifica se os valores na coluna `'platform'` são iguais a um dos valores da lista `handhelds`, que representa os consoles portáteis.
-   `df[df['platform'].isin(handhelds)]` filtra o DataFrame mantendo apenas as linhas retornadas como resultado da verificação da igualdade que executamos na etapa anterior.
-   Por fim, selecionamos apenas duas colunas do DataFrame filtrado: `['name', 'platform']` descartando o resto das colunas. Lembre-se de que usamos colchetes duplos para selecionar várias colunas, por isso o código acima tem `[['name', 'platform']]`.

Poderíamos ter feito a mesma filtragem verificando se `'platform'` era igual a `'3DS'` _ou_ `'DS'` _ou_ `'GB'`, etc. Usar `isin()` é muito mais conveniente quando temos muitas condições para verificar. Imagine se quiséssemos verificar a existência em uma lista com dezenas ou centenas de valores!

A propósito, você se lembra do símbolo de til (`~`), que inverte o resultado? Podemos usá-lo aqui também. Por exemplo, podemos filtrar o DataFrame original extraindo apenas as linhas em que os valores na coluna `'platform'` não estão na lista `handhelds`:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

handhelds = ['3DS', 'DS', 'GB', 'GBA', 'PSP']
print(df[~df['platform'].isin(handhelds)][['name', 'platform']])
```

```
                                                                                                    name platform
0                                            Wii Sports      Wii
1                                     Super Mario Bros.      NES
2                                        Mario Kart Wii      Wii
3                                     Wii Sports Resort      Wii
7                                              Wii Play      Wii
...                                                 ...      ...
16711  SCORE International Baja 1000: The Official Game      PS2
16712                     Samurai Warriors: Sanada Maru      PS3
16713                                  LMA Manager 2007     X360
16714                           Haitaka no Psychedelica      PSV
16716                               Winning Post 8 2016      PSV

[11916 rows x 2 columns]
```

Também podemos verificar a existência de algo usando o método `query()` com a palavra-chave `in` na string de consulta.

Vamos ver como isso funciona em uma filtragem igual à anterior:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

handhelds = ['3DS', 'DS', 'GB', 'GBA', 'PSP']
print(df.query("platform in @handhelds")[['name', 'platform']])
```

```
                                                    name platform
4                               Pokemon Red/Pokemon Blue       GB
5                                                 Tetris       GB
6                                  New Super Mario Bros.       DS
10                                            Nintendogs       DS
11                                         Mario Kart DS       DS
...                                                  ...      ...
16702                           Mezase!! Tsuri Master DS       DS
16703  Eiyuu Densetsu: Sora no Kiseki Material Collec...      PSP
16706                                           Plushees       DS
16710                 Woody Woodpecker in Crazy Castle 5      GBA
16715                                   Spirits & Spells      GBA

[4801 rows x 2 columns]
```

Como alternativa, você pode encontrar as linhas que não estão na lista usando a palavra-chave `not in`:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

handhelds = ['3DS', 'DS', 'GB', 'GBA', 'PSP']
print(df.query("platform not in @handhelds")[['name', 'platform']])
```

```
                                                                                                    name platform
0                                            Wii Sports      Wii
1                                     Super Mario Bros.      NES
2                                        Mario Kart Wii      Wii
3                                     Wii Sports Resort      Wii
7                                              Wii Play      Wii
...                                                 ...      ...
16711  SCORE International Baja 1000: The Official Game      PS2
16712                     Samurai Warriors: Sanada Maru      PS3
16713                                  LMA Manager 2007     X360
16714                           Haitaka no Psychedelica      PSV
16716                               Winning Post 8 2016      PSV

[11916 rows x 2 columns]
```

Como a variável `handhelds` é externa ao DataFrame, temos que iniciá-la com o símbolo `@` na string de consulta. Caso contrário, a pandas vai tentar encontrar uma coluna chamada `'handhelds'` e vai exibir um erro quando não conseguir encontrá-la.

## Tarefas

### Tarefa 1

Imprima uma lista de todos os gêneros únicos no conjunto de dados chamando o método `unique()` na coluna `'genre'`.

CódigoPYTHON

9

1

2

3

4

5

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

  

print(df\['genre'\].unique())

Dica

Mostrar a soluçãoValidar

### Tarefa 2

No pré-código, você tem duas variáveis:

-   `cols`, que contém as colunas de interesse: `'name'` e `'genre'`.
-   `s_genres`, que é uma lista de gêneros começando com a letra "S".

Seu objetivo é usar o método `isin()` com a lista dada `s_genres` para filtrar o DataFrame `df` de maneira a manter apenas as linhas onde o gênero de jogo _não_ comece com a letra "S".

Depois de filtrar o DataFrame, use a variável `cols` para selecionar apenas as colunas `'name'` e `'genre'` e atribuir o resultado a uma variável chamada `df_filtered`. Por fim, imprima.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

9

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

  

cols \= \['name', 'genre'\]

s\_genres \= \['Shooter', 'Simulation', 'Sports', 'Strategy'\]

  

df\_filtered \= df\[~df\['genre'\].isin(s\_genres)\]\[\['name', 'genre'\]\]

print(df\_filtered)

Dica

Mostrar a soluçãoValidar

### Tarefa 3

Filtre todos os gêneros que não começam com "S" novamente, mas desta vez faça isso usando o método `query()`. Você vai precisar usar a palavra-chave `not in` na string de consulta para fazer isso. Use `cols` para selecionar apenas as colunas `'name'` e `'genre'` e atribua o resultado a uma variável chamada `df_filtered`. Por fim, imprima.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

9

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

  

cols \= \['name', 'genre'\]

s\_genres \= \['Shooter', 'Simulation', 'Sports', 'Strategy'\]

  

df\_filtered \= df.query("genre not in @s\_genres")\[cols\]

print(df\_filtered)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-39-208Z.md
### Última modificação: 2025-05-28 19:07:39

# Uso de estruturas de dados externas para filtrar DataFrames - TripleTen

Capítulo 4/11

Filtragem de dados

# Uso de estruturas de dados externas para filtrar DataFrames

Agora que você aprendeu sobre os índices na Pandas e como escrever strings de consulta para filtrar dados com o método `query()`, está com tudo pronto para combinar essas habilidades e filtrar DataFrames usando estruturas de dados externas. Nesta lição, você vai aprender como filtrar usando dicionários, Series e até outros DataFrames.

Para ilustrar o papel de índices na filtragem, vamos criar nossos próprios DataFrames neste capítulo. Vamos revisar brevemente como usar uma lista externa para filtrar o nosso DataFrame com `query()`. Para descobrir se os valores da coluna `'a'` estão na lista `our_list`, vamos escrever a consulta `"a in @our_list"` (a em @our\_list).

```
import pandas as pd

our_list = [2, 5, 10]
df = pd.DataFrame(
    {
        'a': [2, 3, 10, 11, 12],
        'b': [5, 4, 3, 2, 1],
        'c': ['X', 'Y', 'Y', 'Y', 'Z'],
    }
)
print(df)
print()
print(our_list)
print()
print(df.query("a in @our_list"))
```

```
    a  b  c
0   2  5  X
1   3  4  Y
2  10  3  Y
3  11  2  Y
4  12  1  Z

[2, 5, 10]

    a  b  c
0   2  5  X
2  10  3  Y
```

Como podemos ver, `query()` retornou todas as _linhas_ de `df` onde os valores na coluna `'a'` estão presentes em `our_list`. Esses valores são 2 e 10; a coluna `'a'` não contém o valor 5.

Observe que os valores dos índices no DataFrame filtrado permanecem inalterados em relação aos valores originais, 0 e 2 nesse caso.

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/4_4.png)

Às vezes, precisamos criar consultas usando uma variável que contém uma estrutura mais complicada do que um número ou lista independentes: por exemplo, um **dicionário**, **objeto Series** ou **DataFrame**.

## Filtragem com um dicionário

E se a variável armazenar um dicionário em vez de uma lista? Lembre-se de que dicionários consistem em pares chave-valor.

Vamos criar um dicionário e atribuí-lo à variável `our_dict`. Para verificar a presença dos valores da coluna `'a'` entre os _valores_ do dicionário, precisamos usar o método `values()` do dicionário na consulta `"a in @our_dict.values()"`:

```
import pandas as pd

our_dict = {0: 10, 3: 11, 12: 12}
df = pd.DataFrame(
    {
        'a': [2, 3, 10, 11, 12],
        'b': [5, 4, 3, 2, 1],
        'c': ['X', 'Y', 'Y', 'Y', 'Z'],
    }
)
print(df)
print()
print(our_dict)
print()
print(df.query("a in @our_dict.values()"))
```

```
    a  b  c
0   2  5  X
1   3  4  Y
2  10  3  Y
3  11  2  Y
4  12  1  Z

{0: 10, 3: 11, 12: 12}

    a  b  c
2  10  3  Y
3  11  2  Y
4  12  1  Z
```

Cada um dos valores do dicionário (10, 11 e 12) aparece na coluna `'a'` apenas uma vez, então recebemos três linhas de `query()`. Os valores de índice no DataFrame filtrado permanecem inalterados. A filtragem com valores de dicionário é muito parecida à filtragem com listas.

No entanto, para verificar a presença dos valores da coluna `'a'` entre as chaves do dicionário (0, 3 e 12), precisamos usar a consulta `"a in @our_dict.keys()"`. A consulta mais curta `"a in @our_dict"` também funciona, porque verifica as chaves por padrão:

```
import pandas as pd

our_dict = {0: 10, 3: 11, 12: 12}
df = pd.DataFrame(
    {
        'a': [2, 3, 10, 11, 12],
        'b': [5, 4, 3, 2, 1],
        'c': ['X', 'Y', 'Y', 'Y', 'Z'],
    }
)
print(df)
print()
print(our_dict)
print()
print(df.query("a in @our_dict"))
```

```
    a  b  c
0   2  5  X
1   3  4  Y
2  10  3  Y
3  11  2  Y
4  12  1  Z

{0: 10, 3: 11, 12: 12}

    a  b  c
1   3  4  Y
4  12  1  Z
```

## Filtragem com um objeto Series

Agora vejamos um exemplo no qual a estrutura de dados externa é um objeto Series. Da mesma forma que dicionários armazenam pares chave-valor, objetos Series armazenam pares índice-valor. Entretanto, os valores para objetos Series são verificados por padrão.

A consulta `"a in @our_series"` verifica a presença de valores na coluna `'a'` entre os valores de `our_series` em vez de entre os índices.

```
import pandas as pd

our_series = pd.Series([10, 11, 12])
df = pd.DataFrame(
    {
        'a': [2, 3, 10, 11, 12],
        'b': [5, 4, 3, 2, 1],
        'c': ['X', 'Y', 'Y', 'Y', 'Z'],
    }
)
print(df)
print()
print(our_series)
print()
print(df.query("a in @our_series"))
```

```
    a  b  c
0   2  5  X
1   3  4  Y
2  10  3  Y
3  11  2  Y
4  12  1  Z

0    10
1    11
2    12
dtype: int64

    a  b  c
2  10  3  Y
3  11  2  Y
4  12  1  Z
```

Se precisarmos verificar se os valores da coluna `'a'` estão presentes entre os valores de índice do Series, então teremos que usar o atributo `index`: `"a in @our_series.index"`.

Use o atributo `index` de `our_series` abaixo para escrever uma consulta que filtra `df` para manter apenas as linhas onde os valores na coluna `'c'` também estão entre os índices de `our_series`. Imprima o resultado.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

import pandas as pd

  

our\_series \= pd.Series(\[10, 11, 12\], index\=\['X', 'Y', 'T'\])

df \= pd.DataFrame(

{

'a': \[2, 3, 10, 11, 12\],

'b': \[5, 4, 3, 2, 1\],

'c': \['X', 'Y', 'Y', 'Y', 'Z'\],

}

)

  

print(df.query("c in @our\_series.index"))\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

## Filtragem com um DataFrame

Também podemos usar um DataFrame externo para filtrar os dados de duas maneiras:

1.  Filtrar usando seus valores de índice
2.  Filtrar usando valores em colunas específicas

Para uma filtragem com base na inclusão entre os índices de um DataFrame externo, simplesmente fazemos a verificação da mesma maneira que fizemos para os índices de um objeto Series: acessamos o atributo `index` na consulta.

Vamos criar um DataFrame externo chamado `our_df` com valores de índice definidos pela lista `['Z', 'X', 'P']`. Podemos então verificar a inclusão dos valores da coluna `'c'` nos índices de `our_df`:

```
import pandas as pd

df = pd.DataFrame(
    {
        'a': [2, 3, 10, 11, 12],
        'b': [5, 4, 3, 2, 1],
        'c': ['X', 'Y', 'Y', 'Y', 'Z'],
    }
)
our_df = pd.DataFrame(
    {
        'a1': [2, 4, 6],
        'b1': [3, 2, 2],
        'c1': ['A', 'B', 'C'],
    },
    index=['Z', 'X', 'P']
)

print(df)
print()
print(our_df)
print()
print(df.query("c in @our_df.index"))
```

```
    a  b  c
0   2  5  X
1   3  4  Y
2  10  3  Y
3  11  2  Y
4  12  1  Z

   a1  b1 c1
Z   2   3  A
X   4   2  B
P   6   2  C

    a  b  c
0   2  5  X
4  12  1  Z
```

Para verificar se os valores de uma coluna do DataFrame que queremos filtrar (neste caso, `df`) também estão presentes em uma coluna de um DataFrame externo (`our_df`), também precisamos especificar a coluna externa na consulta usando a **notação de ponto.**

A consulta `"a in @our_df.a1"` verifica se algum valor da coluna `'a'` de `df` está presente na coluna `'a1'` de `our_df`.

Podemos usar a notação de ponto para acessar as colunas de um DataFrame como atributos em vez da notação de colchetes, então `our_df['a1']` é equivalente a `our_df.a1`. No entanto, em strings de consulta, apenas a notação de ponto funciona.

Use o que acabou de aprender para filtrar `df` de forma que você mantenha apenas as linhas nas quais os valores da coluna `'a'` também estão presentes na coluna `'b1'` do DataFrame externo `our_df`. Imprima o resultado.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

import pandas as pd

  

df \= pd.DataFrame(

{

'a': \[2, 3, 10, 11, 12\],

'b': \[5, 4, 3, 2, 1\],

'c': \['X', 'Y', 'Y', 'Y', 'Z'\],

}

)

our\_df \= pd.DataFrame(

{

'a1': \[2, 4, 6\],

'b1': \[3, 2, 2\],

'c1': \['A', 'B', 'C'\],

},

index\=\['Z', 'X', 'P'\]

)

  

print(df.query("a in @our\_df.b1"))

Dica

Mostrar a soluçãoValidar

Talvez você tenha notado que a filtragem com base em valores de uma coluna de um DataFrame externo é muito parecida à filtragem com base em um objeto Series externo. Isso porque uma coluna de um DataFrame _é_ um objeto Series!

Agora que você aprendeu sobre os índices e sobre como filtrar usando `query()` com estruturas de dados externas, é hora de avançar para o próximo nível e filtrar com base em várias condições.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-40-558Z.md
### Última modificação: 2025-05-28 19:07:40

# Filtragem com base em várias condições - TripleTen

Capítulo 4/11

Filtragem de dados

# Filtragem com base em várias condições

Você aprendeu muitas novas habilidades de filtragem neste capítulo, mas, até agora, só estávamos filtrando com base em condições únicas. No entanto, frequentemente é necessário fazer uma filtragem mais complexa com base em mais de uma condição lógica. Nesta lição, aprenderemos algumas maneiras de fazer isso.

## Operadores lógicos

Você já tem experiência em combinar condições lógicas do sprint de Python básico. Vamos revisar essas habilidades rapidinho.

Pergunta

Qual linha de código retorna um valor booleano que verifica se a variável `x` está entre 1 e 10 (de modo inclusivo)?

`x >=1, x <=10`

`x > 0 and x < 11`

`x >= 1 or x <= 10`

`x <= 10 and x >= 1`

Isso! Não importa em que ordem as instruções lógicas aparecem nesse caso.

Você conseguiu!

Bom trabalho! Também podemos filtrar DataFrames na pandas com base em várias condições, mas a sintaxe é um pouco diferente. Nesse caso, não podemos usar operadores lógicos, como `and`, `or` ou `not` nas expressões de filtragem. Em vez disso, temos que usar as versões que comparam bit a bit: `&`, `|` e `~`, respectivamente.

Vamos voltar ao conjunto de dados de vendas de videogames.

Digamos que queremos verificar todos os jogos de Wii que _não são_ de esporte:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df['user_score'] = pd.to_numeric(df['user_score'], errors='coerce')

print(df[(df['platform'] == 'Wii') & ~(df['genre'] == 'Sports')].head())
```

```
                         name platform  year_of_release     genre publisher  \
2              Mario Kart Wii      Wii           2008.0    Racing  Nintendo   
7                    Wii Play      Wii           2006.0      Misc  Nintendo   
8   New Super Mario Bros. Wii      Wii           2009.0  Platform  Nintendo   
39    Super Smash Bros. Brawl      Wii           2008.0  Fighting  Nintendo   
49         Super Mario Galaxy      Wii           2007.0  Platform  Nintendo   

    developer  na_sales  eu_sales  jp_sales  critic_score  user_score  
2    Nintendo     15.68     12.76      3.79          82.0         8.3  
7    Nintendo     13.96      9.18      2.93          58.0         6.6  
8    Nintendo     14.44      6.94      4.70          87.0         8.4  
39  Game Arts      6.62      2.55      2.66          93.0         8.9  
49   Nintendo      6.06      3.35      1.20          97.0         8.9
```

Há três componentes importantes para a condição de filtragem `(df['platform'] == 'Wii') & ~(df['genre'] == 'Sports')`:

1.  Cada condição individual é separada por parênteses.
2.  O operador `&` é usado em vez de `and`.
3.  O operador `~` é usado em vez do `not` (e ele precede a condição que queremos negar).

Agora vamos explorar uma condição "or" (ou) obtendo todos os jogos que superaram $1 milhão em vendas em pelo menos _uma_ das três regiões:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df['user_score'] = pd.to_numeric(df['user_score'], errors='coerce')

print(df[(df['na_sales'] >= 1) | (df['eu_sales'] >= 1) | (df['jp_sales'] >= 1)].head())
```

```
                       name platform  year_of_release         genre publisher  \
0                Wii Sports      Wii           2006.0        Sports  Nintendo   
1         Super Mario Bros.      NES           1985.0      Platform  Nintendo   
2            Mario Kart Wii      Wii           2008.0        Racing  Nintendo   
3         Wii Sports Resort      Wii           2009.0        Sports  Nintendo   
4  Pokemon Red/Pokemon Blue       GB           1996.0  Role-Playing  Nintendo   

  developer  na_sales  eu_sales  jp_sales  critic_score  user_score  
0  Nintendo     41.36     28.96      3.77          76.0         8.0  
1       NaN     29.08      3.58      6.81           NaN         NaN  
2  Nintendo     15.68     12.76      3.79          82.0         8.3  
3  Nintendo     15.61     10.93      3.28          80.0         8.0  
4       NaN     11.27      8.89     10.22           NaN         NaN
```

Verificamos as vendas em todas as três regiões: `na`, `eu` e `jp`. Novamente, cada condição individual é separada por parênteses, e usamos o operador `|` em vez de `or`.

Agora é sua vez de tentar. Filtre `df` de modo que você obtenha apenas os jogos lançados nos anos 80. Atribua o resultado a uma variável chamada `df_filtered` e depois imprima as primeiras 5 linhas.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

df\['user\_score'\] \= pd.to\_numeric(df\['user\_score'\], errors\='coerce')

  

df\_filtered \= df\[(df\['year\_of\_release'\] \>= 1980) & (df\['year\_of\_release'\] < 1990)\]

print(df\_filtered.head())

Dica

Mostrar a soluçãoValidar

Muito bem! Agora vamos praticar com uma condição de filtragem mais complexa.

Pergunta

Qual condição de filtragem podemos aplicar a `df` de modo que mantenhamos apenas as linhas nas quais a pontuação de usuários (`'user_score'`) _ou_ a dos críticos (`'critic_score'`) é igual ou maior que 90% _e_ o gênero (`'genre'`) é `'Role-Playing'`, `'Strategy'` ou `'Puzzle'`?

`(df['critic_score'] >= 90) | (df['user_score'] >= 9) & (df['genre'].isin(['Role-Playing', 'Strategy', 'Puzzle']))`

`((df['critic_score'] >= 90) | (df['user_score'] >= 9)) & (df['genre'].isin(['Role-Playing', 'Strategy', 'Puzzle']))`

Isso!

`(df['critic_score'] >= 90) or (df['user_score'] >= 9) and (df['genre'].isin(['Role-Playing', 'Strategy', 'Puzzle']))`

`((df['critic_score'] >= 90) or (df['user_score'] >= 9)) and (df['genre'].isin(['Role-Playing', 'Strategy', 'Puzzle']))`

Muito bem!

## Múltiplas condições com `query()`

Também podemos filtrar com base em várias condições escrevendo strings de consulta para o método `query()`. Vamos filtrar os dados para obter apenas os jogos de Wii que não sejam de esportes, mas desta vez usando uma string de consulta:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df['user_score'] = pd.to_numeric(df['user_score'], errors='coerce')

print(df.query("platform == 'Wii' and genre != 'Sports'").head())
```

```
                         name platform  year_of_release     genre publisher  \
2              Mario Kart Wii      Wii           2008.0    Racing  Nintendo   
7                    Wii Play      Wii           2006.0      Misc  Nintendo   
8   New Super Mario Bros. Wii      Wii           2009.0  Platform  Nintendo   
39    Super Smash Bros. Brawl      Wii           2008.0  Fighting  Nintendo   
49         Super Mario Galaxy      Wii           2007.0  Platform  Nintendo   

    developer  na_sales  eu_sales  jp_sales  critic_score  user_score  
2    Nintendo     15.68     12.76      3.79          82.0         8.3  
7    Nintendo     13.96      9.18      2.93          58.0         6.6  
8    Nintendo     14.44      6.94      4.70          87.0         8.4  
39  Game Arts      6.62      2.55      2.66          93.0         8.9  
49   Nintendo      6.06      3.35      1.20          97.0         8.9
```

Veja como fizemos isso antes:

```
print(df[(df['platform'] == 'Wii') & ~(df['genre'] == 'Sports')].head())
```

Agora, para conectar as duas condições, basta incluir a palavra-chave `and` na string de consulta. Isso é bem mais natural e fácil do que criar uma máscara booleana, não é? Também não foi necessário separar as condições por parênteses aqui, embora não haja problema em incluir parênteses na string de consulta para maior clareza (e, às vezes, isso até será necessário).

Agora é sua vez.

Lembre-se da outra filtragem que fizemos pegando apenas as linhas que superaram $1 milhão em vendas em _pelo menos_ uma das três regiões. Execute a mesma filtragem abaixo, mas desta vez usando `query()`.

Atribua sua string de consulta à variável chamada `q_string`, depois imprima as primeiras 5 linhas do resultado de `query()` em `df` com `q_string` como parâmetro.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

df\['user\_score'\] \= pd.to\_numeric(df\['user\_score'\], errors\='coerce')

  

q\_string \= "na\_sales >= 1 or eu\_sales >= 1 or jp\_sales >= 1"

print(df.query(q\_string).head())

Dica

Mostrar a soluçãoValidar

Observe que strings de consulta podem incluir as palavras-chave `and`/`or` ou os símbolos `&`/`|`, então use a opção que preferir.

## Tarefas

### Tarefa 1

Para completar esta tarefa, você vai precisar aplicar suas habilidades de filtragem. A variável `developers` contém uma lista de empresas. Filtre o DataFrame `df` para incluir apenas os jogos que atendam às seguintes condições:

-   O jogo é vendido nas três regiões (América do Norte, Europa e Japão).
-   As vendas no Japão foram maiores que as vendas combinadas na América do Norte e na Europa.
-   A empresa que desenvolveu o jogo está na lista `developers`.

Recomendamos criar uma string de consulta e atribuí-la a uma variável chamada `q_string`. Depois use essa variável para fazer a filtragem.

É importante observar que não há nenhuma coluna que diz explicitamente onde os jogos foram vendidos, mas você pode deduzir que um jogo _não_ foi vendido em uma região se as vendas de lá forem iguais a 0.

Use a variável `cols` para selecionar apenas as colunas `'name'`, `'developer'`, `'na_sales'`, `'eu_sales'` e `'jp_sales'` do DataFrame filtrado e atribua o resultado à variável `df_filtered`. Imprima o DataFrame inteiro.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

df\['user\_score'\] \= pd.to\_numeric(df\['user\_score'\], errors\='coerce')

  

developers \= \['SquareSoft', 'Enix Corporation', 'Square Enix'\]

cols \= \['name', 'developer', 'na\_sales', 'eu\_sales', 'jp\_sales'\]

  

q\_string \= "jp\_sales > (na\_sales + eu\_sales) & jp\_sales != 0 & na\_sales != 0 & eu\_sales != 0 & developer == @developers"

df\_filtered \= df.query(q\_string)\[(cols)\]

print(df\_filtered)

  

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-41-889Z.md
### Última modificação: 2025-05-28 19:07:42

# Substituição de valores com where() - TripleTen

Capítulo 4/11

Filtragem de dados

# Substituição de valores com where()

Geralmente, filtramos dados porque as perguntas que queremos responder envolvem apenas um subconjunto de informações. Muitas vezes, queremos processar os dados filtrados como parte de nossa AED. Quando o processamento de dados envolve a modificação de valores de colunas, podemos usar o método `where()` para filtrar e modificar ao mesmo tempo, para que só modifiquemos valores com base em condições específicas.

Para entender o `where()`, vamos compará-lo com o método `replace()` que você aprendeu no curso de Python básico. Digamos que queremos modificar todos os valores `'NES'` na coluna `'platform'` para o nome completo, `'Nintendo Entertainment System'`. Podemos fazer isso usando `replace()`:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

df['platform'] = df['platform'].str.replace('NES', 'Nintendo Entertainment System')

# imprimindo apenas as 2 primeiras colunas
print(df.iloc[:, :2].head())
```

```
                       name                       platform
0                Wii Sports                            Wii
1         Super Mario Bros.  Nintendo Entertainment System
2            Mario Kart Wii                            Wii
3         Wii Sports Resort                            Wii
4  Pokemon Red/Pokemon Blue                             GB
```

Chamamos `str.replace()` na coluna de string `'platform'` e todas as instâncias de `'NES'` foram substituídas pelo nome não abreviado. Observe também que usamos `iloc[]` para imprimir nosso resultado, em vez de selecionar uma lista de nomes de colunas que queríamos imprimir, porque foi conveniente fazer isso.

Podemos alcançar o mesmo resultado com o método `where()`:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

df['platform'] = df['platform'].where(df['platform'] != 'NES', 'Nintendo Entertainment System')
print(df.iloc[:, :2].head())
```

```
                       name                       platform
0                Wii Sports                            Wii
1         Super Mario Bros.  Nintendo Entertainment System
2            Mario Kart Wii                            Wii
3         Wii Sports Resort                            Wii
4  Pokemon Red/Pokemon Blue                             GB
```

Chamamos `where()` na coluna `'platform'` e passamos dois argumentos posicionais:

1.  Uma condição lógica: `df['platform'] != 'NES'`. Como sabemos, isso verifica todos os valores na coluna `'platform'` e retorna `True` para as linhas em que `'NES'` **NÃO** é um valor e `False` caso contrário. O resultado é um objeto Series de valores booleanos.
2.  Um novo valor para substituir aqueles valores em `'platform'` para os quais a condição lógica é `False`.

O método `where()` verifica a condição em cada valor na coluna. Se a condição for `True`, `where()` não faz nada; se for `False`, `where()` substitui o valor atual pelo novo.

Também podemos usar `where()` para modificar mais de uma coluna de cada vez. Antes de mostrarmos como isso funciona, vamos testar sua compreensão de `where()` no quiz abaixo.

Pergunta

Qual condição lógica você passaria para `where()` para modificar os valores de colunas apenas quando tanto `'na_sales'` quanto `'eu_sales'` forem zero ou menos que zero? Lembre-se de que `where()` substitui valores apenas quando a expressão for `False`.

`(df['na_sales'] > 0) | (df['eu_sales'] > 0)`

Está correto. Teremos False apenas quando `(df['na_sales'] > 0)` e `(df['eu_sales'] > 0)` retornarem `False`. Quando esse for o caso, a expressão inteira vai resultar em `False`. Se for `False`, where() vai trocar o valor atual pelo novo valor.

`(df['na_sales'] != 0) & (df['eu_sales'] != 0)`

`(df['na_sales'] > 0) & (df['eu_sales'] > 0)`

`(df['na_sales'] == 0) | (df['eu_sales'] == 0)`

Excelente!

Um total de vendas regionais igual a 0 na América do Norte e na Europa pode significar que o jogo nunca foi lançado fora do Japão, então queremos substituir esses valores de 0 pelo valor nulo `None` nessas colunas _apenas quando ambas as colunas contiverem zeros_ (`'na_sales'` e `'eu_sales'`).

Podemos usar a condição de filtragem do quiz para fazer isso usando `where()`:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

df[['na_sales', 'eu_sales']] = df[['na_sales', 'eu_sales']].where((df['na_sales'] > 0) | (df['eu_sales'] > 0), None)
print(df[['name', 'na_sales', 'eu_sales']])
```

```
name  na_sales  eu_sales
0                         Wii Sports     41.36     28.96
1                  Super Mario Bros.     29.08      3.58
2                     Mario Kart Wii     15.68     12.76
3                  Wii Sports Resort     15.61     10.93
4           Pokemon Red/Pokemon Blue     11.27      8.89
...                              ...       ...       ...
16712  Samurai Warriors: Sanada Maru       NaN       NaN
16713               LMA Manager 2007       NaN       NaN
16714        Haitaka no Psychedelica       NaN       NaN
16715               Spirits & Spells       NaN       NaN
16716            Winning Post 8 2016       NaN       NaN

[16717 rows x 3 columns]
```

Para modificar valores em ambas as colunas, precisamos chamar `where()` em `df[['na_sales', 'eu_sales']]`. Observe que usar `where()` nessa tarefa só funciona porque o valor que queremos usar como substituto (`None`) é o mesmo para as duas colunas.

Agora vamos dar uma olhada em como o método `where()` funciona quando precisamos verificar uma inclusão. Notamos que algumas distribuidoras na coluna `'publishers'` aparecem raramente, por exemplo, `'Red Flagship'`, `'Max Five'` e `'989 Sports'`. Embora existam várias outras, para simplificar as coisas, vamos nos concentrar nessas três. Queremos substituir essas distribuidoras pelo valor `'other'` (outro). Veja como podemos fazer isso usando o método `where()`:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

rare_publishers = ['Red Flagship', 'Max Five', '989 Sports']
df['publisher'] = df['publisher'].where(~df['publisher'].isin(rare_publishers), 'other')

print(df[df['publisher'] == 'other'].iloc[:,1:5])
```

```
            platform  year_of_release         genre publisher
5038        PS           1999.0        Sports     other
13365       PS           2001.0  Role-Playing     other
16646      PSV           2016.0        Action     other
```

Observe que passamos `~df['publisher'].isin(rare_publishers)` como uma condição. Isso porque queremos obter `False` nas linhas em que distribuidoras estão na lista `rare_publisher`, para que você possa substituí-las.

## Tarefas

### Tarefa 1

Alguns gêneros no conjunto de dados não estão bem representados. Queremos unir os gêneros menos representados na categoria Diversos substituindo seus valores por `'Misc'`.

Primeiro, conte quantas vezes cada valor aparece na coluna `'genre'` chamando o método `value_counts()` e imprimindo as contagens em ordem crescente.

CódigoPYTHON

9

1

2

3

4

5

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

  

print(df\['genre'\].value\_counts(ascending\=True))

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Crie uma variável chamada `genres` que contém uma lista das duas categorias menos representadas da tarefa anterior: `'Puzzle'` (quebra-cabeças) e `'Strategy'` (estratégia). Depois use `where()` para modificar a coluna `'genre'` de `df` para que os valores na lista `genres` sejam substituídos por `'Misc'`. O pré-código contém o código da tarefa anterior para imprimir os valores únicos e verificar seu resultado.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

  

genres \= \['Puzzle', 'Strategy'\]

df\['genre'\] \= df\['genre'\].where(~df\['genre'\].isin(genres),'Misc')

  

print(df\['genre'\].value\_counts(ascending\=True))

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-43-978Z.md
### Última modificação: 2025-05-28 19:07:45

# Quiz do capítulo - TripleTen

Capítulo 4/11

Filtragem de dados

# Quiz do capítulo

Pergunta

Por padrão, quantos índices existem para um objeto Series e para um objeto DataFrame?

Um e um.

Objetos Series e DataFrame na pandas sempre têm índices, armazenados no atributo `index` (legendas de eixos para objetos Series e legendas de linhas para DataFrame). DataFrames tecnicamente têm dois índices, mas apenas as legendas de linhas são referenciadas ao usar o atributo `index`. Toda vez que você cria um objeto Series ou DataFrame, o atributo `index` correspondente é criado automaticamente com valores padrão se os valores de índice não forem especificados.

Um e um por coluna.

O índice é opcional para ambos, portanto, zero.

Trabalho maravilhoso!

Pergunta

Qual é o tipo de índice padrão para um objeto Series ou um DataFrame?

`RangeIndex` que começa em zero.

O índice padrão para um objeto Series ou um objeto DataFrame é um índice especial, do tipo `RangeIndex`, com números inteiros consecutivos que começam em zero.

`RangeIndex` que começa em um.

`Int64Index` que começa em zero.

`Int64Index` que começa em um.

Fantástico!

Pergunta

Você precisa obter a linha com o índice 4. Qual função funcionaria corretamente?

`df.locate(4)`

`df[4]`

`df.loc[4]`

Os registros de objetos DataFrame podem ser acessados com `loc[]` passando valores de índice e, opcionalmente, nomes de colunas.

`df.iloc[4]`

Você conseguiu!

Pergunta

Você precisa obter a coluna `'age'` para o 4º registro. Qual função funcionaria corretamente?

`df.loc[4, 'age']`

`df.loc[3, 'age']`

Isso retorna o valor de 'age' na posição 4, as posições são enumeradas a partir de 0.

`df.iloc[4, 'age']`

Excelente!

Pergunta

Você precisa obter todos os registros indexados de 5 até e incluindo 50. Qual método funcionaria corretamente?

`df.loc[5:50]`

Ao contrário das fatias comuns em Python, **ambos** o início e o fim são incluídos com **[pandas.DataFrame.loc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html)** _(os materias estão em inglês)_.

`df.loc[5:51]`

`df.iloc[5:50]`

`df.iloc[5:51]`

Você conseguiu!

Pergunta

Você leu os dados do arquivo `clients.csv` em um objeto DataFrame (`df`). O índice criado por padrão com `read_csv()` é o índice `RangeIndex` padrão. Você deseja definir outro índice para o objeto DataFrame, com valores da coluna `client_id`. Como fazer isso?

`df.index = 'client_id'`

`df.index_from('client_id')`

`df.set_index('client_id')`

Isso define os valores no índice com os valores na coluna `client_id` e descarta a coluna `client_id` original.

`df.index = df['client_id'].to_index()`

Muito bem!

Pergunta

Você deseja verificar com `query()` se há um jogo chamado Doom no conjunto de dados de vendas de videogame. Qual dos dois operadores (`=` ou `==`) você usa na condição de consulta?

`=`, por exemplo, `df.query('name = "Doom"')`

`==`, por exemplo, `df.query('name == "Doom"')`

`==` é o operador de comparação.

Nenhum dos operadores pode ser usado nas consultas.

Muito bem!

Pergunta

Você está trabalhando com o conjunto de dados de vendas de videogame mencionado anteriormente no capítulo e quer selecionar os jogos lançados entre 2006 e 2008. Como fazer isso?

Escolha quantas quiser

`df[(df['year_of_release'] >= 2006) & (df['year_of_release'] <= 2008)]`

Construímos dois vetores lógicos e os combinamos com & (AND) para selecionar apenas os registros que satisfaçam ambas as condições.

`df.query('year_of_release >= 2006 and year_of_release <= 2008')`

Embora isso funcione, a sintaxe `query()` pode ser mais direta do que o operador de indexação `[]`.

`df.query('year_of_release in [2006, 2007, 2008]')`

Podemos usar uma `query()` e listar todos os valores possíveis.

`df.query('year_of_release >= "2006" and year_of_release <= "2008"')`

`df[df['year_of_release'].isin([2006, 2007, 2008])]`

Podemos listar todos os valores possíveis e testar os valores da coluna para ver se eles correspondem aos anos que descrevemos.

`df.query('year_of_release >= 2006 and year_of_release < 2008')`

Excelente!

Pergunta

Há dois objetos DataFrame chamados `clients` e `cities`. O `clients` contém informações sobre os clientes, incluindo as cidades de residência deles em `city_id`, o `cities`contém informações sobre cidades, incluindo os identificadores delas em `city_id`e a população em `population`.

Como filtrar clientes em cidades com população superior a 100.000?

Escolha quantas quiser

Isso não é possível sem mesclar ambos os DataFrames em um.

Construindo uma lista de valores de `city_id` com populações de mais de 100.000 e fazendo uma consulta em `clients` com `city_id` na lista construída. `large_cities = cities['population'] > 100000` `clients_in_large_cities = clients[clients['city_id'].isin(large_cities)]`

Construindo uma lista de valores `city_id` com populações superiores a 100.000 e consultando `clients` com `city_id` dessa lista. `large_cities = cities[cities['population'] > 100000]['city_id']` `clients_in_large_cities = clients[clients['city_id'].isin(large_cities)]`

Podemos filtrar o DataFrame com a lista de valores criada usando outra lista.

`large_cities = cities[cities['population'] > 100000]['city_id']` `clients_in_large_cities = clients.query('city_id in @large_cities']`

Podemos filtrar um DataFrame com a lista de valores criada usando outra lista.

`clients_in_large_cities = clients.query('city_id in (cities.population > 100000)']`

Muito bem!

Pergunta

Você deseja preencher os valores ausentes de `year_released` para um determinado videogame com uma constante. Qual função você pode usar?

Escolha quantas quiser

`replace()`

`where()`

`where()` funciona como a instrução if-then. Podemos construir uma condição para verificar se `year_released` de um videogame é um valor ausente e, se a condição for verdadeira, substituir os valores ausentes de `year_released` por alguma constante.

`loc[]`

Podemos construir uma condição para verificar se o `year_released` de um videogame é um valor ausente e, se for o caso, retornar um índice lógico. Esse índice lógico pode ser usado com `loc[]` para substituir os valores ausentes de `year_released` por alguma constante.

`iloc[]`

Podemos construir uma condição para verificar se o `year_released` de um videogame é um valor ausente e, se for o caso, retornar um índice lógico. Esse índice lógico pode ser usado com `iloc[]` para substituir os valores ausentes de `year_released` por alguma constante.

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-45-279Z.md
### Última modificação: 2025-05-28 19:07:45

# Conclusão - TripleTen

Capítulo 4/11

Filtragem de dados

# Conclusão

Você aprendeu várias técnicas usadas na filtragem de dados para processamento e análise.

Pergunta

Agora você é capaz de:

Escolha quantas quiser

Indexar usando `loc[]` e `iloc[]`.

Se ainda não tiver certeza, você pode revisar a lição em [Índices em DataFrames e Series](https://tripleten.com/trainer/data-analyst/lesson/eadf84a2-e0ed-46b0-a334-62004cca6a13/)

Filtrar dados usando strings de consulta e o método `query()`.

Se ainda não tiver certeza, você pode revisar a lição em [Filtragem personalizada usando `query()`](https://tripleten.com/trainer/data-analyst/lesson/c791a30d-0767-4743-8876-624b2c6994a5/)

Filtrar dados com base em condições lógicas complexas.

Se ainda não tiver certeza, você pode revisar a lição em [Filtragem com base em várias condições](https://tripleten.com/trainer/data-analyst/lesson/c7a42381-646b-463b-8c06-6d354843ae48/)

Filtrar e substituir valores ao mesmo tempo usando o método `where()`.

Se ainda não tiver certeza, você pode revisar a lição em [Substituição de valores com `where()`](https://tripleten.com/trainer/data-analyst/lesson/e4a4ca1d-200f-432e-a674-2e353e75a7c6/)

Seu entendimento sobre o material é impressionante!

Você vai usar com frequência essas habilidades de filtragem durante o resto dos seus estudos na TripleTen e em seu trabalho profissional.

Parabéns por ter feito um bom trabalho. Agora que estamos no meio do sprint, é hora de praticar!

### Leve isso com você

Faça download do sumário do capítulo e folha de conclusões para que você possa consultá-los quando necessário.

-   [Resumo: loc\[\] vs. iloc\[\]](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/PT/4_Resumo_loc_vs._iloc.pdf)
-   [Resumo do capítulo: Filtragem de dados](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/PT/4_Resumo_do_capítulo_Filtragem_de_dados.pdf)
-   [Folha de conclusões: Filtragem com base em várias condições](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/PT/4_Folha_de_concluses_Filtragem_com_base_em_vrias_condies.pdf)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-46-589Z.md
### Última modificação: 2025-05-28 19:07:47

# Estudo de caso - TripleTen

Teoria

# Estudo de caso

É hora de colocar todas as abordagens que você aprendeu em prática. Fazer uma pausa na aprendizagem dos materiais e aplicar suas habilidades atuais é importante para garantir que você esteja com tudo pronto para usar seu novo conhecimento.

Este capítulo apresenta um conjunto de pequenas tarefas que você precisa resolver usando um novo conjunto de dados com informação sobre passageiros do Titanic.

Resolver os pequenos problemas apresentados aqui é essencial para concluir o projeto do capítulo. É importante mencionar que você não vai seguir todas as etapas de um projeto real. Às vezes, será necessário apenas tirar alguma conclusão do que você vê sem tomar qualquer medida para resolver o problema. Isso porque seu objetivo principal neste ponto é revisar todo o material que aprendeu.

Com isso em mente, vamos começar.

Estudo de caso

Tarefa11 / 11

1.

O conjunto de dados já foi lido para você. Chame o método `info()` para ter uma ideia melhor dos dados disponíveis.

2.

Vamos conferir as colunas `'deck'` e `'age'`, que têm o maior número de valores ausentes. Use filtragem para obter as linhas com valor ausente na coluna `'deck'` ou na `'age'`. Armazene o resultado na variável `mis_val_rows` e imprima.

3.

Também seria útil examinar as linhas com um valor ausente na coluna `'age'`, mas não na coluna `'deck'`. Há linhas assim? Filtre o DataFrame e armazene o resultado na variável `age_nan`. Por fim, imprima a variável para visualizar os conteúdos.

4.

Vamos examinar a coluna `'deck'` e seus valores. Primeiro, extraia a coluna `'deck'` do DataFrame e armazene-a na variável `deck_col`.

Em seguida, use o método apropriado para contar o número de vezes que cada valor unívoco aparece naquela coluna, incluindo valores ausentes. Armazene o resultado na variável `deck_val`.

Por fim, ordene os resultados da contagem por índice usando o método apropriado. Isso é necessário porque a coluna `'deck'` contém valores de uma letra, como `A`, `B`, `C`, etc. Armazene os resultados ordenados na variável `deck_val`.

Por fim, imprima `deck_val` chamando a função `print()`.

5.

Preencha os valores ausentes na coluna `'deck'` com `'Unknown'` (Desconhecido). Quando terminar, imprima as primeiras linhas do DataFrame.

6.

Por fim, vamos obter o mesmo resultado, mas usando o método `where()`. Quando terminar, imprima as primeiras linhas do DataFrame.

7.

Para verificar os resultados da substituição, filtre o DataFrame para extrair todas as linhas em que o valor da coluna `'deck'` é `'Unknown'`. Use um filtro simples para concluir a tarefa. Salve o DataFrame resultante na variável `deck_unknown` e a imprima.

8.

Agora, vamos obter o mesmo resultado, mas desta vez usando o método `query()`. Armazene o resultado na mesma variável `deck_unknown` e a imprima.

9.

Agora vamos examinar duplicados. Vamos supor que já sabemos que eles existem nos nossos dados. O objetivo é filtrar o DataFrame para extrair apenas linhas duplicadas e armazená-las na variável `duplicates`. Por fim, imprima a variável `duplicates`.

10.

Vamos continuar trabalhando com duplicados. Seu objetivo é contar o número de vezes que cada valor unívoco aparece na coluna `'who'`. Armazene o resultado na variável `who_unique_val` e a imprima.

11.

Reparamos que a coluna `'who'` contém dois valores idênticos: `'child'` e `'Child'`. Para garantir a consistência, vamos converter todos os valores nessa coluna para minúsculas. Substituiremos a coluna original por uma nova em que todos os valores `'Child'` serão substituídos por `'child'`. Depois de fazer a substituição, chame o método `value_counts()` na coluna `'who'` para conferir os resultados.

9

1

2

3

4

5

6

import pandas as pd

  

df \= pd.read\_csv('/datasets/titanic\_updated.csv')

  

df\['who'\] \= df\['who'\].str.lower()

print(df\['who'\].value\_counts())

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-49-496Z.md
### Última modificação: 2025-05-28 19:07:49

# Introdução - TripleTen

Capítulo 6/11

Visualização de dados

# Introdução

Até agora na sua jornada de análise de dados, você aprendeu diferentes maneiras de ler dados, como lidar com valores ausentes e duplicados e como filtrar conjuntos com base em várias condições. Essas habilidades são necessárias para te preparar para análises mais profundas.

Neste capitulo, você vai aprender a visualizar seus dados criando gráficos de aspecto profissional.

O que exatamente você vai aprender:

-   Elementos que compõem bons gráficos
-   Como criar gráficos usando Python
-   Como criar histogramas e gráficos de dispersão, de linha e de barras.
-   Correlação entre variáveis
-   Distribuição de valores para variáveis unitárias

A visualização de dados nos permite identificar rapidamente padrões e relações contidos neles e auxilia na compreensão das informações. Ela também nos ajuda a tirar conclusões, fazer novas perguntas e comunicar nossas descobertas aos outros de forma mais eficaz.

Serão necessárias de 2 a 3 horas para concluir este capítulo. Então vamos começar.

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6_1.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-50-815Z.md
### Última modificação: 2025-05-28 19:07:51

# Visualização de dados: visão geral - TripleTen

Capítulo 6/11

Visualização de dados

# Visualização de dados: visão geral

Antes que você comece a fazer suas próprias criações, queremos usar o início desta lição para discutir a função da visualização de dados e rever os elementos de um bom gráfico. Vamos discutir rapidamente por que isso é importante, que tipos de gráficos usaremos neste capítulo e quais princípios seguir na criação deles.

## Por que visualizar dados?

Existem duas razões principais para criar visualizações como um profissional de dados:

1.  Para facilitar a análise de grandes quantidades de dados e revelar visualmente suas propriedades; em outras palavras, _visualizar para analisar_. Ao visualizar os dados, você pode obter instantaneamente um entendimento melhor da situação, o que seria impossível trabalhando com números brutos.
2.  Para comunicar os resultados da análise aos seus colegas de uma maneira compreensível e efetiva.

Dados representados visualmente costumam ser muito mais fáceis de entender do que dados brutos. Visualizações também podem criar uma resposta emocional que pode ser importante e útil quando você está tentando explicar seu trabalho.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_27_1690872753.png)

### Visualizar para analisar

Aqui está uma tabela de peso corporal de ratos, em uma dieta específica, em diferentes momentos da vida deles:

age

mouse1

mouse2

1

24

18

2

56

36

3

64

50

4

82

68

5

92

72

6

94

72

7

88

74

Percebeu algum padrão?

Vamos conferir um gráfico de dispersão desses dados:

![](https://practicum-content.s3.amazonaws.com/resources/6.2_PT_1692095338.png)

Parece que o peso corporal aumenta linearmente do nascimento até mais ou menos quatro meses, depois disso os ratos tendem a parar de ganhar peso. É muito mais fácil ver a relação entre peso e idade em um gráfico do que em uma tabela com dados numéricos.

### Comunicação de resultados

Outra maneira de usar gráficos é para apresentar nosso trabalho para pessoas que não têm o nosso nível de conhecimento técnico. Partes interessadas e tomadores de decisão normalmente ficam mais envolvidos e são mais facilmente convencidos por imagens do que por um texto ou números.

Aqui está um exemplo de um gráfico de barras que efetivamente passa informações sobre a receita de uma empresa de TI:

![](https://practicum-content.s3.amazonaws.com/resources/6.2.2_PT_1690872793.png)

Esse aspecto dramático dos gráficos tem vantagens e desvantagens. Às vezes, a visualização pode ser errônea se você não levar em conta todas as informações, como, por exemplo, o intervalo de valores para cada eixo e o que aqueles eixos representam. Não se deixe enganar enquanto estiver trabalhando com gráficos. Analise com cuidado o que mostra cada gráfico antes de tirar conclusões precipitadas.

Lembre-se também de que, como analista, você é responsável pelos gráficos que produz. Tente prever quais conclusões eles sugerem e pense se elas são razoáveis e honestas.

## Escolher a visualização correta

Existem muitos tipos de gráficos. Qual deles usar depende de muitos fatores, inclusive o tipo e a dimensão dos dados (ou seja, o número de variáveis representadas nele) e outros fatores que vamos abordar ao longo deste capítulo. Cada tipo pode ser útil para os analistas e leitores à sua maneira.

Aqui estão alguns exemplos dos tipos de gráficos que você vai aprender neste capítulo:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_28_1690872813.png)

Da esquerda para a direita, esses são os gráficos de barras, de dispersão e de linha.

## Elementos de um bom gráfico

Bons gráficos nos ajudam a tirar conclusões a partir dos dados. Eles fornecem uma boa compreensão inicial dos dados sem exigir uma análise profunda. Entretanto, o gráfico abaixo não é útil para isso, já que não está claro o que os dados representam. Vamos conferir alguns elementos que podemos adicionar a esse gráfico que vão torná-lo melhor.

![](https://practicum-content.s3.amazonaws.com/resources/6.2.3_PT_1692095516.png)

### **Títulos**

Todo gráfico deve ter um título descritivo que informa imediatamente ao leitor por que ele foi criado.

Os títulos não devem apenas reiterar o que as legendas dos eixos já informam. Pegue o gráfico sobre o peso corporal dos ratos como exemplo. Ele tem o nome "Taxa de crescimento dos ratos (Dieta #1)", que é muito melhor do que um título como "Peso corporal dos ratos vs. idade". Já fica claro a partir das legendas dos eixos que estamos mostrando o peso corporal e a idade, então um título como esse não traria informações adicionais.

Títulos podem ainda incluir um subtítulo onde a conclusão do gráfico é explicada. Por exemplo, "Taxa de crescimento dos ratos (Dieta #1): o peso aumenta linearmente com a idade até cerca de 4 meses, depois o peso começa a estabilizar".

### **Eixos**

Os eixos devem sempre ser legendados para evitar ambiguidade sobre a quantidade ou as categorias mostradas.

Se um eixo representa alguma medida física (por exemplo, peso, velocidade, tempo, dinheiro) então você sempre deve incluir as unidades daquela medida na legenda. Marcas nos eixos com legendas numéricas ou categóricas também devem ser incluídas para que o leitor tenha noção da escala dos dados.

Geralmente, a melhor opção é representar a variável independente no eixo X e a variável dependente, no eixo Y.

-   A **variável independente** é a causa da variação no gráfico. O valor dela é **independente** das outras variáveis no gráfico.
-   Já a **variável dependente** é o efeito disso. O valor dela **depende** das mudanças na variável independente.

### **Legendas de gráfico**

As legendas de gráfico são úteis quando você está usando mais do que um conjunto de dados no mesmo gráfico.

O gráfico dos ratos inclui uma legenda codificada por cores que diz ao leitor que existem dados para dois ratos diferentes. Isso nos permite traçar uma terceira variável (número de ratos) em um gráfico 2D usando cores diferentes para diferenciar os animais. A legenda do gráfico comunica o significado de cada cor (roxo para o rato 1 e verde para o rato 2).

Nem todos os gráficos necessitam de legenda. Ainda assim, ela pode ajudar a reforçar as informações.

Pergunta

Então, o que mesmo está errado com este gráfico? Dado tudo o que você acabou de aprender, consegue pensar em 5 problemas que o gráfico tem que o impede de transmitir informações de modo claro?

![](https://practicum-content.s3.amazonaws.com/resources/6.2.3_PT_1692095516.png)

Escolha quantas quiser

Ele não tem título.

Um título certamente ajudaria a entender do que se trata o gráfico.

Verde e roxo não é um bom esquema de cores.

Não há legenda de gráfico.

Sem uma legenda de gráfico, é impossível adivinhar o que as linhas verde e roxa estão tentando dizer.

Os eixos deveriam ser trocados.

Isso é verdade. A causa da variação de dinheiro é a variação de tempo, o que torna o dinheiro a variável dependente e tempo a variável independente.

As legendas dos eixos não têm unidades.

Realmente. Não sabemos se estamos falando sobre milhões de dólares em décadas ou alguns dólares em uma semana.

As marcas dos eixos estão muito separadas.

As marcas não têm legendas.

Isso também não ajuda a entender como as variável mudam. É de fato um problema.

Trabalho maravilhoso!

Pergunta

Digamos que você deseja visualizar a relação entre o dinheiro gasto em uma campanha publicitária e o número de usuários que ela atrai. O que vai no eixo Y?

Dinheiro gasto

Número de usuários atraídos

Isso mesmo! Essa é a variável dependente: o efeito, não a causa.

Muito bem!

Isso significa que um bom gráfico deve fornecer informações fáceis de entender em tempo real, sem levar a conclusões erradas sobre os dados. Construir bons gráficos é uma arte que qualquer profissional de dados deve se esforçar para dominar. Vamos aprender como fazer isso bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-52-166Z.md
### Última modificação: 2025-05-28 19:07:52

# O que podemos aprender com visualizações ruins - TripleTen

Capítulo 6/11

Visualização de dados

# O que podemos aprender com visualizações ruins

Às vezes, a melhor maneira de aprender como fazer algo bem é estudar a partir de exemplos em que alguma coisa deu errado.

Nesta lição, vamos revisar alguns gráficos do mundo real que deram errado de um jeito ou de outro. Algumas dessas falhas são sutis, então é bom desenvolver uma noção do que é um bom gráfico.

Vamos explorar alguns exemplos divertidos.

## Uso desnecessário do eixo X

Dê uma olhada neste [artigo](https://www.aljazeera.com/news/2022/3/21/ukraines-poroshenko-urges-gulf-countries-to-increase-oil-output) _(os materiais estão em inglês)_ da Al Jazeera que compara a quantidade de gás natural importado da Rússia em 2019 por diferentes países. Existem duas comparações aqui:

1.  A quantidade de dinheiro gasto na importação do gás (representada pelo tamanho do círculo)
2.  A proporção de gás natural que veio de importação da Rússia (representada pelo eixo Y)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_30_1690873235.png)

Está tudo bem em usar círculos de tamanhos diferentes para representar o dinheiro gasto e cor para representar o continente. O problema aqui é que o eixo X ordena os países em ordem alfabética, o que é irrelevante e não dá nenhuma informação adicional.

Observe o canto inferior direito da imagem. Veja os países cujos nomes começam com a letra **S** grudados uns aos outros, o que dificulta entender qual círculo pertence a qual país. Haveria mais espaçamento entre os nomes se os países não estivessem organizados alfabeticamente.

[Aqui](https://www.gapminder.org/tools/#$model$markers$bubble$encoding$size$data$space@=country&=time;;&scale$domain:null&type:null&zoomed:null;;&y$data$concept=co2_emissions_tonnes_per_person&space@=country&=time;;&scale$domain:null&zoomed:null&type:null;;&x$data$concept=gdppercapita_us_inflation_adjusted&space@=country&=time;;&scale$domain:null&zoomed:null&type:null;;&frame$value=2018;;;;;&chart-type=bubbles&url=v1) _(os materiais estão em inglês)_ está um exemplo melhor de um gráfico similar que analisa a conexão entre as emissões de dióxido de carbono e o PIB de diferentes países.

## Gráfico de pizza sem sentido

Confira este gráfico de pizza da Fox News ([que foi assunto no Business Insider](https://www.businessinsider.com/fox-news-charts-tricks-data-2012-11)) _(os materiais estão em inglês)_ que compara a popularidade dos candidatos do Partido Republicano nas eleições presidenciais dos EUA de 2012:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_31_1690873251.png)

Esse gráfico traz mais perguntas do que respostas. Por que as porções do gráfico de pizza somadas são maiores que 100%? Os entrevistados podiam escolher mais de uma opção? Existiam apenas esses três candidatos na pesquisa?

Por sinal, é melhor [não usar gráficos de pizza](https://scc.ms.unimelb.edu.au/resources-list/data-visualisation-and-exploration/no_pie-charts) _(os materiais estão em inglês)_. Não existe quase nenhum caso em que é melhor usar gráficos de pizza e não gráficos de barras para comunicar informações com clareza.

Presumindo que os entrevistados da pesquisa puderam escolher mais de um candidato, um gráfico de barras seria melhor para esses dados, como este, por exemplo:

![](https://practicum-content.s3.amazonaws.com/resources/6.3.1_PT_1692095858.png)

Esse ainda é um gráfico baseado em um conjunto de dados peculiar. No entanto, lendo da esquerda para a direita, podemos entender facilmente que Palin tem números maiores do que Huckabee que, por sua vez, tem números maiores do que Romney.

Depois de entender a ideia principal, você pode analisar o gráfico em mais detalhes e conferir os números. Além disso, seria útil adicionar uma observação na parte inferior indicando que os entrevistados puderam escolherem mais de um candidato.

## Gráfico de barras enganoso

Outra figura da Fox News que foi analisada pelo [Business Insider](https://www.businessinsider.com/fox-news-charts-tricks-data-2012-11) _(os materiais estão em inglês)_, este é um gráfico de barras visualmente enganoso:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_33_1690873285.png)

O tamanho das barras faz parecer que o número de beneficiários da assistência social nos EUA duplicou de 2009 a 2011, mas dê uma olhada no eixo Y. Em vez de começar do zero, o eixo começa em 94 milhões. As alturas das barras fazem _parecer_ que houve um aumento de 400%, enquanto os números indicam um aumento de 10%.

Podemos consertar isso fazendo o eixo Y começar no 0 e ir até 108 milhões. Isso mostraria visualmente que o aumento foi de 10%.

Perceba que existem casos onde é aceitável _não_ fazer os eixos começarem em zero. Confira [este vídeo](https://www.youtube.com/watch?v=14VYnFhBKcY) _(os materiais estão em inglês)_ para mais informações sobre isso.

## Sobrecarga de informação

Aqui está um gráfico sobre usinas elétricas nos EUA entre 1990 e 2008 apresentada pela [Microsoft](https://www.microsoft.com/en-us/research/blog/geoflow-takes-data-3-d-drive/) _(os materiais estão em inglês)_ usando GeoFlow com o Microsoft Excel:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_34_1690873309.png)

Esse gráfico é um ótimo exemplo de "informação demais". O objetivo de visualizar dados é ter percepções rápidas e fáceis e comunicar claramente essas percepções às outras pessoas.

É difícil dizer em uma primeira olhada o que essa figura está tentando transmitir. Não deveria ser difícil para o público compreender o seu gráfico. Muitas vezes, menos é mais.

## Gráfico de linha enganoso

Mais um gráfico enganoso da Fox News, neste caso de uma postagem do [reddit](https://www.reddit.com/r/CrappyDesign/comments/fv0fpz/the_yaxis_on_this_fox_news_graph_of_coronavirus/) _(os materiais estão em inglês)_. Este gráfico de linha mostra a evolução de novos casos de COVID em 2020:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_35_1690873327.png)

Percebeu alguma coisa estranha?

Olhe atentamente para a escala do eixo Y. É claro, ela não começa em zero, mas essa não é a questão nesse caso. São os intervalos entre as marcas do eixo Y que estão estranhos. A princípio, cada marca representa um aumento de 30 casos. Depois, de repente, a 4º marca representa um aumento de apenas 10 casos (de 90 a 100). As próximas marcas representam 30 casos de novo, mas a 8º marca representa 50, a 9º representa 10 e cada marca depois disso representa 50 casos. O eixo Y está uma bagunça total.

Confira como o gráfico ficaria se as marcas do eixo fossem consistentes, como deveriam ser:

![](https://practicum-content.s3.amazonaws.com/resources/6.3.2_PT_1_1692096003.png)

Código de exemplo para a figura acima

```

    import pandas as pd
    import numpy as np
    import plotly.express as px
    
    cases = [33, 61, 86, 112, 116, 129, 192, 174, 344, 304, 327, 246, 320, 339, 376]
    
    dates = ['March<br>'] * len(cases)
    day = 18
    for i in range(len(dates)):
        dates[i] = dates[i] + str(day)
        day = day + 1
    dates[-1] = 'April<br>1'
    
    labels = dict(date="Date", cases="Number of cases")
    markers = dict(size=30, line=dict(width=2, color='black'), color='white')
    title = dict(text='New Cases Per Day', font=dict(color='white', size=30))
    yaxis = dict(tickmode='linear', tick0=30, dtick=30)
    
    df = pd.DataFrame({'cases': cases, 'date': dates})
    
    fig = px.line(df, y='cases', x='date', text='cases', markers=True, labels=labels, title="New Cases Per Day")
    
    fig.update_xaxes(showgrid=False, color='white', tickangle=0)
    fig.update_yaxes(color='white', gridcolor='#5c5a5c', gridwidth=2, range=[15, 400])
    fig.update_traces(marker=markers, line_color='white', line_width=6)
    fig.update_layout(title=title,
                      title_x=0.5,
                      paper_bgcolor='#070230',
                      plot_bgcolor='#070230',
                      yaxis=yaxis,
                      xaxis_type='category')
    fig.add_annotation(text='TOTAL CASES', 
                        align='right',
                        showarrow=False,
                        font=dict(color='white', size=12),
                        xref='paper',
                        yref='paper',
                        x=1.08,
                        y=1.25)
    fig.add_annotation(text='3,342', 
                        align='right',
                        showarrow=False,
                        font=dict(color='white', size=23),
                        xref='paper',
                        yref='paper',
                        x=1.071,
                        y=1.2)
    
    fig.show()
    
 
```

## Uso confuso de cor

Aqui está outro gráfico sobre casos de COVID nos EUA do CDC, abordado na postagem do blog [Towards Data Science](https://towardsdatascience.com/why-is-this-chart-bad-5f16da298afa) (Rumo à Ciência de Dados) _(os materiais estão em inglês)_:

![](https://practicum-content.s3.amazonaws.com/resources/6.3.3_PT_1_1692096107.png)

À primeira vista, parece que estados com cores mais escuras têm mais casos de COVID. Realmente, isso faria total sentido. Mas olhe atentamente para a legenda do gráfico. É como se as cores tivessem sido atribuídas aleatoriamente. Na melhor das hipóteses, isso torna o gráfico inútil para comunicação. Na pior das hipóteses, ele está enganando de propósito. Além disso, as cores atribuídas a **Nenhum** e **101** a **1.000** são quase as mesmas, o que as torna quase impossíveis de distinguir visualmente.

O conserto para esse gráfico é simples: fazer a gradação de cores se correlacionar com o número de casos. Na verdade, parece que o CDC fez exatamente isso com os [dados atuais](https://covid.cdc.gov/covid-data-tracker/?CDC_AA_refVal=https%3A%2F%2Fwww.cdc.gov%2Fcoronavirus%2F2019-ncov%2Fcases-updates%2Fcases-in-us.html#cases_casesper100klast7days) _(os materiais estão em inglês)_ no seu site.

## Recapitulação

Como você pode ver, visualização ruim está em toda parte. Como profissional de dados, é seu dever evitar esses erros e reconhecê-los quando você os vê. Ao longo do resto deste capítulo, você vai aprender as melhores práticas para apresentar os tipos de gráficos desta lição e além.

Para mais ótimos exemplos de visualizações ruins e recomendações sobre como melhorá-las, navegue pelos blogs e artigos que postamos. Ou acesse outros blogs dedicados a esse tópico, como [este](https://badvisualisations.tumblr.com/) _(os materiais estão em inglês)_ aqui.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-54-043Z.md
### Última modificação: 2025-05-28 19:07:54

# Como criar gráficos em Python com matplotlib - TripleTen

Teoria

# Como criar gráficos em Python com matplotlib

O Python tem várias bibliotecas populares para construir gráficos dos seus dados. Talvez a biblioteca mais comum seja a **[Matplotlib](https://matplotlib.org/stable/index.html)** _(os materiais estão em inglês)_. Você provavelmente vai encontrar muitas menções a essa biblioteca na internet quando procurar soluções para problemas com gráficos em Python.

Recentemente, outras bibliotecas de gráficos mais "simples" foram criadas usando a Matplotlab como base. Em especial, **[Seaborn](https://seaborn.pydata.org/)** _(os materiais estão em inglês)_ e **[Plotly](https://plotly.com/python/)** _(os materiais estão em inglês)_ são opções populares. A Plotly também tem uma biblioteca mais nova chamada **[Plotly Express](https://plotly.com/python/plotly-express/)** _(os materiais estão em inglês)_, que é muito intuitiva e produz gráficos bonitos desde o início.

Mas para muitos tipos de gráficos simples não precisamos usar nenhuma biblioteca externa. A boa e velha pandas tem seus próprios métodos para fazer gráficos a partir dos DataFrames e objetos Series. Nesta lição, você vai aprender a fazer gráficos usando a pandas e a exibi-los usando a Matplotlib.

## O método `plot()` e `plt.show()`

Vamos começar aprendendo sobre o método `plot()` da pandas e o método `show()` da biblioteca `pyplot` da Matplotlib. Aqui está um exemplo de como podemos chamar `plot()` para construir um gráfico na pandas e depois usar `show()` para exibir o gráfico em toda a sua beleza:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]})
print(df)

df.plot()
plt.show()
```

```
   a   b
0  2   4
1  3   9
2  4  16
3  5  25
```

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.4.1.png)

Para desenhar esse gráfico, importamos as bibliotecas `pandas` e `pyplot` da `matplotlib`. Uma técnica popular é usar `plt` como apelido para a biblioteca `pyplot`.

O método `plot()` da `pandas` prepara a imagem com base nos dados, enquanto a função `show()` de `pyplot` apresenta a imagem ao usuário.

Por padrão, o método `plot()` cria um gráfico em que todas as colunas numéricas do DataFrame são traçadas nos mesmos eixos. Os índices ficam no eixo X, e os valores das colunas ficam no eixo Y.

Se for necessário, você também pode traçar uma única coluna:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]})

df['b'].plot()
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.4.2.png)

Em alguns ambientes de desenvolvimento, como no Jupyter Notebook, você não _precisa_ usar `plt.show()` para exibir o gráfico. No entanto, é uma boa prática sempre incluir `plt.show()` para que o gráfico seja exibido em qualquer ambiente que alguém possa executar o seu código.

Outra opção útil é escrever um código que salva os gráficos como arquivos de imagem. Por exemplo, podemos salvar o gráfico acima no formato **PNG** usando a função `savefig()` da `matplotlib`:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]})

df['b'].plot()
plt.savefig('myplot.png')
```

Se você rodar esse código na sua máquina, um arquivo PNG nomeado `myplot.png` será criado no diretório do seu script Python ou Notebook Jupyter. O Python sabe em qual formato salvar o arquivo com base na extensão na string de entrada para `savefig()`.

## Personalização de gráficos com parâmetros `plot()`

Você deve ter percebido que os gráficos que fizemos até agora são muito simples e não cumprem as práticas recomendadas abordadas anteriormente neste capítulo. Bem, `plot()` tem uma tonelada de parâmetros que podemos usar para consertar isso.

### **Títulos**

Títulos para os gráficos são passados no parâmetro `title=` como strings:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]})

df.plot(title='A e B')
plt.show()
```

![](https://practicum-content.s3.amazonaws.com/resources/10_1692096416.png)

Agora temos um bom título para complementar nosso gráfico. É claro, seria melhor escolher alguma coisa mais descritiva do que "A e B" se esses fossem dados reais.

### **Estilo de linha**

Nosso gráfico não está tão ruim, mas um gráfico de linha não é ideal aqui, porque implica que temos pontos de dados ao longo de toda a linha quando, na verdade, temos apenas quatro. Para deixar isso óbvio para qualquer pessoa que olhar para o gráfico, seria melhor usar um marcador de estilo diferente para os pontos de dados.

Para mudar o marcador de estilo, use o parâmetro `style=`. Vamos passar a ele o argumento `'o'` para fazer cada valor ser marcado como um ponto.

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]})

df.plot(title='A e B', style='o')
plt.show()
```

![](https://practicum-content.s3.amazonaws.com/resources/11_1692096454.png)

Existem outros marcadores de estilo também, por exemplo, `style='x'` marca cada ponto com um x:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]})

df.plot(title='A e B', style='x')
plt.show()
```

![](https://practicum-content.s3.amazonaws.com/resources/12_1692096489.png)

Diferentes estilos podem até ser combinados. Por exemplo, quando queremos exibir tanto linhas quanto pontos, podemos usar `style='o-'`:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]})

df.plot(title='A e B', style='o-')
plt.show()
```

![](https://practicum-content.s3.amazonaws.com/resources/13_1692096521.png)

Existem muitos outros [estilos de marcadores](https://matplotlib.org/stable/api/markers_api.html) _(os materiais estão em inglês)_, então fique à vontade para brincar com eles e explorar!

### **Eixos**

Em todos os gráficos que você viu até agora, o eixo horizontal tinha os valores dos índices do DataFrame. Mas e se você quiser desenhar duas colunas, uma em relação a outra? Podemos usar os parâmetros `x=` e `y=` para conseguir isso:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]})

df.plot(x='b', y='a', title='A vs B', style='o')
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.4.7.png)

Aqui a coluna `'a'` está no eixo y e a coluna `'b'`está no eixo X. Perceba que a pandas nomeou o eixo horizontal automaticamente com `'b'`e a legenda padrão do gráfico agora faz referência apenas à coluna `'a'`.

Para mudar a legenda dos eixos, podemos usar os parâmetros `xlabel=` e `ylabel=` do método `plot()`:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]})

df.plot(x='b',
        y='a',
        title='A vs B',
        style='o',
        xlabel="Oi, sou B",
        ylabel="Oi, sou A")

plt.show()
```

![](https://practicum-content.s3.amazonaws.com/resources/14_1692096568.png)

É importante mencionar que há um jeito alternativo de configurar legendas: tanto para o eixo X quanto para o Y. Em vez de especificar `xlabel=` e `ylabel=` no método `plot()`, podemos definir legendas com `plt.xlabel()` e `plt.ylabel()`. Aqui está como:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]})

df.plot(x='b',
        y='a',
        title='A vs B',
        style='o',
        legend=False)

plt.xlabel("Olá, eu sou B") # configurando a legenda de x
plt.ylabel("Olá, eu sou A") # configurando a legenda de y
plt.show()
```

Depois, podemos também definir os limites dos eixos usando os parâmetros `xlim=` **e** `ylim=`. Eles recebem um número ou uma lista de dois números como argumento. Se você passar um único número, ele será o valor mínimo exibido para o respectivo eixo. Se você passar uma lista de dois números, o primeiro será o valor mínimo e o segundo será o valor máximo.

Vamos expandir o intervalo do eixo horizontal de 0 a 30 e definir apenas o valor mínimo do eixo vertical como 0:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]})

df.plot(x='b', y='a', title='A vs B', style='o', xlim=[0, 30], ylim=0)
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.4.10.png)

### **Outras opções de personalização**

Outras funcionalidades podem ser adicionadas aos gráficos se você achar que eles ficariam mais fáceis de ler. Vamos adicionar linhas de grade para ajudar a observar quais valores estamos desenhando. Para fazer isso, defina o parâmetro `grid=` com `True`. Por padrão, esse parâmetro é definido como `False`:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]})

df.plot(x='b', y='a', title='A vs B', style='o', grid=True)
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.4.11.png)

Também podemos gerenciar o tamanho do gráfico com o parâmetro `figsize=` (tamanho da imagem). A largura e o comprimento em polegadas são passados como uma lista: `figsize=[largura, altura]`. Vamos comparar dois gráficos de tamanhos diferentes:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.DataFrame({'a':[2, 3, 4, 5], 'b':[4, 9, 16, 25]})

# construindo um gráfico pequeno
df.plot(x='b', y='a', style='o', xlim=[0, 30], figsize=[2, 2])

# construindo um gráfico grande
df.plot(x='b', y='a', style='o', xlim=[0, 30], figsize=[10, 4])

plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.4.12.png)

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.4.13.png)

### **Recapitulação**

Confira um resumo de todos os métodos e funções que discutimos nesta lição:

-   `plot()`: é o método da pandas que prepara uma imagem a partir dos dados.
-   `show()`: é a função da biblioteca `pyplot` da Matplotlib que apresenta uma imagem ao usuário.
-   `savefig()`: é a função da Matplotlib que salva o gráfico como um arquivo de imagem.
-   `title=`: é o parâmetro para definir o título do gráfico.
-   `style=`: é o parâmetro para alterar o estilo do marcador do gráfico.
-   `x=` e `y=`: são os parâmetros para construir duas colunas relacionadas entre si.
-   `xlabel=` e `ylabel=`: são os parâmetros para definir as legendas dos eixos.
-   `xlim=` e `ylim=`: são os parâmetros para definir os limites dos eixos.
-   `grid=`: é o parâmetro para adicionar linhas de grade.
-   `figsize=`: é o parâmetro para definir o tamanho do gráfico.
-   `legend=`: é o parâmetro para adicionar ou remover a legenda do gráfico.

Como criar gráficos em Python com matplotlib

Tarefa

No pré-código fornecido, importamos as bibliotecas pandas e pyplot da Matplotlib e criamos um DataFrame chamado `df`.

Sua tarefa é criar um gráfico de estrelas rosas usando os dados em `df`. Todos os atributos necessários estão listados abaixo para sua conveniência. Siga as instruções para alcançar o resultado desejado. Se for necessário, consulte a seção de recapitulação para se lembrar dos nomes dos parâmetros.

Usando o método `plot()` da pandas, crie um gráfico da coluna `'a'` em relação à coluna `'c'` de `df` com os seguintes argumentos:

1.  Título "A vs C" (maiúsculas e minúsculas são importantes, você pode copiar e colar o texto)
2.  Marcador em forma de estrela (você pode usar um asterisco `'*'` para isso)
3.  Marcadores сoloridos rosa choque (use o novo parâmetro autoexplicativo `color=` para isso, com o argumento `'hotpink'`)
4.  Tamanho do gráfico de 5 por 5 polegadas
5.  Intervalo do eixo X de 0 a 12
6.  Intervalo do eixo Y de 1 a 6
7.  O eixo X legendado com "C”
8.  O eixo Y legendado com "A”

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

import pandas as pd

import matplotlib.pyplot as plt

  

\# Dados de exemplo

data \= {'x': \[1, 2, 3\], 'y': \[4, 5, 6\], 'z': \[7, 8, 9\]}

df \= pd.DataFrame(data)

  

\# Criando o gráfico com legenda

df.plot(x\='x', y\=\['y', 'z'\], legend\=True)

df.plot(x\='x', y\=\['y', 'z'\], legend\=False)

  

  

  

\# Exibindo o gráfico

plt.show()

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-55-357Z.md
### Última modificação: 2025-05-28 19:07:55

# Gráficos de dispersão - TripleTen

Teoria

# Gráficos de dispersão

Uma parte importante da Análise Exploratória de Dados é desenvolver uma compreensão das relações entre variáveis (ou seja, colunas) nos dados.

Por exemplo, talvez seja necessário entender melhor a relação entre altura e peso, temperatura e vendas de sorvetes, nível de formação e salário ou horas dedicadas ao estudo e notas nas provas. **Gráficos de dispersão** são uma ótima maneira de visualizar isso.

Um **gráfico de dispersão** é quando um único ponto é desenhado para cada conjunto de variáveis, mas os pontos não são conectados por linhas.

Assista ao vídeo abaixo e depois continue lendo para aprender mais.

<iframe class="base-markdown-iframe__iframe" id="player-lMcYL715PZU" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Scatterplots" width="640" height="360" src="https://www.youtube.com/embed/lMcYL715PZU?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fd491db3a-7b32-48d6-ba36-29980aad8d1e%2Ftask%2F9efee7dd-76ff-4069-95f1-fd4717a4dbdb%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Para ilustrar a utilidade dos gráficos de dispersão ainda melhor, vamos examinar um conjunto de dados de altura e peso registrado de 10.000 adultos nos EUA. Vamos começar, como sempre, lendo os dados, imprimindo as primeiras linhas e conferindo as informações gerais sobre o conjunto de dados:

```
import pandas as pd

df = pd.read_csv('/datasets/height_weight.csv')

print(df.head())
print()
df.info()
```

```
      height      weight  age  male
0  67.345391  151.163975   26     0
1  69.131220  203.525148   44     1
2  67.710271  205.752354   42     1
3  62.412362  117.148813   27     0
4  72.967492  229.394555   49     1

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 10000 entries, 0 to 9999
Data columns (total 4 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   height  10000 non-null  float64
 1   weight  10000 non-null  float64
 2   age     10000 non-null  int64  
 3   male    10000 non-null  int64  
dtypes: float64(2), int64(2)
memory usage: 312.6 KB
```

Os dados têm quatro colunas e 10.000 linhas, sem nenhum valor ausente. Cada linha contém dados coletados de um único indivíduo. As colunas são:

-   `'height'`: altura da pessoa em polegadas
-   `'weight'`: peso da pessoa em libras
-   `'age'`: idade da pessoa em anos
-   `'male'`: sexo da pessoa, em que 1 = masculino e 0 = feminino

Agora vamos conferir a descrição numérica dos dados:

```
import pandas as pd

df = pd.read_csv('/datasets/height_weight.csv')

print(df.describe())
```

```
             height        weight           age          male
count  10000.000000  10000.000000  10000.000000  10000.000000
mean      66.367560    164.157357     37.507000      0.500000
std        4.284200     33.881810      7.495236      0.500025
min       53.663133     63.700127     25.000000      0.000000
25%       63.029487    137.599803     31.000000      0.000000
50%       66.276290    163.565827     37.000000      0.500000
75%       69.676237    191.227505     44.000000      1.000000
max       79.598742    280.989699     50.000000      1.000000
```

A partir do resultado de `describe()`, podemos determinar que o intervalo de idade dos adultos no conjunto é de 25 a 50 anos. A coluna `'male'` foi incluída no resultado porque é um tipo de dados numérico do Python, mas tenha em mente que essa é uma variável categórica, então não devemos dar muita atenção a essa informação.

Agora que temos algumas estatísticas resumidas, vamos usar a visualização para ter uma noção melhor dos dados. Vamos investigar a relação entre altura e peso primeiro.

Vamos começar usando o comportamento padrão do método `plot()` para criar um gráfico de linha, colocando altura e peso nos eixos X e Y, respectivamente:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

df.plot(x='height', y='weight')
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.5.1.png)

Opa, que bagunça! Por que nosso gráfico está assim?

Por padrão, o método `plot()` cria um gráfico de linha passando por todas as linhas do DataFrame em ordem. Para cada linha de dados, ele cria um ponto e o conecta ao ponto anterior usando uma linha.

Já que nosso DataFrame não é ordenado por nenhuma coluna em particular, os pontos estão por toda a parte e as linhas os conectando não têm significado, resultando nessa bagunça.

Vamos ordenar os dados por altura e tentar de novo para ver se conseguimos melhorar o gráfico:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

df.sort_values('height').plot(x='height', y='weight')
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.5.2.png)

Já está melhor, mas conectar os pontos de dados por linhas não faz sentido, já que a altura e o peso de uma pessoa não estão relacionados à altura e ao peso de outra. Cada ponto de dados representa uma medida discreta independente.

Ao decidir qual tipo de gráfico usar para os dados, é importante considerar o que cada ponto dos dados representa e como se relaciona ao próximo. Essa deve ser a primeira coisa a ser considerada ao escolher um gráfico para uma apresentação.

Nesse caso, é melhor usar pontos individuais para desenhar dados discretos como esses. É aqui onde os gráficos de dispersão entram em cena. Com um gráfico de dispersão, os dados não precisam ser ordenados, o gráfico terá sempre a mesma aparência, independentemente das linhas estarem ordenadas no DataFrame ou não. Isso também responde à pergunta sobre a relação entre os pontos de dados, já que não há nenhuma relação visível no gráfico de dispersão.

Na lição anterior, aprendemos uma forma indireta de criar gráficos de dispersão incluindo determinados argumentos no parâmetro `style=`.

Por exemplo, podemos usar `'o'` para fazer um gráfico de dispersão onde cada ponto é marcado com um círculo:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

df.plot(x='height', y='weight', style='o')
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.5.3.png)

Agora cada ponto de dados foi desenhado independentemente, deixando claro que há uma relação entre altura e peso. O gráfico de dispersão também permite identificar facilmente pontos atípicos. Entretanto, há uma maneira ainda melhor de criar gráficos de dispersão usando a pandas.

Podemos criar um gráfico de dispersão diretamente usando o parâmetro `kind=` em `plot()`.

Esse parâmetro diz ao `plot()` qual tipo de gráfico ele deve criar. Neste caso, usamos o argumento `'scatter'` para criar um gráfico de dispersão:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

df.plot(x='height', y='weight', kind='scatter')
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.5.4.png)

Os pontos de dados são quase idênticos em ambos os gráficos. Os pontos no segundo gráfico são um pouco menores do que os no primeiro, mas você reparou algumas outras diferenças entre as duas maneiras de criar um gráfico de dispersão?

Se não especificarmos `kind='scatter'`, o eixo Y não é legendado por padrão e ainda ficamos com uma legenda de gráfico desnecessária. Ao criar um gráfico de dispersão explicitamente, ele já sai bem formatado com os eixos legendados.

Existe só mais uma coisa para resolver no nosso gráfico de dispersão. Com tantos pontos, muitos deles ficam sobrepostos, o que dificulta ter uma boa noção da densidade dos pontos no gráfico acima.

Entretanto, podemos lidar com esse problema usando o parâmetro `alpha=`. Esse parâmetro regula a transparência dos pontos e pode aceitar qualquer valor entre 0 (completamente transparente) e 1 (nada transparente). Por padrão, ele recebe o valor 1, indicando que não há transparência.

Use o botão deslizante interativo abaixo para ver como mudanças no parâmetro alpha afetam a aparência do gráfico de dispersão:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Widgets/Data/alpha_scatter/pt_br/alpha.html" style="min-width: 112%; min-height: 565px;"></iframe>

Conforme o valor de alpha diminui, os pontos ficam mais fracos, e assim é mais fácil ver onde está a maior densidade de pontos. A desvantagem é que fica mais difícil ver os pontos nas extremidades da distribuição. O melhor valor alpha a ser usado é uma questão de gosto e do que você quer enfatizar ao desenhar o gráfico.

### Recapitulação

Vamos recapitular o que aprendemos até agora:

-   `sort_values()`: ordena os dados pela coluna passada.
-   `style='o'`: cria um gráfico de dispersão marcando explicitamente cada ponto de dados com um círculo.
-   `kind='scatter'`: cria um gráfico de dispersão usando o método padrão `plot`.
-   `alpha=`: define a transparência dos pontos no gráfico no valor especificado.

Agora que você entende os motivos para a criação de gráficos de dispersão e os métodos usados para isso, é hora de praticar.

Gráficos de dispersão

Tarefa

Para tornar a análise mais interessante e fácil de entender, usaremos o mesmo conjunto de dados de antes, mas, desta vez, vamos analisar a relação entre `height` e `age`, em vez de `weight`. Construa um gráfico de dispersão para mostrar a relação entre essas duas variáveis.

Os dados da altura e peso dos adultos foram lidos na variável `df` no pré-código. Use o argumento `kind='scatter'` para criar o gráfico de dispersão desejado. Dê ao gráfico os seguintes argumentos:

1.  O título "Adult heights" (que significa "Altura de adultos", maiúsculas e minúsculas importam)
2.  O valor 0.36 para o alpha
3.  O tamanho 8 x 6 polegadas
4.  O eixo X legendado como: "Age / years" (Idade / anos)
5.  O eixo Y legendado como: "Height / inches" (Altura / polegadas)

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

import pandas as pd

from matplotlib import pyplot as plt

  

df \= pd.read\_csv('/datasets/height\_weight.csv')

  

df.plot(x\='age',

y\='height',

kind\='scatter',

alpha\=0.3,

figsize\=\[8, 6\],

title\='Adult heights',

xlabel\='Age / years',

ylabel\='Height / inches')

  

plt.show()

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-56-682Z.md
### Última modificação: 2025-05-28 19:07:57

# Correlação - TripleTen

Capítulo 6/11

Visualização de dados

# Correlação

Muitas vezes, o objetivo da análise de dados é mostrar a relação entre dois valores. Por exemplo, digamos que queremos determinar como o preço de um apartamento está relacionado ao tamanho dele. A distância do centro afeta o valor? Qual impacto tem o ano de construção ou o nível de ruído no bairro? Gráficos de dispersão nos ajudam a responder a essas perguntas nos fornecendo uma representação visual dessas relações.

A tendência da alteração de uma variável em relação à alteração de outra variável é chamada de **correlação**.

O gráfico de dispersão que fizemos na última lição mostra que altura e peso são **positivamente correlacionáveis.** Isso faz sentido porque o aumento em uma variável geralmente significa um aumento na outra. Um exemplo de uma **correlação negativa** seria altura da pessoa e tom de voz; normalmente, quanto mais alta a pessoa é, mais baixa é a frequência da sua voz.

## Cálculo do coeficiente de correlação

Uma coisa é olhar para um gráfico de dispersão, outra é ter uma forma numérica de descrever a correlação.

Para quantificar como uma variável tende a mudar quando a outra variável muda, usamos o coeficiente de correlação de Pearson (que também é frequentemente chamado de **coeficiente de correlação**). Esse coeficiente aceita qualquer valor entre -1 e 1.

Um valor de -1 representa uma perfeita correlação negativa, enquanto 1 representa uma perfeita correlação positiva. Em geral, o coeficiente de correlação funciona assim:

-   Se um valor aumenta e o outro também aumenta, o coeficiente de correlação é positivo.
-   Se um permanece o mesmo enquanto o outro muda, o coeficiente é 0.
-   Se um diminui enquanto o outro aumenta, o coeficiente é negativo.

Quanto mais perto o coeficiente estiver de -1 ou 1, mais forte será a correlação. Por outro lado, um valor 0 significa que não existe correlação ou que existe uma conexão complexa não linear que o coeficiente não consegue mostrar.

Para te ajudar a entender a relação entre o coeficiente de correlação e a distribuição de pontos em um gráfico de dispersão, incluímos um gráfico interativo. Alterne o coeficiente de correlação para observar como a propagação dos pontos de dados muda.

Perceba que existem muitos gráficos de dispersão diferentes que podem resultar no mesmo coeficiente de correlação; isso é apenas um exemplo.

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Widgets/Data/corellation/pt_br/corellation.html" style="min-width: 112%; min-height: 600px;"></iframe>

Combine cada gráfico de dispersão com o coeficiente de correlação que melhor descreve sua correlação:

1.  0.99
    
2.  0.5
    
3.  0
    
4.  \-0.5
    
5.  \-0.99
    

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_1_Sprint/5.5.png)

## Calculando o coeficiente de correlação

Na pandas, você pode calcular o coeficiente de correlação de Pearson usando o método `corr()`. Para fazer isso, aplique-o à coluna que contém a primeira variável e passe a coluna com a segunda variável como um parâmetro. A ordem das variáveis não importa. Por exemplo:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

print(df['height'].corr(df['weight']))
```

```
0.9165261045538688
```

Com um coeficiente em torno de 0.9, vemos que a altura e o peso têm uma forte correlação positiva nesse conjunto de dados. Isso corrobora o nosso "senso comum" de que pessoas altas tendem a pesar mais. É claro, existe variação nessa tendência, então não esperamos ter um coeficiente de correlação positiva perfeito de 1.

Pode ser tentador declarar que "a altura de uma pessoa determina seu peso". Mas a correlação por si só não diz nada sobre causa e efeito; sabemos apenas que os fatores estão correlacionados. Para provar (ou refutar) causa e efeito, temos que realizar experimentos controlados. Lembre-se de que correlação não implica causalidade.

Por enquanto, vamos praticar o cálculo de coeficientes de correlação para outros pares de variáveis em nosso conjunto de dados.

## Tarefas

### Tarefa 1

Lembra do gráfico de dispersão que você fez na última lição com as colunas `'height'` e `'age'`? Agora você vai calcular o coeficiente de correlação Pearson dessas colunas e atribuir o resultado à variável chamada `ah_corr`. Por fim, imprima. O resultado confirma o que foi visto no gráfico de dispersão?

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/height\_weight.csv')

  

ah\_corr \= df\['height'\].corr(df\['age'\])

  

print(ah\_corr)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Tente chamar o método `corr()` em todo o DataFrame. Para fazer isso, siga o mesmo procedimento de antes, mas não passe nenhuma variável ao DataFrame `df` ou ao método `corr()`. O que acontece? Imprima o resultado.

CódigoPYTHON

9

1

2

3

4

5

import pandas as pd

  

df \= pd.read\_csv('/datasets/height\_weight.csv')

  

print(df.corr())

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-58-560Z.md
### Última modificação: 2025-05-28 19:07:59

# Matrizes de dispersão - TripleTen

Teoria

# Matrizes de dispersão

## Construção de matrizes de dispersão

Na lição anterior, encontramos uma forte correlação entre altura e peso. Mas na vida real, as coisas raramente são tão simples. Se quisermos explorar fatores que contribuem com o peso das pessoas, há muito mais que deve ser considerado além da altura. Por exemplo, também seria necessário saber como a idade e o sexo se correlacionam com o peso ou como se correlacionam um com o outro.

Infelizmente, é impossível inserir coerentemente todos os quatro parâmetros no mesmo gráfico. No entanto, podemos construir gráficos de dispersão para cada par possível de parâmetros: altura e peso, altura e idade, peso e sexo e assim por diante. Esse conjunto de pares é chamado de **matriz de dispersão**.

Na pandas, podemos construir matrizes de dispersão usando a função `plotting.scatter_matrix()`:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

pd.plotting.scatter_matrix(df, figsize=(9, 9))
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.7.1.png)

Cada célula dessa grade 4x4 mostra a relação entre um par de colunas. Vamos dividi-la em partes; mostraremos cada gráfico abaixo da explicação, e você pode voltar para a matriz para encontrá-lo ali:

-   As células diagonais _não_ mostram relações entre variáveis. Em vez disso, elas são histogramas que exibem a distribuição de valores para cada variável individual. Por exemplo, considere a primeira imagem no canto superior esquerdo. Podemos ver nela que a maioria dos adultos tem entre 64 e 68 polegadas de altura (ou seja, entre 162 e 172 cm). Abordaremos os histogramas em mais detalhe em um dos próximos capítulos.

![](https://practicum-content.s3.amazonaws.com/resources/1_1689593424.png)

![](https://practicum-content.s3.amazonaws.com/resources/2_1689593449.png)

![](https://practicum-content.s3.amazonaws.com/resources/3_1689593472.png)

![](https://practicum-content.s3.amazonaws.com/resources/4_1689593493.png)

-   As células imediatamente abaixo e ao lado do histograma de altura mostram a conexão entre altura e peso. Ambos os gráficos de dispersão altura-peso contêm informações idênticas, mas os eixos estão invertidos. Já vimos esse gráfico de dispersão na última lição e determinamos que ele indica uma forte correlação positiva (um coeficiente de correlação de cerca de 0,9).

![](https://practicum-content.s3.amazonaws.com/resources/5_1_1689593642.png)

![](https://practicum-content.s3.amazonaws.com/resources/6_1_1689593645.png)

-   O gráfico de dispersão da relação idade-altura mostra uma ampla dispersão de pontos sem relação claramente discernível. Você já viu esse gráfico de dispersão e calculou o coeficiente de correlação (aproximadamente 0,01) nas lições anteriores.

![](https://practicum-content.s3.amazonaws.com/resources/7_1689593696.png)

![](https://practicum-content.s3.amazonaws.com/resources/8_1689593699.png)

-   O gráfico idade-peso também tem uma ampla dispersão de pesos para cada idade, mas parece haver uma ligeira correlação positiva se comparado com o gráfico de dispersão idade-altura.

![](https://practicum-content.s3.amazonaws.com/resources/9_1689593755.png)

![](https://practicum-content.s3.amazonaws.com/resources/10_1689593758.png)

-   A linha de baixo (e a última coluna) dos gráficos de dispersão mostram a relação entre sexo e outras variáveis, onde um valor 1 significa que a pessoa é do sexo masculino e 0 significa sexo feminino. Esses gráficos mostram correlações positivas nas relações sexo-altura e sexo-peso, mas não em sexo-idade.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_50_1689593776.png)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_51_1689593789.png)

Com uma única linha de código, nós criamos uma matriz de dispersão que nos permite fazer várias deduções sobre as relações entre variáveis no nosso conjunto de dados. Não seria ótimo se também pudéssemos quantificar todas essas relações?

## Matrizes de correlação

Em uma tarefa da última lição, você chamou o método `corr()` no DataFrame e obteve este resultado:

```
import pandas as pd

df = pd.read_csv('/datasets/height_weight.csv')

print(df.corr())
```

```
          height    weight       age      male
height  1.000000  0.916526  0.010042  0.760690
weight  0.916526  1.000000  0.228538  0.785218
age     0.010042  0.228538  1.000000  0.004750
male    0.760690  0.785218  0.004750  1.000000
```

A tabela resultante é chamada de **matriz de correlação**. Essa matriz contém os coeficientes de correlação para cada par de colunas numéricas no DataFrame.

Coeficientes na diagonal do canto superior esquerdo ao canto inferior direito são sempre iguais a 1, já que qualquer variável se correlaciona perfeitamente consigo mesma. Os elementos das diagonais acima (e abaixo) da diagonal principal contêm os coeficientes para todos os outros pares de colunas. Nesse caso, existem 6 coeficientes de correlação distintos para todos os possíveis pares das 4 colunas numéricas, exceto as diagonais.

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.7.14.png)

Construir uma matriz de correlação é uma maneira muito mais rápida de obter todos os coeficientes de correlação que chamar `corr()` para cada par de colunas. Neste caso, existem apenas 6 pares, mas imagine se tivéssemos dezenas de colunas numéricas!

Matrizes de dispersão

Tarefa

Obtenha os coeficientes de correlação da coluna `'male'` com as outras três colunas. Em vez de chamar `corr()` na coluna `male` três vezes, crie uma matriz de correlação e extraia os três coeficientes que você quer. O resultado deve ser um objeto Series com três elementos, um para cada coeficiente.

Atribua a matriz de correlação à variável `corr_mat` e atribua o Series de coeficientes à variável `male_corr`. Depois imprima `male_corr`.

Use `loc[]` com `'male'` como o primeiro argumento e uma lista dos outros dados como o segundo argumento para extrair esses valores na variável `male_corr`.

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/height\_weight.csv')

  

corr\_mat \= df.corr()

male\_corr \= corr\_mat.loc\['male', \['height', 'weight', 'age'\]\]

print(male\_corr, corr\_mat)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-07-59-851Z.md
### Última modificação: 2025-05-28 19:08:00

# Gráficos de linha - TripleTen

Capítulo 6/11

Visualização de dados

# Gráficos de linha

Lembra quando usamos o método `plot()` sem especificar um argumento para o parâmetro `kind=`? Isso gera um **gráfico de linha**, em que cada ponto dos dados é conectado sequencialmente por uma linha.

Parecia um pouco com isso:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

df.sort_values('height').plot(x='height', y='weight')
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.8.1.png)

No entanto, gráficos de linha não eram apropriados para esses dados, pois implicam uma relação consecutiva entre os pontos de dados.

Entretanto, eles são ótimos quando você tem dados que são conectados cronologicamente e cada ponto de dados tem alguma ligação com o ponto anterior. Dados de temperaturas, de tráfego e da bolsa de valores são bons candidatos para gráficos de linha.

Nesta lição, vamos trabalhar com dados da bolsa de valores da Starbucks (SBUX) de 2015 até 2019. Tiramos esse conjunto de dados do [Yahoo Finance](https://finance.yahoo.com/quote/SBUX/history?p=SBUX) _(os materiais estão em inglês)_ e os modificamos para conter apenas um subconjunto das colunas disponíveis.

Vamos dar uma olhada nos dados:

```
import pandas as pd

df = pd.read_csv('/datasets/sbux.csv')

print(df.head())
print()
df.info()
```

```
         date       open      close    volume
0  2015-01-02  41.064999  40.720001   6886000
1  2015-01-05  40.070000  39.939999  11623800
2  2015-01-06  40.169998  39.615002   7664400
3  2015-01-07  39.875000  40.590000   9732600
4  2015-01-08  41.165001  41.244999  13170600

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1258 entries, 0 to 1257
Data columns (total 4 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   date    1258 non-null   object 
 1   open    1258 non-null   float64
 2   close   1258 non-null   float64
 3   volume  1258 non-null   int64  
dtypes: float64(2), int64(1), object(1)
memory usage: 39.4+ KB
```

Existem quatro colunas no conjunto de dados:

-   `'date'`: a data em que os dados foram registrados
-   `'open'`: o preço de uma ação em dólar americano (USD) no momento da abertura do mercado, ou seja, o primeiro preço do dia
-   `'close'`: o preço de uma ação em USD no momento do encerramento do mercado, ou seja, o último preço do dia
-   `'volume'`: o número total de ações negociadas

Por exemplo, a primeira linha nos diz que em 2 de janeiro de 2015, o preço de abertura das ações da Starbucks foi $41,06, o preço de fechamento foi $40,72 e 6.866,000 ações foram negociadas nesse dia. Cada linha representa um dia de negociação quando o mercado esteve aberto, excluindo finais de semana e alguns feriados.

A partir do resultado de `info()`, podemos ver também que `'date'` tem o tipo de dados string, as outras colunas são numéricas e não existe nenhum valor ausente. Para o nosso propósito aqui, não há problema em representar as datas como strings, mas mais tarde no sprint, você vai a prender sobre tipos de dados especiais para datas e horas que facilitam extrair informações e realizar cálculos com elas.

Para dados temporais como esses, é razoável presumir que o preço em um dia tem alguma relação com o preço do dia anterior. Então vamos construir um gráfico de linhas para visualizar o preço de abertura das ações da Starbucks durante a vida útil do conjunto de dados:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/sbux.csv')

df.plot(x='date', y='open')
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.8.2.png)

Tudo o que tivemos que fazer para construir um gráfico de linhas foi chamar `plot()` no DataFrame sem o argumento `kind=` e especificar quais colunas queríamos desenhar nos eixos X e Y, assim como já fizemos com outros gráficos. Você não precisa especificar o argumento `kind=` como `"line"` porque esse já é o valor padrão.

Agora temos uma ótima representação do preço histórico de abertura das ações da Starbucks. Parece que o preço subiu rapidamente na parte final do gráfico. Por volta de qual data isso ocorreu? É difícil dizer a partir desse gráfico porque as legendas das marcas do eixo X estão sobrepostas umas às outras.

Além de corrigir as marcas das legendas sobrepostas, quais outras melhorias poderíamos fazer nesse gráfico?

Pergunta

O que mais você faria para tornar o gráfico acima mais profissional e comunicativo? Selecione todas as opções que se aplicam:

Escolha quantas quiser

Incluir um título como "Money vs. time" (Dinheiro vs. tempo)

Formatar a legenda do gráfico

Adicionar uma legenda ao eixo Y

**Sem ela, ficaria difícil entender o assunto do gráfico.**

Incluir um título no gráfico, por exemplo, "Starbucks market open" (Starbucks na abertura do mercado)

**Um título torna o gráfico mais bonito e fácil de entender.**

Remover a legenda do gráfico

**Não devemos adicionar coisas só por adicionar. Nesse caso, legendas são inúteis.**

Especificar a unidade do eixo Y como "Money" (dinheiro)

Especificar a unidade do eixo Y como "USD”

Isso nos permite garantir que todos saibam o que estamos mostrando.

Você conseguiu!

Agora vamos usar as nossas habilidades para melhorar o gráfico de acordo com essas respostas.

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/sbux.csv')

df.plot(x='date',
        y='open',
        legend=False,
        title='Starbucks market open',
        xlabel='Date',
        ylabel='Share price / USD',
        rot=45)

plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.8.3.png)

Você já conhece todos os parâmetros usados nesse gráfico, exceto o último, `rot=`. Esse argumento aplica uma rotação nas legendas das marcas do eixo X, em graus. Ele é usado para que as legendas não se sobreponham umas às outras, e agora elas estão presentes em um ângulo de 45% do eixo X.

Agora é sua vez de criar alguns gráficos de linha profissionais.

## Tarefas

### Tarefa 1

Crie um gráfico de linha para o volume de negociações do conjunto de dados das ações da Starbucks. Faça seu gráfico cumprir o seguinte:

1.  Título "Historic SBUX volume" (Volume Histórico das SBUX). Maiúsculas e minúsculas são importantes
2.  Legenda do eixo X: "Date"
3.  Legenda do eixo Y: "Volume"
4.  Marcas das legendas do eixo X rotacionadas em 50 graus
5.  Limite do eixo Y de 1 milhão a 70 milhões (você pode usar `1e6` e `7e7` como os limites inferior e superior respectivamente para evitar os zeros. 1,000,000 = 1e6, ou seja, 1 elevado a sexta potência)
6.  Sem legenda do gráfico

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

import pandas as pd

from matplotlib import pyplot as plt

  

df \= pd.read\_csv('/datasets/sbux.csv')

  

df.plot(x\='date',

y\='volume',

title\='Historic SBUX volume',

xlabel\='Date',

ylabel\='Volume',

ylim\=\[1e6, 7e7\],

legend\=False,

rot\=50)

  

plt.show()

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Crie um gráfico de linha que inclui o preço de abertura e fechamento. Para fazer isso, você pode passar a lista de nomes de colunas, `cols`, fornecida no pré-código como argumento de `y=`. Já que você tem duas variáveis diferentes no mesmo gráfico, certifique-se de incluir a legenda dessa vez. Faça seu gráfico também cumprir o seguinte:

1.  Título "Historic SBUX price" (Histórico de preços SBUX). Maiúsculas e minúsculas são importantes
2.  Legenda do eixo X: "Date"
3.  Legenda do eixo Y: "Share price / USD" (Preço da ação / USD)
4.  Marcas das legendas do eixo X rotacionadas em 50 graus

Não se esqueça de incluir `plt.show()`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

import pandas as pd

from matplotlib import pyplot as plt

  

df \= pd.read\_csv('/datasets/sbux.csv')

cols \= \['open', 'close'\]

  

df.plot(x\='date',

y\=cols,

title\='Historic SBUX price',

xlabel\='Date',

ylabel\='Share price / USD',

rot\=50)

  

plt.show()

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-08-01-182Z.md
### Última modificação: 2025-05-28 19:08:01

# Gráficos de barras - TripleTen

Teoria

# Gráficos de barras

Até agora aprendemos como visualizar relações entre variáveis numéricas em nossos dados, mas e as variáveis categóricas? A maioria dos conjuntos de dados com os quais você vai trabalhar tem uma mistura de dados numéricos e categóricos.

Gráficos de barra são uma ótima ferramenta para comparar categorias definidas por um valor numérico.

Nesta lição, criaremos gráficos de barras para visualizar dados do [Departamento do Censo dos Estados Unidos](https://www.census.gov/) _(os materiais estão em inglês)_ sobre o histórico da população na Califórnia, no Oregon e em Washington. O gráfico de barras é uma ótima opção aqui porque nos permite comparar propriedades numéricas (população) entre as categorias (estados).

Vamos começar imprimindo informações gerais sobre o conjunto de dados:

```
import pandas as pd

df = pd.read_csv('/datasets/west_coast_pop.csv')
df.info()
```

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 10 entries, 0 to 9
Data columns (total 4 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   year    10 non-null     int64  
 1   ca_pop  10 non-null     float64
 2   or_pop  10 non-null     float64
 3   wa_pop  10 non-null     float64
dtypes: float64(3), int64(1)
memory usage: 448.0 bytes
```

Apenas 10 linhas! Bem, então vamos imprimir o DataFrame inteiro:

```
import pandas as pd

df = pd.read_csv('/datasets/west_coast_pop.csv')
print(df)
```

```
   year  ca_pop  or_pop  wa_pop
0  1920   3.554   0.788   1.373
1  1930   5.711   0.956   1.568
2  1940   6.950   1.086   1.740
3  1950  10.680   1.532   2.387
4  1960  15.870   1.772   2.855
5  1970  19.970   2.092   3.413
6  1980  24.290   2.668   4.236
7  1990  29.950   2.859   4.901
8  2000  33.630   3.342   5.811
9  2010  37.270   3.856   6.746
```

Temos dados da Califórnia, do Oregon e de Washington tirados a cada 10 anos, de 1920 a 2010, em unidades de milhões de pessoas.

Vamos comparar a população de cada estado por ano. Para visualizar esses dados em um gráfico de barras, chamamos o método `plot()` no DataFrame e especificamos `kind='bar'` como argumento. Mas precisamos especificar que queremos a coluna `'year'` no eixo X, caso contrário o gráfico não fará muito sentido:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/west_coast_pop.csv')

df.plot(x='year', kind='bar')
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.9.1.png)

Bem como o esperado: um conjunto de três barras, uma para cada estado, para cada ano no conjunto de dados.

Se você não especificar nenhuma coluna para o parâmetro `y=`, a pandas vai criar automaticamente uma barra para cada coluna no DataFrame que não está no eixo X; nesse caso, uma barra para a população de cada estado.

O gráfico de barras facilita tirar algumas conclusões sobre os dados. Por exemplo, podemos ver que a população da Califórnia é consistentemente muito maior do que a dos outros estados. Também vemos que a população cresceu constantemente em todos os estados.

No entanto, se quisermos apresentar esses dados para outras pessoas, precisamos deixá-los mais profissionais:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/west_coast_pop.csv')

df.plot(x='year',
        kind='bar',
        title='West coast USA population growth',
        xlabel='Year',
        ylabel='Population (millions)')

plt.legend(['CA', 'OR', 'WA'])
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/6.9.2.png)

Muito melhor!

Para melhorar o nosso gráfico, usamos uma nova função do Matplotlib chamada `legend()`. Essa função nos permite especificar manualmente as marcas das legendas passando uma lista de nomes, em vez do comportamento padrão de usar os nomes das colunas do DataFrame.

O público para quem vamos apresentar esse gráfico provavelmente não está familiarizado com os detalhes técnicos do nosso conjunto de dados, então nomes como `'ca_pop'` talvez não façam sentido. É muito melhor usar legendas com as siglas comuns dos estados: CA, OR e WA.

Perceba que a ordem da legenda na lista vai corresponder à ordem das colunas no DataFrame, então é importante ordenar as legendas corretamente. Além disso, `plt.legend()` precisa vir depois de chamarmos `plot()` no DataFrame.

Agora é hora de você explorar esse conjunto de dados nas tarefas.

Gráficos de barras

Tarefa

A população da Califórnia é tão maior do que a do Oregon e de Washington que é difícil ter uma noção dos dados para esses dois estados a partir do gráfico que fizemos. Crie um gráfico de barras que mostre apenas as populações do Oregon e de Washington para cada ano no conjunto de dados. Faça isso chamando `plot()` em `df` com argumentos que deem ao seu gráfico as seguintes propriedades:

1.  Inclui apenas os dados para Oregon e Washington especificando o eixo Y, como fizemos na lição anterior
2.  Tem o título "Pacific Northwest population growth" (Crescimento populacional do Noroeste Pacífico). É importante respeitar maiúsculas e minúsculas
3.  Eixo X legendado como "Year" (ano)
4.  Eixo Y legendado como "Population (millions)" (População (milhões))
5.  Legenda do gráfico com marcas "OR" e "WA" para as populações do Oregon e de Washington, respectivamente

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

import pandas as pd

from matplotlib import pyplot as plt

  

df \= pd.read\_csv('/datasets/west\_coast\_pop.csv')

  

df.plot(x\='year',

y\=\['or\_pop', 'wa\_pop'\],

kind\='bar',

title\='Pacific Northwest population growth',

xlabel\='Year',

ylabel\='Population (millions)',

legend\=\['OR', 'WA'\])

  

plt.legend(\['OR', 'WA'\])

plt.show()

  

plt.plot(\[1, 2, 3\], \[4, 5, 6\], label\='Linha 1')

plt.plot(\[1, 2, 3\], \[6, 5, 4\], label\='Linha 2')

plt.legend(\["Descendo", "Subindo"\]) \# Adiciona a legenda

  

plt.show()

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-08-03-398Z.md
### Última modificação: 2025-05-28 19:08:03

# Histogramas - TripleTen

Teoria

# Histogramas

Agora que já estudamos como os gráficos de dispersão, de linha e de barras nos ajudam a entender e analisar as relações _entre_ variáveis nos nossos dados, vamos aprender como construir uma distribuição para uma _única_ variável.

É hora de conhecer os histogramas.

## Histogramas

Um histograma é um gráfico que mostra a frequência em que diferentes valores aparecem em uma variável no conjunto de dados. Embora ele possa se parecer com gráficos de barras, existem algumas diferenças cruciais entre os dois:

-   Gráficos de barras são usados para comparar valores de variáveis discretas, e histogramas são usados para construir distribuições de variáveis _numéricas contínuas_.
-   A ordem das barras no gráfico de barras pode ser modificada por estilo ou objetivo de comunicação. Já a ordem das barras nos histogramas não pode ser modificada.

Em um histograma, o eixo X representa a variável e seu intervalo de valores. O eixo Y representa a frequência de ocorrência de cada valor. E é por isso que não faz sentido alterar a ordem das barras.

Aqui está um exemplo:

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/9_10_1.png)

Esse histograma representa uma variável que assume valores no intervalo de aproximadamente 1 a 9, sendo os valores em torno de 4 e 7 os mais frequentes.

Como a variável pode assumir qualquer valor nesse intervalo, contar cada valor unívoco e traçar as frequências não é útil. Em vez disso, um histograma divide o intervalo de valores em seções chamadas de **barras**. No gráfico acima, uma das barras está destacada para ilustrar os tamanhos das barras.

Todas as barras têm a mesma largura, e a altura de uma barra corresponde ao número de observações que caem no intervalo dela. Portanto, a aparência do histograma depende do número de barras que decidirmos que ele deve ter.

O histograma acima tem 25 barras. Vamos ver o que acontece se criarmos um gráfico com os mesmos dados usando apenas 10 barras:

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/9_10_2.png)

E 100 barras:

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/9_10_3.png)

O gráfico com 10 barras não tem detalhes suficientes, e o com 100 barras tem detalhes demais. Escolher o número correto de barras pode ser [mais arte do que ciência](https://pt.wikipedia.org/wiki/Histograma), mas com prática você pega o jeito.

Se você quer ter uma noção de como o tamanho afeta um histograma, use esta ferramenta interativa para mudar o número de barras e ver o que acontece. Experimente!

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Widgets/Data/histogramm02/pt_br/histogramm02.html" style="min-width: 112%; min-height: 800px;"></iframe>

## Construção de histogramas com DataFrames

Na pandas, há duas maneiras de construir histogramas:

1.  Método `hist()`
2.  Método `plot()` com o argumento `kind='hist'`

Vamos examinar o `hist()` primeiro:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')
df.hist()

plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/9.104.png)

Se `hist()` for chamado em um DataFrame sem nenhum argumento, ele cria um gráfico separado para cada coluna numérica. Esses histogramas parecem familiares? Eles eram os gráficos na diagonal das nossas matrizes de dispersão.

Agora vamos chamar `plot()` no DataFrame para fazer histogramas:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

df.plot(kind='hist')

plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/9.10.5.png)

O comportamento padrão de `plot()` difere significativamente do de `hist()`.

Podemos usar `plot()` para desenhar todos os histogramas no mesmo gráfico. No entanto, não faz sentido fazer isso para esses dados, já que cada uma das quatro colunas representa uma medida física diferente. Então, não é apropriado traçá-las no mesmo eixo.

## Construção de histogramas com colunas

Frequentemente você vai querer explorar uma variável por vez. Podemos construir um histograma com apenas uma coluna usando `hist()` e passando o nome da coluna como argumento no parâmetro `column=`. Vamos dar uma olhada mais detalhada na coluna `'height'`:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

df.hist(column='height')

plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/9.10.6.png)

Como alternativa, podemos traçar o mesmo histograma chamando `hist()` apenas na coluna `'height'`:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

df['height'].hist()

plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/9.10.7.png)

Por padrão, `hist()` usa 10 barras. Entretanto, para esses dados, 10 barras parecem ser insuficientes para capturar o nível de precisão que queremos. Para lidar com isso, podemos usar o parâmetro `bins=` e ajustar o número de barras usadas:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

df.hist(column='height', bins=30)

plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/9.10.8.png)

Muito melhor!

Podemos fazer o mesmo histograma usando `plot()` e o chamando apenas na coluna `'height'`:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

df['height'].plot(kind='hist', bins=30)

plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/9.10.9.png)

O histograma é o mesmo, mas o formato padrão dos gráficos difere entre os métodos. Para analisar um histograma de apenas uma variável, recomendamos usar `plot()`, porque `hist()` não suporta todos os parâmetros de formatação que você aprendeu, como `title=`, `xlabel=` e `ylabel=`.

## Comparação de histogramas de subconjuntos de dados

O histograma da altura acima mostra dois picos de valores mais frequentes. Isso pode ser devido a diferenças na distribuição da altura de homens e mulheres. Podemos investigar essa questão construindo histogramas de altura separados para homens e mulheres no mesmo gráfico.

Tudo o que precisamos fazer é filtrar os dados por sexo e chamar `plot()` na coluna `'height'`. Se fizermos isso duas vezes (uma vez para cada sexo), a pandas vai construir automaticamente os dois no mesmo gráfico, com o segundo histograma à frente do primeiro:

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

# isso imprime o gráfico do sexo masculino
df[df['male'] == 1]['height'].plot(kind='hist', bins=30)

# isso imprime o gráfico do sexo feminino
# incluindo um valor alpha para podermos ver ambos os histogramas
df[df['male'] == 0]['height'].plot(kind='hist', bins=30, alpha=0.8)

plt.legend(['Male', 'Female']) # legenda, seguindo a mesma ordem usada no gráfico anterior
plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/9.10.11.png)

Colocar ambos os histogramas no mesmo gráfico permite ver que a distribuição das alturas dos homens é deslocada para a direita se comparada com a das mulheres. Por exemplo, um homem típico é provavelmente mais alto do que uma mulher típica. E é por isso que o histograma combinado do conjunto de dados tem dois picos.

Observe que, nesse caso, _faz sentido_ construir ambos os histogramas no mesmo gráfico porque ambos representam a mesma variável: altura. Em outros casos, isso talvez não seja útil, e você precisa prestar atenção nisso.

Se precisar, você pode usar os índices para dividir o histograma, usando a mesma lógica usada para criar o filtro da variável 'male'. Para isso, ordene os dados da variável e redefina os índices para que os dados das mulheres sejam os primeiros no DataFrame. Em seguida, corte o DataFrame pela metade para dar uma olhada na metade inferior dos dados.

```
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv('/datasets/height_weight.csv')

df = df.sort_values('male').reset_index(drop=True)

df = df[df.index < 5000]

df.hist(column='height', bins=50)

plt.show()
```

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/9.10.12.png)

## Recapitulação

Histogramas nos dão uma ideia de como nossos dados são distribuídos. Podemos visualizar a distribuição para ter noções cruciais dos dados, como, por exemplo, os valores mais frequentes e a presença de valores atípicos.

-   Podemos usar os métodos `hist()` ou `plot()` para fazer histogramas das nossas colunas numéricas.
-   Chamar `hist()` em um DataFrame cria um histograma separado para cada coluna numérica.
-   Chamar `plot()` com `kind='hist'`cria um histograma para cada coluna numérica no mesmo gráfico.
-   Chamar qualquer método várias vezes também cria cada novo histograma no mesmo gráfico. Isso é útil para comparar distribuições de diferentes subconjuntos de dados, como as alturas de homens e mulheres.

Agora é a sua vez de criar histogramas.

Histogramas

Tarefa2 / 2

1.

Investigue a distribuição de peso em diferentes faixas etárias. Faremos isso em duas etapas. Primeiro divida os conjunto de dados em três DataFrames, filtrando `df` e os atribuindo às seguintes variáveis:

1.  `df_20s`: apenas as linhas em que `'age'` for menor que 30
2.  `df_30s`: apenas as linhas em que `'age'` for maior ou igual a 30 _e_ menor que 40
3.  `df_40s`: apenas as linhas em que `'age'` for maior ou igual a 40 (incluindo a idade 40 anos)

Para verificar se você filtrou corretamente, imprima os seguintes resultados:

1.  A soma dos comprimentos dos três DataFrames (deve haver 10.000 linhas no total)
2.  Os valores mínimo e máximo na coluna `'age'` de `df_20s`
3.  Os valores mínimo e máximo na coluna `'age'` de `df_30s`
4.  Os valores mínimo e máximo na coluna `'age'` de `df_40s`

O pré-código já contém um modelo para você imprimir os resultados; basta finalizar o código.

2.

Na segunda etapa, crie histogramas para cada faixa etária, todos no mesmo gráfico. Para isso, faça o seguinte:

-   Chame `plot()` na coluna `'weight'` de `df_20s`
    -   Defina o número de barras como 20
    -   Dê o nome “Weight / lbs” (Peso / libras) ao gráfico
    -   Legende o eixo Y como "Frequency" (Frequência)
-   Chame `plot()` na coluna `'weight'` de `df_30s`
    -   Defina o número de barras como 20
    -   Defina o valor alfa como 0.6
-   Chame `plot()` na coluna `'weight'` de `df_40s`
    -   Defina o número de barras como 20
    -   Defina o valor alfa como 0.3

Por fim, use a função `legend()` de `matplotlib` para legendar cada histograma com "20s", "30s", e "40s", respectivamente.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

import pandas as pd

from matplotlib import pyplot as plt

  

df \= pd.read\_csv('/datasets/height\_weight.csv')

  

df\_20s \= df\[df\['age'\] < 30\]

df\_30s \= df\[(df\['age'\] \>= 30) & (df\['age'\] < 40)\]

df\_40s \= df\[df\['age'\] \>= 40\]

  

df\_20s\['weight'\].plot(kind\="hist", bins\=20, title\= 'Weight / lbs',ylabel\="Frequency")

  

  

  

df\_30s\['weight'\].plot( kind\="hist", bins\=20, alpha\=0.6)

df\_40s\['weight'\].plot( kind\="hist", bins\=20, alpha\=0.3)

  

  

plt.legend(\["20s", "30s","40s"\])

  

plt.show()

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-08-04-746Z.md
### Última modificação: 2025-05-28 19:08:05

# Quiz do capítulo - TripleTen

Capítulo 6/11

Visualização de dados

# Quiz do capítulo

Pergunta

Quais são as principais razões para criar gráficos?

Escolha quantas quiser

Para analisar dados.

Ao visualizar os dados, podemos obter instantaneamente um entendimento melhor da situação, o que seria impossível ao apenas ler os números e dados brutos.

Para se comunicar com outras pessoas.

Visualização é uma forma eficaz de comunicar os resultados da análise. Isso é um fato especialmente quando seu público não compartilha o mesmo nível de conhecimento técnico que você, ou seja, partes interessadas e tomadores de decisão.

Para apresentar associações complexas.

As visualizações podem ser muito eficazes para revelar associações obscuras entre as variáveis nos dados.

Para tirar conclusões estatisticamente significativas.

Excelente!

Lembra do nosso conjunto de dados sobre videogames? Vamos tentar entender qual foi o gênero mais vendido da Nintendo no Japão. Crie um gráfico de barras da soma de `jp_sales` por gênero da distribuidora Nintendo ao longo dos anos. Filtramos os dados para você.

Faça seu gráfico cumprir o seguinte:

1.  Ele deve ter o título "Nintendo sales in Japan by genre" (Vendas da Nintendo no Japão por gênero). Maiúsculas e minúsculas são importantes
2.  A legenda do eixo X deve ser "Genre"
3.  A legenda do eixo Y deve ser "Japan Sales"
4.  As marcas das legendas do eixo X devem estar rotacionadas em 45 graus
5.  Sem legenda de gráfico

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

import pandas as pd

from matplotlib import pyplot as plt

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

  

df\_nintendo \= df\[df\['publisher'\] \== 'Nintendo'\].groupby('genre')\['jp\_sales'\].sum()

  

df\_nintendo.plot(kind\='bar', x\='genre',

y\='jp\_sales',

legend\=False,

title\='Nintendo sales in Japan by genre',

xlabel\='Genre',

ylabel\='Japan Sales',

rot\=45)

  

plt.show()

Dica

Mostrar a soluçãoValidar

Pergunta

Por que as visualizações ruins devem ser evitadas?

Não devem, uma visualização ruim é melhor do que nenhuma visualização.

Elas podem ser enganosas.

Visualizações ruins podem levar a mais perguntas do que respostas. Elas podem facilmente induzir as pessoas a tomar decisões erradas devido à falta de detalhes ou a uma mudança de foco. Isso seria altamente inadequado já que uma comunicação boa e ética envolve transparência. Como profissional de dados, é seu dever evitar erros na visualização e ser capaz de reconhecê-los para evitar que outras pessoas te enganem.

A visualização é uma arte, cada gráfico deve ser perfeito.

Você conseguiu!

Pergunta

Você chama a função `plot()` para um objeto DataFrame. Que tipo de gráfico é exibido?

Gráfico de barras.

Gráfico de linha.

`plot()` cria um gráfico de linha para cada uma das colunas com dados numéricos por padrão.

Gráfico de dispersão.

Depende do tipo de variável. Ela cria gráficos de linha para variáveis numéricas e gráficos de barras para variáveis categóricas.

Fantástico!

Para entender melhor a história do videogame, precisamos dar uma olhada nos dados do período quando os primeiros jogos foram lançados. Analisar os dados dessas datas nos permite ver como a indústria mudou ao longo do tempo e identificar tendências importantes no design e desenvolvimento de jogos. Vamos continuar explorando o tópico de datas de lançamento de jogos com isso em mente.

-   Crie um histograma da variável `year_of_release` usando o método `hist()`.
-   Legende o eixo x como "Year"; para isso, use a função `xlabel()` do **pyplot**.
-   Legende o eixo y como "Quantity of games" (quantidade de jogos); para isso, use a função `ylabel()` do **pyplot**.
-   Adicione um título ao gráfico: "Number of games for each year" (número de jogos por ano); para isso, use a função `title()`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

import pandas as pd

from matplotlib import pyplot as plt

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

  

df.hist(column\='year\_of\_release')

plt.xlabel('Year')

plt.ylabel('Quantity of games')

plt.title('Number of games for each year')

  

plt.show()

Dica

Mostrar a soluçãoValidar

Pergunta

Você calculou o coeficiente de correlação de Pearson entre `age` e `income` como 0,82. Isso sugere que há uma correlação forte?

Não, consideramos apenas coeficientes de correlação maiores ou iguais a 0,9 como fortes.

Sim, tratamos esse coeficiente de correlação como praticamente significativo.

Nesse caso, o coeficiente de correlação parece forte o suficiente, pois permite explicar uma boa parcela da renda com a idade.

Não, um coeficiente de correlação de Pearson abaixo de 80 não é forte.

Seu entendimento sobre o material é impressionante!

Pergunta

Você deseja analisar rapidamente a correlação entre todas as variáveis em um conjunto de dados. Quais funções você usa?

`scatter_matrix()`

`corr()`

`scatter_matrix()` e `corr()`

`scatter_matrix()` permite verificar visualmente quaisquer correlações complexas, e `corr()` permite obter estimativas objetivas de correlações.

Fantástico!

Pergunta

Você quer ver o crescimento populacional em vários estados ao longo de vários anos. Que tipo de gráfico você usa?

Escolha quantas quiser

Gráfico de linha.

Os gráficos de linha conectam sequencialmente cada ponto de dados com uma linha. Essas conexões implicam que há uma relação entre pontos de dados consecutivos. Colocando os anos no eixo x (em ordem crescente) e o valor da população no eixo y, podemos criar um gráfico de linha para cada estado.

Gráfico de barras.

A versão agrupada de gráficos de barras serviria. Colocando os anos no eixo x (em ordem crescente) e o valor da população no eixo y, podemos criar um gráfico de barras agrupado onde cada grupo tem a população de cada estado por ano como uma barra separada.

Histograma.

Gráfico de dispersão.

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-08-06-043Z.md
### Última modificação: 2025-05-28 19:08:06

# Conclusão - TripleTen

Capítulo 6/11

Visualização de dados

# Conclusão

Uau! Mais um passo em direção ao seu sonho. Agora que você é mestre da visualização, vamos recapitular o que você aprendeu neste capítulo.

Pergunta

Agora você é capaz de:

Escolha quantas quiser

Aplicar as melhores práticas de visualização a seus gráficos

Se ainda não tiver certeza, você pode revisar as lições em [Visualização de dados: visão geral](https://tripleten.com/trainer/data-analyst/lesson/ed6b28b8-8fc0-435a-b887-c27334a669c0/) e [O que podemos aprender com visualizações ruins](https://tripleten.com/trainer/data-analyst/lesson/7ae85381-222f-476d-8318-05ee1028e938/)

Criar gráficos de dispersão para mostrar relações entre variáveis numéricas e visualizar correlações

Se ainda não tiver certeza, você pode revisar as lições em [Gráficos de dispersão](https://tripleten.com/trainer/data-analyst/lesson/d06bdbe0-1d5c-419a-898e-f0968efd723b/task/f7da1e05-7de0-4d52-a26d-74db9233f4c4/)

Calcular o coeficiente de correlação usando o método `corr()`

Se ainda não tiver certeza, você pode revisar as lições em [Correlação](https://tripleten.com/trainer/data-analyst/lesson/62bdfa33-eb02-44d1-bf7c-02254b6a63f1/)

Construir matrizes de dispersão para mostrar relações entre todos os pares de variáveis numéricas nos dados

Se ainda não tiver certeza, você pode revisar as lições em [Matrizes de dispersão](https://tripleten.com/trainer/data-analyst/lesson/f39d0fdd-9d2f-4694-9599-9eaf90549425/task/937ed703-dc95-4bc9-9c6e-6f4f0a2fa5f2/)

Criar gráficos de linha para mostrar relações entre variáveis numéricas com dados temporais

Se ainda não tiver certeza, você pode revisar as lições em [Gráficos de linha](https://tripleten.com/trainer/data-analyst/lesson/c5e763da-13b4-428d-9756-167cee2d4c13/)

Criar gráficos de barras para comparar quantidades entre categorias ou visualizar distribuições de variáveis discretas

Se ainda não tiver certeza, você pode revisar as lições em [Gráficos de barras](https://tripleten.com/trainer/data-analyst/lesson/b9f74d8a-d34c-4893-ba08-48e90ea56206/task/af936d4d-3c2e-4a77-96c2-78bf86687781/)

Desenvolver histogramas para visualizar distribuições de variáveis numéricas contínuas

Se ainda não tiver certeza, você pode revisar as lições em [Histogramas](https://tripleten.com/trainer/data-analyst/lesson/2bc8826e-baff-4e72-99ab-512ce9d8057c/task/e1e335f1-e5e9-4e63-9b3e-c66897d3effd/)

Excelente!

### 🎉

Você aprendeu sobre os principais gráficos — de dispersão, de linha, de barras e histogramas — e também aprendeu como eles podem ser usados para resumir efetivamente a distribuição de dados. Existem muito mais tipos de visualização de dados por aí para você aprender e explorar durante sua carreira. Confira este [vídeo sobre a Arte da Visualização de Dados](https://www.youtube.com/watch?v=AdSZJzb-aX8) _(os materiais estão em inglês)_, que mostra visualizações históricas e técnicas de ponta.

### Recursos extras _(os materiais estão em inglês)_

Reunimos também alguns recursos extras para que você possa continuar a explorar o mundo da visualização dos dados.

Para expandir o conhecimento deste capítulo, confira o livro [Fundamentals of Data Visualization](https://clauswilke.com/dataviz/) (Fundamentos da visualização de dados), de Claus O. Wilke. Você também pode ver [The Data Visualisation Catalogue](https://datavizcatalogue.com/index.html) (O catálogo de visualização de dados) para aprender muitos tipos de visualização de dados.

Se quiser algumas dicas sobre como evitar visualizações ruins, confira [Numbers Shouldn't Lie – An Overview of Common Data Visualization Mistakes](https://www.toptal.com/designers/ux/data-visualization-mistakes) e [The Do’s and Don’ts of Chart Making](https://visme.co/blog/dos-and-donts-chart-making/) (Números não deveriam mentir — uma visão geral sobre erros comuns na visualização de dados e Os certos e errados ao criar um gráfico). Ambos são ótimos artigos com muitas dicas válidas.

Se quiser um guia de estilo geral da Matplotlib, veja [este ótimo recurso](https://jakevdp.github.io/PythonDataScienceHandbook/04.11-settings-and-stylesheets.html). Você também pode ler mais sobre as capacidades da pandas de criar gráficos [aqui](https://pandas.pydata.org/docs/user_guide/visualization.html) e verificar uma lista completa de todos os argumentos válidos de `plot=` [aqui](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html).

Se você quiser rir com alguns exemplos engraçados de "correlação não é causalidade", confira [Correlações Espúrias](http://www.tylervigen.com/spurious-correlations).

Há também dois vídeos de que você talvez goste:

[Six Meta-Rules of Data Visualization](https://www.youtube.com/watch?v=rH--5tes6x8) (Seis meta-regras da visualização de dados).

[Shut up about the y-axis. It shouldn’t always start at zero.](https://www.youtube.com/watch?v=14VYnFhBKcY) (Chega de falar sobre o eixo y. Ele nem sempre precisa começar do zero).

### Leve isso com você

Faça download do sumário do capítulo para que você possa consultá-los quando necessário.

-   [Resumo do capítulo: Visualização de dados](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/PT/6_Resumo_do_captulo_Visualizao_de_dados.pdf)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-05-696Z.md
### Última modificação: 2025-05-28 19:10:06

# Introdução - TripleTen

Capítulo 7/11

Tipos de dados

# Introdução

Existem diferentes tipos de dados, e alguns podem ser mais difíceis de analisar do que outros.

Neste capítulo, você vai aprender a:

-   Identificar quais os tipos de dados do conjunto;
-   Converter dados de um tipo para outro;
-   Corrigir tipos de dados que foram identificados incorretamente;
-   Trabalhar com datas e horas.

![EDA-6-1-01.jpg](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/EDA-6-1-01.jpg)

Serão necessárias de 2 a 3 horas para concluir este capítulo. Junte-se a nós nesta jornada!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-07-052Z.md
### Última modificação: 2025-05-28 19:10:07

# Conjunto de dados à primeira vista - TripleTen

Capítulo 7/11

Tipos de dados

# Conjunto de dados à primeira vista

Para começar, vamos trabalhar com uma versão reduzida do [Conjunto de dados de varejo online](https://archive.ics.uci.edu/ml/datasets/Online+Retail) _(os materiais estão em inglês)_ do Repositório de aprendizado de máquina da Universidade da Califórnia em Irvine:

> Esse é um conjunto de dados internacionais que contém todas as transações ocorridas entre 01/12/2010 e 09/12/2011 em um varejo online sem loja física registrado no Reino Unido. A empresa vende principalmente presentes exclusivos para todas as ocasiões, e muitos dos clientes dela são atacadistas.

**Informações de Atributo**

InvoiceNo

É o número da fatura. Categórico/nominal, é um número inteiro de 6 dígitos atribuído exclusivamente a cada transação. Se esse código começa com a letra 'c', indica um cancelamento.

StockCode

É o código do produto (item). Categórico/nominal, é um número inteiro de 5 dígitos atribuído univocamente a cada produto distinto.

Description

É o nome do produto (item). Categórico/nominal.

Quantity

Representa as quantidades de cada produto (item) por transação. Numérico.

InvoiceDate

Indica a data e hora da fatura. Numérico, representa o dia e a hora em que cada transação foi gerada.

UnitPrice

É o preço unitário. Numérico, representa o preço do produto por unidade em libras esterlinas.

CustomerID

É o número do cliente. Categórico/nominal, é um número inteiro de 5 dígitos atribuído univocamente a cada cliente.

Country

É o nome do país. Categórico/nominal, é o nome do país onde cada cliente reside.

Vamos tentar entender as informações desse conjunto de dados.

Primeiro, vamos carregar o conjunto de dados e exibir as primeiras linhas:

```
import pandas as pd 

df = pd.read_csv('/datasets/OnlineRetail.csv')
print(df.head())
```

```
  InvoiceNo StockCode                          Description  Quantity  \
0    536520     21123  SET/10 IVORY POLKADOT PARTY CANDLES       1.0   
1    536520     21124   SET/10 BLUE POLKADOT PARTY CANDLES       1.0   
2    536520     21122   SET/10 PINK POLKADOT PARTY CANDLES       1.0   
3    536520     84378        SET OF 3 HEART COOKIE CUTTERS       1.0   
4    536520     21985    PACK OF 12 HEARTS DESIGN TISSUES       12.0   

            InvoiceDate UnitPrice  CustomerID         Country  
0  2010-12-01T12:43:00Z      1.25     14729.0  United Kingdom  
1  2010-12-01T12:43:00Z      1.25     14729.0  United Kingdom  
2  2010-12-01T12:43:00Z      1.25     14729.0  United Kingdom  
3  2010-12-01T12:43:00Z      1.25     14729.0  United Kingdom  
4  2010-12-01T12:43:00Z      0.29     14729.0  United Kingdom
```

Com base no resultado acima, que tipo de dados você espera das colunas `'StockCode'` `'Description'` e `'UnitPrice'`?

Pergunta

`'StockCode'`

número inteiro

`'Description'`

string

`'UnitPrice'`

número de ponto flutuante

Você conseguiu!

Parece bastante simples que `'StockCode'`, `'Description'` e `'UnitPrice'` devem conter valores inteiros, strings e pontos flutuantes, respectivamente. No entanto, como diz o ditado, não devemos nos deixar levar por primeiras impressões.

Vamos explorar um pouco mais os dados. Começaremos imprimindo os valores mínimo e máximo da coluna `'StockCode'`:

```
import pandas as pd

df = pd.read_csv('/datasets/OnlineRetail.csv')
print(df['StockCode'].min(), df['StockCode'].max())
```

```
10002 m
```

Quando imprimimos as primeiras linhas, parecia que os dados de `'StockCode'` eram números inteiros. No entanto, agora vemos que o valor máximo é `'m'`, que definitivamente não é um número inteiro.

Na verdade, a coluna representa valores de ID, alguns dos quais parecem números inteiros. Se você observar outros valores nessa coluna, vai encontrar valores como `'82494L'`, `'84029G'`, `'35004G'`, etc.

Em seguida, você pode se fazer a seguinte pergunta: se `'StockCode'` tem valores de string, como é possível que chamar `min()` e `max()` retorne valores em vez de gerar um erro?

Python é perfeitamente capaz de comparar valores de string usando os operadores `>` e `<`. No entanto, as regras para comparar strings são diferentes daquelas para comparar números. O comportamento de um operador depende dos tipos de dados dos operandos.

Considerando os valores inesperados na coluna `'StockCode'`, talvez seja melhor voltarmos um pouco para obter algumas informações mais gerais sobre o conjunto de dados. Vamos usar o método `info()` para fazer isso.

CódigoPYTHON

9

1

2

3

4

import pandas as pd

  

df \= pd.read\_csv('/datasets/OnlineRetail.csv')

df.info()

Mostrar a soluçãoExecutar

Isso definitivamente confirma que `'StockCode'` não usa números inteiros.

Além disso, gostaríamos de apontar mais duas coisas sobre os tipos de dados de nossas colunas:

1.  A coluna `'Quantity'` tem um tipo de dados `float64` em vez de `int64`, embora não se espere que produtos sejam vendidos em quantidades parciais. Isso pode ocorrer se houver valores ausentes, pois `NaN` é um tipo de dados float, que força os números inteiros a se tornarem números de ponto flutuante também. Não é o caso de `'Quantity'`, já que não há valores ausentes. No entanto, é importante observar que números inteiros se tornando flutuantes é comum em sistemas de exportação de dados, como ao converter arquivos do Excel para o formato CSV.
2.  A coluna `'UnitPrice'` tem tipo de dados de `object` em vez de `float64`.

Na próxima lição, aprenderemos como lidar com essas duas coisas, ou seja, converter a coluna `'Quantity'` de `float64` para `int64` e identificar por que a pandas não conseguiu detectar o tipo de dados correto para a coluna `'UnitPrice'`.

Os principais tópicos aprendidos nesta lição são:

-   Não devemos presumir que o Python atribui os tipos de dados que esperamos ao ler os dados.
-   É sempre uma ótima ideia chamar `info()` no DataFrame antes de mergulhar em cálculos e modificações. Faça disso uma rotina.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-08-378Z.md
### Última modificação: 2025-05-28 19:10:08

# Como trabalhar com tipos de dados numéricos e de string - TripleTen

Teoria

# Como trabalhar com tipos de dados numéricos e de string

## Strings e números na pandas

Quando a pandas lê dados de um arquivo de texto, converte automaticamente os dados brutos das colunas em tipos de dados pandas.

No geral, a conversão é simples: uma coluna que contém apenas números no arquivo CSV ou Excel será lida automaticamente como tipo de dados `float` ou `int`. Se uma coluna contiver apenas palavras, será lida como um tipo de dados `object`. A Pandas usa esse tipo de dados para dados de string, e também quando uma coluna contém uma mistura de tipos de dados entre seus valores.

Às vezes, a pandas não consegue inferir o tipo de dados correto. Quando isso acontece, precisamos intervir e converter os valores para o tipo correto.

Outras vezes, a pandas fará exatamente o que foi projetada para fazer, mas isso talvez não seja o que queremos.

Nesta lição, você vai aprender a lidar com essas duas situações.

## Conversão para um tipo de dados específico

O método `astype()` da pandas permite fazer cast (ou converter) entre tipos de dados.

Tal como acontece com muitos outros métodos pandas, esse método existe para DataFrames e Series.

Vamos demonstrar como isso funciona usando alguns exemplos.

Abaixo, vamos criar um DataFrame simples de duas colunas a partir de um dicionário Python. As chaves do dicionário serão usadas como os nomes das colunas no DataFrame. Em seguida, exibiremos as informações sobre os tipos de dados das colunas:

```
import pandas as pd

d = {'col1': [1.0, 2.0], 'col2': [3, 4]}
df = pd.DataFrame(data=d)

print(df)
print()
print('Output of df.info():')
df.info()
```

```
   col1  col2
0   1.0     3
1   2.0     4

Output of df.info():
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2 entries, 0 to 1
Data columns (total 2 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   col1    2 non-null      float64
 1   col2    2 non-null      int64  
dtypes: float64(1), int64(1)
memory usage: 160.0 bytes
```

Agora vamos usar o método `astype()` para converter todos os valores em strings e exibir o DataFrame.

Ao usá-lo, você precisa especificar o tipo de dados que deseja converter entre parênteses. Por exemplo, `df['column'] = df['column'].astype('int')` converte a coluna `'column'` para o tipo de dados de números inteiros.

Esteja ciente de que nem todos os tipos de dados podem ser convertidos entre si usando `astype()`. Todas as operações de conversão que possam alterar significativamente os dados originais devem ser executadas com cuidado.

```
import pandas as pd

d = {'col1': [1.0, 2.0], 'col2': [3, 4]}
df = pd.DataFrame(data=d)

df_str_dtype = df.astype('str')
print(df_str_dtype)
```

```
  col1 col2
0  1.0    3
1  2.0    4
```

Os dados impressos do DataFrame parecem iguais a antes, mas o que o método `info()` nos diz?

```
import pandas as pd

d = {'col1': [1.0, 2.0], 'col2': [3, 4]}
df = pd.DataFrame(data=d)

df_str_dtype = df.astype('str')
df_str_dtype.info()
```

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2 entries, 0 to 1
Data columns (total 2 columns):
 #   Column  Non-Null Count  Dtype 
---  ------  --------------  ----- 
 0   col1    2 non-null      object
 1   col2    2 non-null      object
dtypes: object(2)
memory usage: 160.0+ bytes
```

Podemos ver que, apesar da exibição do DataFrame não ter mudado, as informações sobre os tipos de dados mudaram.

Também podemos usar o método `astype()` em colunas individuais (ou seja, objetos Series). Vamos fazer com que `'col1'` fique com o tipo de dados `int` em vez de `float`.

```
import pandas as pd

d = {'col1': [1.0, 2.0], 'col2': [3, 4]}
df = pd.DataFrame(data=d)

df['col1'] = df['col1'].astype('int')
print(df)
```

```
   col1  col2
0     1     3
1     2     4
```

Dessa vez, em vez do método `info()`, vamos conferir o atributo `dtypes`. Lembra disso?

As colunas em um DataFrame podem ter diferentes tipos de dados ou `dtypes`.

```
import pandas as pd

d = {'col1': [1.0, 2.0], 'col2': [3, 4]}
df = pd.DataFrame(data=d)

df['col1'] = df['col1'].astype('int')
print(df.dtypes)
```

```
col1    int64
col2    int64
dtype: object
```

E assim, alteramos o tipo de dados de uma única coluna. Observe que o 64 em `int64` se refere ao tamanho em bits dos números inteiros na memória de um computador. Você também pode ter outros tipos inteiros como `int32`, `int16`, etc. Basicamente, o tipo com 64 bits pode conter muito mais dígitos do que os tipos com menos bits, mas todos eles se comportam de forma idêntica aos tipos de dados `int`.

## Cuidado com as pegadinhas

Digamos que você carregue um conjunto de dados, e _parece_ que todos os valores de ponto flutuante em uma coluna deveriam ser números inteiros, mas há muitos deles para examinar manualmente. Como saber se é seguro realizar a conversão de `float` para `int` ou se você perderá algumas informações após fazer isso?

Para ilustrar, veja o exemplo a seguir.

```
import pandas as pd

d = {'col1': [1.0, 2.0, 3.0, 4.0], 'col2': [5.0, 6.01, 7.0, 8.0]}
df = pd.DataFrame(data=d)

print(df)
```

```
   col1  col2
0   1.0  5.00
1   2.0  6.01
2   3.0  7.00
3   4.0  8.00
```

Se você não tomar cuidado, vai executar `df.astype('int')` (que passará de 6,01 para 6) sem perceber que acabou de alterar os valores em seu conjunto de dados.

Vamos usar uma nova biblioteca para nos ajudar a lidar com esses casos.

`NumPy` é uma biblioteca Python potente usada para computação científica. Ela apresenta uma nova estrutura de dados chamada vetor, que é semelhante a uma lista, mas com várias vantagens, incluindo a capacidade de realizar operações vetorizadas em vetores inteiros rapidamente. `NumPy` também fornece muitas funções e ferramentas matemáticas úteis para trabalhar com vetores, como ordenação, indexação e transmissão.

Quando estiver lidando com centenas ou milhares (ou mais!) de valores, você pode empregar a ajuda do método [`array_equal()`](https://numpy.org/doc/stable/reference/generated/numpy.array_equal.html) da biblioteca `numpy`. Ele aceita dois vetores e retorna `True` se eles tiverem a mesma forma e elementos e `False` caso contrário. Vamos tentar com `'col1'`:

```
import numpy as np
import pandas as pd

d = {'col1': [1.0, 2.0, 3.0, 4.0], 'col2': [5.0, 6.01, 7.0, 8.0]}
df = pd.DataFrame(data=d)

# verifique se a conversão de 'col1' é segura
np.array_equal(df['col1'], df['col1'].astype('int'))
```

```
True
```

Ótimo, não teremos problemas aqui! Agora vamos tentar `'col2'`:

```
import numpy as np
import pandas as pd

d = {'col1': [1.0, 2.0, 3.0, 4.0], 'col2': [5.0, 6.01, 7.0, 8.0]}
df = pd.DataFrame(data=d)

# verifique se a conversão de 'col2' é segura
np.array_equal(df['col2'], df['col2'].astype('int'))
```

```
False
```

Agora sabemos que não podemos converter `'col2'` de `float` para `int` sem perder alguns detalhes dos dados.

Existem outros exemplos de possíveis armadilhas ao considerar a conversão de tipo de dados, mas o principal é o seguinte:

-   Tenha cuidado ao converter entre tipos de dados e considere se a operação de conversão pode levar a alterações significativas.

## Conversão de strings em valores numéricos

Às vezes, precisamos preservar valores de string que se assemelham a números como tipos de dados de string. Códigos postais ou IDs numéricos são exemplos comuns. Eles são números, mas não faz sentido realizar operações aritméticas com eles.

No entanto, `read_csv()` sempre lê tudo o que se parece com números inteiros como um tipo de dados `int`, mesmo se você quiser que sejam strings.

Outras vezes, acontece o oposto: a pandas confunde valores numéricos com strings e precisamos realizar algumas etapas para fazer a conversão.

Em alguns casos simples, o método `astype()` funciona bem. Abaixo, ilustramos como converter strings que representam números inteiros em inteiros reais:

```
import pandas as pd

d = {'col1': ['1.0', '2.0'], 'col2': ['3', '4']}
df = pd.DataFrame(data=d)

# converter col2 em int
df['col2'] = df['col2'].astype('int')
print(df.dtypes)
```

```
col1    object
col2     int64
dtype: object
```

Em outros casos, as strings não podem ser convertidas em números dessa maneira. Veja o que acontece quando tentamos converter strings que representam valores de ponto flutuante em inteiros:

```
import pandas as pd

d = {'col1': ['1.0', '2.0'], 'col2': ['3', '4']}
df = pd.DataFrame(data=d)

# converter col1 dá um erro
df['col1'] = df['col1'].astype('int')
```

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
...
...
...
ValueError: invalid literal for int() with base 10: '1.0'
```

Para resolver esse problema, usamos um método mais flexível chamado `to_numeric()`:

```
import pandas as pd

d = {'col1': ['1.0', '2.0'], 'col2': ['3', '4']}
df = pd.DataFrame(data=d)

df['col2'] = df['col2'].astype('int')
df['col1'] = pd.to_numeric(df['col1'])
print(df.dtypes)
```

```
col1    float64
col2      int64
dtype: object
```

Ele funciona muito bem se você tiver strings semelhantes a números, como `'72'` ou `'1.394'`. Mas, por padrão, `to_numeric()` não consegue converter strings com caracteres não numéricos ou decimais em um número. Em vez disso, ele mostra um erro.

Mas há boas notícias: `to_numeric()` tem um parâmetro `errors=`! O valor desse parâmetro determina o que `to_numeric()` fará se encontrar um valor inválido:

-   `errors='raise'`: é o argumento padrão, onde valores inválidos geram erros, interrompendo a conversão para números em toda a coluna.
-   `errors='coerce'`: os valores inválidos são substituídos por `NaN`.
-   `errors='ignore'`: os valores inválidos são ignorados e deixados inalterados.

Desta forma:

```
import pandas as pd

d = {'col1': ['1.0', 'B.0'], 'col2': ['3', '4']}
df = pd.DataFrame(data=d)

df['col2'] = df['col2'].astype('int')
df['col1'] = pd.to_numeric(df['col1'], errors='coerce')

print(df.dtypes)
print(df)
```

```
col1    float64
col2      int64
dtype: object
   col1  col2
0   1.0     3
1   NaN     4
```

## Recapitulação

-   Use `astype()` para converter um tipo de dados em outro.
-   Nem todos os tipos de dados podem ser convertidos usando `astype()`; algumas conversões (por exemplo, converter uma string com números float em um número inteiro) podem gerar erros.
-   Tenha cuidado ao realizar operações de conversão que possam resultar em alterações significativas nos dados originais.
-   Ao converter de strings para números, o método `to_numeric()` pode ser uma escolha melhor devido à sua flexibilidade (fornecida pelo parâmetro `errors=`).

Como trabalhar com tipos de dados numéricos e de string

Tarefa3 / 3

1.

Use `numpy` para verificar se não há problema em converter a coluna `'Quantity'` de `float` para `int` sem modificar os valores. Imprima a expressão que avalia isso com `True` ou `False`.

2.

Agora execute a operação de conversão usando o método `astype()`. Converta a coluna `'Quantity'` de `float` para `int` e verifique os resultados chamando o método `info()` no DataFrame.

3.

1.  Converta a coluna `'UnitPrice'` de `object` para `float`.
    -   Use a função `to_numeric()`.
    -   Use o parâmetro `errors=` que substitui valores inválidos por `NaN`.
2.  Verifique os resultados chamando o método `info()` no DataFrame.

9

1

2

3

4

5

6

7

import pandas as pd

  

df \= pd.read\_csv('/datasets/OnlineRetail.csv')

  

df\['UnitPrice'\] \= pd.to\_numeric(df\['UnitPrice'\], errors\='coerce')

df.info()

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-12-393Z.md
### Última modificação: 2025-05-28 19:10:12

# Como trabalhar com datas e horas - TripleTen

Capítulo 7/11

Tipos de dados

# Como trabalhar com datas e horas

A biblioteca pandas é útil para mais do que apenas converter entre strings e números. Ela também pode lidar com dados de data e hora.

Nesta lição, mostraremos como aproveitar isso ao máximo. Assista ao vídeo abaixo e depois continue lendo para saber mais.

Vídeo

<iframe class="base-markdown-iframe__iframe" id="player-CDXC_JDH2Cw" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Working with Dates and Times" width="640" height="360" src="https://www.youtube.com/embed/CDXC_JDH2Cw?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F90fc7f7d-4c6f-4828-8026-ea267985a86d%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Formatos diferentes de datas são usados em todo o mundo. Por exemplo, a ordem **dia/mês/ano** é usada em muitos países, então 8 de março de 2010 seria escrito como 08/03/2010. No entanto, nos EUA, o formato **mês/dia/ano** é o mais usado, então 08/03/2010 seria 3 de agosto.

![](https://practicum-content.s3.amazonaws.com/resources/7.4PT_1692099089.png)

O Excel entende os dois formatos, mas há limites. Ao carregar dados de um banco de dados, você talvez tenha valores como `2019-08-03Z17:25:00` e `03, ago 19, 5:25 PM` no mesmo arquivo, e o Excel definitivamente não vai conseguir organizar essa bagunça.

Então, quando quiser ter certeza de que suas datas e horas estão armazenadas corretamente, ou quiser fazer cálculos com elas, você vai precisar usar o formato **datetime** do Python.

## Conversão para datetime

Imagine que em nosso conjunto de dados `OnlineRetail.csv`, o site da empresa não consiga processar algumas solicitações de compra durante os horários de pico.

A equipe de infraestrutura da empresa pode configurar um processo automatizado para adicionar mais servidores Web virtualizados bem a tempo de processar solicitações adicionais. Eles só precisam de alguém para informar quais são os horários de pico.

Vamos ver se podemos ajudá-los.

Vamos novamente carregar o conjunto de dados e exibir algumas linhas aleatórias.

```
import pandas as pd 

df = pd.read_csv('/datasets/OnlineRetail.csv')
print(df.head())
```

```
InvoiceNo StockCode                          Description  Quantity  \
0    536520     21123  SET/10 IVORY POLKADOT PARTY CANDLES       1.0
1    536520     21124   SET/10 BLUE POLKADOT PARTY CANDLES       1.0
2    536520     21122   SET/10 PINK POLKADOT PARTY CANDLES       1.0
3    536520     84378        SET OF 3 HEART COOKIE CUTTERS       1.0
4    536520     21985    PACK OF 12 HEARTS DESIGN TISSUES       12.0

            InvoiceDate UnitPrice  CustomerID         Country
0  2010-12-01T12:43:00Z      1.25     14729.0  United Kingdom
1  2010-12-01T12:43:00Z      1.25     14729.0  United Kingdom
2  2010-12-01T12:43:00Z      1.25     14729.0  United Kingdom
3  2010-12-01T12:43:00Z      1.25     14729.0  United Kingdom
4  2010-12-01T12:43:00Z      0.29     14729.0  United Kingdom
```

A coluna `'InvoiceDate'` contém a data e hora em que a fatura do pedido foi emitida.

Sabemos que a coluna `'InvoiceDate'` tem o tipo de dados `object` devido às lições anteriores.

Vamos confirmar isso novamente olhando para o atributo `dtype`:

```
import pandas as pd 

df = pd.read_csv('/datasets/OnlineRetail.csv')
print(df['InvoiceDate'].dtype)
```

```

object
```

A partir de agora, `'InvoiceDate'` armazena os valores como um tipo de string. Mas para realizar qualquer tipo de operação de data ou hora com esses valores, precisamos convertê-los para o tipo datetime. Podemos fazer isso usando o método `to_datetime()` da pandas.

## Formatação de valores datetime

O método `to_datetime()` é usado para converter datas de um tipo de dados string para o tipo de dados datetime. Ao chamar o método, precisamos usar o parâmetro `format=`, que recebe uma string especificando como as datas são formatadas. Os códigos de formatação designados pelo símbolo `%` são usados para especificar o formato.

Por exemplo, podemos converter a string `2010-12-17T12:38:00Z` (um formato [ISO 8601](https://pt.wikipedia.org/wiki/ISO_8601)) em um objeto datetime passando a string de formatação correta para o parâmetro `format=`: `%Y-%m-%dT% H:%M:%SZ`.

```
import pandas as pd

string_date = '2010-12-17T12:38:00Z'
datetime_date = pd.to_datetime(string_date, format='%Y-%m-%dT%H:%M:%SZ')

print(type(string_date))
print(type(datetime_date))
print(datetime_date)
```

```
<class 'str'>
<class 'pandas._libs.tslibs.timestamps.Timestamp'>
2010-12-17 12:38:00
```

Após chamar `to_datetime()` com nossa data baseada em string e a string de formatação apropriada, obtivemos um objeto com o tipo de dados **Timestamp**. O tipo `Timestamp` da pandas é equivalente ao tipo `datetime` do Python, então vamos usar o termo "datetime" para nos referirmos a ambos.

Observe que o formato do objeto datetime difere daquele da string original: `2010-12-17T12:38:00Z`. No objeto datetime, temos YYYY-MM-DD HH:MM:SS — `2010-12-17 12:38:00`. Não importa como a string original foi formatada, o tipo datetime terá esse formato consistente.

## Como funcionam os códigos de formatação

Como sabemos que o padrão de código para `'2010-12-17T12:38:00Z'` deveria ser `'%Y-%m-%dT%H:%M:%SZ'`? Porque o código representa uma generalização da string. Cada elemento de data ou hora se torna um símbolo que diz à pandas como interpretá-lo.

Por exemplo, `%Y` no início do padrão especifica que o valor do ano ocorre naquele local das strings em `'InvoiceDate'`. Os traços, dois pontos e as letras `T` e `Z` na string de data e hora são reproduzidos no padrão de formatação: "primeiro o ano, depois um traço, o mês, depois outro traço, etc".

Existem muitos símbolos de formatação diferentes, mas apenas alguns que você vai usar regularmente. Entre eles:

-   `%d` — dia do mês (01 a 31)
-   `%m` — mês (01 a 12)
-   `%Y` — ano de quatro dígitos (2019)
-   `%y` — ano de dois dígitos (19)
-   `%H` — hora no formato de 24 horas
-   `%I` — hora no formato de 12 horas
-   `%M` — minutos (00 a 59)
-   `%S` — segundos (00 a 59)

Para obter uma lista completa de todos os símbolos de formatação de data e hora, consulte a documentação [aqui](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior).

## Prática

Vamos experimentar. Vamos fornecer datetimes como strings, e você deve escrever a string de formatação correspondente que deve ser usada para o argumento `format=` em `to_datetime()`.

Suponha que estamos usando o horário de 24 horas.

Pergunta

`'20-12-2002Z04:31:00'`

'%d-%m-%YZ%H:%M:%S'

Ótimo trabalho! Você passou no teste com mérito. Continue assim!

Muito bem!

Pergunta

Atenção, a data está no formato dos EUA: `'5/13/13 12:04:00'`.

'%m/%d/%y %H:%M:%S'

Ótimo trabalho! Você passou no teste com mérito. Continue assim!

Excelente!

## Verificar a presença de objetos datetime

Agora vamos ver se nossos dados mudam em termos de aparência e tipo depois de usar `to_datetime()`:

```
import pandas as pd 

df = pd.read_csv('/datasets/OnlineRetail.csv')
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%Y-%m-%dT%H:%M:%SZ')

print(df.head())
print()
df.info()
```

```
  InvoiceNo StockCode                          Description  Quantity  \
0    536520     21123  SET/10 IVORY POLKADOT PARTY CANDLES       1.0   
1    536520     21124   SET/10 BLUE POLKADOT PARTY CANDLES       1.0   
2    536520     21122   SET/10 PINK POLKADOT PARTY CANDLES       1.0   
3    536520     84378        SET OF 3 HEART COOKIE CUTTERS       1.0   
4    536520     21985    PACK OF 12 HEARTS DESIGN TISSUES       12.0   

          InvoiceDate UnitPrice  CustomerID         Country  
0 2010-12-01 12:43:00      1.25     14729.0  United Kingdom  
1 2010-12-01 12:43:00      1.25     14729.0  United Kingdom  
2 2010-12-01 12:43:00      1.25     14729.0  United Kingdom  
3 2010-12-01 12:43:00      1.25     14729.0  United Kingdom  
4 2010-12-01 12:43:00      0.29     14729.0  United Kingdom  

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 50000 entries, 0 to 49999
Data columns (total 8 columns):
 #   Column       Non-Null Count  Dtype         
---  ------       --------------  -----         
 0   InvoiceNo    50000 non-null  object        
 1   StockCode    50000 non-null  object        
 2   Description  49857 non-null  object        
 3   Quantity     50000 non-null  float64       
 4   InvoiceDate  50000 non-null  datetime64[ns]
 5   UnitPrice    50000 non-null  object        
 6   CustomerID   31599 non-null  float64       
 7   Country      50000 non-null  object        
dtypes: datetime64[ns](1), float64(2), object(5)
memory usage: 3.1+ MB
```

Observe como os valores impressos parecem ser diferentes de antes. Isso porque a coluna `'InvoiceDate'` agora é datetime. Embora os valores pareçam diferentes, todas as informações das strings originais continuam lá.

Agora, o tipo de dados da coluna datetime é um objeto datetime64\[ns\]. O \[ns\] significa o formato de hora **baseado em nanossegundos** que especifica a precisão do objeto **DateTime**.

## Tarefas

### Tarefa 1

Recebemos um novo conjunto de dados dos nossos colegas. Os dados estão armazenados em um arquivo chamado `position.csv` e possuem informações sobre classificações de mecanismos de pesquisa dos últimos três meses. Leia o arquivo CSV e armazene-o em uma variável chamada `position`. Imprima as primeiras quinze linhas.

Caminho do arquivo: `/datasets/position.csv`

CódigoPYTHON

9

1

2

3

4

import pandas as pd

  

position \= pd.read\_csv('/datasets/position.csv')

print(position.head(15))

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Dê uma olhada nas informações gerais dos dados chamando o método `info()` para `position`.

CódigoPYTHON

9

1

2

3

4

5

import pandas as pd

  

position \= pd.read\_csv('/datasets/position.csv')

  

position.info()

Dica

Mostrar a soluçãoValidar

### Tarefa 3

Processe os dados de tempo na coluna `'timestamp'` convertendo-os de string para datetime. Em seguida, imprima as cinco primeiras linhas na tabela `position` com o método `head()`.

Aqui está um exemplo de data que você pode usar como modelo de formato da Tarefa 1: `2019-02-04T13:22:34`.

CódigoPYTHON

9

1

2

3

4

5

6

import pandas as pd

  

position \= pd.read\_csv('/datasets/position.csv')

  

position\['timestamp'\] \= pd.to\_datetime(position\['timestamp'\], format\='%Y-%m-%dT%H:%M:%S')

print(position.head())

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-13-710Z.md
### Última modificação: 2025-05-28 19:10:14

# Como trabalhar com atributos datetime e fusos horários - TripleTen

Teoria

# Como trabalhar com atributos datetime e fusos horários

## Acesso a atributos de coluna datetime usando `.dt`

Como a pandas usa tipos de dados nativos do Python, bem como alguns de diferentes bibliotecas, às vezes, os tipos de dados podem se tornar um pouco complicados.

Lembre-se de que, na pandas, os objetos datetime são representados pelo tipo de dados Timestamp. Para obter, por exemplo, o atributo `year` do primeiro valor de Timestamp na coluna `'InvoiceDate'`, use o seguinte código:

```
import pandas as pd

df = pd.read_csv('/datasets/OnlineRetail.csv')

# converte 'InvoiceDate' para datetime
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%Y-%m-%dT%H:%M:%SZ')

print(df['InvoiceDate'][0].year) # retorna o ano do primeiro InvoiceDate
```

```
2010
```

Embora possamos acessar todos os atributos de valores Timestamp individuais dessa maneira, não podemos fazer isso para um Series com valores Timestamp. Veja o que acontece quando tentamos obter o atributo `day` de toda a coluna `'InvoiceDate'`:

```
import pandas as pd

df = pd.read_csv('OnlineRetail.csv')
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%Y-%m-%dT%H:%M:%SZ')
df['day'] = df['InvoiceDate'].day
```

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
...
...
...
AttributeError: 'Series' object has no attribute 'day'
```

Obtemos um erro, porque `df['InvoiceDate']` é um objeto Series, que não tem um atributo `day`, embora os valores individuais de Timestamp no objeto Series tenham.

Para obter atributos para colunas inteiras de dados datetime, use o método acessor `.dt`.

Por exemplo, podemos criar um DataFrame `'df_days'` que contém o atributo `day` para cada valor na coluna `'InvoiceDate'`:

```
import pandas as pd 

df = pd.read_csv('/datasets/OnlineRetail.csv')
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%Y-%m-%dT%H:%M:%SZ')

df_days = df['InvoiceDate'].dt.day
print(df_days.sample(5, random_state=42))
```

```
33553    17
9427      6
199       1
12447     6
39489    21
Name: InvoiceDate, dtype: int64
```

Sem erros dessa vez! Lembre-se de que, se quiser acessar os atributos de uma coluna inteira de dados datetime, você vai precisar usar o método acessor `.dt` da coluna datetime e não a própria coluna.

Existem muitos outros [atributos datetime](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-date-components) (a documentação está em inglês) disponíveis na pandas, então use aqueles que forem mais adequados para cada caso.

## Como trabalhar com fusos horários

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/7.4_2PT.png)

Existem alguns cenários comuns relacionados a fusos horários que você vai encontrar ao trabalhar com dados datetime.

Seus dados podem vir de diferentes localizações geográficas, onde cada local os registra usando o próprio horário local. Ou talvez você esteja trabalhando com valores datetime registrados em um fuso horário, mas precisa apresentar os resultados da análise para um público que está em um fuso diferente.

Em ambos os casos, você precisa saber como converter entre diferentes fusos horários sem se confundir. É aí que `.dt.tz_localize()` e `.dt.tz_convert()` são úteis. O primeiro permite que você atribua um fuso horário a uma coluna datetime para que os dados estejam "cientes" do próprio fuso horário. O segundo permite que você converta uma coluna "que reconhece o próprio fuso" em um diferente.

Vamos ver como isso funciona na prática atribuindo o fuso horário UTC à coluna `'InvoiceDate'`:

```
import pandas as pd 

df = pd.read_csv('/datasets/OnlineRetail.csv')
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%Y-%m-%dT%H:%M:%SZ')

df['InvoiceDate'] = df['InvoiceDate'].dt.tz_localize('UTC')

print(df['InvoiceDate'].sample(5, random_state=42))
```

```
33553   2010-12-17 12:38:00+00:00
9427    2010-12-06 09:58:00+00:00
199     2010-12-01 13:21:00+00:00
12447   2010-12-06 16:57:00+00:00
39489   2010-12-21 15:19:00+00:00
Name: InvoiceDate, dtype: datetime64[ns, UTC]
```

Notou que o `dtype` da coluna agora contém informações sobre o fuso horário UTC?

E se precisássemos mostrar nossos dados para alguém que mora em Nova York que prefere ver os valores datetime no fuso horário local?

Nesse caso, passamos `'America/New_York'` para o método `.dt.tz_convert()`:

```
import pandas as pd 

df = pd.read_csv('OnlineRetail.csv')
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%Y-%m-%dT%H:%M:%SZ')

df['InvoiceDate'] = df['InvoiceDate'].dt.tz_localize('UTC')

df['InvoiceDate_NYC'] = df['InvoiceDate'].dt.tz_convert('America/New_York')

print(df['InvoiceDate_NYC'].sample(5, random_state=42))
```

```
33553   2010-12-17 07:38:00-05:00
9427    2010-12-06 04:58:00-05:00
199     2010-12-01 08:21:00-05:00
12447   2010-12-06 11:57:00-05:00
39489   2010-12-21 10:19:00-05:00
Name: InvoiceDate_NYC, dtype: datetime64[ns, America/New_York]
```

Agora vemos duas coisas:

1.  O `dtype` da nova coluna `'InvoiceDate_NYC'` agora contém informações de `America/New_York` em vez de `UTC`.
2.  Os próprios valores datetime são diferentes. Eles são diferentes em exatamente 5 horas, porque Nova York, como a maioria do leste da América do Norte, usa o Horário Padrão do Leste (UTC-5) durante o inverno e o Horário de Verão do Leste (UTC-4) no resto do ano.

Consulte este [artigo da Wikipédia](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones#List) (os materiais estão em inglês) para obter uma lista de todos os nomes de fuso horário padrão que você pode usar com `.dt.tz_convert()`.

Como trabalhar com atributos datetime e fusos horários

Tarefa3 / 3

1.

Crie uma variável `dt_months` com o mês da coluna `'timestamp'` da tabela `position`. Use o método acessor `.dt` com o atributo do mês para extrair o número de `month`.

Imprima as cinco primeiras linhas da tabela `dt_months` usando o método `head()`.

2.

Usando a coluna `'timestamp'`, crie um novo DataFrame chamado `'dt_toronto'` que contém todos os datetimes que estão no fuso horário `'America/Toronto'`. Imprima as primeiras 5 linhas do resultado.

3.

Use o novo DataFrame `'dt_toronto'` para converter o datetime para o fuso horário `'Australia/Brisbane'` e armazene o resultado em uma nova coluna chamada `'dt_brisbane'`. Imprima as 5 primeiras linhas.

99

1

2

3

4

5

6

7

8

9

10

import pandas as pd

  

position \= pd.read\_csv('/datasets/position.csv')

position\['timestamp'\] \= pd.to\_datetime(position\['timestamp'\], format\='%Y-%m-%dT%H:%M:%S')

dt\_toronto \= position\['timestamp'\].dt.tz\_localize('America/Toronto')

  

dt\_brisbane \= dt\_toronto.dt.tz\_convert('Australia/Brisbane')

  

  

print(dt\_brisbane.head())

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-15-066Z.md
### Última modificação: 2025-05-28 19:10:15

# Quiz do capítulo - TripleTen

Capítulo 7/11

Tipos de dados

# Quiz do capítulo

Pergunta

Você recebeu o conjunto de dados de vendas de videogame. Para qual tipo de dados você converte `year_of_release`, que atualmente é um float? Suponha que não há valores NaN em `year_of_release`.

Não é necessário converter, pois já é um float.

int

Dado o intervalo de valores e a capacidade de se livrar da parte fracionária desnecessária contida em floats sem perder o significado, um int seria uma ótima escolha.

string

datetime

Excelente!

Pergunta

Você deseja converter o campo `quantity`, que atualmente é uma string, para um tipo de dados numérico. Alguns dos valores em `quantity` não são números, mas mensagens de status como "Esgotado", "Não é mais fabricado", etc. Qual função você usa para converter `quantity` em um tipo de dados numérico que ignora essas mensagens?

`as_type()`

`to_numeric()`

Essa função fornece o conveniente parâmetro `errors`. Ao definir o parâmetro `errors` como `'coerce'`, todos os valores inválidos são substituídos por `NaN`.

`astype()`

`to_numerical()`

Muito bem!

Pergunta

Você converteu `price` para o tipo de dados `int` e notou que o preço total de todas as mercadorias em estoque diminuiu, o que é um sinal preocupante. O que aconteceu?

Os valores de `price` foram arredondados para o valor inteiro mais próximo.

Os valores de `price` perderam a parte fracionária.

Converter de `float` para `int` é simples assim.

Os valores de `price` foram arredondados para cima e diminuídos em 1.

Trabalho maravilhoso!

Pergunta

Você tem dados sobre clientes dos EUA e os respectivos Números de Seguro Social (SSN) deles. Os primeiros três dígitos de um SSN emitido antes de 2011 identificam a localização geográfica (mais detalhes [aqui](https://en.wikipedia.org/wiki/Social_Security_number#List_of_Social_Security_area_numbers)). Você deseja remover os hifens dos SSNs, desnecessários para nossa análise, e converter os dígitos restantes (o número inteiro) para o tipo de dados inteiro. Existe o risco de perder alguma informação após a conversão?

Não, pois todos os dígitos serão preservados.

Sim, devemos manter o tipo string, pois podemos perder zeros no início do número.

Pode haver zeros à esquerda de um SSN que serão perdidos devido à conversão.

Um SSN não pode ser convertido em um tipo de número inteiro, porque há muitos dígitos para armazenar.

Excelente!

Pergunta

Você converteu uma coluna para datetime com `pd.to_datetime()` e notou que há datas ambíguas, como 2022-04-04, 2022-03-03. Você não tem certeza de qual componente representa o mês. O que você pode fazer para remover essa ambiguidade na conversão?

Não há ambiguidade aqui, então não há necessidade de acrescentar nada.

Podemos adicionar um formato de data datetime relevante.

Podemos passar a máscara de datetime explicitamente para o parâmetro `format` ao chamar `pd.to_datetime()`.

Fantástico!

Pergunta

Você leu dados de um arquivo do Excel e notou que há uma coluna de datetime que também contém o fuso horário, CET. Você deseja convertê-la para EST e localizá-la. Como você pode fazer isso?

Executar `.dt.tz_convert()`.

Executar `.dt.tz_convert()` e `.dt.tz_localize()` na sequência.

Primeiro, precisamos alterar os fusos horários para EST e, em seguida, "remover" o componente de fuso horário ao localizar os valores de datetime.

Executar `.dt.tz_localize()`.

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-17-120Z.md
### Última modificação: 2025-05-28 19:10:17

# Conclusão - TripleTen

Capítulo 7/11

Tipos de dados

# Conclusão

Exploramos muitos assuntos neste capítulo. Agora é hora de resumir:

Pergunta

Agora você é capaz de:

Escolha quantas quiser

Não se deixar enganar pela primeira impressão e sempre verificar os tipos de dados usando `info()`

Se ainda não tiver certeza, você pode revisar a lição em [Conjunto de dados à primeira vista](https://tripleten.com/trainer/data-analyst/lesson/00d1d019-b199-4641-aa9f-0eeefffe03e6/){target=”blank”}

Lidar com a conversão de tipos de dados entre strings e numéricos inteiros

Se ainda não tiver certeza, você pode revisar a lição em [Como trabalhar com tipos de dados numéricos e de string](https://tripleten.com/trainer/data-analyst/lesson/0b7b515f-0383-4ca2-9ac8-7ad8c5693115/task/5c9b7666-bd52-4df6-b101-68817a69e972/){target=”blank”}

Converter valores de datetime de strings para Timestamps

Se ainda não tiver certeza, você pode revisar a lição em [Como trabalhar com datas e horas](https://tripleten.com/trainer/data-analyst/lesson/ef5362d1-ea99-416d-96af-075a3243b760/){target=”blank”}

Extrair componentes individuais de valores de datetime até mesmo de objetos Series

Se ainda não tiver certeza, você pode revisar a lição em [Como trabalhar com atributos datetime e fusos horários](https://tripleten.com/trainer/data-analyst/lesson/d17324d5-e779-4971-a125-0fe29c3d7029/task/8feea5db-887a-42cb-9a6c-6cbcc1cbfe0b/){target=”blank”}

Lidar com conversões de fuso horário

Se ainda não tiver certeza, você pode revisar a lição em [Como trabalhar com atributos datetime e fusos horários](https://tripleten.com/trainer/data-analyst/lesson/d17324d5-e779-4971-a125-0fe29c3d7029/task/8feea5db-887a-42cb-9a6c-6cbcc1cbfe0b/){target=”blank”}

Você conseguiu!

Agora você tem tudo o que precisa para trabalhar de forma eficiente com todos os tipos de dados!

No próximo capítulo, vamos explorar algumas técnicas mais avançadas para criar novas colunas a partir de colunas DataFrame existentes quando temos algum conhecimento de domínio sobre o conjunto de dados.

### Leve isso com você

Faça download do sumário do capítulo para que você possa consultá-los quando necessário.

-   [Resumo do capítulo: Tipos de dados](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/PT/7_Resumo_do_captulo_Tipos_de_dados.pdf)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-18-437Z.md
### Última modificação: 2025-05-28 19:10:18

# Introdução - TripleTen

Capítulo 8/11

Engenharia de características

# Introdução

Um superpoder real de usar uma linguagem de programação para trabalho com dados é a capacidade de modificar e criar novas colunas em tempo real. No aprendizado de máquina, a criação de novas colunas ao processar as colunas originais no conjunto de dados é chamada de **engenharia de características**. Ela permite novas soluções para desafios na análise e modelação de dados, simplificando os problemas ao tornar os dados mais adequados.

Neste capítulo, vamos retornar ao conjunto de dados de vendas de videogames que usamos anteriormente neste sprint. Usando esse conjunto de dados, você vai aprender:

-   Várias maneiras de criar novas colunas usando os dados existentes;
-   Como escrever suas próprias funções;
-   Como usar o método `apply()` para criar novas colunas com base em critérios de processamento complexos que não podem ser satisfeitos usando as funções existentes da pandas.

Serão necessárias de 1,5 a 2 horas de trabalho diligente para concluir este capítulo. Vamos começar!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-19-761Z.md
### Última modificação: 2025-05-28 19:10:20

# Criação de novas colunas com base em valores de outras colunas - TripleTen

Teoria

# Criação de novas colunas com base em valores de outras colunas

Uma das habilidades mais úteis na análise de dados é criar uma nova coluna para resolver problemas específicos das colunas existentes do conjunto. Vejamos de novo o conjunto de dados de vendas de videogames.

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
print(df.head())
```

```
                       name platform  year_of_release         genre publisher  \
0                Wii Sports      Wii           2006.0        Sports  Nintendo   
1         Super Mario Bros.      NES           1985.0      Platform  Nintendo   
2            Mario Kart Wii      Wii           2008.0        Racing  Nintendo   
3         Wii Sports Resort      Wii           2009.0        Sports  Nintendo   
4  Pokemon Red/Pokemon Blue       GB           1996.0  Role-Playing  Nintendo   

  developer  na_sales  eu_sales  jp_sales  critic_score  user_score  
0  Nintendo     41.36     28.96      3.77          76.0         8.0  
1       NaN     29.08      3.58      6.81           NaN         NaN  
2  Nintendo     15.68     12.76      3.79          82.0         8.3  
3  Nintendo     15.61     10.93      3.28          80.0         8.0  
4       NaN     11.27      8.89     10.22           NaN         NaN
```

## Transformações de coluna usando operadores aritméticos

Observe que o DataFrame acima inclui vendas de três regiões: NA (América do Norte), EU (Europa) e JP (Japão). Para criar uma coluna `'total_sales'`, precisamos gerá-la a partir das outras. Isso é fácil de fazer:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

df['total_sales'] = df['na_sales'] + df['eu_sales'] + df['jp_sales']
print(df['total_sales'].head())
```

```
0    74.09
1    39.47
2    32.23
3    29.82
4    30.38
Name: total_sales, dtype: float64
```

Isso funciona porque a maioria das funções matemáticas funcionam em **formato vetorial**: elas são aplicadas a colunas inteiras de uma só vez e não precisam percorrer cada valor em uma coluna. Isso permite que o código seja mais eficiente e conciso.

Com esse código simples, você pode criar uma nova coluna chamada `'total_sales'` no DataFrame. O conteúdo dessa coluna será a soma das vendas nas três regiões, linha por linha.

Podemos usar esse método para gerar colunas a partir de fórmulas úteis. Por exemplo, se quisermos calcular a parcela das vendas da EU, basta fazer o seguinte:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
# cria a coluna total_sales e a preenche
df['total_sales'] = df['na_sales'] + df['eu_sales'] + df['jp_sales']

# cria a coluna eu_sales_share e a preenche
df['eu_sales_share'] = df['eu_sales'] / df['total_sales']
print(df['eu_sales_share'].head())
```

```
0    0.390876
1    0.090702
2    0.395904
3    0.366533
4    0.292627
Name: eu_sales_share, dtype: float64
```

E é isso!

## Criação de colunas booleanas

Digamos que queremos uma coluna para indicar se algo é verdade. Podemos criá-la usando os operadores de comparação `==`, `<`, `>=`, etc. Por exemplo, vamos criar uma coluna que verifica se a distribuidora é a Nintendo:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

# cria a coluna is_nintendo e a preenche
df['is_nintendo'] = df['publisher'] == 'Nintendo'
print(df['is_nintendo'].head())
```

```
0    True
1    True
2    True
3    True
4    True
Name: is_nintendo, dtype: bool
```

Observe como isso é semelhante ao que vimos nas aulas de filtragem de dados. A maior parte da filtragem de dados vem da aplicação de colunas booleanas como uma **"máscara"** sobre os dados. Lembre-se de que também podemos fazer isso usando o método `isin()`, que verifica se um valor está em uma lista:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

# Garante que estamos comparando letras minúsculas
print(df['platform'].str.lower().isin(['gb', 'wii']).head())
```

```
0     True
1    False
2     True
3     True
4     True
Name: platform, dtype: bool
```

Aqui, primeiro transformamos o conteúdo da coluna em minúsculas para evitar erros de maiúsculas e minúsculas, como `'wii'` vs. `'Wii'`.

## Colunas categóricas

É raro que trabalhar com dados brutos de string seja útil para análise de dados; colunas string geralmente precisam ser processadas de alguma forma.

Se a coluna string representar um conjunto de categorias, é muito melhor tratar esses valores diretamente como categorias.

Converter uma coluna para um tipo de dados categórico, em vez de manter o tipo string, pode economizar memória e tornar a análise mais rápida, especialmente em grandes conjuntos de dados. Isso ocorre porque colunas categóricas armazenam apenas um único número (o ID da categoria) para cada entrada, em vez do texto completo dela. Além disso, trabalhar com categorias pode facilitar alguns tipos de análise, por exemplo, quando precisamos agrupar dados por categoria ou filtrar informações com base em várias categorias de uma só vez. Isso pode ser feito usando o tipo de dados `categorical`.

Observe os valores únivocos na coluna `'platform'`:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

print(df['platform'].unique())
```

```
['Wii' 'NES' 'GB' 'DS' 'X360' 'PS3' 'PS2' 'SNES' 'GBA' 'PS4' '3DS' 'N64'
 'PS' 'XB' 'PC' '2600' 'PSP' 'XOne' 'WiiU' 'GC' 'GEN' 'DC' 'PSV' 'SAT'
 'SCD' 'WS' 'NG' 'TG16' '3DO' 'GG' 'PCFX']
```

Podemos converter `'platform'` de uma coluna string para uma coluna categórica usando o método `astype()` que você aprendeu no capítulo anterior:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

df['platform'] = df['platform'].astype('category')
print(df['platform'].head())
```

```
0    Wii
1    NES
2    Wii
3    Wii
4     GB
Name: platform, dtype: category
Categories (31, object): ['2600', '3DO', '3DS', 'DC', ..., 'WiiU', 'X360', 'XB', 'XOne']
```

Observe que existem apenas 31 categorias, embora existam 16.719 entradas. Quando a coluna é armazenada como strings, precisamos manter o texto completo das 16.719 entradas. Quando armazenada como uma categoria, armazenamos apenas um único número (o ID da categoria). Perfeito!

Criação de novas colunas com base em valores de outras colunas

Tarefa

Você tem a tarefa de analisar qual é o jogo mais vendido em média em todos os mercados.

Você precisa:

-   Calcular a média das colunas `'jp_sales'`, `'na_sales'` e `'eu_sales'` e armazená-la em uma nova coluna chamada `'average_sales'` (média das vendas).
-   Ordenar os valores do DataFrame por `average_sales`, em ordem decrescente. Use o método `sort_values` passando os argumentos corretos para os parâmetros `by=` e `ascending=`.
-   Depois, imprima os primeiros cinco valores do DataFrame com a nova coluna.

99

1

2

3

4

5

6

7

8

9

10

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

  

df\['average\_sales'\] \= (df\['na\_sales'\] + df\['eu\_sales'\] + df\['jp\_sales'\])/3

df \= df.sort\_values(by\='average\_sales', ascending\=False)

  

  

  

print(df.head())

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-22-109Z.md
### Última modificação: 2025-05-28 19:10:22

# Criação de colunas categóricas com apply() - TripleTen

Teoria

# Criação de colunas categóricas com apply()

Na lição anterior, você aprendeu a criar novas colunas numéricas fazendo cálculos com base em outras colunas numéricas dos dados. Nesta lição, você vai aprender a criar novas colunas categóricas para resumir dados numéricos em outras colunas. Essa técnica frequentemente simplifica a análise e pode tornar seus resultados mais fáceis de entender.

Vamos conferir de novo a coluna `'year_of_release'` do conjunto de dados de videogame. Em particular, queremos saber o intervalo de anos que nossos dados cobrem:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

print(df['year_of_release'].min(), df['year_of_release'].max())
```

```
1980.0 2020.0
```

Uau, 40 anos de jogos! Vamos dar uma olhada nesses valores contando quantos jogos temos para cada ano.

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df_year_of_release = df['year_of_release'].value_counts()

print(df_year_of_release)
```

```
2008.0    1427
2009.0    1426
2010.0    1255
2007.0    1197
2011.0    1136
2006.0    1006
2005.0     939
2002.0     829
2003.0     775
2004.0     762
2012.0     653
2015.0     606
2014.0     581
2013.0     544
2016.0     502
2001.0     482
1998.0     379
2000.0     350
1999.0     338
1997.0     289
1996.0     263
1995.0     219
1994.0     121
1993.0      60
1981.0      46
1992.0      43
1991.0      41
1982.0      36
1986.0      21
1983.0      17
1989.0      17
1990.0      16
1987.0      16
1988.0      15
1984.0      14
1985.0      14
1980.0       9
2017.0       3
2020.0       1
Name: year_of_release, dtype: int64
```

Recebemos uma resposta, mas ela não está na ordem correta. Precisamos ordenar o resultado. O método value\_counts() retornou uma contagem dos anos, onde os anos são os índices do novo DataFrame e as contagens são os valores. Então, vamos ordenar o resultado por índice para exibi-lo em ordem cronológica.

Para isso, usaremos o método `sort_index()`. Ele funciona da mesma forma que o bom e velho `sort_values()`, mas é aplicado aos índices em vez dos valores:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df_year_of_release = df['year_of_release'].value_counts().sort_index()

print(df_year_of_release)
```

```
1980.0       9
1981.0      46
1982.0      36
1983.0      17
1984.0      14
1985.0      14
1986.0      21
1987.0      16
1988.0      15
1989.0      17
1990.0      16
1991.0      41
1992.0      43
1993.0      60
1994.0     121
1995.0     219
1996.0     263
1997.0     289
1998.0     379
1999.0     338
2000.0     350
2001.0     482
2002.0     829
2003.0     775
2004.0     762
2005.0     939
2006.0    1006
2007.0    1197
2008.0    1427
2009.0    1426
2010.0    1255
2011.0    1136
2012.0     653
2013.0     544
2014.0     581
2015.0     606
2016.0     502
2017.0       3
2020.0       1
Name: year_of_release, dtype: int64
```

Que conclusões podemos tirar desses dados? A análise dos jogos de 2008 seria significativamente diferente da análise dos jogos de 2009? Se quisermos comparar os dados de 2000 com outros anos, precisaremos compará-los com os dados de 1986 e de 1987 separadamente, ou seria melhor agrupar essas comparações para obter dados sobre os jogos lançados antes de 2000 comparados aos lançados depois de 2000?

É difícil produzir conclusões estatisticamente relevantes para jogos de cada ano em concreto quando os jogos no conjunto de dados estão tão distribuídos por todos os anos de modo tão amplo. Podemos resolver esse problema juntando os jogos em categorias para que cada grupo seja grande o suficiente para tirarmos conclusões significativas.

## Categorização

Precisamos realizar uma **categorização**, o que significa agrupar dados em novas categorias que vamos criar. Nesse caso, vamos agrupar os jogos em quatro categorias com base em geração.

-   Lançamentos de antes de 2000 vão para a categoria `'retro'`
-   Lançamentos entre 2000 e 2009 (incluindo esse ano) vão para a categoria `'modern'`
-   Lançamentos a partir de 2010 vão para a categoria `'recent'`
-   Aqueles sem dados do ano de lançamento vão para a categoria `'unknown'`

Queremos colocar cada jogo em uma dessas quatro categorias e armazenar o resultado em uma nova coluna.

Não há na pandas uma função pronta para fazer isso. No entanto, podemos escrever nossa própria função, adaptada especificamente às necessidades aqui. A função deve aceitar o ano de lançamento como entrada e retornar como saída a categoria da geração para aquele ano.

Veja como ficará nossa função personalizada, `era_group()`:

```
def era_group(year):
    """
    A função retorna a geração dos jogos conforme o ano de lançamento, usando as seguintes regras:
    —'retro'   for year < 2000
    —'modern'  for 2000 <= year < 2010
    —'recent'  for year >= 2010
    —'unknown' for missing year values (NaN)
    """

    if year < 2000:
        return 'retro'
    elif year < 2010:
        return 'modern'
    elif year >= 2010:
        return 'recent'
    else:
        return 'unknown'
```

Agora precisaremos testar a função para cada regra. Então, vamos verificar em quais categorias três jogos de anos diferentes se enquadram:

```
print(era_group(1983))
print(era_group(2009))
print(era_group(2021))
print(era_group(np.nan))
```

```
retro
modern
recent
unknown
```

Excelente! A função funciona como esperado. Em seguida, criaremos uma nova coluna para registrar as categorias de geração.

## O método `apply()`

Para criar uma coluna com as gerações usando a função personalizada `era_group()`, precisaremos chamar o método `apply()`, que recebe valores de uma coluna DataFrame e aplica uma função a eles.

Nesse caso, `apply()` deve ser aplicada à coluna `'year_of_release'`, porque `'year_of_release'` contém os dados que a função usa como parâmetro. A própria função `era_group()` torna-se então o argumento que passamos para o método `apply()`.

```
import pandas as pd

def era_group(year):
    """
    A função retorna a geração dos jogos conforme o ano de lançamento, usando as seguintes regras:
    —'retro'   for year < 2000
    —'modern'  for 2000 <= year < 2010
    —'recent'  for year >= 2010
    —'unknown' for missing year values (NaN)
    """

    if year < 2000:
        return 'retro'
    elif year < 2010:
        return 'modern'
    elif year >= 2010:
        return 'recent'
    else:
        return 'unknown'

df = pd.read_csv('/datasets/vg_sales.csv')

df['era_group'] = df['year_of_release'].apply(era_group)
print(df.head())
```

```
              name platform  year_of_release         genre publisher  \
0                Wii Sports      Wii           2006.0        Sports  Nintendo   
1         Super Mario Bros.      NES           1985.0      Platform  Nintendo   
2            Mario Kart Wii      Wii           2008.0        Racing  Nintendo   
3         Wii Sports Resort      Wii           2009.0        Sports  Nintendo   
4  Pokemon Red/Pokemon Blue       GB           1996.0  Role-Playing  Nintendo   

  developer  na_sales  eu_sales  jp_sales  critic_score  user_score era_group  
0  Nintendo     41.36     28.96      3.77          76.0         8.0    modern  
1       NaN     29.08      3.58      6.81           NaN         NaN     retro  
2  Nintendo     15.68     12.76      3.79          82.0         8.3    modern  
3  Nintendo     15.61     10.93      3.28          80.0         8.0    modern  
4       NaN     11.27      8.89     10.22           NaN         NaN     retro
```

É muito simples criar uma coluna usando nossa função personalizada: apenas uma linha de código.

Agora vamos analisar os dados das gerações com o método `value_counts()`:

```
import pandas as pd

def era_group(year):
    """
    A função retorna a geração dos jogos conforme o ano de lançamento, usando as seguintes regras:
    —'retro'   for year < 2000
    —'modern'  for 2000 <= year < 2010
    —'recent'  for year >= 2010
    —'unknown' for missing year values (NaN)
    """

    if year < 2000:
        return 'retro'
    elif year < 2010:
        return 'modern'
    elif year >= 2010:
        return 'recent'
    else:
        return 'unknown'

df = pd.read_csv('/datasets/vg_sales.csv')

df['era_group'] = df['year_of_release'].apply(era_group)
print(df['era_group'].value_counts())
```

```
modern     9193
recent     5281
retro      1974
unknown     269
Name: era_group, dtype: int64
```

Os dados estão prontos para análise.

Agora é sua vez. Nas tarefas, pratique usar `apply()` para categorizar os videogames pelas classificações e analisar como isso se correlaciona com as vendas.

Criação de colunas categóricas com apply()

Tarefa3 / 3

1.

Comece escrevendo uma função chamada `score_group()` que coloca os jogos em categorias com base nas pontuações dos críticos. Categorize as pontuações de acordo com o seguinte:

-   O valor `'low'` para pontuações abaixo de 60
-   O valor `'medium'` para pontuações entre 60 e 79, incluindo esse número
-   O valor `'high'` para pontuações de 80 para cima
-   O valor `'no score'` para pontuações com valores ausentes

A função `score_group()` deve ter um parâmetro de entrada numérico chamado `score`. A saída deve ser uma string que devolve a categoria da pontuação.

Certifique-se de que sua função produza a saída correta quando os valores 10, 65, 99 e `np.nan` forem passados para ela. Escrevemos uma instrução `print()` separada para cada chamada de função.

Não hesite em copiar a função do exemplo anterior e adaptá-la a suas necessidades atuais.

2.

Adicione uma coluna `'score_group'` à tabela `df` aplicando a função `score_group()` à coluna `'critic_score'`, usando o método `apply()` para isso. Imprima as primeiras 5 linhas para certificar-se de que a nova coluna foi criada corretamente.

O pré-código contém a função `score_group()` da última tarefa (ela pode estar um pouco diferente da sua, mas a funcionalidade é a mesma).

3.

Agora calcule o total de vendas na América do Norte para cada categoria de pontuação dos críticos.

Faça o seguinte:

-   Agrupe pela sua nova coluna `'score_group'` usando o método `groupby()`
-   Calcule a soma da coluna `'na_sales'` do DataFrame **agrupado** usando `sum()`
-   Imprima o resultado

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

import pandas as pd

import numpy as np

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

  

def score\_group(score):

if score < 60:

return 'low'

elif score < 80:

return 'medium'

elif score \>= 80:

return 'high'

else:

return 'no score'

  

df\['score\_group'\] \= df\['critic\_score'\].apply(score\_group)

  

df\_grouped \= df.groupby('score\_group')

df\_sum \= df\_grouped\['na\_sales'\].sum()

  

  

print(df\_sum)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-23-418Z.md
### Última modificação: 2025-05-28 19:10:23

# Criação de categorias com funções de tratamento de linhas - TripleTen

Teoria

# Criação de categorias com funções de tratamento de linhas

Você conseguiu realizar uma análise simplificada sobre a correlação entre a pontuação dada pelos críticos de um jogo e o sucesso comercial dele usando uma função que você criou e o método `apply()`. Sua função personalizada recebia um único argumento, então só era necessário chamar `apply()` na coluna com os valores que você queria usar como parâmetro de entrada.

Mas e se quisermos criar categorias baseadas em valores de mais de uma coluna? Nesse caso, podemos escrever uma função que receba uma linha inteira como entrada e extraia os valores necessários para criar novas categorias.

Ilustraremos isso com o conjunto de dados de videogames. Mas, desta vez, vamos simplificar as coisas eliminando todas as linhas que tenham valores ausentes:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

df.dropna(inplace=True)
df.info()
```

```
<class 'pandas.core.frame.DataFrame'>
Int64Index: 7943 entries, 0 to 16707
Data columns (total 11 columns):
 #   Column           Non-Null Count  Dtype
---  ------           --------------  -----
 0   name             7943 non-null   object
 1   platform         7943 non-null   object
 2   year_of_release  7943 non-null   float64
 3   genre            7943 non-null   object
 4   publisher        7943 non-null   object
 5   developer        7943 non-null   object
 6   na_sales         7943 non-null   float64
 7   eu_sales         7943 non-null   float64
 8   jp_sales         7943 non-null   float64
 9   critic_score     7943 non-null   float64
 10  user_score       7943 non-null   object
dtypes: float64(5), object(6)
memory usage: 744.7+ KB
```

Descartamos muitas linhas, mas agora não precisamos nos preocupar com valores ausentes. Além disso, sempre podemos adicioná-las novamente após nossa análise preliminar para ver como isso afeta os resultados.

Agora vamos escrever uma função chamada `era_sales_group()` que coloca os jogos nas seguintes categorias conforme o ano de lançamento _e_ as vendas totais:

-   Lançamentos de antes de 2000 com menos de US$ 1 milhão em vendas vão para a categoria `'retro'`
-   Lançamentos entre 2000 e 2009 (incluindo esse ano) com menos de US$ 1 milhão em vendas vão para a categoria `'modern'`
-   Lançamentos de antes de 2010 com US$ 1 milhão ou mais em vendas vão para a categoria `'classic'`
-   Lançamentos a partir de 2010 com menos de US$ 1 milhão em vendas vão para a categoria `'recent'`
-   Lançamentos a partir de 2010 com US$ 1 milhão ou mais em vendas vão para a categoria `'big hit'`

Veja como a função fica, e também uma amostra do resultado:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

df.dropna(inplace=True)

def era_sales_group(row):
    """
    A função retorna a geração dos jogos conforme o ano de lançamento, usando as seguintes regras:
    —'retro'   para ano < 2000 e o total de vendas < $1 milhões
    —'modern'  para 2000 <= ano < 2010 e o total de vendas < $1 milhões
    —'recent'  para ano >= 2010 e o total de vendas < $1 milhões
    —'classic' para ano < 2010 e o total de vendas >= $1 milhões
    —'big hit' para ano >= 2010 e o total de vendas >= $1 milhões
    """

    year = row['year_of_release']
    na_sales = row['na_sales']
    eu_sales = row['eu_sales']
    jp_sales = row['jp_sales']
    
    total_sales = na_sales + eu_sales + jp_sales
    
    if year < 2000:
        if total_sales < 1:
            return 'retro'
        else:
            return 'classic'
    if year < 2010:
        if total_sales < 1:
            return 'modern'
        else:
            return 'classic'
    if year >= 2010:
        if total_sales < 1:
            return 'recent'
        else:
            return 'big hit'

row = df.iloc[0] # use a primeira linha como exemplo de parâmetro de entrada

print(row)
print()
print('Esse jogo é', era_sales_group(row))
```

```
name               Wii Sports
platform                  Wii
year_of_release        2006.0
genre                  Sports
publisher            Nintendo
developer            Nintendo
na_sales                41.36
eu_sales                28.96
jp_sales                 3.77
critic_score             76.0
user_score                  8
Name: 0, dtype: object

Esse jogo é classic
```

Assim como a função `era_group()` da lição anterior, a função `era_sales_group()` também tem um parâmetro, `row`. Mas, nesse caso, espera-se que o parâmetro `row` seja uma única linha inteira do DataFrame.

A função precisa de quatro valores da linha para realizar a categorização: o valor do ano de lançamento e três valores para calcular o total de vendas. Observe que a função usa os nomes das colunas de `df` para extrair esses valores, então ela não funcionaria em outro DataFrame com colunas diferentes. Ela é específica para os dados em `vg_sales.csv`.

Usamos `df.iloc[0]` para pegar a primeira linha de `df` e testar a função, que produziu a saída `'classic'`. Se verificarmos as regras de categorização, isso faz sentido. O Wii Sports foi lançado antes de 2010 e certamente teve mais de US$ 1 milhão em vendas – muito, muito mais!

## Usos da função com linhas personalizadas

Testamos `era_sales_group()` para o caso `'classic'`, mas e quanto às outras quatro categorias? Seria uma tarefa tediosa pesquisar nos dados tentando encontrar linhas para cada caso. No entanto, podemos criar nossas próprias linhas com os valores que queremos testar. Basta converter uma lista de nomes de colunas e uma lista de valores de linhas em um objeto Series:

```
column_names = ['year_of_release', 'na_sales', 'eu_sales', 'jp_sales']
row_values = [2000, 0.1, 0.25, 0]

row = pd.Series(row_values, index=column_names)

print(row)
print()
print('Esse jogo é', era_sales_group(row))
```

```
year_of_release    2000.00
na_sales              0.10
eu_sales              0.25
jp_sales              0.00
dtype: float64

Esse jogo é modern
```

Outro benefício de testar a função dessa maneira é que não precisamos criar valores para colunas que a função não usa. Nesse caso, nossa linha tem valores só para as colunas `'year_of_release'`, `'na_sales'`, `'eu_sales'` e `'jp_sales'`.

Para finalizar os testes, vamos verificar a função com os vários valores de entrada que criamos:

```
cols = ['year_of_release', 'na_sales', 'eu_sales', 'jp_sales']

row_1 = pd.Series([1989, 0, 0, 0.6], index=cols) # esperado 'retro'
row_2 = pd.Series([1989, 1, 2, 0], index=cols)   # esperado 'classic'
row_3 = pd.Series([2006, 0.3, 0, 0], index=cols) # esperado 'modern'
row_4 = pd.Series([2020, 0, 0.4, 0], index=cols) # esperado 'recent'
row_5 = pd.Series([2020, 1, 1, 1], index=cols)   # esperado 'big hit'

print(row_1, row_2, row_3, row_4, row_5, sep='\n\n')
print()

rows = [row_1, row_2, row_3, row_4, row_5]

for row in rows:
    print('Esse jogo é', era_sales_group(row))
```

```
year_of_release    1989.0
na_sales              0.0
eu_sales              0.0
jp_sales              0.6
dtype: float64

year_of_release    1989
na_sales              1
eu_sales              2
jp_sales              0
dtype: int64

year_of_release    2006.0
na_sales              0.3
eu_sales              0.0
jp_sales              0.0
dtype: float64

year_of_release    2020.0
na_sales              0.0
eu_sales              0.4
jp_sales              0.0
dtype: float64

year_of_release    2020
na_sales              1
eu_sales              1
jp_sales              1
dtype: int64

Esse jogo é retro
Esse jogo é classic
Esse jogo é modern
Esse jogo é recent
Esse jogo é big hit
```

Parece que a função de categorização está funcionando conforme o esperado. Para a etapa final, vamos usar a função para criar uma nova coluna.

## Criação de colunas

Agora que você entende melhor como as funções de tratamento de linhas funcionam, vamos aprender como usá-las com `apply()` para criar novas colunas.

Nesse caso, queremos criar uma coluna `'game_category'` que categoriza cada jogo conforme a saída da função `era_sales_group()`. Assim como na lição anterior, chamaremos o método `apply()`. Desta vez, no entanto, existem duas grandes diferenças:

1.  O método `apply()` é chamado em todo o DataFrame `df` em vez de em uma única coluna
2.  Precisamos usar o parâmetro `axis=` ao chamar o método `apply()`

Por padrão, o parâmetro `axis=` é definido como 0, que significa que `apply()` passa valores de colunas para a função usada como parâmetro de entrada. Se quisermos que `apply()` passe valores de linhas para a função, precisamos definir `axis=1`.

Assim, a nova coluna `'game_category'` pode ser criada da seguinte maneira:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df.dropna(inplace=True)

def era_sales_group(row):
    """
    A função retorna a geração dos jogos conforme o ano de lançamento, usando as seguintes regras:
    —'retro'   para ano < 2000 e o total de vendas < $1 milhões
    —'modern'  para 2000 <= ano < 2010 e o total de vendas < $1 milhões
    —'recent'  para ano >= 2010 e o total de vendas < $1 milhões
    —'classic' para ano < 2010 e o total de vendas >= $1 milhões
    —'big hit' para ano >= 2010 e o total de vendas >= $1 milhões
    """

    year = row['year_of_release']
    na_sales = row['na_sales']
    eu_sales = row['eu_sales']
    jp_sales = row['jp_sales']
    
    total_sales = na_sales + eu_sales + jp_sales
    
    if year < 2000:
        if total_sales < 1:
            return 'retro'
        else:
            return 'classic'
    if year < 2010:
        if total_sales < 1:
            return 'modern'
        else:
            return 'classic'
    if year >= 2010:
        if total_sales < 1:
            return 'recent'
        else:
            return 'big hit'

df['game_category'] = df.apply(era_sales_group, axis=1)
print(df.sample(5, random_state=321))
```

```
                                    name platform  year_of_release      genre  \
7654          Dave Mirra Freestyle BMX 2       XB           2001.0     Sports   
6815          Burnout 2: Point of Impact       XB           2003.0     Racing   
9824   NPPL: Championship Paintball 2009     X360           2008.0    Shooter   
34            Call of Duty: Black Ops II      PS3           2012.0    Shooter   
14940  Agatha Christie's The ABC Murders      PS4           2016.0  Adventure   

                   publisher        developer  na_sales  eu_sales  jp_sales  \
7654   Acclaim Entertainment     Z-Axis, Ltd.      0.15      0.04      0.00   
6815   Acclaim Entertainment  Criterion Games      0.18      0.05      0.00   
9824        Activision Value         FUN Labs      0.10      0.01      0.00   
34                Activision         Treyarch      4.99      5.73      0.65   
14940               Microids         Microids      0.01      0.01      0.00   

       critic_score  user_score game_category  
7654           76.0         8.2        modern  
6815           88.0         5.1        modern  
9824           44.0         7.3        modern  
34             83.0         5.3       big hit  
14940          67.0         6.2        recent
```

Podemos ver que a coluna foi criada e que podemos começar a analisá-la. Por exemplo, podemos usar o método `value_counts()` para obter o número de jogos que pertencem a cada categoria:

```
print(df['game_category'].value_counts())
```

```
modern     3907
recent     1784
classic     748
big hit     407
retro        43
Name: game_category, dtype: int64
```

Esperamos que você aprecie a capacidade e a versatilidade das funções personalizadas com `apply()`. Continue aprimorando suas habilidades de criação de colunas nas tarefas.

Criação de categorias com funções de tratamento de linhas

Tarefa2 / 2

1.

Escreva uma função chamada `avg_score_group()` que tenha um parâmetro chamado `row`. O parâmetro `row` deve ser um objeto Series da pandas. A função deve calcular a pontuação média de classificação para cada jogo e, em seguida, retornar uma string que distribui cada jogo em uma das seguintes categorias:

-   Um valor `'low'` para pontuações médias abaixo de 60
-   Um valor `'medium'` para pontuações médias de 60 a 79, incluindo esse número
-   Um valor `'high'` para pontuações médias de 80 para cima

Para calcular a pontuação média, `avg_score_group()` deve extrair valores de `row` com os nomes das colunas em `'critic_score'` e `'user_score'`. A fórmula para calculá-la é `avg_score = (critic_score + user_score * 10) / 2`.

Fizemos os testes para você, o programa deve imprimir `low`, `medium` e `high`, nessa ordem.

2.

Agora é hora de testar sua nova função. Crie três linhas personalizadas com os seguintes nomes e valores de variáveis:

-   `row_1` — pontuação dos críticos de 66 e pontuação dos usuários de 3.6
-   `row_2` — pontuação dos críticos de 72 e pontuação dos usuários de 8.1
-   `row_3` — pontuação dos críticos de 99 e pontuação dos usuários de 9.4

Todas as variáveis `row` devem ser objetos Series com valores de índice `'critic_score'` e `'user_score'` para que `avg_score_group()` possa extrair os valores corretos.

O pré-código define a função `avg_score_group()` da última tarefa, embora ela talvez esteja um pouco diferente da sua solução. Sua tarefa é imprimir o resultado da chamada `avg_score_group()` com cada uma das três entradas acima.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

df.dropna(inplace\=True)

  

def avg\_score\_group(row):

critic\_score \= row\['critic\_score'\]

user\_score \= row\['user\_score'\]

avg\_score \= (critic\_score + user\_score \* 10) / 2

if avg\_score < 60:

return 'low'

if avg\_score < 80:

return 'medium'

if avg\_score \>= 80:

return 'high'

  

  

\# crie as linhas de entrada de teste aqui

row\_1 \= pd.Series(\[66, 3.6\], index\=\['critic\_score', 'user\_score'\])

row\_2 \= pd.Series(\[72, 8.1\], index\=\['critic\_score', 'user\_score'\])

row\_3 \= pd.Series(\[99, 9.4\], index\=\['critic\_score', 'user\_score'\])

\# imprima os resultados da chamada da função com as entradas de teste na ordem necessária

print(avg\_score\_group(row\_1))

print(avg\_score\_group(row\_2))

  

print(avg\_score\_group(row\_3))

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-24-730Z.md
### Última modificação: 2025-05-28 19:10:25

# Quiz do capítulo - TripleTen

Capítulo 8/11

Engenharia de características

# Quiz do capítulo

Pergunta

O que é a engenharia de características?

É a adição de novas características a um modelo de aprendizado de máquina.

É a criação de novas variáveis a partir das existentes.

No aprendizado de máquina, o processo de criação de novas colunas como resultado do processamento de dados das colunas originais no conjunto é chamado de **engenharia de características**. As novas colunas destinam-se a ajudar a encontrar padrões úteis do conjunto de dados para algoritmos de aprendizado de máquina, o que melhora a qualidade geral do modelo.

É a implementação de características em um produto de software.

Você conseguiu!

Pergunta

Qual transformação provavelmente não faz parte da engenharia de características?

Somar todas as vendas de diferentes regiões.

Multiplicar a quantidade vendida pelo preço.

Converter datas de strings para o tipo de dados `datetime`.

Essa é uma transformação de tipo de dados e, portanto, é provável que não ajude a melhorar a qualidade do modelo. Extrair diferentes componentes de tempo desses valores como colunas separadas poderia ajudar e até ser considerado engenharia de características.

Copiar ano e mês de uma coluna de tipo de dados `datetime` para novas colunas separadas.

Fantástico!

Pergunta

Qual destes casos seria considerado o **pior** para escolher como tipo `category`?

Sabores de sorvete

Números primos

Há muitos números primos, teoricamente um número infinito deles. Seria difícil encontrar cenários em que você precisaria tratá-los como uma categoria.

Classes de medalhas olímpicas

Nomes de fabricantes de smartphones

Muito bem!

Lembra da coluna `average_sales` que criamos duas lições atrás? Vamos transformá-la em uma função e aplicar ao nosso DataFrame. Agora queremos que você:

-   Crie uma nova função chamada `average_sales()` que aplica a fórmula que usamos antes: `('na_sales' + 'eu_sales' + 'jp_sales') / 3`
-   Crie uma nova coluna chamada `'avg_sales'` aplicando a função `average_sales()` ao DataFrame.
-   Imprima as 5 primeiras linhas para ter certeza de que está tudo certo com a nova coluna.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

df.dropna(inplace\=True)

  

def average\_sales(row):

return (row\['na\_sales'\] + row\['eu\_sales'\] + row\['jp\_sales'\]) / 3

  

df\['avg\_sales'\] \= df.apply(average\_sales,axis\=1)

  

  

  

  

print(df.head())

  

Dica

Mostrar a soluçãoValidar

Pergunta

Você deseja executar uma função personalizada para computar categorias com base em valores de uma coluna. Qual função da pandas você usa?

`apply()`

Isso permite aplicar uma função personalizada ao longo de um objeto Series ou um eixo do objeto DataFrame.

`astype()`

`run()`

Seu entendimento sobre o material é impressionante!

Pergunta

Você deseja executar uma função personalizada chamada `post_code_and_address_to_territory_group` para calcular categorias com base em valores de colunas diferentes. Como você chama a função `apply()` para fazer isso?

`df.apply(post_code_and_address_to_territory_group, axis=0)`

`df.apply(post_code_and_address_to_territory_group, axis=1)`

Correto. Precisamos definir o parâmetro `axis` como 1 se quisermos que `apply()` passe os valores da linha da função. Se tivéssemos definido o parâmetro `axis` como 0, `apply()` teria passado os valores da coluna da função.

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-26-725Z.md
### Última modificação: 2025-05-28 19:10:27

# Conclusão - TripleTen

Capítulo 8/11

Engenharia de características

# Conclusão

Bom trabalho aprendendo a modelar os dados na forma que você deseja! Usar a transformação de dados de maneira eficaz para projetar novas características em conjuntos é uma habilidade fundamental para qualquer profissional de dados.

Pergunta

Agora você é capaz de:

Escolha quantas quiser

Criar novas colunas usando operações básicas em outras colunas

Se ainda não tiver certeza, você pode revisar a lição em [Criação de novas colunas com base em valores de outras colunas](https://tripleten.com/trainer/data-analyst/lesson/15c3f968-717e-4bba-9647-79a6b67a76fb/task/c1ee7144-f406-478e-ad31-4b9382e1777c/)

Usar o tipo de dados `category` como uma forma de representar strings unívocas com valores numéricos distintos

Se ainda não tiver certeza, você pode revisar a lição em [Criação de colunas categóricas com `apply()`](https://tripleten.com/trainer/data-analyst/lesson/62246252-fd2f-4321-9d2c-54c53bb8527d/task/0a6108ee-0568-4cd3-b174-6326e20bbd8c/)

Criar colunas mediante um processamento mais complexo usando funções personalizadas e o método `apply()`

Se ainda não tiver certeza, você pode revisar a lição em [Criação de colunas categóricas com `apply()`](https://tripleten.com/trainer/data-analyst/lesson/62246252-fd2f-4321-9d2c-54c53bb8527d/task/0a6108ee-0568-4cd3-b174-6326e20bbd8c/)

Usar `apply()` com funções que recebem linhas inteiras do DataFrame como argumento

Se ainda não tiver certeza, você pode revisar a lição em [Criação de categorias com funções de tratamento de linhas](https://tripleten.com/trainer/data-analyst/lesson/b6bdd963-cbfe-4566-8ed7-98bdff832bf4/task/5e8cd413-5542-47db-a80e-96888e4f4f12/)

Você conseguiu!

Incrível! Você arrasou! Vamos continuar aprendendo!

### Leve isso com você

Faça download do sumário do capítulo para que você possa consultá-los quando necessário.

-   [Resumo do capítulo: Engenharia de características](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/PT/8_Resumo_do_captulo_Engenharia_de_caractersticas.pdf)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-28-041Z.md
### Última modificação: 2025-05-28 19:10:28

# Introdução - TripleTen

Capítulo 9/11

Transformações de dados

# Introdução

Boas-vindas ao capítulo sobre transformações de dados!

Neste capítulo, você vai examinar mais profundamente o agrupamento e a combinação de dados de diferentes fontes.

O que exatamente você vai aprender:

-   Como funciona a função `groupby()`;
-   Uma nova maneira de agrupar dados usando tabelas dinâmicas;
-   Como combinar dados de diferentes DataFrames usando concatenação e junção.

Vamos continuar usando o conjunto de dados sobre vendas de videogames para te ajudar a aprender essas técnicas importantes de análise de dados.

Serão necessárias de 2 a 2,5 horas para concluir este capítulo. Vamos lá!

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/9.1.1.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-29-305Z.md
### Última modificação: 2025-05-28 19:10:29

# Processamento de dados agrupados com agg() - TripleTen

Teoria

# Processamento de dados agrupados com agg()

Você já deve ter se familiarizado bastante com o método `groupby()` na pandas. Ele permite reformatar um conjunto de dados agrupando linhas com base em valores em uma ou mais colunas. Nesta lição, vamos revisar `groupby()` e aprender informações mais detalhadas sobre como ele funciona.

Vamos dar mais uma olhada no conjunto de dados de vendas de videogames usando `dropna()` para excluir valores ausentes mais uma vez.

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df.dropna(inplace=True)

print(df.head())
```

```
                    name platform  year_of_release     genre publisher  \
0             Wii Sports      Wii           2006.0    Sports  Nintendo   
2         Mario Kart Wii      Wii           2008.0    Racing  Nintendo   
3      Wii Sports Resort      Wii           2009.0    Sports  Nintendo   
6  New Super Mario Bros.       DS           2006.0  Platform  Nintendo   
7               Wii Play      Wii           2006.0      Misc  Nintendo   

  developer  na_sales  eu_sales  jp_sales  critic_score  user_score  
0  Nintendo     41.36     28.96      3.77          76.0         8.0  
2  Nintendo     15.68     12.76      3.79          82.0         8.3  
3  Nintendo     15.61     10.93      3.28          80.0         8.0  
6  Nintendo     11.28      9.14      6.50          89.0         8.5  
7  Nintendo     13.96      9.18      2.93          58.0         6.6
```

Digamos que queremos obter a pontuação média de críticos para cada gênero:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df.dropna(inplace=True)

mean_score = df.groupby('genre')['critic_score'].mean()
print(mean_score)
```

```
genre
Action          67.852100
Adventure       66.422053
Fighting        69.693931
Misc            67.414508
Platform        70.000000
Puzzle          70.694915
Racing          69.621160
Role-Playing    72.823776
Shooter         70.968894
Simulation      69.983444
Sports          74.110643
Strategy        72.949458
Name: critic_score, dtype: float64
```

O índice do Series `mean_score` é a "chave de `groupby()`", nesse caso, os valores unívocos da coluna `'genre'`. Fazer uma operação `groupby()` altera o índice de linha dos dados para as chaves que usamos para agrupamento.

Para agrupar os dados por várias colunas diferentes, passamos uma lista para o método `groupby()`:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df.dropna(inplace=True)

grp = df.groupby(['platform', 'genre'])
print(grp['critic_score'].mean())
```

```
platform  genre       
3DS       Action          63.727273
          Adventure       68.333333
          Fighting        68.857143
          Misc            69.100000
          Platform        72.444444
                            ...    
XOne      Role-Playing    80.777778
          Shooter         77.903226
          Simulation      53.000000
          Sports          71.935484
          Strategy        70.000000
Name: critic_score, Length: 197, dtype: float64
```

Agora temos a pontuação média de críticos para cada gênero em cada plataforma. Já que agrupamos os dados por duas colunas, o resultado é um objeto Series multi-índice com dois valores de índice para cada pontuação média; nesse caso, `'platform'` e `'genre'`.

Aqui, a variável `grp` é um objeto que contém o DataFrame agrupado antes de processarmos cada grupo com o método `mean()`. Ele é conhecido como um objeto "em espera". Se tentarmos imprimir o objeto `grp`, uma representação textual do objeto será exibida.

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df.dropna(inplace=True)

grp = df.groupby(['platform', 'genre'])
print(grp)
```

```
<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000022EA82A75B0>
```

Quando imprimimos `df.groupby('column_name')`, não vemos a impressão de uma tabela como veríamos ao imprimir `df`. Em vez disso, vemos o tipo de dados do objeto agrupado (`DataFrameGroupBy`) e uma string (`0x0000022EA82A75B0`) representando o local na memória do computador onde o objeto está armazenado. Não há resultado a ser exibido até processarmos os grupos.

### Split-apply-combine

O objeto `DataFrameGroubBy` é uma parte de um framework de processamento de dados chamado **split-apply-combine** (ou seja, "divida-aplique-combine"):

1.  **divida** os dados em grupos
2.  **aplique** uma função de agregação estatística a cada grupo
3.  **combine** os resultados de cada grupo

No código abaixo, ilustramos cada um dos três componentes desse padrão:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df.dropna(inplace=True)

grp = df.groupby(['platform', 'genre'])
mean_scores = grp['critic_score'].mean()
print(mean_scores)
```

```
platform  genre
3DS       Action          63.727273
          Adventure       68.333333
          Fighting        68.857143
          Misc            69.100000
          Platform        72.444444
                            ...
XOne      Role-Playing    80.777778
          Shooter         77.903226
          Simulation      53.000000
          Sports          71.935484
          Strategy        70.000000
Name: critic_score, Length: 197, dtype: float64
```

**Dividimos** os dados em grupos com `df.groupby(['platform', 'genre'])`, **aplicamos** o método `mean()` e **combinamos** o resultado em um objeto Series com `grp['critic_score'].mean()`.

Claro, podemos pular a criação dos objetos `grp` e `mean_scores` e deixar a pandas fazer todos os três passos em uma linha de código:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df.dropna(inplace=True)

print(df.groupby(['platform', 'genre'])['critic_score'].mean())
```

```
platform  genre       
3DS       Action          63.727273
          Adventure       68.333333
          Fighting        68.857143
          Misc            69.100000
          Platform        72.444444
                            ...    
XOne      Role-Playing    80.777778
          Shooter         77.903226
          Simulation      53.000000
          Sports          71.935484
          Strategy        70.000000
Name: critic_score, Length: 197, dtype: float64
```

### O método `agg()`

Até agora, aplicamos apenas uma única função aos grupos. Mas e se quisermos calcular estatísticas de resumo diferentes para colunas diferentes? Como exemplo, a pontuação média dos críticos e o total de vendas japonesas de cada grupo. Podemos fazer isso usando o método `agg()`, que é a abreviação de "aggregate" (agregar).

O método `agg()` usa um dicionário como parâmetro de entrada, no qual as chaves são nomes das colunas e os valores correspondentes são as funções de agregação que você quer aplicar a elas:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df.dropna(inplace=True)

agg_dict = {'critic_score': 'mean', 'jp_sales': 'sum'}

grp = df.groupby(['platform', 'genre'])
print(grp.agg(agg_dict))
```

```
                       critic_score  jp_sales
platform genre                               
3DS      Action           63.727273      6.60
         Adventure        68.333333      0.66
         Fighting         68.857143      0.46
         Misc             69.100000      1.22
         Platform         72.444444      5.94
...                             ...       ...
XOne     Role-Playing     80.777778      0.01
         Shooter          77.903226      0.13
         Simulation       53.000000      0.00
         Sports           71.935484      0.02
         Strategy         70.000000      0.00

[197 rows x 2 columns]
```

Isso é incrível, não é? Dessa forma, você pode até aplicar suas próprias funções personalizadas com `agg()`. Confira:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df.dropna(inplace=True)

def double_it(sales):
    sales = sales.sum() * 2 # multiplica a soma anterior por 2
    return sales

agg_dict = {'jp_sales': double_it}

grp = df.groupby(['platform', 'genre'])
print(grp.agg(agg_dict))
```

```
                                                    jp_sales
platform genre
3DS      Action           13.20
         Adventure         1.32
         Fighting          0.92
         Misc              2.44
         Platform         11.88
...                        ... 
XOne     Role-Playing      0.02
         Shooter           0.26
         Simulation        0.00
         Sports            0.04
         Strategy          0.00

[197 rows x 1 columns]
```

### Recapitulação

Vamos lembrar de tudo o que vimos nesta lição:

-   Método `groupby()`: usado para agrupar dados de acordo com uma ou mais colunas em um DataFrame
-   `grp = df.groupby(['platform', 'genre'])` agrupa por duas colunas ao mesmo tempo
-   Método `agg()`: usado para computar estatísticas agregadas para cada grupo em um DataFrame agrupado. Passe um dicionário para ele com itens `'column':'function'`
-   Framework de divisão-aplicação-combinação (split-apply-combine): um framework de processamento de dados que descreve o processo de dividir um conjunto de dados em grupos, aplicar uma função a cada grupo e, em seguida, combinar os resultados de volta em uma única estrutura de dados
-   Método `mean()`: usado para calcular a média de um conjunto de valores
-   Método `sum()`: usado para calcular a soma de um conjunto de valores
-   Método `dropna()`: usado para remover valores ausentes (NaN) de um DataFrame
-   Objeto `DataFrameGroupBy`: possui o DataFrame agrupado antes de ser processado com uma função de agregação

Processamento de dados agrupados com agg()

Tarefa

É hora de conferir mais de perto as vendas de videogames de cada gênero.

O pré-código cria uma coluna `'total_sales'` como você fez antes. Você vai usar essas colunas, então preste atenção nesses nomes.

O pré-código agrupa o DataFrame `df` pela coluna `'genre'` e atribui o objeto agrupado resultante à variável `grp`.

E agora você vai:

-   Criar um dicionário para calcular para cada gênero:
    -   Soma das vendas totais
    -   Média de vendas na América do Norte (NA)
    -   Média de vendas na Europa (EU)
    -   Média de vendas no Japão (JP)
-   Atribuir o dicionário a uma variável chamada `agg_dict` com as tuplas descritas acima.
-   Atribuir o resultado de `agg()` a uma variável chamada `genre`.
-   Imprimir `genre`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

df\['total\_sales'\] \= df\['na\_sales'\] + df\['eu\_sales'\] + df\['jp\_sales'\]

  

grp \= df.groupby('genre')

  

agg\_dict \= {'total\_sales':'sum', 'na\_sales':'mean', 'eu\_sales':'mean', 'jp\_sales':'mean', }

  

  

genre \= grp.agg(agg\_dict)

  

print(genre)

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-31-322Z.md
### Última modificação: 2025-05-28 19:10:31

# Tabelas dinâmicas - TripleTen

Teoria

# Tabelas dinâmicas

Esperamos já ter conseguido convencer você de que `groupby()` é uma excelente ferramenta para agrupar dados e realizar análises mais complexas. Embora seja possível usar `groupby()` para calcular as propriedades agregadas dos dados, a pandas também oferece tabelas dinâmicas como um método alternativo para agrupar e analisar dados.

As tabelas dinâmicas são uma ótima ferramenta para sintetizar conjuntos de dados e explorar suas diferentes dimensões. Elas são muito populares em aplicativos de planilha como o Excel, mas é ainda mais eficiente criá-las de forma programática na pandas.

Vamos dar mais uma olhada nos dados de vendas de videogames abaixo:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
print(df.head())
```

```
                       name platform  year_of_release         genre publisher  \
0                Wii Sports      Wii           2006.0        Sports  Nintendo   
1         Super Mario Bros.      NES           1985.0      Platform  Nintendo   
2            Mario Kart Wii      Wii           2008.0        Racing  Nintendo   
3         Wii Sports Resort      Wii           2009.0        Sports  Nintendo   
4  Pokemon Red/Pokemon Blue       GB           1996.0  Role-Playing  Nintendo   

  developer  na_sales  eu_sales  jp_sales  critic_score  user_score  
0  Nintendo     41.36     28.96      3.77          76.0         8.0  
1       NaN     29.08      3.58      6.81           NaN         NaN  
2  Nintendo     15.68     12.76      3.79          82.0         8.3  
3  Nintendo     15.61     10.93      3.28          80.0         8.0  
4       NaN     11.27      8.89     10.22           NaN         NaN
```

Suponha que queremos determinar o total de vendas europeias para cada gênero em cada plataforma. As tabelas dinâmicas fornecem um método rápido e conveniente para isso. Primeiro, vamos examinar o código e depois analisá-lo. Para ficar mais simples, removeremos as linhas com valores ausentes.

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df.dropna(inplace=True)

pivot_data = df.pivot_table(index='genre',
                            columns='platform',
                            values='eu_sales',
                            aggfunc='sum'
                           )
print(pivot_data)
print()
print(type(pivot_data))
```

```
platform       3DS    DC     DS    GBA    GC     PC     PS    PS2    PS3  \
genre                                                                      
Action        8.41   NaN  11.25   5.95  6.11  15.32  21.15  63.97  94.17   
Adventure     0.46  0.24   1.17   1.07  1.01   1.58   0.32   3.27   6.34   
Fighting      0.84  0.00   0.28   0.85  2.73   0.11   5.63  15.46  13.69   
Misc          1.21   NaN  25.10   2.19  1.90   1.19   1.69   9.73   7.62   
Platform      8.41  0.00  13.00  10.62  5.19   0.33   6.09  17.11   7.26   
Puzzle        1.02   NaN  15.72   1.34  0.74   0.13   0.10   1.11   0.04   
Racing        4.45  0.00   8.43   3.43  2.13   2.59  13.03  36.59  28.09   
Role-Playing  3.10  0.00   6.00   3.96  2.36  24.32   8.70  16.33  16.81   
Shooter       0.27  0.00   0.43   0.38  2.79  18.28   2.31  31.38  65.43   
Simulation    4.40  0.00  12.21   0.56  1.74  22.35   0.44  10.08   2.71   
Sports        0.56  0.05   0.94   1.52  3.75   6.47   5.66  47.85  30.62   
Strategy      0.20   NaN   1.29   0.73  0.48  14.81   0.10   2.26   1.04   

platform        PS4    PSP   PSV    Wii  WiiU   X360     XB   XOne  
genre                                                               
Action        35.66  13.08  2.79  21.87  4.81  64.30   9.82  10.11  
Adventure      1.55   0.85  0.56   2.43  0.05   3.50   0.46   0.76  
Fighting       2.40   2.10  0.44   4.86  0.34   8.17   2.13   0.48  
Misc           1.15   0.83  0.31  47.91  3.39  15.13   0.80   0.99  
Platform       2.84   3.02  1.22  21.13  6.27   3.58   2.21   0.28  
Puzzle         0.02   0.76  0.03   2.61  0.38   0.02   0.02    NaN  
Racing         5.49   9.30  0.44  17.47  2.15  21.14   6.45   4.11  
Role-Playing   8.01   3.47  1.06   1.88  0.42  17.32   2.83   2.75  
Shooter       28.83   5.12  1.49   4.49  1.57  70.02  14.56  14.03  
Simulation     0.47   1.79   NaN   6.21  0.04   3.56   1.31   0.01  
Sports        25.81   7.46  0.43  73.08  0.68  25.74   9.23   8.65  
Strategy       0.21   1.35   NaN   0.20  0.32   2.04   0.42   0.06  

<class 'pandas.core.frame.DataFrame'>
```

Criamos uma tabela dinâmica usando o método `pivot_table()`. Os seguintes parâmetros foram usados:

-   `index=` — a coluna cujos valores se tornam o índice na tabela dinâmica
-   `columns=` — a coluna cujos valores se tornam colunas na tabela dinâmica
-   `values=` — a coluna cujos valores queremos agregar na tabela dinâmica
-   `aggfunc=` — a função de agregação que queremos aplicar a valores em cada grupo linha-coluna

Cada célula na tabela dinâmica acima contém o total de vendas na Europa para uma determinada combinação de um gênero e uma plataforma. Também imprimimos o tipo de dados da tabela dinâmica para mostrar que ela é um DataFrame da pandas, que você já conhece.

Usar uma tabela dinâmica é conveniente aqui porque nos permite excluir facilmente todas as colunas do `df` que não interessam para a análise. Pode ser mais fácil também para as pessoas lerem do que o resultado equivalente do uso de `groupby()`, como você pode ver abaixo.

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
df.dropna(inplace=True)

groupby_data = df.groupby(['genre', 'platform'])['eu_sales'].mean()
print(groupby_data)
print()
print(type(groupby_data))
```

```
genre         platform
Action        3DS         0.152909
              DS          0.150000
              GBA         0.116667
              GC          0.088551
              PC          0.115188
              PS          0.783333
              PS2         0.263251
              PS3         0.409435
              PS4         0.424524
              PSP         0.176757
              PSV         0.075405
              Wii         0.179262
              WiiU        0.123333
              X360        0.274786
              XB          0.082521
              XOne        0.198235
Adventure     3DS         0.051111
              DC          0.120000
              DS          0.037742
              GBA         0.118889
              GC          0.077692
              PC          0.043889
              PS          0.064000
              PS2         0.081750
              PS3         0.317000
              PS4         0.110714
              PSP         0.065385
              PSV         0.056000
              Wii         0.110455
              WiiU        0.050000
              X360        0.233333
              XB          0.025556
              XOne        0.152000
Fighting      3DS         0.120000
              DC          0.000000
              DS          0.020000
              GBA         0.065385
              GC          0.113750
              PC          0.036667
              PS          0.375333
              PS2         0.211781
              PS3         0.240175
              PS4         0.218182
              PSP         0.070000
              PSV         0.048889
              Wii         0.186923
              WiiU        0.113333
              X360        0.145893
              XB          0.068710
              XOne        0.096000
Misc          3DS         0.121000
              DS          0.660526
              GBA         0.182500
              GC          0.118750
              PC          0.396667
              PS          0.422500
              PS2         0.180185
              PS3         0.185854
              PS4         0.143750
              PSP         0.039524
              PSV         0.038750
              Wii         0.684429
              WiiU        0.282500
              X360        0.244032
              XB          0.053333
              XOne        0.082500
Platform      3DS         0.467222
              DC          0.000000
              DS          0.371429
              GBA         0.221250
              GC          0.117955
              PC          0.033000
              PS          0.380625
              PS2         0.244429
              PS3         0.330000
              PS4         0.258182
              PSP         0.151000
              PSV         0.203333
              Wii         0.586944
              WiiU        0.418000
              X360        0.170476
              XB          0.085000
              XOne        0.070000
Puzzle        3DS         0.127500
              DS          0.302308
              GBA         0.223333
              GC          0.123333
              PC          0.032500
              PS          0.050000
              PS2         0.185000
              PS3         0.040000
              PS4         0.020000
              PSP         0.069091
              PSV         0.030000
              Wii         0.186429
              WiiU        0.126667
              X360        0.010000
              XB          0.020000
Racing        3DS         0.890000
              DC          0.000000
              DS          0.468333
              GBA         0.163333
              GC          0.068710
              PC          0.057556
              PS          0.685789
              PS2         0.279313
              PS3         0.445873
              PS4         0.366000
              PSP         0.238462
              PSV         0.088000
              Wii         0.582333
              WiiU        2.150000
              X360        0.297746
              XB          0.084868
              XOne        0.293571
Role-Playing  3DS         0.119231
              DC          0.000000
              DS          0.067416
              GBA         0.127742
              GC          0.107273
              PC          0.293012
              PS          0.310714
              PS2         0.139573
              PS3         0.215513
              PS4         0.333750
              PSP         0.049571
              PSV         0.034194
              Wii         0.072308
              WiiU        0.210000
              X360        0.298621
              XB          0.166471
              XOne        0.305556
Shooter       3DS         0.135000
              DC          0.000000
              DS          0.015926
              GBA         0.029231
              GC          0.069750
              PC          0.140615
              PS          0.192500
              PS2         0.247087
              PS3         0.540744
              PS4         0.847941
              PSP         0.170667
              PSV         0.298000
              Wii         0.154828
              WiiU        0.314000
              X360        0.454675
              XB          0.136075
              XOne        0.452581
Simulation    3DS         0.488889
              DC          0.000000
              DS          0.330000
              GBA         0.070000
              GC          0.174000
              PC          0.272561
              PS          0.088000
              PS2         0.224000
              PS3         0.150556
              PS4         0.235000
              PSP         0.223750
              Wii         0.238846
              WiiU        0.040000
              X360        0.127143
              XB          0.062381
              XOne        0.010000
Sports        3DS         0.140000
              DC          0.050000
              DS          0.042727
              GBA         0.080000
              GC          0.056818
              PC          0.202187
              PS          0.353750
              PS2         0.244133
              PS3         0.266261
              PS4         0.629512
              PSP         0.124333
              PSV         0.071667
              Wii         1.001096
              WiiU        0.113333
              X360        0.183857
              XB          0.076281
              XOne        0.279032
Strategy      3DS         0.066667
              DS          0.047778
              GBA         0.121667
              GC          0.068571
              PC          0.118480
              PS          0.025000
              PS2         0.059474
              PS3         0.130000
              PS4         0.052500
              PSP         0.096429
              Wii         0.033333
              WiiU        0.320000
              X360        0.107368
              XB          0.032308
              XOne        0.030000
Name: eu_sales, dtype: float64

<class 'pandas.core.series.Series'>
```

É um formato bem diferente, não é? Observe também que o resultado de `groupby()` retorna um objeto Series, enquanto `pivot_table()` retorna um DataFrame. Escolher usar `groupby()` ou `pivot_table()` é uma questão de preferência pessoal, e com o tempo você vai desenvolver uma intuição sobre qual ferramenta é a mais conveniente para cada tarefa.

Tabelas dinâmicas

Tarefa

Filtramos o conjunto de dados de videogames de modo que ele contenha apenas os jogos lançados a partir de 2000. Crie uma tabela dinâmica a partir do conjunto de dados filtrado que possui o valor médio das vendas no Japão para cada combinação de gênero e ano de lançamento.

-   Os gêneros são o índice
-   As colunas da tabela dinâmica são os anos de lançamento
-   Use a coluna apropriada como os valores a serem agregados
-   Use a função de agregação apropriada

Atribua o resultado à variável chamada `df_pivot` e depois a imprima.

99

1

2

3

4

5

6

7

8

9

10

11

12

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

df \= df\[df\['year\_of\_release'\] \>= 2000\]

  

  

df\_pivot \= df.pivot\_table(index\='genre',columns\='year\_of\_release', values\='jp\_sales',aggfunc\='mean')

  

  

  

print(df\_pivot)

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-32-663Z.md
### Última modificação: 2025-05-28 19:10:33

# Combinação de DataFrames com concat() - TripleTen

Teoria

# Combinação de DataFrames com concat()

Nesta lição, vamos continuar usando o conjunto de dados de vendas de videogames. Como lembrete de como é a estrutura dele, aqui estão as primeiras linhas:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')
print(df.head())
```

```
                       name platform  year_of_release         genre publisher  \
0                Wii Sports      Wii           2006.0        Sports  Nintendo   
1         Super Mario Bros.      NES           1985.0      Platform  Nintendo   
2            Mario Kart Wii      Wii           2008.0        Racing  Nintendo   
3         Wii Sports Resort      Wii           2009.0        Sports  Nintendo   
4  Pokemon Red/Pokemon Blue       GB           1996.0  Role-Playing  Nintendo   

  developer  na_sales  eu_sales  jp_sales  critic_score  user_score  
0  Nintendo     41.36     28.96      3.77          76.0         8.0  
1       NaN     29.08      3.58      6.81           NaN         NaN  
2  Nintendo     15.68     12.76      3.79          82.0         8.3  
3  Nintendo     15.61     10.93      3.28          80.0         8.0  
4       NaN     11.27      8.89     10.22           NaN         NaN
```

Queremos saber algumas estatísticas gerais sobre distribuidoras de jogos:

-   A pontuação média dada a elas por críticos
-   O total de vendas

Como você já viu, podemos fazer isso usando `groupby()`. Primeiro, vamos obter a pontuação média de avaliações de cada distribuidora:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

mean_score = df.groupby('publisher')['critic_score'].mean()
print(mean_score)
```

```
publisher
10TACLE Studios                 42.000000
1C Company                      73.000000
20th Century Fox Video Games          NaN
2D Boy                          90.000000
3DO                             57.470588
                                  ...    
id Software                     85.000000
imageepoch Inc.                       NaN
inXile Entertainment            81.000000
mixi, Inc                             NaN
responDESIGN                          NaN
Name: critic_score, Length: 581, dtype: float64
```

Vamos também obter o número de vendas. A maneira mais simples de fazer isso é usar mais um `groupby()`:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

df['total_sales'] = df['na_sales'] + df['eu_sales'] + df['jp_sales']
num_sales = df.groupby('publisher')['total_sales'].sum()
print(num_sales)
```

```
publisher
10TACLE Studios                 0.11
1C Company                      0.08
20th Century Fox Video Games    1.92
2D Boy                          0.03
3DO                             9.52
                                ... 
id Software                     0.02
imageepoch Inc.                 0.04
inXile Entertainment            0.09
mixi, Inc                       0.87
responDESIGN                    0.13
Name: total_sales, Length: 581, dtype: float64
```

Observe que o índice para ambos os resultados é a coluna `'publisher'`, porque agrupamos por `'publisher'` em ambos os casos. Já que ambos os resultados têm o mesmo índice, podemos juntar os resultados em um DataFrame com facilidade usando a função `concat()` da pandas:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

mean_score = df.groupby('publisher')['critic_score'].mean()

df['total_sales'] = df['na_sales'] + df['eu_sales'] + df['jp_sales']
num_sales = df.groupby('publisher')['total_sales'].sum()

df_concat = pd.concat([mean_score, num_sales], axis='columns')
print(df_concat)
```

```
                              critic_score  total_sales
publisher                                              
10TACLE Studios                  42.000000         0.11
1C Company                       73.000000         0.08
20th Century Fox Video Games           NaN         1.92
2D Boy                           90.000000         0.03
3DO                              57.470588         9.52
...                                    ...          ...
id Software                      85.000000         0.02
imageepoch Inc.                        NaN         0.04
inXile Entertainment             81.000000         0.09
mixi, Inc                              NaN         0.87
responDESIGN                           NaN         0.13

[581 rows x 2 columns]
```

Em geral, `concat()` espera uma lista de objetos Serie e/ou DataFrame. Para obter esse resultado, passamos uma lista de variáveis Series para `concat()` e definimos `axis='columns'` para garantir que elas foram combinadas como colunas.

Observe que os nomes originais das colunas foram mantidos no DataFrame concatenado.

Podemos renomear colunas com o método `columns`. Ele pode ser chamado em um DataFrame e passar uma lista de novos nomes de coluna para substituir os existentes. Os novos nomes devem ser passados na mesma ordem que os nomes das colunas originais.

Vamos renomear a coluna `'critic_score'` já que agora ela representa uma média:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

mean_score = df.groupby('publisher')['critic_score'].mean()

df['total_sales'] = df['na_sales'] + df['eu_sales'] + df['jp_sales']
num_sales = df.groupby('publisher')['total_sales'].sum()

df_concat = pd.concat([mean_score, num_sales], axis='columns')
df_concat.columns = ['avg_critic_score', 'total_sales']
print(df_concat)
```

```
                              avg_critic_score  total_sales
publisher                                                  
10TACLE Studios                      42.000000         0.11
1C Company                           73.000000         0.08
20th Century Fox Video Games               NaN         1.92
2D Boy                               90.000000         0.03
3DO                                  57.470588         9.52
...                                        ...          ...
id Software                          85.000000         0.02
imageepoch Inc.                            NaN         0.04
inXile Entertainment                 81.000000         0.09
mixi, Inc                                  NaN         0.87
responDESIGN                               NaN         0.13

[581 rows x 2 columns]
```

Em geral, é uma boa ideia renomear colunas após elas serem agrupadas e processadas para que os nomes descrevam melhor como elas foram processadas.

Você deve ter notado que podemos obter o mesmo resultado acima usando `agg()`. No entanto, `concat()` é bastante versátil. Podemos usá-lo para concatenar DataFrames juntos:

-   Por linhas, assumindo que eles possuem o mesmo número de colunas
-   Por colunas, se eles tiverem o mesmo número de linhas

Para concatenar linhas de DataFrames diferentes, podemos usar `concat()` e definir `axis='index'` (ou excluir o parâmetro, já que `axis='index'` é o argumento padrão). Como alternativa, podemos usar números inteiros para o argumento `index=`, onde `index=0` concatena em linha e `index=1`, em coluna.

Aqui está um exemplo onde filtramos os dados para obter dois DataFrames diferentes de acordo com gênero e depois os recombinamos em um DataFrame:

```
import pandas as pd

df = pd.read_csv('/datasets/vg_sales.csv')

rpgs = df[df['genre'] == 'Role-Playing']
platformers = df[df['genre'] == 'Platform']

df_concat = pd.concat([rpgs, platformers])
print(df_concat[['name', 'genre']])
```

```
                                                   name         genre
4                              Pokemon Red/Pokemon Blue  Role-Playing
12                          Pokemon Gold/Pokemon Silver  Role-Playing
20                        Pokemon Diamond/Pokemon Pearl  Role-Playing
25                        Pokemon Ruby/Pokemon Sapphire  Role-Playing
27                          Pokemon Black/Pokemon White  Role-Playing
...                                                 ...           ...
16356                                    Strider (2014)      Platform
16358                                Goku Makaimura Kai      Platform
16603  The Land Before Time: Into the Mysterious Beyond      Platform
16710                Woody Woodpecker in Crazy Castle 5      Platform
16715                                  Spirits & Spells      Platform
```

Pronto, os dois DataFrames foram costurados em um! Lembre-se de que isso funciona aqui porque os dois DataFrames pequenos têm as mesmas colunas.

Combinação de DataFrames com concat()

Tarefa2 / 2

1.

Já lemos os dados para você, criamos uma coluna `'total_sales'` e calculamos o total de vendas para cada plataforma na variável `total_sales`.

Sua tarefa é calcular o número total de distribuidoras que criaram um jogo em cada plataforma usando `nunique()`. Atribua o resultado à variável chamada `num_pubs`, depois a imprima.

2.

Combine `total_sales` e `num_pubs` por colunas em um DataFrame chamado `platforms` usando `concat()`. Altere os nomes das colunas em `platforms` para `'total_sales'` e `'num_publishers'`, respectivamente, e imprima `platforms`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

import pandas as pd

  

df \= pd.read\_csv('/datasets/vg\_sales.csv')

df\['total\_sales'\] \= df\['na\_sales'\] + df\['eu\_sales'\] + df\['jp\_sales'\]

  

total\_sales \= df.groupby('platform')\['total\_sales'\].sum()

num\_pubs \= df.groupby('platform')\['publisher'\].nunique()

  

platforms \= pd.concat(\[total\_sales, num\_pubs\], axis\=1)

  

  

platforms.columns \= \['total\_sales', 'num\_publishers'\]

  

print(platforms)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-33-964Z.md
### Última modificação: 2025-05-28 19:10:34

# Combinação de DataFrames com merge() - TripleTen

Teoria

# Combinação de DataFrames com merge()

Você acabou de aprender como combinar DataFrames concatenando-os por linhas ou colunas com `concat()`.

Concatenar DataFrames preserva o número total de dados. Por exemplo, combinar um DataFrame que tem duas colunas e três linhas com outro que tem as mesmas duas colunas e cinco linhas vai resultar em um DataFrame com duas colunas e oito linhas. O número total de células antes e depois da concatenação é dezesseis.

Nesta lição, você vai aprender como combinar DataFrames usando o método `merge()` de forma que afete a quantidade de dados com os quais você está trabalhando.

Considere o seguinte exemplo: dois estudantes de literatura concordam que um deles vai escrever metade da lista de leitura do quadro enquanto o outro assiste ao YouTube. Depois, o primeiro vai até a lanchonete da escola enquanto o segundo copia o resto da lista. Por fim, eles vão juntar as listas. Trabalho em equipe! Vamos ver como ficou:

```
import pandas as pd

first_pupil_df = pd.DataFrame(
    {
        'author': ['Alcott', 'Fitzgerald', 'Steinbeck', 'Twain', 'Hemingway'],
        'title': ['Little Women',
                  'The Great Gatsby',
                  'Of Mice and Men',
                  'The Adventures of Tom Sawyer',
                  'The Old Man and the Sea'
                 ],
    }
)
second_pupil_df = pd.DataFrame(
    {
        'author': ['Steinbeck', 'Twain', 'Hemingway', 'Salinger', 'Hawthorne'],
        'title': ['East of Eden',
                  'The Adventures of Huckleberry Finn',
                  'For Whom the Bell Tolls',
                  'The Catcher in the Rye',
                  'The Scarlett Letter'
                 ],
    }
)

print(first_pupil_df)
print()
print(second_pupil_df)
```

```
       author                         title
0      Alcott                  Little Women
1  Fitzgerald              The Great Gatsby
2   Steinbeck               Of Mice and Men
3       Twain  The Adventures of Tom Sawyer
4   Hemingway       The Old Man and the Sea

      author                               title
0  Steinbeck                        East of Eden
1      Twain  The Adventures of Huckleberry Finn
2  Hemingway             For Whom the Bell Tolls
3   Salinger              The Catcher in the Rye
4  Hawthorne                 The Scarlett Letter
```

### Junção interna (inner merge)

Vamos usar o método `merge()` para combinar entradas com os mesmos autores. O nome da coluna na qual vai ocorrer a junção é passado para o parâmetro `on=`, nesse caso o `'author'`:

```
both_pupils = first_pupil_df.merge(second_pupil_df, on='author')
print(both_pupils) 
```

```
      author                       title_x                             title_y
0  Steinbeck               Of Mice and Men                        East of Eden
1      Twain  The Adventures of Tom Sawyer  The Adventures of Huckleberry Finn
2  Hemingway       The Old Man and the Sea             For Whom the Bell Tolls
```

O resultado contém apenas os autores que estão presentes em **ambos** DataFrames originais.

O novo DataFrame inclui todas as colunas dos DataFrames originais, mas apenas as linhas com autores compartilhados são mantidas. Como os dois DataFrames originais possuem uma coluna chamada `'title'`, a pandas adicionou os sufixos `_x` e `_y` para diferenciá-las no DataFrame unido. Observe que o DataFrame unido tem apenas 9 células, em comparação com as 20 células dos DataFrames originais: a quantidade de dados mudou.

Esse modo de junção é chamado de **junção interna**. Há outros tipos de junção que podem ser especificados usando o parâmetro `how=` em `merge()`. `'inner'` é o argumento padrão para `how=`, então não precisamos incluí-lo acima.

### Junção externa (outer merge)

A diferença entre **junção externa** e a interna é que, no caso da externa, _todos_ os valores na coluna especificada são mantidos a partir de ambos os DataFrames originais, mas o novo DataFrame tem valores ausentes nos lugares onde não há correspondências. É melhor ilustrar isso com um exemplo:

```
both_pupils = first_pupil_df.merge(second_pupil_df, on='author', how='outer')
print(both_pupils)
```

```
       author                       title_x  \
0      Alcott                  Little Women   
1  Fitzgerald              The Great Gatsby   
2   Steinbeck               Of Mice and Men   
3       Twain  The Adventures of Tom Sawyer   
4   Hemingway       The Old Man and the Sea   
5    Salinger                           NaN   
6   Hawthorne                           NaN   

                              title_y  
0                                 NaN  
1                                 NaN  
2                        East of Eden  
3  The Adventures of Huckleberry Finn  
4             For Whom the Bell Tolls  
5              The Catcher in the Rye  
6                 The Scarlett Letter
```

Há 7 autores únicos em ambos os DataFrames originais; cada um deles é representado por uma linha no DataFrame unido. Para os autores no primeiro DataFrame que não se encontram no segundo (no caso, `'Alcott'` e `'Fitzgerald'`), há valores `NaN` na coluna que veio do segundo DataFrame ( `'title_y'`) e vice-versa. Observe também que agora temos 21 células de dados.

### Junção à esquerda (left merge)

O último tipo de junção com merge que gostaríamos de discutir é a **junção à esquerda**, que podemos fazer passando `how='left'` para `merge()`. Em uma junção à esquerda, todos os valores do DataFrame esquerdo (aquele no qual nós chamamos `merge()`) estão presentes no DataFrame unido. Os valores do DataFrame direito (aquele que passamos como entrada para `merge()`) só são mantidos se tiverem correspondência aos valores da coluna especificada no DataFrame esquerdo. É melhor explicar usando um exemplo:

```
both_pupils = first_pupil_df.merge(second_pupil_df, on='author', how='left')
print(both_pupils)
```

```
       author                       title_x  \
0      Alcott                  Little Women   
1  Fitzgerald              The Great Gatsby   
2   Steinbeck               Of Mice and Men   
3       Twain  The Adventures of Tom Sawyer   
4   Hemingway       The Old Man and the Sea   

                              title_y  
0                                 NaN  
1                                 NaN  
2                        East of Eden  
3  The Adventures of Huckleberry Finn  
4             For Whom the Bell Tolls
```

Como você pode ver, todos os autores e títulos do primeiro estudante estão presentes no DataFrame unido, enquanto as linhas com `'Salinger'` e `'Hawthorne'` do segundo estudante não se encontram aqui, porque esses autores não estavam no DataFrame do primeiro estudante.

A junção à esquerda contém 15 células de dados, o que é diferente do número de células original e do número nas outras junções que fizemos.

Observe que existe também uma junção à direita (`how='right'`). No entanto, ela funciona de forma idêntica a uma junção à esquerda, exceto que o DataFrame unido mantém todos os valores do DataFrame direito em vez do esquerdo. O mesmo resultado pode ser obtido executando uma junção à esquerda e alternando a ordem dos DataFrames.

Para entender melhor, aqui está um diagrama de Venn que ilustra todas as opções de junção que discutimos:

![Untitled](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_2/PT/9_5_PT.png)

### Consideração sobre os nomes das colunas

Há dois aspectos sobre todas as junções que fizemos até agora que precisamos abordar:

1.  Os sufixos `_x` e `_y` foram adicionados aos nomes das colunas `'title'` no DataFrame unido
2.  A coluna na qual fizemos a junção tinha o mesmo nome em ambos os DataFrames — `'author'`

Ao unir DataFrames na pandas, é importante garantir que as colunas não tenham os mesmos nomes. Se for o caso, a pandas vai adicionar automaticamente os sufixos `_x` e `_y`. No entanto, esses sufixos não são muito descritivos. Para definir sufixos melhores, passe uma lista de strings de sufixos para o parâmetro `suffixes=` em `merge()`:

```
both_pupils = first_pupil_df.merge(second_pupil_df,
                                   on='author',
                                   suffixes=['_1st_student', '_2nd_student']
                                  )
print(both_pupils)
```

```
      author             title_1st_student                   title_2nd_student
0  Steinbeck               Of Mice and Men                        East of Eden
1      Twain  The Adventures of Tom Sawyer  The Adventures of Huckleberry Finn
2  Hemingway       The Old Man and the Sea             For Whom the Bell Tolls
```

Os nomes das colunas agora indicam explicitamente de onde as colunas vieram. É sempre uma boa prática usar nomes de colunas descritivos como esse. Observe que a primeira string na lista de sufixos é anexada ao nome da coluna do DataFrame esquerdo e a segunda string é anexada ao DataFrame direito.

Quanto ao segundo ponto, nem sempre é o caso que as colunas em que você quer fazer junção têm o mesmo nome. Você poderia mudar os nomes para serem iguais antes de junção, mas isso pode causar confusão.

Em vez disso, a função `merge()` tem os parâmetros `left_on=` e `right_on=`, que você pode usar em vez de `on=` se as colunas tiverem nomes diferentes. Para ilustrar como isso funciona, vamos recriar os DataFrames para que um tenha uma coluna `'authors'` e o outro tenha uma coluna `'author'`:

```
import pandas as pd

first_pupil_df = pd.DataFrame(
    {
        'authors': ['Alcott', 'Fitzgerald', 'Steinbeck', 'Twain', 'Hemingway'],
        'title': ['Little Women',
                  'The Great Gatsby',
                  'Of Mice and Men',
                  'The Adventures of Tom Sawyer',
                  'The Old Man and the Sea'
                 ],
    }
)
second_pupil_df = pd.DataFrame(
    {
        'author': ['Steinbeck', 'Twain', 'Hemingway', 'Salinger', 'Hawthorne'],
        'title': ['East of Eden',
                  'The Adventures of Huckleberry Finn',
                  'For Whom the Bell Tolls',
                  'The Catcher in the Rye',
                  'The Scarlett Letter'
                 ],
    }
)

both_pupils = first_pupil_df.merge(second_pupil_df,
                                   left_on='authors',
                                   right_on='author'
                                  )
print(both_pupils)
```

```
     authors                       title_x     author  \
0  Steinbeck               Of Mice and Men  Steinbeck   
1      Twain  The Adventures of Tom Sawyer      Twain   
2  Hemingway       The Old Man and the Sea  Hemingway   

                              title_y  
0                        East of Eden  
1  The Adventures of Huckleberry Finn  
2             For Whom the Bell Tolls
```

Dessa vez, a pandas vai resultar em `'authors'` e `'author'`, mas eles estarão alinhados e bem posicionados para que você possa entender os resultados e como eles se relacionam.

### O método `drop()`

Agora temos o resultado da junção interna, que contém algumas informações duplicadas porque tanto `'author'` quanto `'authors'` foram retidos dos DataFrames originais, como vimos anteriormente.

Se quisermos remover a informação duplicada, podemos usar o método `drop()` com `axis='columns'` para indicar que queremos remover uma coluna e não uma linha:

```
both_pupils = first_pupil_df.merge(second_pupil_df,
                                   left_on='authors',
                                   right_on='author'
                                  )
print(both_pupils.drop('author', axis='columns'))
```

```
     authors                       title_x                             title_y
0  Steinbeck               Of Mice and Men                        East of Eden
1      Twain  The Adventures of Tom Sawyer  The Adventures of Huckleberry Finn
2  Hemingway       The Old Man and the Sea             For Whom the Bell Tolls
```

### Recapitulação

-   Junção interna: mantém apenas as linhas com valores compartilhados na coluna especificada de ambos os DataFrames originais. É feita com `how='inner'` (argumento padrão).
-   Junção externa: mantém todos os valores na coluna especificada de ambos os DataFrames originais, com valores ausentes onde não há correspondência. É feita com `how='outer'`.
-   Junção à esquerda: mantém todos os valores do DataFrame esquerdo, com valores do DataFrame direito mantidos apenas para valores que correspondem à coluna especificada no esquerdo. É feita com `how='left'`.
-   Junção à direita: mantém todos os valores do DataFrame direito, com valores do DataFrame esquerdo mantidos apenas para valores que correspondem à coluna especificada no direito. É feita com `how='right'`.
-   Método `drop()`: remove uma coluna de um DataFrame usando o parâmetro `axis='columns'`.

Agora é sua vez de testar suas habilidades de junção nas tarefas com um novo conjunto de dados.

Combinação de DataFrames com merge()

Tarefa2 / 2

1.

Temos dois conjuntos de dados, `df_orders` e `df_members`, lidos em DataFrames no pré-código.

Cada linha da tabela `df_orders` representa um pedido de serviço. A tabela inclui uma coluna `'user_id'` que registra qual cliente fez cada pedido e uma coluna `'id'` que identifica o pedido.

Cada linha na tabela `df_members` representa um cliente e inclui uma coluna `'id'` que o identifica exclusivamente.

Sua tarefa é juntar as duas tabelas de forma que você fique apenas com os clientes que realmente fizeram pedidos.

-   Escolha o tipo certo de junção
-   Faça a junção de `df_members` como o DataFrame à esquerda, usando a coluna de id do usuário
-   Faça a junção de `df_orders` como o DataFrame à direita, usando a coluna de id do usuário, não o id do pedido
-   Inclua os sufixos `'_member'` (à esquerda) e `'_order'` (à direita)
-   Atribua o resultado da junção a uma variável chamada `df_merged`
-   Imprima `df_merged`
-   Não remova colunas com drop por enquanto

2.

Vamos limpar um pouco as coisas.

-   Elimine a coluna duplicada (nesse caso, `'user_id'`)
-   Atribua o resultado de volta ao DataFrame `df_merged`
-   Imprima o DataFrame unido

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

import pandas as pd

  

df\_members \= pd.read\_csv('/datasets/new\_members.csv')

df\_orders \= pd.read\_csv('/datasets/recent\_orders.csv')

  

df\_merged \= df\_members.merge(df\_orders,

left\_on\='id',

right\_on\='user\_id',

suffixes\=\['\_member', '\_order'\])

  

df\_merged \= df\_merged.drop('user\_id', axis\='columns')

  

  

  

  

print(df\_merged)

  

  

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-36-423Z.md
### Última modificação: 2025-05-28 19:10:36

# Quiz do capítulo - TripleTen

Capítulo 9/11

Transformações de dados

# Quiz do capítulo

Pergunta

Considerando o conjunto de dados de vendas de jogos, o que faz `df.groupby(['platform'])`?

Agrupa a coluna `'platform'` por valores de índice.

Isso divide o DataFrame em diferentes grupos de acordo com valores distintos na coluna `platform`.

A operação `groupby()` altera o índice de linha dos dados para os parâmetros pelos quais estamos agrupando.

Categoriza a coluna `platform` conforme os valores dentro da coluna.

Fantástico!

Pergunta

Precisamos calcular a pontuação média aritmética dos críticos para cada gênero. Quais soluções são corretas?

Escolha quantas quiser

`df.groupby('genre').apply(['critic_score'].mean())`

`df.groupby('genre')['critic_score'].mean()`

Isso calcula a média aritmética para cada gênero.

`df.groupby('genre').agg(critic_score=('critic_score', 'mean'))`

Isso calcula a média aritmética para cada gênero. É provável que essa seja a mais conveniente e transparente entre todas as soluções válidas, sintaticamente.

`df.groupby('genre').mean('critic_score')`

Você conseguiu!

Pergunta

Como `pivot_table()` e `groupby()` (com uma função de agregação) são diferentes?

Podemos usar uma gama mais ampla de funções de agregação com `groupby()`.

A função `groupby()` (quando usada com funções de agregação) costuma produzir resultados no formato "longo" (um objeto Series por função de agregação), enquanto `pivot_table()` pode produzir resultados no formato "amplo" (muitos objetos Series por função de agregação).

Podemos usar apenas uma função com `pivot_table()` enquanto com `groupby()` podemos usar várias.

`pivot_table()` não pode ser usado para agregar valores, e `groupby()` pode ser usado para isso.

Trabalho maravilhoso!

Pergunta

Quais afirmações sobre `concat()` e `merge()` são verdadeiras?

Escolha quantas quiser

A função `concat()` é mais adequada para juntar DataFrames como se fossem uma fila. Isso fica claro porque, por padrão, o parâmetro **`axis`** é `0`.

`concat()` pode juntar DataFrames ao longo do eixo do índice (`axis=0`) ou do eixo da coluna (`axis=1`). Dito isso, considerando que o padrão do parâmetro **`axis`** é `0,` parece mais conveniente usar `concat()` para juntar vários DataFrames como uma fila ou pilha (um em cima do outro).

`concat()` e `merge()` são totalmente intercambiáveis apesar das diferenças de sintaxe.

A função `merge()` é mais adequada para "costurar" DataFrames uns nos outros.

`merge()` pode juntar DataFrames ao longo do eixo do índice ou do eixo da coluna, dependendo de quais nomes de índice e coluna são passados nos parâmetros `left_on`, `right_on`, mas é usado principalmente para juntar dois DataFrames com base em valores comuns nas colunas (ou índices).

`concat()` e `merge()` são totalmente diferentes e, portanto, não podem ser comparados.

Fantástico!

Pergunta

Qual das seguintes afirmações sobre a função `concat()` é verdadeira?

Ela é usada para costurar DataFrames horizontalmente.

Ela só pode unir DataFrames ao longo do eixo da coluna.

Ela é mais adequada para unir DataFrames com base em colunas comuns.

Ela pode unir DataFrames ao longo do eixo do índice ou do eixo da coluna.

Essa é a afirmação correta. A função `concat()` pode unir DataFrames ao longo do eixo do índice ou do eixo da coluna, dependendo do valor do parâmetro `axis`.

Muito bem!

Pergunta

Você deseja unir um DataFrame que tem uma lista de jogos e um DataFrame que tem uma lista de vendas de jogos em diferentes regiões usando a função `merge()` e indexando com os valores de `game_id` (unívocos para cada jogo). Os dados de vendas de alguns dos jogos não estão disponíveis, mas você gostaria de manter a lista original de jogos no resultado. Que tipo de junção você usa ao chamar `merge()` para a lista de jogos, ou seja, `games.merge(sales, …)`?

`games.merge(sales, left_on='game_id', right_on='game_id')`

`games.merge(sales, left_on='game_id', right_on='game_id', how='left')`

Isso usa apenas as chaves do DataFrame esquerdo (aquele com a lista de jogos), portanto, o resultado inclui todos os jogos no DataFrame `games`, independentemente da presença das suas chaves no DataFrame `sales`.

`games.merge(sales, left_on='game_id', right_on='game_id', how='right')`

`games.merge(sales, left_on='game_id', right_on='game_id', how='outer')`

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-37-688Z.md
### Última modificação: 2025-05-28 19:10:38

# Conclusão - TripleTen

Capítulo 9/11

Transformações de dados

# Conclusão

Parabéns por aprender como transformar dados brutos conforme o necessário para resolver problemas! As habilidades que você adquiriu neste capítulo serão úteis em todas as áreas da sua carreira como profissional de dados.

Pergunta

Agora você é capaz de:

Escolha quantas quiser

Usar `groupby()` para agrupar mais de uma coluna

Se ainda não tiver certeza, você pode revisar a lição em [Processamento de dados agrupados com `agg()`](https://tripleten.com/trainer/data-analyst/lesson/2cd377e1-820a-465f-a5f6-5d70d3fa6ec9/task/62a3f67a-5cf9-454e-87af-27ecda10c3d5/)

Aplicar `agg()` para agrupar dados de forma mais eficaz

Se ainda não tiver certeza, você pode revisar a lição em [Processamento de dados agrupados com `agg()`](https://tripleten.com/trainer/data-analyst/lesson/2cd377e1-820a-465f-a5f6-5d70d3fa6ec9/task/62a3f67a-5cf9-454e-87af-27ecda10c3d5/)

Criar tabelas dinâmicas

Se ainda não tiver certeza, você pode revisar a lição em [Tabelas dinâmicas](https://tripleten.com/trainer/data-analyst/lesson/a85facb4-f24c-4f40-bc32-1d4689d74e58/task/7fafb369-527a-4f33-8be6-5f7d914c1f91/)

Combinar DataFrames com concatenação

Se ainda não tiver certeza, você pode revisar a lição em [Combinação de DataFrames com `concat()`](https://tripleten.com/trainer/data-analyst/lesson/b42eaab6-e2af-4a14-b355-d51d2608a7d6/task/c6d5ccc6-c812-47b1-b768-2d6ea2ad3915/)

Combinar DataFrames com junção

Se ainda não tiver certeza, você pode revisar a lição em [Combinação de DataFrames com `merge()`](https://tripleten.com/trainer/data-analyst/lesson/3ae57234-4214-45e5-8ed6-da658665294a/task/ec522b10-a1c0-4c05-b7de-4b11eaacd2b8/)

Fantástico!

Pronto, agora você é mestre das combinações! Continue assim e prepare-se para o projeto!

### Leve isso com você

Faça download do sumário do capítulo para que você possa consultá-los quando necessário.

-   [Resumo do capítulo: Transformações de dados](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/PT/9_Resumo_do_captulo_Transformaes_de_dados.pdf)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-38-987Z.md
### Última modificação: 2025-05-28 19:10:39

# Conheça o segundo projeto - TripleTen

Capítulo 10/11

Projeto

# Conheça o segundo projeto

Você explorou o grande e maravilhoso mundo da manipulação de dados 🎉, aprendeu muita coisa e agora está com tudo pronto para iniciar o seu segundo projeto neste programa.

Desta vez, você vai realizar uma tarefa que poderia receber em um trabalho real de especialista de dados. Você vai precisar passar por todas as etapas do processo: estudar os dados, limpá-los tratando com cuidado valores ausentes e duplicados, responder a perguntas analíticas e visualizar suas conclusões.

> 💡 Como de costume, ao terminar o projeto, entregue-o para ser revisado. Você vai receber comentários em até 24 horas. Faça mudanças conforme indicado pelo revisor e entregue uma versão atualizada. Talvez você receba novos comentários, e é normal ter várias versões de mudanças antes que o projeto seja aceito. Quando o revisor aceitar o seu projeto, ele será considerado concluído.

Antes de você iniciar, temos mais algumas **dicas**:

1.  **Leia as instruções com atenção:** você está apenas começando sua jornada no mundo de dados, então fornecemos instruções detalhadas sobre o que fazer. Leia tudo atentamente para evitar os erros mais comuns e alcançar os melhores resultados;
2.  **Use os materiais do sprint:** lembre-se de que este é um projeto "com consulta", o que significa que você pode usar livremente todos os materiais do sprint, principalmente a parte prática. OBS: preste atenção especial ao estudo de caso;
3.  **Faça perguntas:** é normal não saber algo. Se tiver dificuldades com uma determinada parte do projeto, sinta-se à vontade para pedir ajuda aos tutores ou aos colegas no Discord.

Boa sorte no seu caminho para se tornar mestre da manipulação de dados!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-41-056Z.md
### Última modificação: 2025-05-28 19:10:41

# Sprint 3 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 10/11

Projeto

# Sprint 3 - Projeto

**DATASETS**

-   [instacart\_orders.csv](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/datasets/instacart_orders.csv)
    
-   [products.csv](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/datasets/products.csv)
    
-   [order\_products.csv](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/order_products.csv.zip)
    
-   [aisles.csv](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/datasets/aisles.csv)
    
-   [departments.csv](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_2_sprint/datasets/departments.csv)
    

Parabéns por concluir o sprint sobre AED! É hora de aplicar o conhecimento e as habilidades que você adquiriu a um estudo de caso analítico.

Quando terminar o projeto, envie seu trabalho ao revisor na plataforma para avaliação. Você vai receber feedback em até 48 horas. Use o feedback para fazer alterações e, em seguida, envie a nova versão de volta ao revisor do projeto.

Você talvez receba mais feedback referente à nova versão. Isso é completamente normal. Não é incomum passar por vários ciclos de feedback e revisão.

Seu projeto será considerado concluído assim que o revisor do projeto o aprovar.

## Descrição do projeto

Neste projeto, você vai trabalhar com dados da Instacart.

A Instacart é uma plataforma de entrega de supermercado onde os clientes podem fazer um pedido de supermercado e receber a compra em casa, semelhante ao funcionamento do Uber Eats e do iFood. Esse conjunto de dados específico foi [lançado publicamente](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2) \*(os materiais estão em inglês) pela Instacart em 2017 para uma [competição Kaggle](https://www.kaggle.com/c/instacart-market-basket-analysis/overview) _(os materiais estão em inglês)_. Os dados reais podem ser baixados na página da Kaggle.

O conjunto de dados que fornecemos foi modificado a partir do original. Reduzimos o tamanho dele para que seus cálculos sejam executados mais rapidamente e incluímos valores ausentes e duplicados. Também tivemos o cuidado de preservar as distribuições dos dados originais quando fizemos as alterações.

Sua missão é limpar os dados e preparar um relatório que forneça informações sobre os hábitos de compra dos clientes da Instacart. Após responder a cada pergunta, escreva uma breve explicação dos seus resultados em uma célula Markdown no seu notebook Jupyter.

Neste projeto, você vai precisar criar gráficos para apresentar seus resultados. Certifique-se de que todos os gráficos que você criar têm um título, eixos rotulados e uma legenda, se necessário; e inclua `plt.show()` no final de cada célula com um gráfico.

Assista a este vídeo para mais dicas sobre o projeto:

<iframe class="base-markdown-iframe__iframe" id="player-KtCrkV4CFz8" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="EDA Project to Data Wrangling" width="640" height="360" src="https://www.youtube.com/embed/KtCrkV4CFz8?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F078eb029-d762-4f21-8c07-bfb555d6a3e9%2Ftask%2F3650dcf2-48de-4df5-b2c5-a6330e4324c4%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

## Dicionário de dados

Há cinco tabelas no conjunto de dados, e você vai precisar usar todas elas para pré-processar os dados e fazer a AED. Abaixo está um dicionário que lista as colunas de cada tabela e descreve os dados contidos nelas.

-   `instacart_orders.csv`: cada linha corresponde a um pedido no aplicativo da Instacart
    -   `'order_id'`: é o número de identificação exclusivo de cada pedido
    -   `'user_id'`: é o número de identificação exclusivo da conta de cada cliente
    -   `'order_number'`: é o número de vezes que o cliente fez um pedido
    -   `'order_dow'`: é o dia da semana em que o pedido foi feito (0 é domingo)
    -   `'order_hour_of_day'`: é a hora do dia em que o pedido foi feito
    -   `'days_since_prior_order'`: é o número de dias desde que o cliente fez seu pedido anterior
-   `products.csv`: cada linha corresponde a um produto exclusivo que os clientes podem comprar
    -   `'product_id'`: é o número de identificação exclusivo de cada produto
    -   `'product_name'`: é o nome do produto
    -   `'aisle_id'`: é o número de identificação exclusivo de cada categoria de corredor do supermercado
    -   `'department_id'`: é o número de identificação exclusivo de cada categoria de departamento do supermercado
-   `order_products.csv`: cada linha corresponde a um item incluído em um pedido
    -   `'order_id'`: é o número de identificação exclusivo de cada pedido
    -   `'product_id'`: é o número de identificação exclusivo de cada produto
    -   `'add_to_cart_order'`: é a ordem sequencial em que cada item foi colocado no carrinho
    -   `'reordered'`: 0 se o cliente nunca comprou o produto antes, 1 se já o comprou
-   `aisles.csv`
    -   `'aisle_id'`: é o número de identificação exclusivo de cada categoria de corredor do supermercado
    -   `'aisle'`: é o nome do corredor
-   `departments.csv`
    -   `'department_id'`: é o número de identificação exclusivo de cada categoria de departamento do supermercado
    -   `'department'`: é o nome do departamento

## Instruções para concluir o projeto

**Etapa 1:** abra os arquivos de dados (`/datasets/instacart_orders.csv`, `/datasets/products.csv`, `/datasets/aisles.csv`, `/datasets/departments.csv` e `/datasets/order_products.csv`) e dê uma olhada no conteúdo geral de cada tabela.

Observe que os arquivos têm formatação fora do padrão, então você vai precisar definir certos argumentos em `pd.read_csv()` para ler os dados corretamente. Dê uma olhada nos arquivos CSV para ter uma ideia de quais deveriam ser esses argumentos.

Observe que `order_products.csv` contém _muitas_ linhas de dados. Quando um DataFrame tem muitas linhas, `info()` não imprime as contagens de valores não nulos por padrão. Se quiser imprimi-las, inclua `show_counts=True` quando chamar `info()`.

**Etapa 2:** faça o pré-processamento dos dados da seguinte maneira:

-   Verifique e corrija os tipos de dados (por exemplo, certifique-se de que as colunas de ID sejam números inteiros)
-   Identifique e preencha valores ausentes
-   Identifique e remova valores duplicados

Certifique-se de explicar que tipos de valores ausentes e duplicados você encontrou, como você os preencheu ou removeu, por que escolheu esses métodos e por que você acha que esses valores ausentes e duplicados estavam presentes no conjunto de dados.

**Etapa 3:** quando os dados estiverem processados e prontos, execute a seguinte análise:

**\[A\] (é necessário concluir tudo para passar)**

1.  Verifique se os valores nas colunas `'order_hour_of_day'` e `'order_dow'` na tabela `orders` fazem sentido (ou seja, os valores da coluna `'order_hour_of_day'` variam de 0 a 23 e os da `'order_dow'` variam de 0 a 6).
2.  Crie um gráfico que mostre quantas pessoas fazem pedidos a cada hora do dia.
3.  Crie um gráfico que mostre em que dia da semana as pessoas fazem compras.
4.  Crie um gráfico que mostre quanto tempo as pessoas esperam até fazer seu próximo pedido e comente sobre os valores mínimo e máximo.

**\[B\] (é necessário concluir tudo para passar)**

1.  Há diferenças entre as distribuições de `'order_hour_of_day'` na quarta-feira e no sábado? Construa histogramas para ambos os dias no mesmo gráfico e descreva as diferenças que você notou.
2.  Construa um gráfico de distribuição para o número de pedidos que os clientes fazem (ou seja, quantos clientes fizeram apenas 1 pedido, quantos fizeram apenas 2, quantos apenas 3, etc.)
3.  Quais são os 20 produtos comprados com mais frequência? Exiba os números de identificação (ID) e nomes.

**\[C\] (é necessário concluir pelo menos duas perguntas para passar)**

1.  Quantos itens as pessoas normalmente compram em um pedido? Como fica a distribuição?
2.  Quais são os 20 principais itens incluídos mais frequentemente em pedidos repetidos? Exiba os números de identificação (ID) e nomes.
3.  Para cada produto, que proporção de pedidos em que ele aparece são pedidos repetidos? Crie uma tabela com colunas para ID do produto, nome do produto e a proporção de pedidos repetidos.
4.  Para cada cliente, que proporção dos produtos comprados são pedidos repetidos?
5.  Quais são os 20 principais itens que as pessoas colocam nos carrinhos antes de todos os outros? Exiba o ID do produto, nome e o número de vezes que ele foi o primeiro a ser adicionado a um carrinho.

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-5f303af1-ecf6-42ca-a81a-4364a545d937.containerhub.tripleten-services.com/doc/tree/078eb029-d762-4f21-8c07-bfb555d6a3e9.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-47-008Z.md
### Última modificação: 2025-05-28 19:10:47

# Feedback do Sprint 3 - TripleTen

Capítulo 11/11

Conclusão

# Compartilhe sua opinião

**Olá!**

Você acabou de concluir um sprint e enviar seu projeto. Parabéns por alcançar este marco! 🚀

Agora é o momento perfeito para dar seu feedback sobre o sprint. Sua opinião vai nos ajudar a aprimorar ainda mais o processo de aprendizagem.

Este questionário curto vai levar apenas 2 minutos. Todas as respostas vão ser confidenciais: não vamos compartilhá-las de forma que identifique você.

Agradecemos por nos ajudar a evoluir com você!

Como você avalia a experiência com os materiais didáticos, as tarefas e os projetos neste sprint?

1

2

3

4

5

Respostas: 1 – Muito insatisfatória; 5 – Muito satisfatória.

Avançar

1 — 7

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-10-48-318Z.md
### Última modificação: 2025-05-28 19:10:48

# Conclusão - TripleTen

Capítulo 11/11

Conclusão

# Conclusão

![](https://practicum-content.s3.amazonaws.com/resources/eda-conc_1691132683.jpg)

Parabéns por concluir o sprint de Disputa de dados! Esperamos que, depois de todo seu trabalho e esforço, você se sinta muito mais confiante em como agir sempre que começar a trabalhar com um novo conjunto de dados.

As habilidades que você aprendeu neste sprint são fundamentais para qualquer tipo de carreira de dados que você tenha no futuro, e você terá várias oportunidades de praticar elas durante o resto da sua jornada no TripleTen.

Para recapitular, você deve ter adquirido estas habilidades ao final deste sprint:

-   Ler dados de diferentes fontes e formatos
-   Obter uma visão geral dos dados
-   Lidar com valores ausentes e duplicados
-   Criar gráficos claros e representativos dos dados e resultados analíticos
-   Filtrar e agrupar dados para uma análise agregada mais avançada
-   Criar novas colunas processando colunas existentes
-   Combinar dados por meio de junção e concatenação

Agora você está com tudo pronto para encarar a Análise estatística de dados!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-31-943Z.md
### Última modificação: 2025-05-28 19:18:32

# Introdução - TripleTen

Capítulo 1/6

Introdução

# Introdução

## Análise estatística de dados

Olá, este é o sprint sobre análise estatística de dados!

Você deve ter notado a rapidez com que produtos são desenvolvidos hoje em dia. Basta se lembrar do número de atualizações sugeridas para seus aplicativos móveis no mês passado. Temos certeza de que você atualizou seu aplicativo favorito pelo menos uma vez.

O mais interessante é que a versão final da atualização que você baixa no seu telefone passa por testes extensivos e é comparada com outras opções alternativas, também conhecidas como hipóteses. Mas como as empresas tomam as decisões finais sobre qual opção escolher? É aqui que a estatística entra em ação. Dados estatísticos nos permitem medir numericamente as principais análises relacionadas ao produto e entender qual opção é melhor e por quê. Além disso, eles ajudam as empresas a terem segurança nas decisões finais ao fornecer uma medida de confiança na comparação de alternativas.

Neste sprint, você vai:

-   Avaliar variáveis contínuas e discretas usando histogramas de vários tipos;
-   Tirar conclusões sobre dados com base em métricas estatísticas;
-   Se familiarizar com a teoria da probabilidade;
-   Definir tipos de distribuição e aprender a calcular tanto a distribuição normal como a binomial;
-   Aprender a formular e testar hipóteses.

Você vai praticar as habilidades adquiridas na nossa plataforma online resolvendo várias tarefas de produtos que vão surgir durante o processo de trabalho.

Ao final do curso, você vai concluir um projeto individual: resolver uma tarefa analítica da indústria de telecomunicações. Você vai testar várias hipóteses e formular algumas recomendações sobre futuras estratégias publicitárias para os planos de dados da operadora.

Assim, vamos continuar melhorando seu conjunto de habilidades!

![](https://practicum-content.s3.amazonaws.com/resources/Pre-processamento_de_Dados_1713354815.png)

![](https://practicum-content.s3.amazonaws.com/resources/Analise_Exploratoria_1713354825.png)

![](https://practicum-content.s3.amazonaws.com/resources/Analise_Estatistica_1713354846.png)

![](https://practicum-content.s3.amazonaws.com/resources/Ferramentas_de_Desenvolvimento_de_Software_1713354900.png)

![](https://practicum-content.s3.amazonaws.com/resources/Python_1713354856.png)

### Quanto tempo isso vai levar?

Ao começa a estudar estatística e teoria da probabilidade, leia a teoria com bastante atenção. Pode ser difícil colocar estatística na cabeça no começo, mas não se desespere. Essa teoria vai ser uma boa base para resolver tarefas mais complexas no futuro. Você vai precisar de 30 a 40 horas para concluir o material, dependendo do seu conhecimento prévio e hábitos de estudo. Se você sentir que está ficando para trás, entre em contato com a nossa Equipe de Orientação. Como sempre, temos o compromisso de ajudar você em todas as etapas do caminho.

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_1707138950.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-33-336Z.md
### Última modificação: 2025-05-28 19:18:33

# Introdução - TripleTen

Capítulo 2/6

Estatística descritiva

# Introdução

Neste capítulo, você vai aprender os fundamentos da estatística descritiva e como aplicar seus aprendizados para resolver tarefas simples da vida real.

Você vai descobrir por experiência própria a importância do conhecimento de estatística descritiva para todas as pessoas e organizações que trabalham com dados.

Ao final deste capítulo, que vai levar de 3 a 4 horas para ser concluído, você será capaz de:

-   comparar variáveis discretas e contínuas;
-   usar barras para construir histogramas de frequência a partir de variáveis contínuas;
-   visualizar as medidas básicas de localização;
-   explicar o que é dispersão e calcular variância e desvio padrão;
-   usar a regra dos três sigmas para distribuições normais.

Pode ser desafiador aprender todos esses conceitos estatísticos e, afinal, por que você precisa deles? Primeiramente, porque todos os dados que as empresas coletam sobre usuários ou produtos seguem alguns padrões comuns de distribuição. Ao entender essas distribuições, as empresas podem tirar conclusões sobre o que os usuários desejam ou o que está faltando nos produtos. E é por isso que é importante aprender sobre esse novo material. Então vamos começar!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-34-692Z.md
### Última modificação: 2025-05-28 19:18:35

# Variáveis contínuas e discretas - TripleTen

Capítulo 2/6

Estatística descritiva

# Variáveis contínuas e discretas

No sprint de manipulação de dados, você aprendeu sobre variáveis categóricas e quantitativas.

Lembre-se de que uma variável quantitativa (às vezes chamada de "variável numérica") pega valores numéricos de um intervalo (por exemplo, 1, 2, 3.78, -100.5), enquanto uma variável categórica (às vezes chamada de "variável qualitativa") pega valores de um conjunto limitado de valores não numéricos.

Por exemplo, a escolaridade é uma variável categórica que só pode assumir um número limitado de valores exatos, como PhD, mestrado, etc. Embora possa haver muitas opções, especialmente quando levamos em consideração todas as séries do ensino médio e fundamental, a quantidade total das opções ainda é um conjunto limitado.

Pergunta

Escolha a alternativa em que **todas** as variáveis são **qualitativas**.

Tempo de espera de ônibus, preço do ingresso de teatro, peso de recém-nascido

Numeração de calçado, tipo sanguíneo, cidades do Brasil

Raças de cachorro, tamanhos de roupa (PP, P, M, G, GG, XGG), eventos olímpicos

Códigos de Classificação Internacional de Doenças, número de estudantes em um grupo, pratos italianos

Fantástico!

Agora vamos dar uma olhada mais de perto nas variáveis quantitativas. Elas podem ser **discretas** ou **contínuas**.

![](https://practicum-content.s3.amazonaws.com/resources/4.2.2.2PT_1692885087.png)

**Variáveis contínuas** assumem valores numéricos a qualquer nível de precisão. Isso significa que, por exemplo, tempo de espera (que pode ser medido em horas, minutos, segundos – por exemplo, 2,5 minutos) ou peso (quilos, gramas – por exemplo, 4.275 kg) são exemplos de variáveis contínuas, já que esses valores podem ser medidos em uma escala contínua.

Em contraste, variáveis **discretas** só podem ser medidas em números inteiros sem unidades menores. Por exemplo, não podemos ter 4,25 pítons ou 2,4 olhos em um gato. Essas medidas precisam ser variáveis discretas.

![](https://practicum-content.s3.amazonaws.com/resources/Chapter_1_PT_1692885108.png)

Entretanto, há situações em que variáveis discretas podem conter valores que não sejam números inteiros. Imagine uma loja online que vende enchimento para pufes. Já que os clientes nem sempre precisam de um saco inteiro de enchimento para usar em um pufe, a loja pode disponibilizar opções com metade dessa quantidade. Nesse caso, o número de sacos vendidos em um mês poderia ser qualquer número não negativo divisível por um meio: 0, ½, 1, 1½, 2…

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_17_1692000156.png)

Portanto, essa variável assume valores que não são números inteiros (como ½, 1½), mas ainda é discreta. Em outras palavras, não poderíamos ter valores como 3/4 ou 5,62 nessa sequência.

Para concluir, variáveis discretas assumem números com um degrau exato entre os valores. Agora vamos praticar!

Pergunta

Selecione a opção na qual todas as variáveis são contínuas:

Idade, medida em anos inteiros

O tempo de carregamento de uma página, o peso de um pacote

Número de pessoas na fila para cumprimentar a Lady Gaga

Número de aulas em um curso de estatística, tempo de estudo de análise de dados

Fantástico!

Pergunta

Selecione as opções em que todas as variáveis são discretas:

Escolha quantas quiser

Número de páginas em um livro

O tempo de duração de um filme

Número de convidados

Tempo gasto estudando análise de dados

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-37-027Z.md
### Última modificação: 2025-05-28 19:18:37

# Histogramas de frequência - TripleTen

Capítulo 2/6

Estatística descritiva

# Histogramas de frequência

Lembra dos histogramas que criamos no sprint sobre manipulação de dados? Veja um histograma que você obtém ao jogar dois dados 1.000 vezes. Temos os possíveis valores no eixo horizontal e frequência no eixo vertical.

Você consegue descobrir por que ele fica assim? Por que um total de 7 é mais frequente do que 12?

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_18_1692000296.png)

Quando jogamos pares de dados, esperamos que a soma 2 apareça, em média, uma vez a cada 36 jogadas. De forma parecida, esperamos que a soma 3 apareça duas vezes (como 1+2 e 2+1), a soma 4 apareça três vezes (como 1+3, 2+2 e 3+1) e assim por diante. O histograma acima mostra o que realmente acontece quando jogamos os dados. Como resultado, esperamos que a média dos números jogados seja 7, e o histograma mostra exatamente isso.

Histogramas funcionam para variáveis discretas, pois elas possuem um conjunto predeterminado de valores específicos, e cada valor pode ser usado na medição de frequência.

Mas como você construiria um histograma para uma variável contínua? Lembre-se de que os valores das variáveis contínuas podem ter qualquer grau de precisão e ser muito próximos sem serem idênticos (por exemplo, 5 e 5,00001). Desse modo, cada valor individual teria a frequência igual a 1, e o histograma não forneceria nenhuma informação útil:

![](https://practicum-content.s3.amazonaws.com/resources/_4_1_1692000322.png)

Vamos discutir histogramas para variáveis contínuas logo mais. Mas, primeiro, vamos testar sua intuição.

Pergunta

Qual é a melhor maneira de construir um histograma para uma variável contínua?

Transformar a variável em uma variável discreta arredondando todos os valores

Transformar a variável em uma variável categórica dividindo os valores em intervalos e usando cada um como um possível valor

Você conseguiu!

Então: como criamos um histograma para variáveis contínuas? Uma ideia que faz sentido é dividir o intervalo de possíveis valores em intervalos e então contar o número de vezes que os valores aparecem em cada um.

Ao criar um histograma, podemos ajustar o número de barras usadas especificando o parâmetro `bins=`. Por exemplo, definir `bins=4` cria 4 barras, o que funciona bem para variáveis discretas. No entanto, para variáveis contínuas, precisamos passar uma lista com os limites dos intervalos, e não um único número inteiro, como 4.

Compare os dois histogramas abaixo. O azul foi criado com o parâmetro `bins=` definido em `4`, enquanto o laranja foi construído com uma lista com os limites dos intervalos:

```
import pandas as pd

data = pd.Series([11, 20, 22, 31, 32, 33, 41, 42, 43, 44, 51, 52, 53, 54, 55, 61, 62, 63, 64, 65, 66, 71, 72, 73, 74, 75, 76, 77, 81, 82, 83, 84, 85, 86, 87, 88, 91, 92, 93, 94, 95, 96, 97, 98, 99])

data.hist(bins=4, alpha=0.5)  # constrói um histograma com quatro barras

data.hist(
    bins=[11, 20, 30, 40, 50, 60, 70, 80, 90, 99], alpha=0.7
)  # constrói um histograma com nove barras, cujos limites estão listados, e com um argumento alpha que gera um gráfico opaco
```

![](https://practicum-content.s3.amazonaws.com/resources/_1_1_1692000377.png)

Ao passar uma lista como um valor para `bins=`, criamos 9 barras: de `11` a `20`, de `20` a `30` e assim por diante. O parâmetro `alpha=` é usado para ajustar a transparência, permitindo que os dois gráficos sejam exibidos em um só.

## Tarefas

### Tarefa 1

Suponha que a empresa em que você trabalha atualizou a interface do carrinho de compras do site dela. A sua equipe quer saber se os usuários estão achando essa mudança complicada, então você decide analisar o tempo de compras – o número de segundos gastos pelo cliente entre o início do processo de pagamento e a conclusão da compra.

Um dia após a nova interface ter sido lançada, você recebeu leituras de dados suficientes para construir um histograma.

Você recebeu um conjunto de dados que contém o tempo de compras em segundos armazenados na variável `pur_time`. Seu objetivo é criar um histograma com os limites das barras em `[15, 30, 45, 60, 75, 90]` e ajustar a transparência para 0.7.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

import pandas as pd

  

\# o conjunto de dados de tempo de compra (pur\_time)

pur\_time \= pd.Series(\[36, 44, 73, 32, 44, 29, 63, 60, 55, 74, 61, 26, 76, 40, 39, 28, 69, 61, 54, 58, 47, 41, 70, 51, 58, 36, 71, 47, 74, 59, 50, 78, 59, 48, 67, 53, 67, 52, 38, 55, 53, 53, 43, 77, 44, 63, 63, 54\])

pur\_time.hist(

bins\=\[15, 30, 45, 60, 75, 90\], alpha\=0.7

)

\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

## Tarefa 2

Usando os dados de tempo de compras da tarefa 1 de novo, construa dois histogramas com os seguintes limites de intervalos:

-   `[15, 35, 55, 75, 90]`
-   `[15, 45, 55, 90]`

Determine a transparência de ambos em 0.5.

CódigoPYTHON

9

1

2

3

4

5

6

import pandas as pd

  

\# vamos chamar o conjunto de dados \`pur\_time\` (purchase time – tempo de compra)

pur\_time \= pd.Series(\[36, 44, 73, 32, 44, 29, 63, 60, 55, 74, 61, 26, 76, 40, 39, 28, 69, 61, 54, 58, 47, 41, 70, 51, 58, 36, 71, 47, 74, 59, 50, 78, 59, 48, 67, 53, 67, 52, 38, 55, 53, 53, 43, 77, 44, 63, 63, 54\])

pur\_time.hist(bins\=\[15, 35, 55, 75, 90\], alpha\=0.5)\# escreva seu código aqui

pur\_time.hist(bins\=\[15, 45, 55, 90\], alpha\=0.5)\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-38-336Z.md
### Última modificação: 2025-05-28 19:18:38

# Histogramas de densidade - TripleTen

Capítulo 2/6

Estatística descritiva

# Histogramas de densidade

Como observamos na última tarefa da lição anterior, histogramas têm algumas desvantagens quando trabalhamos com variáveis contínuas. A frequência de valores em cada intervalo depende bastante dos limites desses intervalos, também conhecidos como barras. Em cenários da vida real, pode ser necessário definir larguras desiguais para as barras. Podemos esperar que tamanhos maiores de barras possam resultar em alterações na frequência que elas capturam. Falando de forma geral, quanto maior o intervalo, mais alta a frequência.

Por exemplo, considere o exemplo abaixo de um histograma com 3 barras de larguras diferentes:

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_1_1692885163.png)

Vamos dar uma olhadinha nas barras e fazer algumas anotações:

-   A primeira barra tem uma largura de 2 (de 2 a 4) e uma frequência de 3.
-   A segunda barra tem uma largura de 4 (de 4 a 8) e uma frequência de 6.
-   A última barra tem uma largura de 2 (de 8 a 10) e uma frequência de 4.

Vamos calcular as áreas das barras e compará-las. Para refrescar a memória, a área de um retângulo é a largura multiplicada pela altura, então:

-   A área da primeira barra é 2 x 3 = 6
-   A área da segunda barra é 4 x 6 = 24
-   A área da terceira barra é 2 x 4 = 8

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_2_1692885188.png)

Embora a segunda barra tenha uma frequência apenas duas vezes maior do que a primeira (6 vs 3), a área dela é quatro vezes maior (24 vs 6). Como podemos lidar com diferenças nos tamanhos das áreas ao comparar frequências que caem em barras diferentes?

Uma abordagem alternativa seria exibir **densidades de frequência** em vez de frequências. Mas o que são densidades de frequência e como podemos mudar de frequências normais para elas? Vamos explorar esse conceito agora.

Para calcular a densidade de uma frequência, divida a frequência (a altura de uma barra) pela largura da barra:

-   A primeira barra tem uma densidade de frequência de 1,5 (3 dividido por 2).
-   A segunda barra tem uma densidade de frequência de 1,5 (6 dividido por 4).
-   A terceira barra tem uma densidade de frequência de 2 (4 dividido por 2).

Vamos reconstruir o gráfico, mas, desta vez, o eixo Y vai exibir a densidade de frequência, não a frequência normal:

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_3_1692885210.png)

Um histograma que exibe densidades de frequência em vez de frequências, conforme descrito, é chamado de **histograma de densidade**.

Agora vamos calcular as áreas das barras no histograma de densidade que acabamos de construir:

-   A área da primeira barra é 2 x 1,5 = 3
-   A área da segunda barra é 4 x 1,5 = 6
-   A área da terceira barra é 2 x 2 = 4

Os histogramas de densidade, assim como histogramas normais, também são usados para exibir a distribuição de frequência. Essas áreas representam as diferenças reais entre as frequências. Precisamente, cada área representa uma frequência. Quanto maior a área da barra, mais alta é a frequência. Essa é uma maneira alternativa para visualizar frequências.

Pergunta

Você tem um intervalo e as frequências dele:

Idade

Frequência

20 - 40

44

Seu objetivo é calcular a densidade da frequência.

44 / 20 = 2,2

Certo. Calculamos a densidade de uma frequência dividindo a frequência pela largura de um intervalo.

20 / 44 = 0,4545

\-20 / 44 = -0,4545

Você conseguiu!

Geralmente, ao construir gráficos de densidade de frequência, podemos começar a usar curvas, em vez de barras, para visualizar a distribuição. Vamos considerar outro exemplo que indica isso claramente:

![](https://practicum-content.s3.amazonaws.com/resources/2.4_pt_1_1693816395.png)

A curva de densidade exibida acima conecta as partes superiores das barras e ilustra a distribuição de atrasos em minutos. Mantivemos as barras perto da curva para mostrar como a curva de densidade e o histograma de densidade estão relacionados entre si, mas já não precisamos das barras.

Tudo funciona da mesma maneira que histogramas de densidade: a área abaixo da curva entre dois valores corresponde à frequência dos valores localizados no intervalo. Entretanto, há uma diferença no eixo Y. Você reparou? Agora o intervalo dos valores é diferente, e o título do eixo é `Densidade`. O que aconteceu? Não houve nenhuma magia: é só outra convenção comumente utilizada para visualizar densidades de frequência.

Agora os números no eixo Y são as porcentagens. Por exemplo, `0.01` é 1% da área total abaixo da curva, `0.02` é 2% da área total abaixo da curva, etc. A área total é o número total de instâncias no conjunto de dados.

Concluindo, tanto histogramas de densidade quanto curvas de densidade fornecem a mesma informação de histogramas de frequência normais: as frequências. Entretanto, a diferença é que os valores reais de frequência se tornaram as áreas.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-39-666Z.md
### Última modificação: 2025-05-28 19:18:40

# Medidas de localização e variabilidade - TripleTen

Capítulo 2/6

Estatística descritiva

# Medidas de localização e variabilidade

### Medidas de localização

No sprint anterior, você aprendeu sobre medianas e médias, que são valores usados para descrever dados numéricos. Esses valores também são chamados de **medidas de localização** ou de posição. Medianas e médias permitem que você encontre um valor aproximado que caracterize ou represente determinado conjunto de dados.

Tanto a mediana quanto a média têm as próprias vantagens. Para calcular a média, você precisa de _todos os valores do conjunto de dados_. Isso porque quando calculamos a média, somamos todos os valores e dividimos o número resultante pela contagem total. Às vezes, a média também é chamada de **medida algébrica de localização.**

A mediana representa os valores no meio de uma distribuição. Vamos recapitular brevemente o que é a mediana:

Pergunta

Confira esta distribuição de dados numéricos:

```
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
```

Selecione a resposta que exibe a mediana da distribuição.

(4 + 5 ) / 2 = 4.5

(5 + 6) / 2 = 5.5

5

Excelente!

A mediana é uma **medida de localização estrutural.** Você pode mudar quase todos os valores do conjunto de dados, e ainda assim a mediana permanece a mesma. Como exemplo, vamos continuar trabalhando com o conjunto de dados do quiz e calcular a média e a mediana em Python.

```
data = pd.Series([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
print('A média é', data.mean())
print('A mediana é', data.median())
```

```
A média é 5.0
A mediana é 5.0
```

Agora vamos ver como a média e a mediana são afetadas quando modificamos os dados. Primeiro, dividimos todos os valores estritamente menores do que 5 por 10 e multiplicamos todos os valores estritamente maiores do que 5 por 10 (o número 5 deve ser deixado como está). Vamos encontrar as novas mediana e média do conjunto de dados atualizado.

```
data_new = pd.Series([0, 0.1, 0.2, 0.3, 0.4, 5, 60, 70, 80, 90, 100])
print('A nova média é', data_new.mean())
print('A nova mediana é', data_new.median())
```

```
A nova média é 36.90909090909091
A nova mediana é 5.0
```

A média mudou, mas a mediana permaneceu a mesma. Não é fácil manter a média a mesma: qualquer alteração em um número maior do que a média precisa de uma mudança de contrabalanço em um número menor que ela.

Além disso, se houver valores atípicos no conjunto de dados, será necessário usar a mediana, como você aprendeu no sprint de manipulação de dados. Tenha em mente que a média de um conjunto de dados contendo valores atípicos pode ser enganosa. Afinal, quando Bill Gates entra em um bar, a média da fortuna de todo mundo lá sobe para milhões.

Vamos imaginar que você tem uma startup: um site de financiamento coletivo, e blogueiros o usam para arrecadar doações. A maioria das pessoas doam valores baixos, mas há vários fãs que mandam aos ídolos várias centenas de dólares.

Pergunta

Qual medida funcionaria melhor para fins promocionais, se quiséssemos convencer blogueiros a usar o serviço? Qual medida revela mais sobre quanto a maioria das pessoas doa?

A doação média (a média) é melhor para promover aos blogueiros, e a mediana diz mais sobre o quanto as pessoas geralmente doam.

A mediana é melhor para promover aos blogueiros, e a doação média diz mais sobre o quanto as pessoas geralmente doam.

As medidas são igualmente úteis.

Muito bem!

### Medidas de variabilidade

Apenas as medidas de localização não são suficientes para compreender os dados. Você também vai querer saber como os valores estão **dispersos** (ou espalhados) em torno da média ou mediana.

Para a medida estrutural de localização, a mediana, temos uma medida estrutural de dispersão. Essa medida é constituída pelos **quartis**.

A mediana representa o valor no meio de uma distribuição, e os quartis representam outros locais importantes. Que locais são esses?

-   O primeiro quartil (Q1) representa o valor no percentil 25 da distribuição dos dados. O percentil 25 é o valor abaixo do qual estão 25% dos dados.
-   O segundo quartil (Q2) representa o valor no percentil 50 da distribuição dos dados, que é equivalente à mediana. Da mesma forma que o percentil 25, o percentil 50 é o valor abaixo do qual estão 50% dos dados.
-   O terceiro quartil (Q3) representa o valor no percentil 75 da distribuição dos dados.

Veja como isso fica quando consideramos um exemplo simples de distribuição de idades:

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_18_1693486051.png)

Muitas vezes, visualizamos quartis em uma forma de um diagrama de caixa (ou boxplot). Para o exemplo acima, ele fica assim:

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_6_1692885321.png)

Confira o que diz o diagrama de caixa:

-   O valor mínimo é 1 ano de idade. Ele está localizado no final do bigode esquerdo.
-   O primeiro quartil: 2 anos de idade.
-   O segundo quartil: 3 anos de idade.
-   O terceiro quartil: 5 anos de idade.
-   O máximo é 6 anos de idade. Ele está localizado no final do bigode direito.

A distância entre o Q1 e o Q3 é chamada de **intervalo interquartil** (ou **IQR** – sigla do inglês para "interquartile range"). Ele mede a amplitude da meia metade dos dados. Em outras palavras, é o intervalo dos 50% dos dados no meio da amostra.

Pergunta

Veja o diagrama de caixa abaixo. Qual é o valor da mediana?

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_19_1692001114.png)

33

31

27

Seu entendimento sobre o material é impressionante!

Pergunta

Veja o diagrama de caixa abaixo. Qual é o valor mínimo?

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_20_1692001164.png)

27

20

24

Seu entendimento sobre o material é impressionante!

Para construir diagramas de caixa em Python, é comum usar a biblioteca Seaborn, que fornece capacidades de visualização parecidas às da biblioteca Matplotlib que usamos antes, mas com uma funcionalidade um pouco diferente. A primeira diferença é que precisamos importar a Seaborn.

```
import seaborn as sns
```

Depois, construímos um diagrama de caixa assim:

```
import pandas as pd

dataset = pd.Series([1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 5, 5, 6, 6])
sns.boxplot(dataset)
```

![](https://practicum-content.s3.amazonaws.com/resources/download_1_1_1692001274.png)

Observe que, para construir esse diagrama de caixa, usamos os mesmos dados de quando o vimos pela primeira vez. Portanto, o gráfico resultante é idêntico ao anterior, e a única diferença é que o nosso gráfico está orientado verticalmente.

Mas como medir a dispersão em torno da medida algébrica de localização, a média?

A primeira (e mais fácil) abordagem é encontrar a distância média entre a média e todos os outros valores. Vamos calculá-la agora.

## Tarefas

### Tarefa 1

Seus objetivos são:

1.  Encontrar a média do conjunto de dados e armazená-la na variável `mean_value` usando o método apropriado.
2.  Calcular a distância entre cada valor e a média e armazenar o valor resultante na variável `spacing_all`. Nesta etapa, use operações aritméticas simples com Series da Pandas. Isso vai resultar em operações elemento por elemento, o que significa que uma operação será realizada em cada elemento de um objeto Series, produzindo outro objeto Series.
3.  Em seguida, calcular a distância média e armazená-la na variável `spacing_all_mean`.
4.  Imprimir a variável `spacial_all_mean`.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

import pandas as pd

  

data \= pd.Series(\[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\])

mean\_value \= data.mean() \# encontre o valor médio em um conjunto

spacing\_all \= data \- mean\_value \# para cada elemento no conjunto de dados, encontre sua distância da média

spacing\_all\_mean \= spacing\_all.mean() \# calcule a distância média

  

print(spacing\_all\_mean )

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-41-604Z.md
### Última modificação: 2025-05-28 19:18:41

# Variância - TripleTen

Capítulo 2/6

Estatística descritiva

# Variância

### Variância

Na lição anterior, calculamos a distância média entre cada valor e a média, e ela acabou sendo 0. Como isso é possível quando todos os valores no conjunto de dados são diferentes? Vamos rever o conjunto de dados: `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`. A média desse conjunto era `5`.

Calculamos a distância a partir de cada elemento do conjunto de dados até a média. Para isso, subtraímos a média de cada valor no conjunto de dados. Confira como:

-   0 - 5 = -5
-   1 - 5 = -4
-   2 - 5 = -3
-   …
-   8 - 5 = 3
-   9 - 5 = 4
-   10 - 5 = 5

E para calcular a distância média, somamos todas as diferenças. Algumas dessas distâncias eram positivas, e outras eram negativas, então elas se anularam, o que resultou em uma soma de 0.

Agora vamos nos livrar dos sinais e tornar todos os valores positivos elevando-os ao quadrado:

-   (0 - 5)² = -5² = 25
-   (1 - 5)² = -4² = 16
-   (2 - 5)² = -3² = 9
-   …
-   (8 - 5)² = 3² = 9
-   9 - 5 = 4² = 16
-   10 - 5 = 5² = 25

Os novos valores já não representam a distância média entre os valores do conjunto de dados e a média. Em vez disso, eles representam a média dos _quadrados_ da distância. Se somarmos esses valores e os dividimos pela contagem total, obtemos uma métrica chamada **variância**. Ela é calculada usando a seguinte fórmula:

σ2\=∑(xi−μ)2n\\sigma^{2} = \\frac{\\sum \\left ( x\_{i} - \\mu \\right )^{2}}{n}σ2\=n∑(xi​−μ)2​

Na fórmula acima:

-   _σ²_ é a variância
-   _Σ_ é o sinal de soma
-   _xᵢ_ representa um valor do conjunto de dados
-   _µ_ é a média
-   _n_ é o número de valores no conjunto de dados

O sinal de sigma (_σ²_ ) foi escolhido para a variância porque ele denota o som "s", como em "standard" (padrão). Na próxima lição, você vai aprender que a raiz quadrada da variância é o **desvio padrão**.

Então, o que a variância significa? Ela mostra o grau de espalhamento no conjunto de dados. Quanto mais espalhados estão os dados, maior é a variância em relação à média.

Confira o gráfico abaixo. Há 3 distribuições com variâncias diferentes.

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_7_1692885367.png)

Como você pode ver, quanto maior o espalhamento no conjunto de dados, mais alta a variância.

Pergunta

Em qual caso a variância é zero?

Quando os valores são organizados simetricamente ao redor da média

Quando os valores estão organizados simetricamente ao redor da mediana

Quando todos os valores do conjunto de dados são iguais

Muito bem!

### Covariância

Muitas vezes, será do nosso interesse saber como uma variável depende de outra. Por exemplo, como o preço de uma casa depende do tamanho dela.

Para verificar esse tipo de dependência, podemos calcular a **covariância** entre duas variáveis multiplicando suas diferenças pela média. A covariância é uma medida da dependência linear.

Covariaˆncia\=∑in(xi−x‾)(yi−y‾)n\\text{Covariância} = \\frac{\\sum\_{i}^{n} \\left (x\_{i} - \\overline{x})(y\_{i} - \\overline{y} \\right )}{n}Covariaˆncia\=n∑in​(xi​−x)(yi​−y​)​

-   Continuando com o exemplo, _xᵢ_ representa o tamanho de uma casa em metros quadrados, e _yᵢ_ representa o preço em dólares.
-   _n_ é o número de valores no conjunto de dados (o número de casas e os respectivos preços).
-   _Σ_ é o símbolo de soma.
-   x̄ é o tamanho da média das casas no conjunto de dados.
-   ȳ é a média dos preços das casas no conjunto de dados.

Se a covariância for igual a zero, isso indica que não há nenhuma relação direcional entre as variáveis. O valor de covariância pode ser qualquer valor do infinito negativo até o infinito positivo. Um valor negativo indica uma relação inversa, enquanto um valor positivo indica uma relação direta.

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_8_1692885398.png)

Pergunta

O que uma covariância negativa indica? Selecione todas as opções que se aplicam.

Escolha quantas quiser

Isso significa que enquanto uma variável diminui, a tendência é que a outra também diminua.

Quanto mais distante de 0 estiver o valor da covariância, maior será a dependência entre duas variáveis.

É isso aí. E quando a covariância for 0, não haverá nenhuma dependência.

Valores negativos e próximos de 0 indicam uma relação negativa fraca.

Correto! Quanto mais próximo de 0 o valor, menor é a dependência entre as variáveis.

Isso significa que se uma variável diminuir, a tendência é que a outra aumente ou diminua.

Muito bem!

### Implementação em Python

Para calcular a variância em Python, vamos precisar da biblioteca **NumPy**, a irmã mais velha da biblioteca Pandas que você já conhece. Enquanto a Pandas é boa para estruturas, a NumPy é usada para funções matemáticas complexas. Ela é importada assim:

```
import numpy as np
```

A variância de um conjunto de dados pode ser encontrada usando o método `var()`:

```
import numpy as np

x = [1, 2, 3, 4, 5, 6] # conjunto de dados

variance = np.var(x)
print(variance)
```

```
2.9166666666666665
```

E calculamos a covariância assim:

```
import numpy as np

x = [1, 2, 3, 4, 5, 6] # conjunto de dados 1
y = [41, 62, 89, 96, 108, 115] # conjunto de dados 2

covariance_matrix = np.cov(x,y) # calculando a matriz de covariância
covariance = covariance_matrix[0][1] # extraindo a covariância como um valor
print(covariance)
```

```
51.5
```

A função `cov()` na NumPy retorna um vetor 2D, no qual o valor no índice `[0][1]` é a covariância entre `x` e `y`.

## Tarefas

### Tarefa 1

Calcule a variância do conjunto de dados `data = pd.Series([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])`. Imprima os resultados.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

import numpy as np

  

data \= pd.Series(\[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\])

  

variance \= np.var(data)

print(variance)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-42-917Z.md
### Última modificação: 2025-05-28 19:18:43

# Desvio padrão e distribuição normal - TripleTen

Capítulo 2/6

Estatística descritiva

# Desvio padrão e distribuição normal

### Introdução ao desvio padrão

Se os dados iniciais fossem medidos em, digamos, metros, usuários ou dólares, a variância seria medida nos quadrados dessas mesmas medidas. Acontece que, muitas vezes, isso é pouco prático para o uso cotidiano.

Para voltar às unidades originais, precisamos (isso mesmo!) tirar a raiz quadrada da variância. O valor resultante é conhecido como **desvio padrão** que, às vezes, é chamado de std.

Você provavelmente já viu "std" no resultado do método `describe()`. Se não, confira um exemplo:

```
import pandas as pd

s = pd.Series([1, 2, 3, 4, 5, 6])
s.describe()
```

```
count    6.000000
mean     3.500000
std      1.870829
min      1.000000
25%      2.250000
50%      3.500000
75%      4.750000
max      6.000000
dtype: float64
```

Entre as várias métricas de dispersão, o desvio padrão é o mais usado, já que ele tem as mesmas unidades de medida que os dados.

O desvio padrão é denotado pelo sigma minúsculo. O sigma foi escolhido porque tem a denotação do som do "s", assim como em "standard" (padrão), e a variância é sigma ao quadrado (ou seja, desvio padrão ao quadrado). Então a fórmula para o desvio padrão é esta:

σ\=∑(xi−μ)2n\\sigma = \\sqrt{\\frac{\\sum \\left ( x\_{i} - \\mu \\right )^{2}}{n}}σ\=n∑(xi​−μ)2​​

O desvio padrão de um conjunto de dados pode ser calculado com o método `std()` da biblioteca NumPy:

```
import numpy as np

x = [1, 2, 3, 4, 5, 6] # conjunto de dados

standard_deviation = np.std(x)
print(standard_deviation)
```

```
1.707825127659933
```

Imagine que você já sabe a variância de um conjunto de dados. Nesse caso, você pode usar o método `sqrt()` da NumPy para calcular o desvio padrão:

```
import numpy as np

variance = 3.5
standard_deviation = np.sqrt(variance)
print(standard_deviation)
```

```
1.8708286933869707
```

### Distribuição normal

O desvio padrão, sobre o qual acabamos de aprender, é uma métrica muito potente quando trabalhamos com distribuições normais. Mas espera aí: o que é uma distribuição normal?

A **distribuição normal**, também conhecida como a **distribuição Gaussiana**, é a distribuição mais usada para muitas variáveis. Ela é conhecida por sua curva simétrica em forma de sino e pode ser usada em várias aplicações estatísticas, como análises de pesquisas, controle de qualidade e até alocação de recursos. Geralmente ela tem essa aparência:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_21_1692002139.png)

Como sabemos, distribuições normais são simétricas em volta da média, ou do centro de um gráfico. Isso significa que os dados próximos da média ocorrem com mais frequência do que os dados que estão longe dela.

Uma distribuição desse tipo pode ser definida por dois parâmetros:

-   **a média** (ou o valor médio), que é o valor máximo do gráfico em relação ao qual o gráfico é sempre simétrico
-   **o desvio padrão**, que determina a dispersão a partir da média

Assim como na variância, um desvio padrão menor (comparado à média) produz uma curva mais íngreme, enquanto um desvio padrão maior (também comparado à média) produz uma curva mais plana:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_22_1692002158.png)

Se os dados seguirem uma distribuição normal, uma regra importante será aplicada: a regra dos três sigmas. Essa regra afirma que quase todos os valores (99,7%) caem em um intervalo específico. Este é o intervalo:

(μ−3σ,μ+3σ)(\\mu - 3\\sigma, \\mu + 3\\sigma)(μ−3σ,μ+3σ)

Lembre-se de que µ representa a média e σ representa o desvio padrão. Em outras palavras, 99,7% de todos os valores estão em um intervalo de 3 desvios padrão à esquerda e à direita da média. Aqui está uma ilustração da regra dos três sigmas para distribuições normais:

![](https://practicum-content.s3.amazonaws.com/resources/4.2.9PT_1692885443.png)

Essa regra nos permite não apenas encontrar um intervalo que certamente contém quase todos os valores da variável, mas também nos ajuda a detectar valores que estão fora dela. Tais valores são chamados de **atípicos**. Eles merecem atenção, já que a presença deles nos dados pode produzir informações importantes sobre a variável ou revelar erros de medição e problemas parecidos.

## Tarefas

### Tarefa 1

Encontre o desvio padrão do conjunto de dados `data = pd.Series([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])` e armazene-o na variável `standard_dev`. Em seguida, imprima os resultados.

CódigoPYTHON

9

1

2

3

4

5

6

7

import pandas as pd

import numpy as np

  

data \= pd.Series(\[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\])

  

standard\_dev \= np.std(data)

print(standard\_dev)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Um usuário comum leva 3 segundos para ler uma mensagem em um site. Essa é a média do conjunto de dados.

Em seguida, você descobre que os dados (distribuídos normalmente) possuem uma variância de 0,25 segundo.

Usando a regra empírica descrita acima, precisamos calcular o tempo que uma mensagem precisa ser exibida para que 99,7% dos usuários a vejam. Para fazer isso, siga estas etapas:

1.  Calcule o desvio padrão da variância e armazene o resultado na variável `adv_std`.
2.  Com base na média e no desvio padrão, calcule o limite superior do intervalo que represente 99,7% dos usuários. Armazene o valor resultante na variável `adv_time`.

Imprima o texto `O tempo de exibição da mensagem é` seguido do seu resultado.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

import numpy as np

  

adv\_mean \= 3

adv\_var \= 0.25

adv\_std \= np.sqrt(adv\_var)

  

adv\_time \= adv\_mean + (3 \* adv\_std)

print('O tempo de exibição da mensagem é', adv\_time)

Dica

Mostrar a soluçãoValidar

### Tarefa 3

Sua empresa vai sediar uma competição de conhecimentos gerais em um shopping.

Como você já organizou competições desse porte antes, sabe que uma média de 3% dos participantes responde às questões corretamente, com um desvio padrão de 0,4%.

Sua expectativa é de que 6 mil pessoas compareçam à competição. Você precisa descobrir quantas pessoas podem ganhar para preparar um número suficiente de prêmios. Ou seja, você quer obter um intervalo para o número de vencedores, de modo que haja 99,7% de chance de que o número real esteja dentro desse intervalo (isto é, entre _x_ e _y_). Use a regra dos três sigmas para calcular esse intervalo.

Depois de calcular, imprima os resultados assim: `Intervalo: ... - ...` (atenção aos espaços em torno do hífen!)

Para tornar essa tarefa menos difícil, vamos dividi-la em etapas menores:

1.  Comece calculando a média esperada de respostas corretas dos participantes. Com base na porcentagem média dos participantes que responderam corretamente (3% ou 0.03) e no número esperado de participantes (6000), calcule a média e armazene o valor resultante na variável `quiz_mean`.
2.  Em seguida, usando o número total de participantes esperados (6000) e uma porcentagem fornecida para o desvio padrão (0.4% ou 0.004), calcule o desvio padrão do número das pessoas que vão comparecer. Esse valor, σ, deve ser armazenado na variável `quiz_std`.
3.  Por fim, determine um intervalo para o número de vencedores. Conforme a regra dos três sigmas, esse intervalo se estende três desvios padrão à esquerda e à direita da média. Calcule os limites desse intervalo e armazene o limite inferior na variável `quiz_bottom_line` e o limite superior na variável `quiz_top_line`.
4.  Imprima os resultados no formato exigido.

CódigoPYTHON

9

1

2

3

4

5

6

7

quiz\_mean \= 6000 \* 0.03

quiz\_std \= 6000 \* 0.004

  

quiz\_bottom\_line \= quiz\_mean \- quiz\_std\*3

quiz\_top\_line \= quiz\_mean + quiz\_std\*3

  

print(f"Intervalo: {quiz\_bottom\_line} - {quiz\_top\_line}")

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-44-244Z.md
### Última modificação: 2025-05-28 19:18:44

# Dados assimétricos e valores atípicos - TripleTen

Capítulo 2/6

Estatística descritiva

# Dados assimétricos e valores atípicos

## Dados assimétricos

Em vários casos, os dados da vida real são distribuídos normal e simetricamente. No entanto, conjuntos de dados podem ser assimétricos, ou "dispersos", tanto em uma direção positiva quanto negativa.

![](https://practicum-content.s3.amazonaws.com/resources/4.2.10PT_1692885479.png)

Para saber se os dados são assimétricos, observe o histograma e localize um pico, que representa os valores mais frequentes. Uma distribuição de dados com mais valores à direita do pico do que à esquerda (como acima) é chamada **assimétrica à direita**. Isso é **assimetria positiva**.

Por exemplo, ao conduzir uma pesquisa e conferir as idades dos participantes, talvez a maioria das respostas seja de idosos e não de jovens. Isso seria uma distribuição assimétrica à direita.

Se o conjunto de dados tem mais valores à esquerda, ele é **assimétrico à esquerda**. Essa é, como você pode adivinhar, uma **assimetria negativa**. Continuando com o exemplo da pesquisa, se a maioria dos entrevistados fossem jovens e não idosos, os resultados seriam assimétricos com concentração à esquerda.

Agora vamos praticar!

Pergunta

Esses dados são assimétricos para qual direção?

![](https://practicum-content.s3.amazonaws.com/resources/_______1_1692002421.jpg)

À esquerda

À direita

O conjunto de dados é simétrico

Muito bem!

Você aprendeu sobre diagramas de caixa nas lições anteriores. Eles são uma ótima maneira de ilustrar a assimetria de uma distribuição. Vamos ilustrar com um exemplo. O gráfico abaixo contém dois subgráficos, e cada um exibe uma distribuição de tempo gasto. O subgráfico esquerdo mostra a distribuição na forma de um diagrama de caixa, enquanto o direito mostra um histograma normal girado em 90 graus no sentido anti-horário, o que facilita a comparação com o diagrama à esquerda.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_23_1692002468.png)

Primeiro, vamos focar no histograma à direita, que tem uma clara assimetria à esquerda. Podemos concluir isso porque a maioria dos valores está à esquerda do pico, que está próximo de 0.

O diagrama de caixa à esquerda mostra Q1, a mediana (ou Q2) e Q3. Anteriormente, aprendemos que um diagrama de caixa exibe os valores mínimo e máximo, localizados nos finais dos bigodes. No entanto, o que a seção preta em negrito no intervalo entre 650 e 1.000 representa? Ainda não aprendemos sobre isso.

Se ampliarmos a imagem, veremos que a seção em negrito não é uma linha, mas pontos.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_24_1692002484.png)

Esses são **valores atípicos**, extremos que se desviam significativamente do padrão geral dos valores de um conjunto de dados. Mas e como determinar se um valor tem um desvio significativo do padrão geral? Há uma regra para isso: se algum valor estiver 1,5 vez o intervalo interquartil (IQR, na sigla em inglês) à esquerda do Q1 ou 1,5 vez o IQR à direita do Q3, ele é considerado atípico.

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_9_1692885514.png)

Agora vamos praticar!

Pergunta

Você tem uma distribuição com os seguintes parâmetros:

-   Q1 = 25
-   Q3 = 41

Qual é o IQR?

16

20

41

Fantástico!

Pergunta

Continuaremos trabalhando com a mesma distribuição com os seguintes parâmetros:

-   Q1 = 25
-   Q3 = 41

Você precisa responder se 64 é um valor atípico ou não. Selecione a resposta correta.

Não, não é.

Sim, é atípico.

Fantástico!

Existe ainda outra maneira de determinar a assimetria de um conjunto de dados que não precisa de gráficos: basta comparar a média e a mediana.

Lembre-se de que a mediana não é afetada por valores atípicos, mas a média é. Como a média é muito mais sensível a valores atípicos, ela acaba sendo muito mais próxima deles do que a mediana. Portanto, em conjuntos de dados assimétricos à direita, a média é maior do que a mediana, e vice-versa: nos conjuntos assimétricos à esquerda, a média é menor do que a mediana.

Se pensarmos nisso, faz todo o sentido. Imagine que você tem uma distribuição perfeitamente simétrica, de tal forma que a média é igual à mediana. Agora mova um único ponto dos dados (um que já é maior do que a média) bem para a direita. Note que: a) a média vai aumentar, b) a mediana vai permanecer a mesma e c) os dados vão se tornar assimétricos à direita.

Uma observação aos interessados: definir a assimetria em termos da relação entre a média e a mediana é um bom ponto de partida, mas nem sempre é preciso. Pode haver conjuntos de dados assimétricos à direita cuja média é menor do que a mediana, e vice-versa. Isso acontece com mais frequência com distribuições discretas, nos casos em que a variável aleatória tem apenas alguns valores e alguns deles aparecem muito mais do que outros. Para distribuições contínuas, isso acontece quando uma distribuição é bimodal ou multimodal (ou seja, tem dois ou mais picos).

É sempre uma boa ideia criar um gráfico e olhar para a distribuição em vez de basear a conclusão apenas em um par de números que a descrevem.

Se você quiser saber mais sobre as sutilezas da assimetria, confira [este artigo](http://jse.amstat.org/v13n2/vonhippel.html) _(os materiais estão em inglês)_.

Pergunta

Qual sentença é verdadeira sobre o conjunto de dados ilustrado no histograma abaixo?

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_25_1692002637.png)

A mediana é maior do que a média.

A média é maior do que a mediana.

A mediana e a média são aproximadamente iguais.

Fantástico!

Pergunta

Usuários chegam ao seu site por propagandas contextuais e de mídias sociais. Os dois diagramas de caixa abaixo mostram o total de pedidos (em dólares) dos usuários dos dois canais.

Em que tipo de propaganda é necessário investir para maximizar a receita?

![](https://practicum-content.s3.amazonaws.com/resources/4.2.10.3PT_1692885546.png)

Em ambos os tipos: tanto as propagandas contextuais quanto as de mídias sociais são importantes.

Anúncios contextuais

Propagandas de mídias sociais

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-46-187Z.md
### Última modificação: 2025-05-28 19:18:46

# Quiz do capítulo - TripleTen

Capítulo 2/6

Estatística descritiva

# Quiz do capítulo

Pergunta

Combine cada conjunto com o tipo correspondente:

(0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5 ... 100)

Variável discreta quantitativa

Para começar, decida se é possível aplicar operações aritméticas nela. Se sim, é quantitativa. Caso contrário, categórica. Com certeza é possível aplicá-las nesse caso.

Qualquer valor entre -100 e 100

Variável contínua quantitativa

Você pode aplicar operações aritméticas aqui, e o intervalo é contínuo, ao invés de serem pontos discretos.

('vermelho', 'verde', 'azul')

Variável categórica

Não é possível executar nenhuma operação aritmética? Definitivamente categórica.

('um', 'dois', 'três')

Variável categórica

Não se confunda com nomes de números: eles são apenas nomes, e não números. Eles talvez se refiram a três grupos que estão sendo estudados.

Fantástico!

Pergunta

Imagine que você tem um objeto Series, `data`, que contém três números inteiros entre 0 e 99. Qual código produz o seguinte resultado?

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_26_1692002905.png)

`data.hist(bins = 3)`

`data.hist(bins = [0, 10, 40, 100])`

`data.hist(bins = 10, 40, 100)`

`data.hist(bins = [0, 10, 30, 60])`

Trabalho maravilhoso!

Pergunta

Por que histogramas não são bons para visualizar variáveis contínuas?

Se fizermos alterações nos limites das barras, vamos acabar com gráficos diferentes para a mesma distribuição

Mesmo que os valores dos limites das barras sejam alterados, o gráfico da distribuição não muda

Fantástico!

Pergunta

Uma colecionadora de pedras cria um histograma de densidade de frequência mostrando os pesos de suas pedras:

![](https://practicum-content.s3.amazonaws.com/resources/4.2.12PT_1692885586.png)

Quantas das pedras são mais leves do que 100 gramas?

60

120

180

200

Fantástico!

Pergunta

Cerca de 99% dos dados na distribuição normal podem ser encontrados em qual intervalo do eixo X? Essa é a regra dos três sigmas.

3 sigmas

3 variâncias

3 desvios padrão

6 desvios padrão

Fantástico!

Pergunta

Vamos passar para medições de localização. Temos a seguinte lista:

1, 2, 4, 7, 10, 11, 12, 14, 15

O que acontece se substituirmos o último valor por 100? Escolha a resposta correta. Podem existir várias respostas corretas.

Escolha quantas quiser

A mediana permanece a mesma

A mediana vai aumentar

A média vai diminuir

A média vai aumentar

Você conseguiu!

Pergunta

Quando precisamos saber a distância entre os valores e a média, calculamos a variância. Para cada ponto de dados, descobrimos qual é sua distância em relação à média e então elevamos o resultado ao quadrado. Para finalizar, pegamos todos esses valores e encontramos sua média.

Escolha a fórmula correta para a variância:

σ2\=(μ−xi)2n\\sigma^{2} = \\frac{\\left ( \\mu - x\_{i} \\right )^{2}}{n}σ2\=n(μ−xi​)2​

σ2\=∑(μ−xi)n\\sigma^{2} = \\frac{\\sum \\left ( \\mu - x\_{i} \\right )}{n}σ2\=n∑(μ−xi​)​

σ2\=∑(xi−μ)2n\\sigma^{2} = \\frac{\\sum \\left ( x\_{i} -\\mu \\right )^{2}}{n}σ2\=n∑(xi​−μ)2​

σ2\=∑(μ+xi)2n\\sigma^{2} = \\frac{\\sum \\left ( \\mu + x\_{i} \\right )^{2}}{n}σ2\=n∑(μ+xi​)2​

Excelente!

Pergunta

Você precisa encontrar a variância da seguinte lista:

data = \[1, 2, 4, 7, 10, 11, 12, 14, 15\]

Qual é a abordagem correta?

`import pandas as pd variance = pd.var(data)`

`import pandas as pd variance = pd.variance(data)`

`import numpy as np variance = np.var(data)`

`import numpy as np variance = var(data)`

`import numpy as np variance = np.variance(data)`

Muito bem!

Pergunta

Você é uma criatura evoluída matematicamente. Calcule a variância da lista abaixo de forma manual.

data = \[1, 2, 3\]

0.777777

3/2

3/4

2/3

Muito bem, você conseguiu calcular a variância!

Trabalho maravilhoso!

Pergunta

Considere o código a seguir.

```
import numpy as np
data = [1,2,3,4,5,10,20,30]
var = np.var(data)
```

Qual código retorna o desvio padrão desses dados? Podem existir várias respostas corretas.

Escolha quantas quiser

`np.sqrt(data)`

`np.sqrt(var)`

`np.std(data)`

`np.std(var)`

`pd.std(var)`

Trabalho maravilhoso!

Pergunta

Observe o gráfico a seguir:

![](https://practicum-content.s3.amazonaws.com/resources/2.9_pt_1693471594.png)

Quais sentenças sobre a distribuição são verdadeiras? Podem existir várias respostas corretas.

Escolha quantas quiser

A média é maior do que a mediana

O dados são assimétricos à esquerda.

Há uma assimetria negativa

Há uma assimetria positiva

Excelente!

Pergunta

Você tem os seguintes dados:

![](https://practicum-content.s3.amazonaws.com/resources/2.9.2_pt_1717138023.png)

Escolha o diagrama de caixa que corresponde a essa distribuição.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_9_1691581866.png)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_10_1691581894.png)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_11_1691581912.png)

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_12_1691581930.png)

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-47-487Z.md
### Última modificação: 2025-05-28 19:18:47

# Conclusão - TripleTen

Capítulo 2/6

Estatística descritiva

# Conclusão

Parabéns! Você agora se familiarizou com o básico da estatística descritiva.

Pergunta

Agora você é capaz de:

Escolha quantas quiser

Distinguir valores contínuos e discretos

Se ainda não tiver certeza, você pode revisar a lição em [Variáveis contínuas e discretas](https://tripleten.com/trainer/data-analyst/lesson/0cbb599d-5b2f-46dc-a411-4fe0a70f68a4/).

Construir histogramas para valores discretos e contínuos

Se ainda não tiver certeza, você pode revisar as lições em [Histogramas de frequência](https://tripleten.com/trainer/data-analyst/lesson/acdd320c-7ca5-4862-9c1b-f12c93902387/) e [Histogramas de densidade](https://tripleten.com/trainer/data-analyst/lesson/6a55013c-49b5-471e-8a5f-91ed32657cbc/).

Calcular as principais medidas de localização e variabilidade

Se ainda não tiver certeza, você pode revisar a lição em [Medidas de localização e variabilidade](https://tripleten.com/trainer/data-analyst/lesson/8dc0ee99-5861-4871-bbd2-b015b8bc91d6/).

Entender e ler o que diagramas de caixa ilustram

Se ainda não tiver certeza, você pode revisar a lição em [Medidas de localização e variabilidade](https://tripleten.com/trainer/data-analyst/lesson/8dc0ee99-5861-4871-bbd2-b015b8bc91d6/).

Calcular variância e desvio padrão

Se ainda não tiver certeza, você pode revisar as lições em [Variância](https://tripleten.com/trainer/data-analyst/lesson/76157367-66f2-4ba4-8875-1b9caea55b7c/) e [Desvio padrão e distribuições normais](https://tripleten.com/trainer/data-analyst/lesson/40ef2c0c-5da7-43f4-aa27-20bac8a9f8fb/).

Identificar se os dados estão dispersos e que impacto isso pode ter nas principais estatísticas dos dados

Se ainda não tiver certeza, você pode revisar a lição em [Dados assimétricos e valores atípicos](https://tripleten.com/trainer/data-analyst/lesson/d1d58e52-c9e4-4c78-b961-b3e2473ae570/).

Seu entendimento sobre o material é impressionante!

Agora você já é capaz de aprender como os dados se comportam de uma maneira mais abstrata: é hora de aprender sobre a teoria da probabilidade.

### Leve isso com você

Faça download do sumário do capítulo e folha de conclusões para que você possa consultá-los quando necessário.

-   [Resumo do Capítulo: Estatística Descritiva](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_Sprint_3/PT/1.Resumo_do_Capítulo_Estatística_Descritiva.pdf)
-   [Folha de Conclusões: Estatística Descritiva](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_Sprint_3/PT/1.Folha_de_concluses_Estatstica_descritiva.pdf)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-48-822Z.md
### Última modificação: 2025-05-28 19:18:49

# Introdução - TripleTen

Capítulo 3/6

Teoria da probabilidade

# Introdução

### É possível prever o futuro?

Ninguém pode prever o futuro com 100% de confiança, mas os profissionais em estatística são capazes de calcular a probabilidade de diferentes resultados em várias situações. Embora isso não resolva todos os problemas, pode facilitar as nossas vidas ao torná-las mais previsíveis.

Neste capítulo, você vai aprender a calcular as probabilidades de diferentes eventos usando dados diferentes.

### O que você vai aprender:

-   Fórmulas básicas para calcular a probabilidade de eventos únicos e múltiplos;
-   Fundamentos da análise combinatória;
-   Tipos da distribuição de características, incluindo distribuição binomial e normal;
-   Uso do Python para resolver problemas relacionados ao cálculo de probabilidade.

### No final deste capítulo, você será capaz de:

-   Avaliar o número de resultados possíveis;
-   Calcular as chances de eventos acontecerem, como a probabilidade de clientes comprarem a versão premium de um produto;
-   Automatizar cálculos de probabilidades com Python;
-   Usar o poder da matemática ao planejar o trabalho em condições incertas.

Este capítulo apresenta a você muitos novos conceitos que te permitirão se sentir confortável em entrevistas de emprego e avaliações de testes. Por exemplo, você vai aprender como lidar com perguntas em entrevistas, como "Qual é a probabilidade de obter uma soma de 9 jogando dois dados?" ou "Quantas palavras podem ser compostas a partir de 'Palavra' se trocarmos as letras?”

Este capítulo vai levar um pouco mais de tempo do que o habitual, cerca de 4 a 5 horas.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-51-264Z.md
### Última modificação: 2025-05-28 19:18:51

# Como prever o futuro? - TripleTen

Capítulo 3/6

Teoria da probabilidade

# Como prever o futuro?

Há uma velha piada:

-   Qual é a chance de encontrar um dinossauro durante uma corrida pela manhã?
    
-   50%: ou você vai encontrá-lo ou não vai.
    

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_34_1692620855.png)

Mesmo se você nunca estudou sobre a teoria da probabilidade, entende que calculá-la desse jeito não faz sentido. Então, como fazer isso do jeito certo? Vamos descobrir.

### Definições básicas

Para começar, vamos definir os principais termos. Imagine um cenário em que você joga um dado e visa obter um resultado igual ou maior do que 5. Aqui estão alguns termos para descrever a situação:

Termo

Definição

Exemplo

Experimento

Qualquer procedimento que pode ser repetido

Jogar um dado

Tentativa

Uma repetição de um experimento

Cada tentativa de jogar um dado

Resultado

Um resultado de um experimento executado uma vez

Conseguir o resultado 1, 2, 3, 4, 5 ou 6

Evento

Um resultado simples ou um conjunto de resultados aos quais pode ser atribuída uma probabilidade

Conseguir o resultado 5 ou 6 (qualquer resultado possível que seja igual ou maior que 5)

Todos esses termos estão relacionados com a **teoria da probabilidade**, um ramo da matemática que lida com a determinação das probabilidades associadas a eventos aleatórios.

### A fórmula principal

Quando você joga um dado, todos os resultados possíveis têm a mesma probabilidade de ocorrer. Já que há seis resultados possíveis e cada um deles é igualmente provável, a probabilidade de cada resultado é 1/6.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_35_1692620874.png)

Na estatística, usamos a letra _P_ para denotar a probabilidade e a letra _A_ para denotar um evento de interesse. Portanto, a notação _P(A)_ descreve a probabilidade do evento _A_.

A probabilidade _P(A)_ pode ser calculada da seguinte maneira:

# de resultados que satisfac¸am o evento# do total de resultados\\frac{\\text{\\# de resultados que satisfaçam o evento}} {\\text{\\# do total de resultados}}# do total de resultados# de resultados que satisfac¸​am o evento​

Se o evento _A_ for definido como "conseguir um número igual ou maior do que 5", então _P(A)_ é (1 + 1) / 6 = 1/3.

![](https://practicum-content.s3.amazonaws.com/resources/26_07_DA_DS-03_1691758724.png)

Pergunta

Um dado é jogado. Seja o evento _A_ "jogar um número par". Como fica o _P(A)_?

3/6

2/6

1

1/6

Excelente!

Muitos outros cenários possuem essa propriedade. Por exemplo, ao tirar uma carta aleatoriamente de um baralho de 52 cartas, todas as cartas têm a mesma possibilidade de serem tiradas. Se o evento _A_ for definido como "tirar um oito de copas" (ou alguma outra carta específica), então a probabilidade _P(A)_ será 1/52.

Podemos generalizar esse cenário. Se…

-   um experimento é executado uma vez,
-   a probabilidade de cada resultado é a mesma e
-   existem _n_ resultados possíveis,

então a probabilidade de ocorrência de cada um dos resultados é 1/_n_.

Isso revela uma propriedade crucial da probabilidade: **a soma das probabilidades de todos os resultados possíveis em um experimento deve ser igual a 100% ou 1**. Isso garante que um dos resultados possíveis vai ocorrer.

Ao jogar um dado, há seis resultados possíveis, e cada um possui uma probabilidade de 1/6. A soma dessas probabilidades é 1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6 = 1. Assim, temos uma garantia de que o resultado do dado vai ser 1, 2, 3, 4, 5 ou 6.

Pergunta

Imagine que você está organizando uma rifa com dez bilhetes numerados de 1 a 10, onde o bilhete #1 é o vencedor e os outros não. Um cliente vem comprar um bilhete. Quantos resultados possíveis existem?

1

10

2

1/10

Você conseguiu!

### Eventos e espaços amostrais

O conjunto de todos os resultados possíveis de um experimento é chamado de **espaço amostral** e costuma ser representado como _S_.

Para ilustrar, considere o exemplo de jogar um dado uma vez. O espaço amostral associado a esse evento é o seguinte:

S\={1,2,3,4,5,6}S = \\{1, 2, 3, 4, 5, 6\\}S\={1,2,3,4,5,6}

![](https://practicum-content.s3.amazonaws.com/resources/26_07_DA_DS-02_1691758775.png)

Vamos explorar como calcular a probabilidade de um evento do espaço amostral.

Por exemplo, vamos supor que o evento A seja "conseguir um número menor do que 3.” Depois de examinarmos o espaço amostral, observamos que há dois números menores do que 3. Desse modo, a probabilidade do evento A denotada por P(A), onde P representa a probabilidade, é 2 (o número de resultados que satisfazem o evento A) dividido por 6 (o comprimento de S).

![](https://practicum-content.s3.amazonaws.com/resources/26_07_DA_DS-04_1691758800.png)

Pergunta

Vamos voltar para o exemplo com bilhetes de rifa. Temos o mesmo conjunto que antes, mas os resultados mudaram. O bilhete #1 ganha $100, os bilhetes #2 e #3 ganham $10 cada, o bilhete 4 ganha outro bilhete da rifa e o resto dos bilhetes não são vencedores. Como fica _S_ nessa situação?

{100$, 10$, novo bilhete da rifa, bilhetes não vencedores}

{dinheiro, novo bilhete da rifa, bilhetes não vencedores}

{vencedores, não vencedores}

{#1 ($100), #2 ($10), #3 ($10), #4 (novo bilhtete), #5 (-), #6 (-), #7 (-), #8 (-), #9 (-), #10 (-)}

Seu entendimento sobre o material é impressionante!

Às vezes, o espaço amostral é pequeno, e é possível listar todos os possíveis resultados, mas, conforme o número de experimentos aumenta, o espaço amostral aumenta bastante.

Agora vamos jogar dois dados ao invés de um só. Este é o espaço amostral quando dois dados são jogados:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_13_1691589816.png)

O documento fala sobre o espaço amostral de jogar dois dados, onde cada par é o resultado do primeiro e do segundo dado. Ao adicionar apenas um dado, o espaço amostral cresce de 6 resultados para 36. Como cada dado é independente do outro, todos os 36 resultados têm a mesma chance de ocorrer. As probabilidades ainda podem ser calculadas da mesma maneira.

Pergunta

Dois dados são jogados. Seja o evento _A_ a probabilidade de "a soma dos dois dados ser 5." Como fica o _P(A)_?

8/36

5/36

4/36

1/36

Muito bem!

### A lei dos grandes números

Cada evento é associado a uma probabilidade. Em sistemas complexos de grandes populações, calcular a probabilidade real de um evento pode ser muito difícil, ou até impossível. É aí que contamos com as estimativas.

Ao estimar a probabilidade de um evento, podemos confiar em um teorema muito útil, a **lei dos grandes números**. Essa lei afirma que quanto mais vezes um experimento é repetido, mais próxima estará a frequência relativa de determinado evento (o número de vezes que o evento ocorre dividido pelo número de vezes que o experimento é repetido) de sua verdadeira probabilidade.

Por exemplo, já sabemos que, ao jogarmos uma moeda, a probabilidade de sair cara é de 50%. Entretanto, se não tivéssemos essa informação, poderíamos estimar a probabilidade jogando uma moeda muitas vezes e analisando o número de caras e o número de jogadas.

A lei dos grandes números afirma que conforme o número de jogadas se aproxima do infinito, a frequência relativa desse evento vai se aproximar da real probabilidade do evento.

As probabilidades de eventos complexos são estimadas fazendo experimentos muitas vezes e usando frequências relativas para estimar as probabilidades.

### Probabilidades de espaço amostral em Python

Em Python, cálculos simples de probabilidades baseados em espaços amostrais como esses são feitos com facilidade usando o operador lógico `==` e a função `len()`.

-   Usar `==` identifica as linhas que vão satisfazer o evento.
-   `len()` fornece a contagem das linhas que satisfazem ao evento.

Aqui está um exemplo:

Você criou uma playlist no Spotify com duas colunas: `'Artist'` (artista) e `'Song'` (música).

```
import pandas as pd

cool_rock = pd.DataFrame(
    {
        'Artist': [
            'Queen',
            'Queen',
            'Queen',
            'Pink Floyd',
            'Nirvana',
            'AC/DC',
            'AC/DC',
            'Scorpions',
            'Scorpions',
            'Scorpions',
        ],
        'Song': [
            'The Show Must Go On',
            'Another One Bites The Dust',
            'We Will Rock You',
            'Wish You Were Here',
            'Smells Like Teen Spirit',
            'Highway To Hell',
            'Back in Black',
            'Wind Of Change',
            'Still Loving You',
            'Send Me An Angel',
        ],
    }
)
print(cool_rock)
```

```
                Artist                        Song
0       Queen         The Show Must Go On
1       Queen  Another One Bites The Dust
2       Queen            We Will Rock You
3  Pink Floyd          Wish You Were Here
4     Nirvana     Smells Like Teen Spirit
5       AC/DC             Highway To Hell
6       AC/DC               Back in Black
7   Scorpions              Wind Of Change
8   Scorpions            Still Loving You
9   Scorpions            Send Me An Angel
```

Assumindo que você vai colocá-la no modo aleatório, cada música na lista de artista + músicas tem a mesma probabilidade de tocar primeiro. Esta lista é o nosso espaço amostral.

Qual é a probabilidade de que a primeira música tocada seja `"Smells Like Teen Spirit"`?

Para identificar os resultados satisfatórios para esse evento, use `==` e use `len()` duas vezes: uma para calcular a quantidade de resultados que satisfazem o evento e outra para contar o tamanho do espaço amostral.

```
print(
    len(cool_rock[cool_rock['Song'] == 'Smells Like Teen Spirit'])
    / len(cool_rock)
)
```

```
0.1
```

Você aprendeu muitos termos novos! Para conferir que você entendeu tudo, tente resolver as tarefas abaixo.

---

Após ativar o modo aleatório, você precisa calcular a probabilidade de que a primeira música seja da banda `'Queen'`. Escreva o código para esse cálculo.

Use a variável `desired_outcomes` para armazenar o número de resultados desejados, `total_outcomes` para armazenar o número total de resultados e `probability` para o resultado final. Feito isso, imprima `probability`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

import pandas as pd

  

cool\_rock \= pd.DataFrame(

{

'Artist': \[

'Queen',

'Queen',

'Queen',

'Pink Floyd',

'Nirvana',

'AC/DC',

'AC/DC',

'Scorpions',

'Scorpions',

'Scorpions',

\],

'Song': \[

'The Show Must Go On',

'Another One Bites The Dust',

'We Will Rock You',

'Wish You Were Here',

'Smells Like Teen Spirit',

'Highway To Hell',

'Back in Black',

'Wind Of Change',

'Still Loving You',

'Send Me An Angel',

Dica

Mostrar a soluçãoValidar

Muito bem! Agora que você aprendeu os fundamentos da teoria da probabilidade, você está com tudo pronto para se aprofundar em tópicos mais específicos.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-52-582Z.md
### Última modificação: 2025-05-28 19:18:52

# Eventos independentes e multiplicação de probabilidades - TripleTen

Capítulo 3/6

Teoria da probabilidade

# Eventos independentes e multiplicação de probabilidades

Agora vamos considerar um exemplo mais complexo.

Suponha que você jogou dois dados, cada um com a probabilidade igual de obter um número de 1 a 6. Digamos que temos interesse na soma total quando os dois dados são jogados. Podemos visualizar todos os resultados possíveis assim:

![](https://practicum-content.s3.amazonaws.com/resources/____1691590023.jpg)

Esse é um diagrama de espaço amostral. Os números ao longo dos eixos representam os resultados de cada dado, e as interseções representam a soma dos resultados dos dois dados.

O diagrama mostra que há apenas um resultado (dos 36 possíveis) em que a soma resulta em 2, em comparação aos outros seis em que a soma é igual a 7.

### Eventos mais complicados

Podemos usar o último exemplo para examinar eventos mais complicados. Por exemplo, considere a questão de quantos resultados dão uma soma maior do que 8 quando jogamos dois dados. Contar o número de células azuis abaixo (ou seja, as células que satisfazem esse evento) retorna uma resposta de 10.

![](https://practicum-content.s3.amazonaws.com/resources/____1_1691590040.jpg)

Podemos reformular a pergunta assim: qual é a probabilidade de jogar dois dados e obter uma soma maior do que 8?

Se definirmos o evento _A_ como "obter uma soma maior do que 8", então a probabilidade de _A_ é _P(A)_ = 10/36 = 27,7%.

Pergunta

O diagrama do espaço amostral abaixo representa um evento para o experimento de "calcular a soma total obtida ao jogar dois dados". Selecione todas as afirmações corretas sobre esse evento.

![](https://practicum-content.s3.amazonaws.com/resources/____2_1691590115.jpg)

A soma total é maior do que 5.

A soma total é 6 ou menos.

Cada dado mostra um resultado igual ou menor que 5.

Cada dado exibe um valor de 1 a 4.

Fantástico!

### Intersecção de eventos

É possível que um resultado seja associado a múltiplos eventos. Por exemplo, os seguintes eventos compartilham resultados:

-   Evento _A_ – a soma de dois dados é maior que 8
-   Evento _B_ – os dados exibem o mesmo número

![](https://practicum-content.s3.amazonaws.com/resources/image_1691590176.jpg)

Observe que se ambos os dados exibem 5 ou 6, então ambos os eventos _A_ e _B_ ocorrem.

### Diagramas de Venn

Para ilustrar a ocorrência de ambos os eventos (como _A_ e _B_ acima, quando ambos os dados exibiram 5 ou 6), podemos usar um **diagrama de Venn**. Ele não exibe pontos amostrais, apenas a intersecção relevante entre os eventos:

![](https://practicum-content.s3.amazonaws.com/resources/4.3.4.2PT_1_1693469725.png)

Eventos que não podem ocorrer simultaneamente no mesmo experimento são chamados **mutuamente exclusivos** — eles não se intersectam no diagrama de Venn:

![](https://practicum-content.s3.amazonaws.com/resources/4.3.4PT_1692885857.png)

A probabilidade de intersecção de eventos mutuamente exclusivos é 0.

Se os eventos _A_ e _B_ são mutuamente exclusivos, quer dizer que _P(A e B)_ = 0. Como você pode ver acima, se o evento _A_ ocorre, não há como o evento _B_ também ter ocorrido.

Se eventos mutuamente exclusivos cobrem todo o espaço amostral, então a soma de suas probabilidades será igual a 1.

Por exemplo, vamos considerar os seguintes eventos:

-   Os dois dados exibem o mesmo número (células vermelhas na imagem abaixo).
-   O primeiro dado exibiu um número maior do que o segundo (células verdes).
-   O segundo dado exibiu um número maior do que o primeiro (células azuis).

Todos os pontos amostrais estão inclusos nesses eventos; eles cobrem todo o espaço amostral. As probabilidades são 6/36, 15/36 e 15/36, respectivamente. Juntas, elas somam 1: (6 + 15 + 15) / 36 = 1.

![](https://practicum-content.s3.amazonaws.com/resources/3.3_es_1694513000.png)

Pergunta

Quais eventos são mutuamente exclusivos?

O primeiro dado exibe 1. O segundo exibe um número maior do que o primeiro.

O primeiro dado exibe 6. O segundo exibe um número menor do que o primeiro.

O primeiro dado exibe um número maior do que o segundo, enquanto o segundo exibe um número maior do que o primeiro.

Ambos os dados exibem o número 5.

Excelente!

### Eventos independentes e dependentes

Eventos são chamados de **independentes** se a ocorrência de um não afeta a probabilidade de outro.

Se o evento _A_ = _João paga os 10 reais extras por uma garantia de um ano ao finalizar a compra_" e _B_ = _Sofia clica em um clickbait_, então o fato de João pagar ou não pela garantia não vai afetar a decisão de Sofia. Podemos afirmar que esses dois eventos são independentes.

Se o evento _A_ = _chove_ e _B_ = _você vai para o trabalho de guarda-chuva_, então a probabilidade de B vai depender da probabilidade de A. Esses são eventos **dependentes**.

Com um diagrama de Venn, é fácil verificar se os eventos são mutuamente exclusivos. Entretanto, não é tão fácil determinar a independência deles. Você precisa conferir se o produto das probabilidades dos eventos é igual à probabilidade da intersecção. Para encontrar a probabilidade da intersecção entre eventos independentes (de ambos eventos ocorrerem), precisamos encontrar o produto das probabilidades de cada evento.

### Probabilidades de eventos independentes

Essa é a parte boa em relação a eventos independentes…

Se os eventos _A_ e _B_ são independentes, então _P(A e B)_ é igual a _P(A)_ x _(P(B)_.

Isso quer dizer que você precisa apenas saber as probabilidades individuais para calcular a probabilidade de ambos ocorrerem.

**Observação**: Isso NÃO é caso para eventos dependentes.

### Exemplo

Sejam os eventos A e B iguais a…

-   A = _o primeiro dado exibe 3_
-   B = _o segundo dado exibe 5_

Esses eventos são independentes: o resultado do primeiro dado não tem efeito no segundo. Já que cada lançamento de um dado tem a mesma probabilidade de resultar em um valor de 1 a 6, sabemos que _P(A)_ = 1/6 e _P(B)_ = 1/6. Usando a fórmula apresentada acima, podemos calcular _P(A e B)_ = 1/6 × 1/6 = 1/36.

Visualmente, percebemos que esse cálculo está correto ao observarmos 1 resultado entre os 36 que satisfazem tanto A quanto B.

![](https://practicum-content.s3.amazonaws.com/resources/image_2_1691590286.jpg)

Aqui estão algumas questões para testar sua compreensão.

Pergunta

Os eventos "um dado exibe um valor de 2 a 4" e "o outro exibe um valor de 3 a 5" são independentes no mesmo espaço amostral?

Dependentes

Independentes

Precisamos saber mais sobre os dados. Eles foram jogados pela mesma pessoa?

Seu entendimento sobre o material é impressionante!

O caso do quiz acima pode ser ilustrado da seguinte maneira:

![](https://practicum-content.s3.amazonaws.com/resources/____5_1691590464.jpg)

Pergunta

Os eventos "um dos dados exibe 2" e "os dados exibem 7 no total" são independentes no espaço amostral?

Dependentes

Independentes

Precisamos aprender mais sobre os dados. Eles foram jogados pela mesma pessoa?

Trabalho maravilhoso!

O caso desse quiz pode ser ilustrado assim:

![](https://practicum-content.s3.amazonaws.com/resources/____6_1691659567.jpg)

Pergunta

29% dos usuários de um site se identificaram com o gênero masculino, 25% com feminino e o restante não especificou um gênero. Qual é a probabilidade de que, quando dois usuários são selecionados aleatória e independentemente, o gênero do primeiro usuário será masculino e o do segundo será desconhecido?

11,5%

7,25%

13,34%

Excelente!

Pergunta

O site é composto por seções, essas seções são compostas por subseções e as subseções são compostas por páginas, como demonstrado no gráfico.

![](https://practicum-content.s3.amazonaws.com/resources/4.3.4.3PT_1692885904.png)

Um usuário vai da página inicial do site para uma das seções, depois para uma subseção e depois para uma página. Sempre que um link é clicado, cada opção possível é igualmente provável. Qual é a probabilidade de que ele vai abrir a subseção necessária?

Aproximadamente 5,27%

Aproximadamente 2,9%

Aproximadamente 4,63%

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-53-897Z.md
### Última modificação: 2025-05-28 19:18:54

# Fundamentos da combinatória e mais problemas de probabilidades - TripleTen

Capítulo 3/6

Teoria da probabilidade

# Fundamentos da combinatória e mais problemas de probabilidades

Nesta lição, vamos continuar resolvendo problemas relacionados à tarefa de descobrir as probabilidades de ocorrência de diferentes eventos. Além disso, aprenderemos novos métodos para resolver problemas mais desafiadores e complexos.

Imagine que você está trabalhando em um projeto cujo objetivo é desenvolver um novo aplicativo de celular. O projeto acabou de começar, e você recebeu um convite para participar do processo de resolução de problemas relacionados ao recrutamento.

Os recrutadores selecionaram 5 candidatos para a posição de programador, mas apenas 3 são necessários para o projeto. Os dois candidatos restantes irão para outras equipes. Você conhece pessoalmente um dos candidatos e gostaria de trabalhar com ele.

Pergunta

A gerente de RH planeja entrevistar cada um dos 5 candidatos. Se a ordem das entrevistas é aleatória, qual é a probabilidade do seu conhecido ser entrevistado primeiro?

1/5

3/5

5/3

0.5

Excelente!

A gerente de RH pediu assistência para selecionar opções da sequência em que os candidatos serão convidados para as entrevistas. Como podemos criar essa lista?

### Permutações. Como calcular o fatorial

O problema que você está enfrentando é contar o número de possíveis **permutações**. Aqui está outro exemplo desse problema:

Uma escola online forneceu três cursos gratuitos a um estudante:

-   Como escrever um currículo eficaz?
-   Assistência profissional na orientação de carreira.
-   Melhore suas habilidades em entrevistas!

Esses cursos podem ser feitos em qualquer ordem. Quantos percursos educacionais diferentes podem ser criados a partir desses cursos?

Para calcular o número de permutações de _n_ elementos, os matemáticos usam esta fórmula:

Pn\=n!P\_n = n!Pn​\=n!

Você talvez não tenha encontrado o sinal "!" em expressões matemáticas até agora. Ele é chamado de **fatorial** e significa o produto de números naturais de 1 a _n_. Ou seja:

n!\=1⋅2⋅3⋅...⋅(n−1)⋅nn! = 1 \\cdot 2 \\cdot 3 \\cdot ... \\cdot (n-1) \\cdot nn!\=1⋅2⋅3⋅...⋅(n−1)⋅n

Então, para os três cursos diferentes, há 3! = 1 x 2 x 3 = 6 percursos educacionais diferentes.

### Como calcular fatoriais em Python

Quando o valor de _n_ é pequeno, _n!_ pode ser calculado de cabeça. Para valores maiores de _n_, é mais conveniente automatizar o processo. Para fazer isso, você pode usar a função `factorial()` do módulo `math` da biblioteca padrão de Python.

```
# Importe a função factorial do módulo math
from math import factorial

# Defina o número necessário de cursos
courses_amount = 3

# Calcule o fatorial de 3 (o valor da variável courses_amount)
result = factorial(courses_amount)

# Imprima o resultado
print(result)
```

```
6
```

Agora você está com tudo pronto para resolver a tarefa sobre o número de opções de entrevista por conta própria.

Cinco candidatos foram escolhidos para o projeto. A gerente de RH quer entrevistar cada um deles, mas não consegue decidir em que ordem conversar com os candidatos. Para fazer isso, ela precisa descobrir quantas maneiras existem para formar equipes de três pessoas selecionando-as de cinco candidatos.

Salve o resultado dos cálculos na variável `lists_amount`. Imprima essa variável.

CódigoPYTHON

9

1

2

3

4

5

6

from math import factorial

  

candidates\_amount \= 5

  

lists\_amount \= factorial(candidates\_amount)

print(lists\_amount)

Dica

Mostrar a soluçãoValidar

A gerente de RH tem uma nova ideia: talvez seja melhor procurar a equipe ideal diretamente, em vez de selecionar candidatos individuais. Para fazer isso, a gerente precisa descobrir quantas maneiras existem para formar equipes de três pessoas selecionando-as de cinco candidatos. Isso vai permitir que ela agende entrevistas para as equipes. Você consegue ajudá-la a resolver esse problema?

### Combinações

Essa é uma nova tarefa para você e, antes de resolvê-la, precisamos aprender uma nova fórmula: a fórmula para calcular o número de combinações de _k_ elementos de _n_ opções possíveis. A fórmula é a seguinte:

Cnk\=n!k!⋅(n−k)!C\_n^k = \\frac{n!}{k! \\cdot (n-k)!}Cnk​\=k!⋅(n−k)!n!​

Por exemplo, se um café vende 10 tipos de sorvetes e você quer comprar três sabores diferentes, você terá:

C103\=10!3!⋅(10−3)!\=10!3!⋅7!\=1⋅2⋅3⋅4⋅5⋅6⋅7⋅8⋅9⋅10(1⋅2⋅3)⋅(1⋅2⋅3⋅4⋅5⋅6⋅7)\=120C\_{10}^3 = \\frac{10!}{3! \\cdot (10-3)!} = \\frac{10!}{3! \\cdot 7!} = \\frac{1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdot 5 \\cdot 6 \\cdot 7 \\cdot8 \\cdot 9 \\cdot 10}{(1 \\cdot 2 \\cdot 3) \\cdot (1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdot 5 \\cdot 6 \\cdot 7)} = 120C103​\=3!⋅(10−3)!10!​\=3!⋅7!10!​\=(1⋅2⋅3)⋅(1⋅2⋅3⋅4⋅5⋅6⋅7)1⋅2⋅3⋅4⋅5⋅6⋅7⋅8⋅9⋅10​\=120

O mesmo pode ser calculado em Python:

```
from math import factorial

# Defina os valores para as variáveis n e k
n = 10
k = 3

# Faça os cálculos
combinations = factorial(n) / (factorial(k) * factorial(n-k))
print(combinations)
```

```
120.0
```

Agora você está com tudo pronto para ajudar a gerente de RH e determinar o número de entrevistas necessárias para conversar com todas as possíveis combinações de equipes.

Conclua o código abaixo para determinar o número de maneiras de formar equipes de três pessoas selecionando-as de cinco candidatos. Salve o resultado dos cálculos na variável `combinations` e o imprima.

CódigoPYTHON

9

1

2

3

4

5

6

7

from math import factorial

  

n \= 5

k \= 3

  

combinations \= factorial(n) / (factorial(k) \* factorial(n\-k))

print(combinations)

Dica

Mostrar a soluçãoValidar

### Uso de fórmulas de análise combinatória para calcular probabilidades

Sua habilidade de contar possíveis resultados para eventos complexos permite determinar as probabilidades de diferentes resultados ou combinações de resultados.

Para concluir todas as tarefas a seguir, você vai precisar usar fórmulas para calcular permutações ou combinações para determinar o número total de resultados possíveis. Em seguida, divida o número de resultados desejados por esse resultado para determinar a probabilidade.

## Tarefas

### Tarefa 1

Você e seus amigos estão desenvolvendo um jogo de missões que consiste em 10 tarefas diferentes. As tarefas podem ser concluídas em qualquer ordem, mas apenas uma sequência de conclusão permite que os jogadores ganhem o prêmio principal. Qual é a probabilidade de ganhar o prêmio principal, assumindo que a probabilidade de escolher cada tarefa em qualquer etapa da missão seja a mesma?

Conclua o código abaixo para calcular a probabilidade. Use as seguintes variáveis:

-   `tasks` — para armazenar o número de tarefas;
-   `permutations` — para armazenar o número de permutações;
-   `probability` — para armazenar a probabilidade de escolher a única sequência das tarefas que permite aos jogadores ganhar o prêmio principal.

CódigoPYTHON

9

1

2

3

4

5

6

7

from math import factorial

  

tasks \= 10

permutations \= factorial(tasks)\# Calcule o número total de possíveis sequências de tarefas aqui

probability \= 1/permutations\# Calcule a probabilidade de selecionar a única combinação vencedora aqui

  

print(probability)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Os clientes estavam insatisfeitos com a dificuldade de vencer o prêmio principal, então você alterou as regras. Agora os jogadores podem escolher três tarefas para começar o jogo. A ordem das tarefas não importa, o importante é a combinação delas. Se os jogadores conseguirem adivinhar a "combinação secreta", eles recebem um código promocional.

Para garantir equidade, você precisa calcular a probabilidade de receber o código promocional usando as seguintes variáveis:

-   `tasks` — número total de tarefas disponíveis
-   `chosen` — número de tarefas escolhidas no começo do jogo
-   `combinations` — número total de possíveis combinações de 3 tarefas a partir das 10 disponíveis
-   `probability` — probabilidade final de receber o código promocional

Implemente um programa que calcule a probabilidade de receber o código promocional usando as variáveis acima.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

9

from math import factorial

  

tasks \= 10

chosen \= 3

  

combinations \= factorial(tasks) / (factorial(chosen) \* factorial(tasks\-chosen))\# Calcule o número de combinações disponíveis

probability \= 1/combinations

  

print(probability)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-18-58-746Z.md
### Última modificação: 2025-05-28 19:18:59

# A ideia de distribuição - TripleTen

Capítulo 3/6

Teoria da probabilidade

# A ideia de distribuição

Agora que você sabe como experimentos, resultados e eventos se relacionam com as probabilidades, está com tudo pronto para conhecer o conceito de uma **variável aleatória**.

### Variáveis aleatórias

Variáveis aleatórias são aquelas que assumem valores aleatórios. O termo "aleatório" aqui significa que não podemos prever o valor assumido pela variável com 100% de precisão.

Por exemplo, seja _X_ a altura da próxima pessoa que passar pela sua porta. _X_ é uma variável aleatória com um intervalo de valores possíveis. O valor real que _X_ vai assumir é desconhecido até que alguém passe pela porta. Nesse ponto, o valor de _X_ será observado.

Na teoria da probabilidade, nosso interesse está em saber todos os valores possíveis que _X_ pode assumir e as probabilidades associadas a esses valores.

### Variáveis aleatórias definidas como números

Experimentos possuem resultados que podem ser descritos de maneira quantitativa ou qualitativa. Por exemplo, que número vai aparecer quando um dado for jogado, se um visitante de uma loja online fará uma compra, quantas pessoas vão visitar um café no final de semana e quanto dinheiro cada uma vai gastar.

Para trabalhar com esses resultados, precisamos definir uma variável aleatória numericamente. Isso nos permite projetar os resultados do experimento em um eixo numérico. Por exemplo, para os visitantes de uma loja online, podemos denotar o resultado como 1 se eles fazem uma compra e como 0 se não.

Uma vez que tenhamos decidido como representar os resultados numericamente, podemos trabalhar com os números, o que costuma ser mais fácil.

### Variáveis aleatórias podem ser discretas ou contínuas

Como todas as variáveis quantitativas, variáveis aleatórias podem ser **discretas** ou **contínuas**. Por exemplo, o tempo gasto em um site é uma variável contínua, enquanto o número de compras é um exemplo de uma variável discreta.

Seja _X_ o tempo gasto em um site.

Seja _Y_ o número de compras em uma loja online.

_X_ e _Y_ são variáveis aleatórias.

Pergunta

Selecione a afirmação correta sobre variáveis aleatórias:

Uma variável aleatória deve ser definida numericamente, é por isso que todas as variáveis aleatórias são discretas.

Não podemos predizer com 100% de precisão qual valor uma variável aleatória vai assumir.

A cor do primeiro carro que passar perto da sua casa é um exemplo de uma variável aleatória.

Uma variável aleatória pode assumir um valor específico que não sabemos antecipadamente.

Muito bem!

### Distribuições de probabilidade

Ótimo! Sabemos como calcular a probabilidade de valores discretos, como a probabilidade de obter um determinado valor jogando um dado. Mas e quanto a casos mais complexos? Como podemos descobrir a probabilidade de que:

-   Um cliente vai gastar mais de $50 na loja?
-   Menos de 5% das peças produzidas em uma fábrica terão defeitos?
-   Pelo menos 25 pessoas vão comprar ingressos para uma sessão de cinema?

Até mesmo para valores discretos, os cálculos podem não ser tão simples como eram no caso dos dados.

![](https://practicum-content.s3.amazonaws.com/resources/26_07_DA_DS-07_PT_1692885957.png)

Para aprender a resolver esses problemas, precisamos primeiro aprender sobre o conceito da **distribuição de probabilidade**.

Na estatística, uma distribuição de probabilidade é uma função matemática que mostra a probabilidade de diferentes resultados para um experimento. Nesta lição, vamos mostrar tipos diferentes de distribuição de probabilidade usando descrições verbais em vez de fórmulas matemáticas.

Vamos explorar os tipos mais comuns de distribuição de probabilidade.

Tipo de distribuição

Descrição

Exemplo

Distribuição binomial

É uma distribuição de probabilidade discreta que descreve o número de sucessos em um número fixo de tentativas.

O resultado de uma conversão de ligações publicitárias em compras: sucesso – a compra é feita, fracasso – o cliente recusa comprar.

Distribuição uniforme

É uma distribuição de probabilidade contínua onde cada valor em um determinado intervalo tem a mesma chance de ser selecionado.

Os resultados de um lançamento de dados: cada resultado tem a mesma probabilidade.

Distribuição normal

É uma distribuição de probabilidades discreta ou contínua em que os dados tendem a ser agrupados em torno de uma média ou um valor médio.

Resultados do ENEM: a maioria dos estudantes no término da educação básica recebem uma nota média. Quanto mais próximos os resultados estejam do valor mínimo ou máximo, com menos frequência eles ocorrem.

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_10_1692885981.png)

A imagem acima apresenta um gráfico de resultados do exame SAT (análogo do ENEM nos EUA), em que o eixo X representa as notas e o eixo Y representa a frequência das notas. O gráfico mostra que 55% dos estudantes obtiveram resultados médios, com notas entre 1.000 e 1.250. Notas muito altas e muito baixas foram obtidas por apenas 1-2% de todos os participantes do teste.

Pergunta

Quais destes são exemplos de uma distribuição normal?

Escolha quantas quiser

Distribuição dos visitantes do site naqueles que clicaram no link e naqueles que não clicaram

Resultados de uma prova (passou ou reprovou)

Tamanhos de calçados masculinos

Quantidade de dinheiro gasto durante as férias

Muito bem!

### Uso do Python para trabalhar com distribuição

Para criar conjuntos de dados com vários tipos de distribuição, vamos usar a biblioteca NumPy. A biblioteca trabalha com vetores de dados com muitas dimensões e possui jeitos rápidos de fazer cálculos complexos com muitos números de uma vez.

Como você pode lembrar, o objeto principal da biblioteca NumPy é um objeto **ndarray** (N-dimensional array, ou seja, vetor N-dimensional), que é um vetor multidimensional que contém elementos do mesmo tipo. O tamanho de um vetor NumPy é determinado no momento de sua criação e não pode ser modificado durante a execução do código.

Para criar um ndarray, usamos a função `np.array()`. Como um argumento, você precisa especificar uma lista de dados que devem ser colocados em um vetor:

```
import numpy as np

data = np.array([1, 3, 5, 7, 11, 13, 17, 19, 23, 317])
```

Você pode acessar os elementos de um ndarray usando índices, assim como fez com listas:

```
print('O primeiro elemento:', data[0])
print('O último elemento:', data[-1])
print('Todos os elementos, exceto o primeiro e o último:', data[1:-1])
```

```
O primeiro elemento: 1
O último elemento: 317
Todos os elementos, exceto o primeiro e o último: [ 3  5  7 11 13 17 19 23]
```

A NumPy pode ser usada para gerar números aleatórios com uma distribuição especificada, usando a função `normal()` do módulo `random`. Para gerar um vetor de 20 números que seguem uma distribuição normal, use o seguinte código:

```
import numpy as np

data = np.random.normal(size = 20)
print(data)
```

```
[-2.17697439 -1.2377386  -0.93172467  0.55590602 -0.50272766  1.2035185
 -0.89225077 -0.68392353 -0.56675546  0.9228669  -0.72231798 -0.0259272
  0.29761004  0.73541293  1.36204941 -0.95317916  1.29747695 -0.58452232
  1.46041457 -0.00316409]
```

Observe que o `data` gerado pode variar, porque os números são gerados aleatoriamente.

Para gerar um vetor com uma média específica e desvio padrão, precisamos especificar os valores desses parâmetros:

```
import numpy as np

mean = 15  # ponto médio entre 1 e 30
std_dev = 5  # desvio padrão arbitrário

data = np.random.normal(mean, std_dev, size = 20)
print(data)
```

```
[10.62711882  7.8798645  18.14475199 11.91370968 22.47197371 11.51284177
 16.4413512  11.45685538 19.32405617 11.29332392 19.76644576 17.96411027
 20.49640535 11.51180174  9.27728821 17.24437757 14.49474732 13.56894342
 17.21277733 11.32012319]
```

Agora estamos com tudo pronto para trabalhar com os dados. Vamos supor que o vetor contém o peso dos pacotes com compras dos 30 primeiros visitantes de uma loja. Quantos visitantes saíram com um pacote que pesava mais do que a média?

```
# Declare uma variável para armazenar o número de usuários
# Como ainda não começamos a contar, 
# o número de clientes com um pacote pesado é zero.
visitors_number = 0

# Crie um ciclo para iterar sequencialmente sobre os pesos de todos os pacotes.
for weight in data:

    # Se o peso do pacote atual for maior do que a média, aumente o contador
    if weight > mean:
        visitors_number += 1

# Imprima o número de usuários com pacotes pesados.
print('Número de usuários com pacotes pesados:', visitors_number)
```

```
Número de usuários com pacotes pesados: 9
```

Agora é hora de praticar:

## Tarefas

### Tarefa 1

30 estudantes fizeram uma prova. As notas deles estão armazenadas na variável `exam_results`. Se um estudante obteve menos de 20 pontos, não passou na prova. Escreva um programa que conta quantos estudantes reprovaram e armazene o resultado na variável `failed_students`. Imprima o resultado.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

import numpy as np

  

exam\_results \= np.array(

\[

42, 56, 59, 76, 43, 34, 62, 51, 50, 65,

66, 50, 46, 5, 79, 99, 51, 26, 35, 8,

34, 47, 64, 58, 61, 12, 30, 63, 20, 68

\]

)

  

failed\_students \= 0

  

for score in exam\_results:

if score < 20:

failed\_students += 1

  

print('O número dos estudantes que reprovaram:', failed\_students)

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Vamos continuar com os resultados da prova. Agora precisamos contar não apenas os estudantes que reprovaram, mas também aqueles que obtiveram resultados excelentes (90 pontos ou mais), resultados bons (entre 70 e 89 pontos), resultados médios (entre 50 e 69) e suficientes (entre 20 e 49).

Crie um dicionário `summarized_data` e escreva o código para preenchê-lo com os dados necessários.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

import numpy as np

  

exam\_results \= np.array(

\[

42, 56, 59, 76, 43, 34, 62, 51, 50, 65,

66, 50, 46, 5, 79, 99, 51, 26, 35, 8,

34, 47, 64, 58, 61, 12, 30, 63, 20, 68

\]

)

  

summarized\_datai \= {'excellent': 90,

'good': 70,

'average': 50,

'passable': 20,

'failed': 0}

summarized\_data \= {'excellent': 0,

'good': 0,

'average': 0,

'passable': 0,

'failed': 0}

for n in exam\_results:

for chave, limite in summarized\_datai.items():

if n \>= limite: \# Corrigindo a lógica de classificação

summarized\_data\[chave\] += 1

break

  

\# Código para exibir resultados na tela. Não o altere!

for result in summarized\_data:

print(result, '-', summarized\_data\[result\])

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-19-00-045Z.md
### Última modificação: 2025-05-28 19:19:00

# Como trabalhar com uma distribuição binomial - TripleTen

Capítulo 3/6

Teoria da probabilidade

# Como trabalhar com uma distribuição binomial

No mundo real, há muitos experimentos com dois resultados possíveis. Por exemplo, se uma pessoa vai ganhar na loteria ou não, se uma fatia de pão vai cair com a manteiga virada para baixo ou para cima ou se um cliente fará o upgrade para uma conta premium ou recusar a oferta. Quando executamos um experimento uma vez e há dois resultados possíveis, estamos lidando com um **experimento simples binomial**, ou **Ensaio de Bernoulli**.

Geralmente, chamamos um resultado de _sucesso_ e o outro de _fracasso_. Se a probabilidade de sucesso é ppp, então a probabilidade de fracasso deve ser igual a (1−p)(1-p)(1−p), já que há apenas dois resultados possíveis e eles devem somar 1.

A distribuição de probabilidade descrita aqui é conhecida como distribuição binomial.

### Experimentos binomiais

Se executarmos um experimento de dois resultados um determinado número de vezes, obteremos um **experimento binomial**. Por exemplo, se jogarmos uma moeda 100 vezes, podemos querer determinar a probabilidade de tirar exatamente 50 caras e 50 coroas.

Se as tentativas são independentes uma da outra, então a probabilidade de uma determinada combinação de sucessos e fracassos é o produto das probabilidades de cada sucesso e fracasso individual.

Por exemplo, para determinar a probabilidade da combinação _sucesso-sucesso-fracasso-sucesso_ **nessa ordem** (ao contrário de apenas "três sucessos e um fracasso"), calculamos o produto p×p×(1−p)×pp \\times p \\times (1-p) \\times pp×p×(1−p)×p, onde ppp é a probabilidade de sucesso.

A tarefa começa a ficar complicada se não nos preocuparmos com a ordem exata dos sucessos e fracassos.

Pergunta

Suponha que em 88% dos casos o primeiro clique de um usuário é em um banner de propaganda, e nos 12% de casos restantes, os cliques são feitos em outro lugar e o usuário acaba em outra página.

Qual a probabilidade de que, entre dois usuários diferentes, um vai clicar no banner e o outro não?

0,88 ⋅ 0,12 = 0,1056 = 10,56%

0,88 ⋅ 0,12 ⋅ 2 = 0,2112 = 21,12%

0,88 ⋅ 0,12 / 2 = 0,0528 = 5,28%

Fantástico!

Nesse experimento, há quatro resultados possíveis que constituem nosso espaço amostral, o que pode ser ilustrado assim:

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_11_1692886025.png)

Se olharmos para o espaço amostral, fica claro por que precisamos multiplicar as probabilidades de um usuário clicar no banner e do outro não clicar nele. Isso porque há dois resultados possíveis.

A ordem específica normalmente não é importante. É mais útil encontrar a probabilidade de um número específico de sucessos em um determinado número de tentativas. Em tais espaços amostrais, a variável aleatória "número de sucessos" (ou fracassos) é usada, e o que importa é o número de cada um.

Valores

Probabilidades

0 sucessos

81%

1 sucessos

18%

2 sucessos

1%

Quando repetimos um experimento apenas duas vezes, não aparece nenhum problema nos cálculos. Mas e se precisarmos calcular as probabilidades para 10, 100 ou 1.000 usuários?

### Cálculo da quantidade de sucessos _k_ de _n_ tentativas

Como você deve se lembrar, precisamos encontrar um jeito de calcular o número de maneiras que podemos obter _k_ sucessos em _n_ tentativas, sem compilar uma tabela com todos os resultados possíveis. Podemos fazer isso usando esta fórmula:

Cnk\=n!k!(n−k)!C\_{n}^{k} = \\frac{n!}{k!(n-k)!}Cnk​\=k!(n−k)!n!​

Se aplicarmos essa fórmula ao exemplo de cliques no banner, podemos considerar _n_ como o número total de usuários e _k_ como o número de cliques no banner. A fórmula nos ajuda a calcular o número de maneiras em que podemos selecionar um grupo de _k_ usuários do total _n_ dos usuários que vão clicar no banner e não em outro link.

Por exemplo, o número de maneiras em que exatamente 7 do total de 10 usuários clicam no banner é:

C107\=10!7!(10−7)!\=10!7!⋅3!\=120C\_{10}^{7} = \\frac{10!}{7!(10-7)!} = \\frac{10!}{7!⋅3!} = 120C107​\=7!(10−7)!10!​\=7!⋅3!10!​\=120

Para encontrar o mesmo resultado em Python, podemos usar o código:

```
from math import factorial

c = factorial(10)/(factorial(7)*factorial(3))
print(c)
```

```
120.0
```

Escreva o código para encontrar o número de maneiras em que exatamente 90 pessoas do total de 100 usuários vão clicar no banner: armazene o resultado na variável `c`.

CódigoPYTHON

9

1

2

3

4

5

from math import factorial

  

c \= factorial(100)/(factorial(90)\*factorial(100\-90))

  

print(c)

Dica

Mostrar a soluçãoValidar

### Encontrar a probabilidade de _n_ sucessos

Muito bem, podemos calcular o número de maneiras em que podemos obter _k_ sucessos em _n_ tentativas. Mas como podemos determinar a probabilidade de alcançar tal número de resultados bem-sucedidos?

Lembre-se de que, embora haja muitas combinações que resultem em _k_ sucessos, todas elas têm a mesma probabilidade. Desse modo, precisamos:

1.  Encontrar a probabilidade de apenas uma maneira de obter _k_ sucessos do total de _n_ experimentos;
2.  Encontrar o número total de combinações que resultam em _k_ sucessos; e
3.  Multiplicar a probabilidade por esse número.

Vamos dar uma olhada mais de perto no primeiro ponto. Considere um exemplo em que os 7 primeiros cliques serão no banner e o resto dos cliques não. A probabilidade desse resultado é a seguinte:

0.88⋅0.88⋅0.88⋅0.88⋅0.88⋅0.88⋅0.88⋅0.12⋅0.12⋅0.12\=0.887⋅0.123≈0.00070.88 \\cdot 0.88 \\cdot 0.88 \\cdot 0.88 \\cdot 0.88 \\cdot 0.88 \\cdot 0.88 \\cdot 0.12 \\cdot 0.12 \\cdot 0.12 = 0.88^7 \\cdot 0.12^3 \\approx 0.0007 0.88⋅0.88⋅0.88⋅0.88⋅0.88⋅0.88⋅0.88⋅0.12⋅0.12⋅0.12\=0.887⋅0.123≈0.0007

Em geral, a fórmula fica assim:

P\=pk⋅qn−kP = p^k \\cdot q^{n-k}P\=pk⋅qn−k

onde:

-   _p_ é a probabilidade de sucesso
-   _q_ é a probabilidade de fracasso
-   _k_ é o número de tentativas bem-sucedidas
-   _n_ é o número total de tentativas

Portanto, podemos ver que a probabilidade de obter _k_ sucessos do total de _n_ tentativas é

P\=Cnk⋅pk⋅qn−kP = C\_n^k \\cdot p^k \\cdot q^{n-k}P\=Cnk​⋅pk⋅qn−k

Agora você sabe como calcular a probabilidade de obter uma quantidade fixa de sucessos quando sabemos a quantidade de tentativas e a probabilidade de sucesso, e como criar uma distribuição de probabilidade completa para cada quantidade possível de sucessos: de 0 a _n_. Essa distribuição é chamada de **distribuição binomial**.

### A distribuição binomial

Vamos listar as condições necessárias para uma variável aleatória ter uma distribuição binomial:

-   Uma quantidade finita e fixa de tentativas (_n)_ é conduzida;
-   Cada tentativa é um experimento binomial simples com exatamente dois resultados;
-   As tentativas são independentes uma da outra;
-   A probabilidade de sucesso (_p)_ é a mesma para todas as _n_ tentativas.

O widget interativo abaixo mostra a probabilidade de obter um número diferente de sucessos em _n_ experimentos binomiais. Você pode ajustar a probabilidade de sucesso em um experimento (_p_) e o número de experimentos (_n_) e ver como os dados mudam.

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Widgets/Data/binominal_distribution/pt_br/binomial.html" style="min-width: 112%; min-height: 580px;"></iframe>

Quando _p_ é 5% e o experimento é repetido 30 vezes, a probabilidade de mais de 8 sucessos é microscopicamente pequena. Mas quando a probabilidade de sucesso é 90%, há pouca chance de obtermos poucos sucessos. Por exemplo, com 26 tentativas, a quantidade de sucessos será quase sempre maior do que 18. Pode conferir!

## Tarefas

### Tarefa 1

Os notebooks da Pineapple são caros, mas bastante populares entre geeks de TI: 60% de clientes querem comprar um computador quando vêm à loja. Os produtos da Banana são mais baratos, mas não tão populares: apenas 20% dos visitantes da loja fazem uma compra. Vamos supor que a loja só tem equipamentos da Pinapple à venda. Qual é a probabilidade de que 50 do total de 80 clientes farão uma compra durante um dia?

Armazene o resultado na variável `probability` e a imprima.

Não se esqueça de que em Python o sinal `**` é usado para exponenciação.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

from math import factorial

  

p \= 0.6\# A probabilidade de um cliente fazer uma compra

q \= 1\-p\# A probabilidade de um cliente NÃO fazer uma compra

n \= 80 \# O número total de visitantes

k \= 50\# O número de visitantes que esperamos que façam uma compra

c \= factorial(n)/(factorial(k)\*factorial(n\-k))

probability \= c\* p\*\*k\*q\*\*(n\-k)\# Escreva aqui o código para os cálculos

  

print(probability)

Dica

Mostrar a soluçãoValidar

### Tarefa **2**

Suponha que há um grande shopping perto da loja da Pineapple que tem uma loja que vende computadores da Banana. Durante o dia, 160 pessoas visitam essa loja. Qual é a probabilidade de que 50 delas vão comprar notebooks?

Lembre-se de que apenas 20% dos usuários querem comprar um computador da Banana.

Armazene o resultado na variável `probability` e a imprima.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

from math import factorial

  

p \= 0.2\# A probabilidade de um cliente fazer uma compra

q \= 1\-p\# A probabilidade de um cliente NÃO fazer uma compra

n \= 160\# O número total de visitantes

k \= 50\# O número de visitantes que esperamos que façam uma compra

c \= factorial(n)/(factorial(k)\*factorial(n\-k))

  

probability \= c\* p\*\*k\*q\*\*(n\-k)\# Escreva aqui o código para os cálculos

  

print(probability)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-19-01-371Z.md
### Última modificação: 2025-05-28 19:19:01

# Como trabalhar com uma distribuição normal - TripleTen

Capítulo 3/6

Teoria da probabilidade

# Como trabalhar com uma distribuição normal

Nesta lição, veremos uma distribuição que é extremamente importante na estatística: a **distribuição normal**, também conhecida como **curva em forma de sino**.

Ela aparece em muitos contextos diferentes e pode descrever tanto a altura de uma população quanto taxas de falha de equipamentos.

A distribuição normal não é a única observada na vida real, mas é a mais comum, com uma importância teórica fundamental.

### Centro e amplitude de uma distribuição normal

Uma distribuição normal possui dois parâmetros-chave: média e variância.

_X_ ∼ (_μ_, _σ^2_)

Essa notação pode ser lida como: a variável _X_ é normalmente distribuída com uma média de mi (μ) e uma variância de sigma ao quadrado (σ²), ou seja, um desvio padrão de sigma.

Cerca de 68% dos pontos de dados de uma distribuição normal estão próximos da média, em um desvio padrão dela (σ), enquanto aproximadamente 95% dos pontos de dados estão dentro de dois desvios padrão e cerca de 99,7% estão dentro de três desvios padrão. Esse conceito é chamado de **regra dos 3 sigmas**.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_59_1693470588.png)

É comum que os parâmetros de distribuição sejam conhecidos e que você precise encontrar a probabilidade de um valor ocorrer em certo intervalo.

Pergunta

Suponha que o número de horas que os clientes passam na academia cada semana segue uma distribuição normal com uma média de 4 e um desvio padrão de 1. Qual é o percentual dos clientes que passam mais de 6 horas por semana na academia?

menos de 3%

cerca de 5%

0.3%

pelo menos 50%

Seu entendimento sobre o material é impressionante!

O widget interativo abaixo mostra a distribuição normal resultante. A média (1.000) está no centro, e as outras linhas verticais estão espaçadas com intervalos de um desvio padrão. As porcentagens são as probabilidades associadas com cada intervalo.

Tente alterar os valores da média (μ) e do desvio padrão (σ) para ver como isso afeta a distribuição normal.

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Widgets/Data/normal_distribution/pt_br/nd_01_page.html" style="min-width: 112%; min-height: 580px;"></iframe>

O que acontece com as variações de probabilidade quando σ aumenta? Como você pode ver, elas se expandem. Um desvio padrão maior indica menos certeza sobre onde certo valor pode estar localizado, ou seja, uma variação maior em que ele pode ser encontrado.

### Cálculo de probabilidades em dados normalmente distribuídos usando Python

O Python pode fazer esses cálculos por nós. Para isso, precisamos de métodos da biblioteca **SciPy** e do seu módulo **stats**.

A SciPy é uma biblioteca Python gratuita de código aberto usada para computação científica e técnica. Ela contém módulos para otimização, álgebra linear, integração, processamento de sinais e imagens e muitos outros fins.

Nesta lição, vamos examinar a função `stats.norm()`, que gera uma distribuição normal com parâmetros indicados (média e desvio padrão). Também vamos abordar os métodos `norm.cdf` e `norm.ppf`, que permitem encontrar respostas a perguntas específicas.

Vamos examinar mais de perto esses métodos. Para ilustrar, considere o seguinte exemplo: na Índia, estudantes adultos gastam em média $5.000 para aprender uma nova profissão, com um desvio padrão de $1.500.

Nome

Sintaxe

Explicação

Exemplo

Função de distribuição acumulada

norm.cdf()

Calcula a probabilidade de uma variável aleatória ser inferior ou igual a um determinado valor

Qual é a probabilidade de que um estudante pode aprender uma nova profissão gastando menos de $4.000?

Função quantil (ou função de ponto percentual)

norm.ppf()

Calcula o valor da variável aleatória que corresponde a uma determinada probabilidade

Qual é o custo máximo para os 10% de estudantes que gastam menos dinheiro com educação?

Vamos ver como podemos responder a essas perguntas usando Python.

```
# Importando o módulo stats
from scipy import stats as st

# Criando um objeto com dados normalmente distribuídos com uma média de 5000
# e um desvio padrão de 1500
data = st.norm(5000, 1500)

# Criando uma variável para armazenar o custo desejado
desired_cost = 4000

# Calculando a probabilidade de obter o valor desired_cost
probability = data.cdf(desired_cost)

print(probability)
```

Resultado

```
0.2524925375469229
```

As chances são maiores que 25%. Pelo menos é possível 😉!

E agora, passamos para a segunda pergunta:

```
# Começando com a mesma importação e a criação de distribuição
from scipy import stats as st

data = st.norm(5000, 1500)

# Definindo o valor de probabilidade. Vamos procurar o limiar do valor das despesas de ensino
# para os 10% de estudantes que gastaram a menor quantidade de dinheiro.
target_level = 0.1

# Encontrando o custo que não ultrapasse o custo dos 10% de estudantes mais econômicos
cost = data.ppf(target_level)

print(cost)
```

Resultado

```
3077.6726516830995
```

Agora sabemos que 10% dos estudantes não gastam mais de $3.077 com os estudos. Eles realmente sabem como economizar dinheiro!

Pergunta

A pontuação média na prova da certificação de Analista de Dados é 1.000, com um desvio padrão de 100. Você precisa encontrar a probabilidade de obter entre 900 e 1.100 pontos na prova.

Qual código pode ser usado para isso?

`st.norm(1000, 100).ppf(1100) + st.norm(1000, 100).ppf(900)`

`st.norm(1000, 100).cdf(1100) + st.norm(1000, 100).cdf(900)`

`st.norm(1000, 100).ppf(1100) - st.norm(1000, 100).ppf(900)`

`st.norm(1000, 100).cdf(1100) - st.norm(1000, 100).cdf(900)`

Muito bem!

## Tarefas

### Tarefa 1

A quantidade de visitantes mensais de uma loja online possui uma distribuição normal com uma média de 100.500 e um desvio padrão de 3.500.

Encontre a probabilidade de que no próximo mês o site da loja terá:

-   Menos de 92.000 visitantes
-   Mais de 111.000 visitantes

Conclua o código conforme os comentários e use as instruções `print()` no pré-código para exibir os resultados.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

from scipy import stats as st

  

mu \= 100500\# qual é a média da distribuição?

sigma \= 3500\# qual é o desvio padrão da distribuição?

  

more\_threshold \= 111000\# qual é o limite superior para o número de visitantes?

fewer\_threshold \= 92000\# qual é o limite inferior para o número de visitantes?

  

p\_more\_visitors \= 1 \- st.norm(mu, sigma).cdf(more\_threshold)\# calcule a probabilidade de que haja mais visitantes do que o limiar superior

p\_fewer\_visitors \= st.norm(mu, sigma).cdf(fewer\_threshold)\# calcule a probabilidade de que haja menos visitantes do que o limiar inferior

  

print(f'Probabilidade de que haja mais de {more\_threshold} visitantes: {p\_more\_visitors}')

print(f'Probabilidade de que haja menos de {fewer\_threshold} visitantes: {p\_fewer\_visitors}')

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Outra loja online, Fancy Pants, vende produtos para presente para uma faixa bem estreita de clientes corporativos. As vendas semanais de conjuntos de jogos de xadrez de luxo feitos de presas de mamute possui uma distribuição normal com uma média de 420 e desvio padrão de 65.

A equipe do estoque está decidindo quantos conjuntos precisa encomendar. Ela quer que a possibilidade de vender todos eles na semana seguinte seja de 90%. Quantos ela precisa encomendar?

CódigoPYTHON

9

1

2

3

4

5

6

7

8

9

from scipy import stats as st

  

mu \= 420

sigma \= 65

prob \= 0.9

  

n\_shipment \= st.norm(mu, sigma).ppf(1 \- prob)

  

print('Necessidade de encomendar itens:', int(n\_shipment))

Dica

Mostrar a soluçãoValidar

### Tarefa 3

Os preços de pedidos feitos em uma loja online possuem uma distribuição normal com uma média de $24 e um desvio padrão de $3,20.

Alguns clientes optam pela entrega rápida, que possui um valor fixo independente do valor da compra.

Os clientes costumam não gostar quando o custo da entrega é igual ao custo do pedido. Qual deveria ser o custo da entrega rápida para não ser maior do que o preço dos pedidos em 75% dos casos?

CódigoPYTHON

9

1

2

3

4

5

6

7

8

9

from scipy import stats as st

  

mu \= 24\# coloque seu código aqui: qual é a média de distribuição?

sigma \= 3.2\# coloque seu código aqui: qual é o desvio padrão da distribuição?

threshold \= 0.75\# coloque seu código aqui: qual porcentagem dos pedidos deve custar mais do que o dobro do custo da entrega?

  

max\_delivery\_price \= st.norm(mu, sigma).ppf(1 \- threshold)\# coloque seu código aqui: o custo máximo de entrega

  

print('Custo máximo do serviço de entrega:', max\_delivery\_price)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-19-04-742Z.md
### Última modificação: 2025-05-28 19:19:05

# Combinação de distribuições binomial e normal - TripleTen

Capítulo 3/6

Teoria da probabilidade

# Combinação de distribuições binomial e normal

### Valor esperado

Digamos que temos uma variável aleatória. Ela pode assumir certos valores, e cada valor possui uma probabilidade associada a ele. O **valor esperado** da variável aleatória é o valor "típico" que associamos a ela. Para encontrar esse valor, precisamos repetir o experimento várias vezes para aproximar as probabilidades de cada valor, e então encontrar a média ponderada.

Um experimento binomial consiste de _n_ tentativas de um experimento simples (Bernoulli) com dois resultados. Se a quantidade de tentativas for alta o bastante, a distribuição binomial pode ser modelada pela distribuição normal. Nesta lição, vamos mostrar como isso funciona.

### Exemplos de valores esperados

Conheça Afonso: Afonso é um engenheiro de software que escreve 50 blocos de código por dia. A cada cinco vezes, o bloco é executado perfeitamente de primeira (_p_ = 0,2).

Isso significa que, diariamente, Afonso está realizando um experimento binomial. Para o experimento binomial, o valor esperado pode ser calculado ao multiplicar o número de tentativas (n) pela probabilidade de sucesso (p), ou n ⋅ p. Nesse caso, seria 50 ⋅ 0,2 = 10. Sobre a variância, em uma distribuição binomial, ela pode ser calculada assim: n ⋅ p ⋅ (1 − p). Usando essa fórmula, temos 50 ⋅ 0,2 ⋅ 0,8 = 8 (ou seja, com um desvio padrão de √8 = 2√2).

Isso não quer dizer que ele escreve 10 blocos de código perfeitos por dia. Um dia, ele pode estar cansado e escrever apenas 5; no outro, ele pode se sentir ótimo e escrever 15.

Mas, ao longo do tempo, a quantidade média diária vai se aproximar de 10, e por volta de 99% dos valores vão cair em 10 ± 3 ⋅ 2√2. Em qualquer dia, você pode esperar que entre 2 e 18 blocos funcionem perfeitamente na primeira tentativa.

---

Vamos ver outro exemplo

Pergunta

Para ter acesso a um quadro de avisos para profissionais em estatística, os usuários precisam fazer um exame de admissão. Ele é difícil, e apenas 5% das pessoas passam. Além disso, apenas 100 usuários podem fazer o teste por dia, e há uma lista de espera com 1.000 nomes.

Os moderadores estão estudando como novos usuários aparecem. Eles querem modelar a afluência como um experimento binomial.

Que parâmetros eles devem usar?

_p_ = 0,1. _n_ = 100

_p_ = 0,05. _n_ = 100

_p_ = 0.1, _n_ = 1,000

_p_ = 0,05. _n_ = 1,000

Você conseguiu!

### A aproximação normal da distribuição binomial

Se houver 50 ou mais tentativas, os resultados de um experimento binomial podem ser representados como dados normalmente distribuídos. Combinar dois tipos de distribuição de probabilidade tão diferentes pode parecer um pouco confuso, então vamos examinar essa ideia em mais detalhes.

Considere a distribuição binomial com _n_ = 50 e _p_ = 0,8. Usando a fórmula para a probabilidade de obter _k_ sucessos de _n_ tentativas, recebemos os seguintes resultados:

-   30 sucessos de 50 tentativas têm uma probabilidade de 0,0006
-   35 sucessos de 50 tentativas têm uma probabilidade de 0,0299
-   40 sucessos de 50 tentativas têm uma probabilidade de 0,1398
-   45 sucessos de 50 tentativas têm uma probabilidade de 0,0295

Se calcularmos os resultados para cada número de sucessos e criarmos um gráfico com base neles, teremos um gráfico muito parecido com uma curva de distribuição normal.

![](https://practicum-content.s3.amazonaws.com/resources/_5_1691661536.png)

Por que isso é útil? Devido a características úteis e previsíveis da distribuição normal, como a regra dos três sigmas.

### Exemplo

Uma companhia decidiu fazer publicidade online. A empresa de publicidade diz que, em média, 15% dos usuários clicam nos anúncios dela. Isso são 750 visitas a cada 5.000 visualizações.

A companhia coloca anúncios, compra 5.000 impressões (visualizações) e consegue apenas 715 visitas. A equipe de marketing fica irritada. A promessa não era de 750 visitas? Vamos usar argumentos estatísticos para acalmá-la.

Recapitulando: quando um usuário vê um anúncio, há 15% de chance de que ele vai clicar nele. Isso nos dá uma distribuição binomial com _n_ = 5.000 e _p_ = 0,15.

Vamos usar uma distribuição normal para aproximar a binomial e achar a probabilidade de conseguir 715 cliques ou menos.

Pense — temos o valor do eixo X e queremos a probabilidade de conseguir esse número ou menos. De qual método precisamos?

Entendeu?

Precisamos de `norm.cdf()`:

```
# Importando as bibliotecas necessárias
from scipy import stats as st
import math as mt

# Definindo valores dos parâmetros
binom_n = 5000
binom_p = 0.15
clicks = 715

# Calculando o valor esperado de sucessos e sigma
mu = binom_n * binom_p
sigma = mt.sqrt(binom_n * binom_p * (1 - binom_p))

# Calculando a probabilidade de conseguir 715 cliques ou menos
p_clicks = st.norm(mu, sigma).cdf(clicks)
print(p_clicks)
```

```
0.08284191945650154
```

A probabilidade de conseguir 715 visitas ao invés de 750 não é muito alta, por volta de 8,3%. Porém, não é pequena o suficiente para que esse evento quase nunca aconteça. Considerando essa probabilidade, podemos afirmar que para cada 100 companhias que colocam anúncios com a empresa de publicidade, 8 vão receber 715 cliques ou menos. Isso significa que não é de jeito nenhum um evento raro, e que a queixa da equipe de marketing não faz sentido.

## Tarefa

Uma companhia manda para seus clientes uma newsletter mensal com novidades e ofertas de parceiros. Ela sabe que 40% dos clientes abrem a newsletter.

Um parceiro está planejando uma campanha de publicidade e gostaria de alcançar aproximadamente 9.000 clientes. Calcule a probabilidade de que as expectativas do parceiro sejam atendidas se a newsletter for enviada para 23.000 pessoas.

No último exemplo, criamos uma variável chamada `clicks`; aqui, crie uma chamada `threshold` e salve o valor 9.000 nela. O total de clientes deve ser `binom_n` e a probabilidade de que a newsletter seja aberta deve ser `binom_p`.

Salve a probabilidade de que as expectativas do cliente sejam atendidas como `p_threshold` e imprima.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

from scipy import stats as st

import math as mt

  

binom\_n \= 23000

binom\_p \= 0.4

threshold \= 9000

  

\# Calculando o valor esperado de sucessos e sigma

mu \= binom\_n \* binom\_p

sigma \= mt.sqrt(binom\_n \* binom\_p \* (1 \- binom\_p))

  

\# Calculando a probabilidade de conseguir 715 cliques ou menos

p\_threshold \= 1 \- st.norm(mu, sigma).cdf(threshold)

\# escreva seu código aqui

print(p\_threshold)

\# escreva seu código aqui

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-19-06-073Z.md
### Última modificação: 2025-05-28 19:19:06

# Quiz do capítulo - TripleTen

Capítulo 3/6

Teoria da probabilidade

# Quiz do capítulo

Pergunta

Você conduziu uma pesquisa entre os clientes de uma cafeteria e descobriu que 25% deles gostam de salada de fruta, 30% gostam de sorvetes e o resto prefere cappuccino.

Três amigos entram na cafeteria. Qual é a probabilidade de que todos eles gostem de cappuccino?

20,25%

9,11%

3,36%

0,45%

Fantástico!

Pergunta

Qual é a probabilidade de tirar um ás aleatoriamente de um baralho de 52 cartas?

1/52

1/4

4/52

1/14

Seu entendimento sobre o material é impressionante!

Pergunta

Qual é o tamanho do espaço amostral na tarefa anterior?

1

4

13

52

Você conseguiu!

Pergunta

Um mágico afirma que consegue tirar 4 cartas aleatórias do baralho e todas essas cartas serão reis. Qual é a probabilidade de obter 4 reis se a seleção das cartas for verdadeiramente aleatória?

0%

cerca de 1%

cerca de 0,00037%

quase 0,05%

Você conseguiu!

Pergunta

Digamos que fizemos uma playlist (ordenada) de 100 músicas. Criamos um experimento simples: pegamos uma música da playlist aleatoriamente e vemos se ela cai nas posições 11-20. Se cair, consideramos que o teste foi um sucesso.

Conduzimos N tentativas, depois calculamos a frequência relativa com que essas músicas caem naquele intervalo:

frequeˆncia relativa\=nuˊmero de sucessosN\\text{frequência relativa} = \\frac{\\text{número de sucessos}}{N}frequeˆncia relativa\=Nnuˊmero de sucessos​

Quando a frequência relativa será mais próxima de 0,1?

Quando N = 30

Quando N = 10

Quando N = 100

Quando N = 1.000

Você conseguiu!

Pergunta

Considere estes dois eventos:

"O pedido #12345 foi entregue à agência do correio e aguarda pela retirada pelo destinatário.”

"O pedido #12345 foi entregue ao domicílio do destinatário.”

Esses eventos são:

Mutuamente exclusivos

Dependentes

Independentes

Paralelos

Muito bem!

Pergunta

Considere estes eventos:

"O pedido #12345 foi entregue à agência do correio.”

"O pedido #12345 deve ser pago com cartão de crédito.”

Os eventos são:

Mutuamente exclusivos

Independentes

Dependentes

Paralelos

Seu entendimento sobre o material é impressionante!

Pergunta

Ordenamos 1.000 arquivos de áudio pelo tipo de voz: masculino e feminino.

Com que tipo de distribuição estamos lidando?

Normal

Binomial

Poisson

Uniforme

Muito bem!

Pergunta

Determine o valor esperado do histograma abaixo:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_16_1691662644.png)

0

0.8

1.5

4

Excelente!

Pergunta

Uma empresa está realizando uma campanha promocional distribuindo panfletos com um cupom de desconto para possíveis clientes. De acordo com dados estatísticos, 11% de quem receber um panfleto irá até a loja para ver o produto. Qual é a probabilidade de que, depois que a empresa distribuir 5000 panfletos, menos de 500 pessoas irão até a loja?

cerca de 1%

cerca de 10%

exatamente 50%

11%

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-19-07-419Z.md
### Última modificação: 2025-05-28 19:19:07

# Conclusão - TripleTen

Capítulo 3/6

Teoria da probabilidade

# Conclusão

Parabéns! Este foi um capítulo difícil, mas você conseguiu concluí-lo! Tínhamos certeza de que você se sairia bem com uma probabilidade de 100%.

Pergunta

Agora você é capaz de:

Escolha quantas quiser

Calcular as probabilidades de eventos individuais e suas combinações

Se ainda não tiver certeza, você pode revisar a lição em [Eventos independentes e multiplicação de probabilidades](https://tripleten.com/trainer/data-analyst/lesson/2be57e7b-f1eb-444f-83cd-824b9b94012a/)

Determinar o número de combinações possíveis para eventos independentes e calcular a probabilidade correspondente usando Python

Se ainda não tiver certeza, você pode revisar a lição em [Fundamentos da combinatória e mais problemas de probabilidades](https://tripleten.com/trainer/data-analyst/lesson/6055d51d-d602-4f12-95f1-cebf9638debe/)

Usar o conhecimento de diferentes tipos de distribuição de dados para resolver problemas de negócios reais relacionados a estimação de probabilidades

Se ainda não tiver certeza, você pode revisar as lições em [Como trabalhar com uma distribuição binomial](https://tripleten.com/trainer/data-analyst/lesson/e5026261-048c-4f8a-8659-15a7b96ff48c/), [Como trabalhar com uma distribuição normal](https://tripleten.com/trainer/data-analyst/lesson/efc25954-7f66-466e-bdf7-021393515682/) e [Combinação de distribuições binomial e normal](https://tripleten.com/trainer/data-analyst/lesson/33aa3e47-9ee9-42e3-85af-74b0940a4d5d/)

Você conseguiu!

Não esperamos que você aprenda todos os conceitos de cabeça após estudá-los uma vez. Você talvez precise de mais tempo para entender o material completamente, então não hesite em consultar as lições necessárias sempre que precisar.

Nos vemos no próximo capítulo, onde você vai aprender a testar hipóteses usando as estatísticas que já conhece.

### Leve isso com você

Faça download do sumário do capítulo e folha de conclusões para que você possa consultá-los quando necessário.

-   [Resumo do Capítulo: Teoria da Probabilidade](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_Sprint_3/PT/Resumo_do%20capitulo_Teoria_da_probabilidade.pdf)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-20-27-285Z.md
### Última modificação: 2025-05-28 19:20:27

# Introdução - TripleTen

Capítulo 4/6

Testes de hipóteses

# Introdução

Testar hipóteses é uma ferramenta estatística. Usando essa ferramenta, você pode fazer estimativas sobre grandes populações usando pequenas amostras. Por exemplo, podemos determinar se os usuários de um site vão gostar de uma nova funcionalidade com base nos dados coletados de uma pequena parcela deles.

Você também pode fazer previsões sobre os parâmetros de distribuição de populações. Continuando com o exemplo dos usuários de um site, podemos predizer a distribuição do tempo que os usuários vão passar no site após a nova funcionalidade. Pode ser que os usuários gostem da nova funcionalidade e o tempo que passam no site aumente.

### O que você vai aprender:

-   Como o teorema central do limite (expresso de forma diferente) descreve distribuições amostrais;
-   Como aplicar a lógica de testar hipóteses estatísticas;
-   Como testar uma hipótese sobre se a média de uma _população estatística_ é igual a um determinado valor;
-   Como testar uma hipótese sobre se as médias de duas populações estatísticas são iguais.

O capítulo vai levar de 3 a 4 horas para ser concluído. Depois dele, estará na hora de trabalhar no projeto do sprint.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-20-28-613Z.md
### Última modificação: 2025-05-28 19:20:28

# Amostras aleatórias e médias amostrais - TripleTen

Capítulo 4/6

Testes de hipóteses

# Amostras aleatórias e médias amostrais

Na teoria da probabilidade, aprendemos sobre experimentos, resultados e distribuições de probabilidade. Ou seja, modelos ideais que, sob certas condições, descreviam teoricamente os dados observados.

A lógica de conduzir um teste estatístico de uma hipótese é um pouco diferente. Para começar, vamos tirar conclusões sobre um grande conjunto de dados, ou **população estatística**, com base em **amostras**. Por exemplo, se houver dados sobre todas as visitas a um site em um ano, incluindo a hora exata e o tempo gasto no site, não é necessário analisar todo o conjunto de dados. Tudo o que você precisa é de uma parte pequena, mas **representativa** dos dados que reflita o comportamento de toda a população estatística.

A maneira mais fácil de garantir que essa seleção de dados seja representativa é fazer uma **amostra aleatória**. Elementos aleatórios são selecionados de todo o conjunto de dados usando um gerador de números aleatórios. Usando esses elementos, podemos tirar conclusões sobre a população como um todo.

Alguns conjuntos de dados podem ter várias partes de tamanho desigual que diferem muito em relação ao parâmetro que você está estudando. Nesses casos, é uma boa ideia pegar amostras aleatórias proporcionais de cada parte e depois combiná-las. O resultado é uma **amostra estratificada**, que é mais representativa do que uma amostra aleatória seria. Chamamos de "estratificada" porque dividimos a população em **estratos**, ou grupos, unidos por uma característica comum. Esses estratos são então usados para produzir amostras aleatórias.

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_12_1692886111.png)

Aqui está um exemplo: ao estudar o comportamento dos clientes de companhias aéreas, seria fácil negligenciar os clientes da classe executiva porque há poucos deles no geral. No entanto, o comportamento desses consumidores difere muito dos outros clientes, então vale a pena garantir que eles estejam representados na amostra.

Pergunta

Qual das seguintes situações requer uma amostra estratificada?

Variável experimental: número de ligações telefônicas em um dia. População estatística: residentes de bairros menos ricos em Palermo.

Variável experimental: despesas com benefícios de empregados. População: funcionários de uma grande empresa.

Variável experimental: nível de cegueira de faixa, que é o percentual de anúncios despercebidos online. População estatística: usuários das cinco redes sociais mais populares.

Variável experimental: número de bebidas tropicais consumidas em uma praia na Tailândia. População estatística: turistas de diferentes países.

Muito bem!

Para a maioria das tarefas que você vai fazer, uma amostra aleatória regular será suficiente.

Com uma amostra, você pode tirar conclusões sobre a população – mais precisamente, sobre os parâmetros estatísticos dela. Geralmente é suficiente estimar valores médios e variância para tirar uma conclusão sobre **igualdade ou desigualdade em relação aos valores médios das populações**. Continuando com o exemplo de uma companhia aérea, talvez seja do nosso interesse saber o número médio de voos realizados pelos clientes cada ano e se esse número é igual a 5. Em outras palavras, queremos determinar se o número médio de voos por cliente é igual a 5.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_36_1692702802.png)

O que podemos dizer sobre a média e a variância de uma população com base na média e na variância que calculamos para uma amostra (também chamadas de **média amostral** e **variância amostral**)? Quase tudo, se a amostra for grande o suficiente. Precisamos de pelo menos 30 valores, mas 50 ou mais é melhor. Em outras palavras, se temos dados sobre apenas 30-50 clientes da companhia e esses clientes representam a população, podemos tirar conclusões sobre a população (todos os clientes) com base apenas nos dados sobre esses 30-50 clientes.

### O teorema central do limite

O teorema central do limite (também conhecido por teorema do limite central) é um dos teoremas mais importantes que precisamos entender.

Esta é uma forma de explicar o teorema do limite central: caso exista um número suficiente de observações em uma amostra, a **distribuição da amostragem** da média amostral de qualquer população estatística é distribuída normalmente em torno da média dessa população. Em outras palavras, quando o tamanho da amostra aumenta, a média dos dados da amostra se aproxima da média de toda a população.

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_Sprint_3/PT/Some-Pages_214_ptbr.gif"></iframe>

A animação acima ilustra bem que quando o tamanho da amostra é pequeno, as médias da amostra diferem de modo significativo da média da distribuição normal da população. Entretanto, quando o tamanho da amostra aumenta, as médias da amostra se aproximam gradualmente da média da população.

Agora vamos discutir a distribuição amostral da média amostral e associá-la a tudo o que aprendemos até agora, usando o exemplo da companhia aérea. Primeiro, a definição: a distribuição amostral da média amostral é o conjunto de valores das médias de todas as possíveis amostras de um determinado tamanho retiradas de uma determinada população estatística.

Vamos discutir o que essa definição diz:

-   A população estatística nesse caso representa todos os clientes da companhia aérea no ano passado.
-   Uma amostra é um conjunto de clientes selecionados aleatoriamente. Lembre-se de que cada amostra tem um determinado tamanho, por exemplo, 30 clientes por amostra.
-   A média amostral é a média calculada para uma determinada amostra. Por exemplo, se selecionarmos aleatoriamente 30 clientes e calcularmos o número médio de voos que eles realizaram no ano passado, o valor resultante será a média amostral.
-   Se calcularmos a média amostral para todas as amostras possíveis, teremos muitos valores de médias amostrais que constituem a distribuição.

Depois de estudar a distribuição binomial, você deve estar com tudo pronto para descobrir quantas amostras de tamanho _k_ você pode selecionar de uma população de tamanho _n_:

Cnk\=n!k!(n−k)!C\_{n}^{k} = \\frac{n!}{k!(n-k)!}Cnk​\=k!(n−k)!n!​

Esse número aumenta bem rápido à medida que _n_ e _k_ aumentam. Aqui estão vários exemplos:

Tamanho da população estatística: 100, tamanho da amostra: 8

Número de amostras diferentes desse tamanho: 186087894300

Tamanho da população estatística: 500, tamanho da amostra: 20

Número de amostras diferentes desse tamanho: 2,6671985128374 \* 10 elevado a 35 (mais do que o número estimado de estrelas no universo observável).

Se pegarmos todas as amostras possíveis de um determinado tamanho de uma determinada população estatística e calcularmos a média de cada uma, obteremos um conjunto de dados normalmente distribuído em torno da média da população estatística. De acordo com o teorema central do limite, a distribuição sempre será assim:

![](https://practicum-content.s3.amazonaws.com/resources/4.4.2PT_1692886149.png)

Qual é a variância? Isso depende do tamanho da amostra. Quanto maior ela for, menor será o desvio padrão da média amostral. Quanto maior a amostra, mais precisa será a média de toda a população.

O desvio padrão da média amostral em relação à média da população estatística é chamado de **erro padrão** e é encontrado usando esta fórmula:

E.S.E.\=Sn\\text{E.S.E.} = \\frac{S}{\\sqrt{n}}E.S.E.\=n​S​

**E.S.E.** é uma sigla em inglês para "estimated standard error": erro padrão estimado.

Ele é "estimado" porque temos apenas uma amostra. Não sabemos o erro exato e o _estimamos_ com base nos dados que temos.

**S** representa o desvio padrão da amostra.

**n** é o tamanho da amostra. Como a raiz quadrada de _n_ está no denominador, o erro padrão diminui à medida que o tamanho da amostra aumenta.

Não precisamos calcular os resultados dessas fórmulas; elas são incorporadas a funções predefinidas para testar hipóteses em Python. Resumimos elas aqui para ajudar você a entender como hipóteses são testadas.

Pergunta

Com relação à média amostral, o teorema central do limite implica que:

Não é possível estimar a média de uma população usando a média de uma amostra.

É possível estimar a média de uma população estatística usando a média de uma amostra, mas isso é muito impreciso e só pode ser feito para populações estatísticas normalmente distribuídas.

É possível estimar a média de uma população estatística usando a média de uma amostra. Quanto maior a amostra, mais precisa será a média da população – mas apenas para populações estatísticas normalmente distribuídas.

É possível estimar a média de uma população estatística usando a média de uma amostra. Quanto maior a amostra, mais precisa será a média da população – e isso funciona para populações estatísticas com qualquer distribuição.

Seu entendimento sobre o material é impressionante!

Pergunta

Com relação à média amostral, o teorema central do limite implica que:

As médias amostrais são normalmente distribuídas em torno de um valor próximo à média da população estatística.

As médias amostrais são normalmente distribuídas em torno de um valor igual à média da população estatística.

As médias amostrais podem ser distribuídas de várias maneiras, mas sua média é próxima da média da população estatística.

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-20-29-910Z.md
### Última modificação: 2025-05-28 19:20:30

# Formulação de hipóteses bicaudais - TripleTen

Capítulo 4/6

Testes de hipóteses

# Formulação de hipóteses bicaudais

### Formulação de uma hipótese

Dados obtidos experimentalmente não confirmam hipóteses. Essa é a nossa maior limitação. Os dados só podem contradizer a hipótese ou, inversamente, mostrar resultados extremamente improváveis (desde que a hipótese seja verdadeira). No entanto, em ambos os casos, não temos base para afirmar que a hipótese foi comprovada.

Se os dados não contradizem a hipótese, ela apenas não é rejeitada. Mas, caso fosse muito improvável obter esses dados, assumindo que a hipótese seja verdadeira, temos um motivo para rejeitar a hipótese.

Uma hipótese estatística afirma algo sobre um parâmetro da distribuição. As hipóteses típicas neste programa serão sobre as médias das populações estatísticas e serão assim:

-   A média de uma população é igual a um determinado valor;
-   As médias de duas populações são iguais entre si;
-   A média de uma população é maior (ou menor) que um determinado valor;
-   A média de uma população é maior (ou menor) que a média de outra população estatística.

Na lição anterior, discutimos os clientes de uma companhia aérea. Vamos continuar com esse exemplo. Suponha que o departamento de marketing dessa companhia realizou uma campanha bem agressiva para incentivar os clientes a voar mais. Agora a empresa quer saber se a campanha deu algum resultado. Podemos responder a essa pergunta testando uma hipótese.

Vamos dar uma olhada nas etapas para testar uma hipótese estatística. Podemos fazer isso bem rápido, pois o Python possui funções internas para testes. Para interpretar corretamente os resultados dessas funções, é importante entender como tudo funciona.

Vamos começar com a **hipótese nula** **H₀**. Por exemplo, "A média da população em questão é igual a A", onde _A_ é algum número.

Na estatística, H₀ costuma representar a ideia de que não há diferença entre grupos. A hipótese nula assume que não há nenhuma alteração ou efeito. Por exemplo, para uma companhia aérea, a hipótese nula seria "o número de voos por cliente não mudou após a campanha". Geralmente, uma hipótese é considerada verdadeira até que algum dado para negá-la seja apresentado.

Em outras palavras, usamos a hipótese nula para garantir que os valores obtidos na experimentação não sejam extremos e estejam dentro do intervalo esperado.

Por exemplo, se queremos descobrir se o número de voos aumentou, a hipótese nula será: "Não, ele não aumentou". Se estivermos investigando se há diferença entre populações, a hipótese nula será: "Não, não há diferença e os parâmetros em questão são iguais".

A **hipótese alternativa**, **H₁**, é baseada em H₀. Para essa H₀, H₁ seria: "A média da população não é igual a A".

Nesse exemplo, a hipótese alternativa poderia afirmar que o número de voos por cliente mudou após a campanha e que esse número não é igual ao número antes da campanha.

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_17_1693468934.png)

Como testamos uma hipótese?

Primeiro, criamos uma distribuição com base na suposição de que H₀ é verdadeira. O teorema central do limite diz que a distribuição deve ser normal e a média é o valor proposto em H₀. A variância é desconhecida, mas pode ser estimada usando as amostras.

Para ilustrar isso, vamos deixar de lado o exemplo da companhia aérea por um momento e considerar outra distribuição amostral da média amostral que se parece com o seguinte:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Widgets/Data/normal_distribution/pt_br/nd_03_page.html" style="min-width: 112%; min-height: 580px;"></iframe>

Vamos ver as partes desse widget interativo mais de perto. A área sob a curva é igual à probabilidade de que um valor amostral caia nesse intervalo. Podemos verificar esse fato somando toda a área sob a curva.

Mova os controles deslizantes na parte superior para os valores máximos. Qual é a probabilidade resultante? Exato, é 99,99%. Não é possível obter 100%, porque a distribuição normal nunca atinge zero.

Com base no gráfico, esperamos que quanto mais afastada da média de H₀ a amostra estiver, menos provável é que essa diferença seja por acaso. Entretanto, como vimos, nunca poderemos ter 100% de certeza. Mesmo quando consideramos valores extremos, como 600 e 1.400, eles não garantem 100% de certeza.

E como, então, determinamos se precisamos rejeitar a hipótese nula? Definimos um limiar para a **significância estatística**.

Para fazer isso, mova o controle deslizante acima para que os limites inferior e superior fiquem ±200 da média (de 1.000). Tudo bem se você não conseguir arrastar o controle deslizante para os valores exatos. Aqui, precisamos apenas de uma ideia geral.

Agora veja a porcentagem que obtivemos. É em torno de 95%, não é? O que essa porcentagem nos diz? Ela nos diz que se espera que 95% dos valores estejam entre os limites inferior e superior de ±200 da média. Mas e quanto aos valores que estão fora desse intervalo? Vamos testar sua intuição!

Pergunta

Suponha que fizemos uma alteração (para ter uma ideia de tal alteração, pode pensar em uma campanha que visa aumentar o número de voos no nosso exemplo da companhia aérea) e obtivemos experimentalmente um novo valor da média de 1.250. Qual é a probabilidade de obter esse número? Em outras palavras, quais são as chances de que não há diferença estatística entre esse valor e a média de H₀?

~97%

~95%

~5%

~3%

Fantástico!

Um limiar de 5% (2,5% de cada lado – esquerdo e direito) é uma medida comum de significância estatística. Quando você arrasta o controle deslizante de modo que os limites inferior e superior sejam ±200 da média, as duas áreas cinzentas à esquerda e à direita representam as duas zonas de 2,5%.

Na próxima lição, você vai aprender que quando queremos verificar se um valor cai na área cinzenta dos 2,5% à esquerda da média ou dos 2,5% à direita da média, isso é chamado de uma **hipótese bicaudal**.

O objetivo de um teste de uma hipótese bicaudal é mostrar se a média amostral é significativamente maior ou menor do que a média de uma população. Um exemplo disso é o que fizemos no quiz anterior. Definimos o limite de significância estatística como 5% e queríamos verificar um valor de 1.250. Como 1.250 cai na área dos 2,5% à direita da média, podemos concluir que o nosso resultado experimental cumpre o requisito de significância estatística. Portanto, rejeitamos a hipótese nula.

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_37_1692703102.png)

É claro, se quiséssemos algo mais rigoroso, poderíamos definir um nível de significância diferente. Outros valores comuns são 1% ou até mesmo 0,01%. Você pode brincar com valores diferentes no gráfico acima para ter uma ideia mais intuitiva disso.

Agora você sabe como formular tanto uma hipótese nula como alternativa, bem como ajustar valores de significância comuns lidando com uma hipótese. A seguir, você vai aprender como testar uma hipótese bicaudal em Python. Continue com a gente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-20-32-150Z.md
### Última modificação: 2025-05-28 19:20:32

# Testes de hipóteses em Python. Valores-p - TripleTen

Capítulo 4/6

Testes de hipóteses

# Testes de hipóteses em Python. Valores-p

Em Python, há funções que podemos usar para testar hipóteses. Você não precisa escolher um nível de significância ou descobrir se um valor está ou não em um intervalo crítico. Uma função retorna a **diferença estatística** entre a média e o valor com o qual você está comparando, e a significância estatística dessa estatística é o **valor-p** (da palavra "probabilidade").

A diferença estatística é o número de desvios padrão entre os valores comparados quando ambas as distribuições são convertidas para uma distribuição normal padrão com média 0 e desvio padrão 1. Esse número não é muito orientativo.

É sensato usar o valor-_p_ para decidir aceitar ou rejeitar a hipótese nula. O valor-_p_ é a probabilidade de obter um resultado pelo menos tão extremo quanto o que você está considerando, supondo que a hipótese nula esteja correta. O valor-p é representado como uma porcentagem e indica a área embaixo da curva que corresponde a uma probabilidade.

Se esse valor for maior que 10%, você definitivamente não deve rejeitar a hipótese nula. Se o valor-_p_ for menor que isso, é possível que você deva rejeitar a hipótese nula. Os valores limiares convencionais são 5% e 1%.

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_14_1692886228.png)

Se você reconhecer que 5% de probabilidade é muito baixo e rejeitar a hipótese nula com base nisso, então a cada 20 ou mais experimentos você vai rejeitar a hipótese não porque esteja errada, mas porque o valor real acabou sendo muito distante do valor proposto. Cabe sempre ao analista decidir qual limiar considerar suficiente.

Se não houver muitos valores em cada amostra (não mais do que 20–30), a distribuição correspondente à hipótese nula será quase normal, só um pouco ampla. Em outras palavras, dados insuficientes produzem alta variância. Tal distribuição é chamada da **distribuição de Student**. Essa é uma distribuição de probabilidade que possui uma forma de sino, o que a torna parecida à distribuição normal, mas tem caudas mais pesadas. Ela é usada para estimar parâmetros das populações para amostras de tamanhos pequenos ou variâncias desconhecidas.

A distribuição normal supões que o desvio padrão da população é conhecido. Já a distribuição de Student não faz essa suposição. O número de valores observados (menos 1) usados para construir a distribuição de Student é oficialmente chamado do número de "graus de liberdade" (k) e é relacionado com o tamanho da amostra. À medida que o número de graus de liberdade (k) aumenta, a distribuição se aproxima de uma distribuição normal com a mesma média:

![](https://practicum-content.s3.amazonaws.com/resources/4.4.3PT_1692886261.png)

A distribuição de Student também é chamada de **distribuição t**, e o teste estatístico que a usa é chamado de **teste t**.

Como analistas, não precisamos nos preocupar se a distribuição t com a qual trabalhamos está próxima de uma distribuição normal ou não, porque as funções da biblioteca `scipy` a tornam normal automaticamente, então um teste estatístico pode ser conduzido.

A função `scipy.stats.ttest_1samp (array, popmean)` é usada para testar hipóteses do tipo "a média da população é igual a x".

`ttest` significa teste t. `1samp` significa que estamos trabalhando com uma amostra e comparando-a com um determinado valor.

Passamos estes parâmetros para a função:

-   `array` é um vetor que contém a amostra.
-   `popmean` é a média proposta que estamos testando.

Quando chamada, a função retorna a estatística de diferença entre `popmean` e a média da amostra do vetor `array`.

Ela também retorna a significância estatística bilateral, que é a probabilidade de que a média de toda a população estatística se desvie do valor proposto pelo menos tanto quanto vemos na amostra (aumentando ou diminuindo em qualquer direção).

Agora vamos ver como isso funciona na prática. Seu parceiro de negócios diz que o site que você criou se tornou um meio de atrair usuários. Ele diz que os usuários passam duas horas por dia no site. Em outras palavras, a hipótese nula é que os usuários passam 2 horas no site.

Uma amostra de 200 pessoas foi retirada dos registros do tempo gasto no site. Vamos testar a hipótese do seu parceiro. Para isso, vamos conduzir um teste e comparar o valor-p resultante com o limiar que definiremos como 5%.

```
from scipy import stats as st
import numpy as np
import pandas as pd

time_on_site = pd.read_csv('user_time.csv')

interested_value = 120 # tempo gasto no site

alpha = .05 # significância estatística crítica (limiar)

# executando um teste
results = st.ttest_1samp(
    time_on_site, 
    interested_value)

# imprimindo o valor-p resultante
print('valor-p: ', results.pvalue)

# comparando o valor-p com o limiar
if (results.pvalue < alpha):
    print("Rejeitamos a hipótese nula")
else:
    print("Não podemos rejeitar a hipótese nula")
```

```
valor-p:  [0.27702871]
Não podemos rejeitar a hipótese nula
```

O valor-p resultante é 0.27702871, ou 27,7%. Ele representa a probabilidade de obter uma média do tempo igual a 2 horas. Como a probabilidade é bem grande, parece que os usuários realmente passam cerca de duas horas no site.

Usamos um teste bilateral porque não nos importávamos se o valor resultante fosse maior ou menor que o valor proposto pelo seu parceiro (ou seja, se era maior ou menor do que 2 horas). Preste atenção a como você declara os resultados. No início da lição, observamos que os dados nunca podem provar ou confirmar uma hipótese. Isso é de importância fundamental. Se tivermos apenas uma amostra, não sabemos nada sobre a população estatística inteira _com certeza._ Se tivéssemos certeza, não precisaríamos de um teste estatístico.

Só o que podemos fazer são suposições sobre uma população estatística e calcular a probabilidade de obtermos uma determinada amostra se nossas suposições estiverem corretas. Se a probabilidade for relativamente alta, os dados não nos dão motivos para rejeitar uma suposição. Se a probabilidade for baixa, então, a partir dos dados fornecidos, podemos concluir que nossa suposição provavelmente está incorreta (mas não podemos _refutá-la_ ou _provar_ o contrário).

### Tarefa

Você gerencia uma rede de estações de aluguel de scooters de mobilidade chamada Scooters Get You There. São 20 locais no centro da cidade e cada um tem no máximo 50 scooters. Você quer testar a hipótese de que no mês passado havia em média 30 scooters disponíveis em cada estação durante o dia. Um grupo urbano chamado "Esquilo" destacou a importância desse número em seu estudo de mobilidade de moradores. Se houver menos scooters na estação, os usuários podem pensar que não vão poder alugar um quando precisarem, mas se houver mais, as pessoas talvez pensem que não vão conseguir estacionar as scooters após um passeio porque não haverá espaços livres.

A cada hora, as estações enviam o número de scooters disponíveis para o servidor. Você baixou os números das 13h às 16h dos últimos 30 dias. Teste sua hipótese usando essa amostra. Definia um limiar de 5% para significância estatística.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

from scipy import stats as st

import pandas as pd

  

scooters \= pd.Series(\[15, 31, 10, 21, 21, 32, 30, 25, 21,

28, 25, 32, 38, 18, 33, 24, 26, 40, 24, 37, 20, 36, 28, 38,

24, 35, 33, 21, 29, 26, 13, 25, 34, 38, 23, 37, 31, 28, 32,

24, 25, 13, 38, 34, 48, 19, 20, 22, 38, 28, 31, 18, 21, 24,

31, 21, 28, 29, 33, 40, 26, 33, 33, 6, 27, 24, 17, 28, 7,

33, 25, 25, 29, 19, 30, 29, 22, 15, 28, 36, 25, 36, 25, 29,

33, 19, 32, 32, 28, 26, 18, 48, 15, 27, 27, 27, 0, 28, 39,

27, 25, 39, 28, 22, 33, 30, 35, 19, 20, 18, 31, 44, 20, 18,

17, 28, 17, 44, 40, 33,\])

  

optimal\_value \= 30\# escreva seu código aqui

  

alpha \= 0.05\# defina a significância estatística crítica

  

results \= st.ttest\_1samp(

scooters,

optimal\_value)\# realize o teste t

  

print('valor-p: ',results.pvalue) \# extraia o valor-p dos resultados do teste

  

if (results.pvalue < alpha): \# compare o valor-p com o limiar alpha:

print('Rejeitamos a hipótese nula')

else:

print("Não podemos rejeitar a hipótese nula")

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-20-33-464Z.md
### Última modificação: 2025-05-28 19:20:33

# Formulação de hipóteses unicaudais - TripleTen

Capítulo 4/6

Testes de hipóteses

# Formulação de hipóteses unicaudais

Quais são as hipóteses comuns que testamos?

-   A média de uma população estatística é igual a um determinado valor;
-   As médias de duas populações estatísticas são iguais entre si;
-   A média de uma população estatística é maior (ou menor) que um determinado valor;
-   A média de uma população estatística é maior (ou menor) que a média de outra população estatística.

A hipótese bicaudal, sobre a qual aprendemos na lição anterior, é eficaz para abordar os dois primeiros itens da lista, mas como fica o gráfico quando apenas as alterações em um lado forem importantes, como nos dois últimos itens da lista? O intervalo crítico (também conhecido como a "área cinzenta", ou seja, o intervalo em que o valor em questão deve diminuir para que a hipótese nula seja rejeitada) será apenas de um lado, não dos dois, como podemos ver abaixo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Widgets/Data/normal_distribution/pt_br/nd_02_page.html" style="min-width: 112%; min-height: 580px;"></iframe>

Brinque com o controle deslizante. Veja que quando o arrastamos, alteramos a área cinzenta apenas à direita da média. Lembre-se de que, ao arrastar o controle, definimos um limiar para a significância estatística. Anteriormente, dissemos que ele pode ser 5%, 1% ou até menos que isso. Agora vamos testar sua intuição:

Pergunta

Suponha que queremos verificar se a média da amostra é significativamente maior do que a média de H₀ e definimos nosso limiar de significância como 5%.

Se o resultado experimental for 1.190, podemos rejeitar a hipótese nula? Use a ferramenta interativa acima para responder a essa pergunta.

Sim, rejeitamos H₀

Não rejeitamos H₀

Não temos informações suficientes para responder à pergunta

Você conseguiu!

O que acabamos de fazer é chamado de um **teste unicaudal (ou unilateral) de hipótese**.

Observe que a área cinzenta representa 5% da área total. Isso porque definimos o limiar de significância como 5%. Compare o resultado com o teste bicaudal, em que também definimos o limiar de significância em 5%, mas dividimos esses 5% em duas áreas: uma à esquerda da média e outra à direita. Como no teste unicaudal todos os 5% de significância estão localizados à direita da média, essa área está mais próxima da média em comparação com o teste bicaudal. Em outras palavras, para um teste unilateral da hipótese, esse nível de significância corresponde a 1,645 desvio padrão; para um teste bilateral, ele corresponde a 1,96.

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_15_1692886313.png)

Isso é assim para qualquer nível de significância, não apenas 5%: para um teste unilateral, caímos no intervalo crítico (e rejeitamos a hipótese nula) com menos desvio (ou um número menor de desvios padrão) do que em um teste bilateral. Isso acontece porque, com um teste unilateral, a probabilidade tomada para significância estatística corresponde a um lado, e com um teste bilateral ela é dividida igualmente em um possível desvio para a direita e para a esquerda.

Não existe nenhuma função especial para testes unilaterais em Python, embora exista um para testes bilaterais. Se você precisar de um teste unilateral, faça um teste bilateral e divida o valor _p_ por 2. Dessa forma, você pode obter a significância estatística unilateral do desvio do valor em questão do valor previsto. Isso funciona porque, por padrão, o teste calcula a probabilidade de desvio em ambas as direções. Como um teste bilateral é simétrico, o valor-_p_ será exatamente duas vezes maior do que seria se você estivesse buscando o desvio de apenas um lado.

![](https://practicum-content.s3.amazonaws.com/resources/26_07_DA_DS-08_PT_1692886340.png)

Ao testar a hipótese, compare a média amostral com o valor previsto – o primeiro é maior que o segundo? Quando a hipótese alternativa afirma que o valor real é menor que o previsto, mas a média amostral é maior (em qualquer valor), não há base para rejeitar a hipótese nula para a hipótese alternativa. Mas se o valor observado for menor que o valor proposto, aplique um teste estatístico.

---

### Exemplo

Você está vendendo melancias online. Para vender todas antes do final da estação, você contratou alguns profissionais de desenvolvimento web para criar uma página de destino chamada Watermelon Life. Observando as estatísticas, você notou que quanto mais as pessoas rolavam pelo site (quanto mais blocos visualizavam), mais compravam melancias. A média de blocos visualizados é 4,867.

Por insistência dos designers, você alterou os primeiros blocos para atender a novas diretrizes, e o número de pedidos não mudou. Mas talvez os usuários tenham começado a fazer compras mais rápido. Vamos verificar se é esse o caso: se for, os usuários agora decidem fazer compras após visualizar apenas os primeiros blocos da página de destino, portanto, o número de blocos que veem deve ser menor.

Usaremos uma amostra de 100 compradores aleatórios. O conjunto de dados é o número de blocos visualizados da página de destino. A nossa hipótese nula é que o número de blocos visualizados da página de destino é maior ou igual a 4,867

```
from scipy import stats as st
import pandas as pd

screens = pd.Series([4, 2, 4, 5, 5, 4, 2, 3, 3, 5, 2, 5, 2, 2, 2, 3, 3, 4, 8, 3, 4, 3, 5, 5, 4, 2, 5, 2, 3, 7, 5, 5, 6,  5, 3, 4, 3, 6, 3, 4, 4, 3, 5, 4, 4, 8, 4, 7, 4, 5, 5, 3, 4, 6, 7, 2, 3, 6, 5, 6, 4, 4, 3, 4, 6, 4, 4, 6, 2, 6, 5, 3, 3, 3, 4, 5, 3, 5, 5, 4, 3, 3, 3, 1, 5, 4, 3, 4, 6, 3, 1, 3, 2, 7, 3, 6, 6, 6, 5, 5])

prev_screens_value = 4.867 # número médio de blocos visualizados

alpha = 0.05  # nível de significância

results = st.ttest_1samp(screens, prev_screens_value)

# teste unilateral: o valor-p será reduzido pela metade
print('valor-p: ', results.pvalue / 2)

# teste unilateral à esquerda:
# rejeitar a hipótese apenas se a média da amostra for significativamente menor que o valor proposto
if (results.pvalue / 2 < alpha) and (screens.mean() < prev_screens_value):
    print("Rejeitamos a hipótese nula")
else:
    print("Não podemos rejeitar a hipótese nula")
```

```
valor-p:  1.3358596895543794e-06
Rejeitamos a hipótese nula
```

Como você pode ver no código, em um teste unilateral com a hipótese alternativa "O número de blocos visualizados _diminuiu_", a hipótese nula é rejeitada se duas condições forem atendidas:

— O valor observado é menor do que o previsto.

— A diferença entre os valores é estatisticamente significativa (dividimos o valor-_p_ que obtivemos do teste bilateral por 2).

Se o teste fosse unilateral à direita com a hipótese alternativa "O valor observado é maior que o previsto", um dos sinais "<" mudaria para ">". As últimas linhas de código ficariam assim:

```
if (results.pvalue / 2 < alpha) and (screens.mean() > prev_screens_value):
    print("Rejeitamos a hipótese nula")
else:
    print("Não podemos rejeitar a hipótese nula")
```

```
Não podemos rejeitar a hipótese nula
```

A propósito, podemos escrever o valor-_p_ que obtivemos (1,3358596895543794e-06) como 1,3358596895543794 vezes 10 elevado à potência de menos 6 (ou dividido por 10 elevado a 6, ou dividido por um milhão). Basicamente, esse é um número muito, muito pequeno.

### Tarefa

Em 1º de junho de 2019, você fez um curso com um coach e empresário famoso chamado Robby Tobbinson. Se aplicar as técnicas de negócio consciente que aprendeu, você recebeu a garantia de que seu projeto online vai gerar pelo menos US$ 800 por dia. Talvez mais. Em apenas um mês. Ele promete.

Promessas são boas, mas testes estatísticos são melhores. Vamos ver o que os números nos dizem.

Use um conjunto de dados com receita diária do último mês para testar sua hipótese. A hipótese é que sua receita média diária seja igual ou superior a US$ 800.

Lembre-se: a hipótese que contém o sinal de igual geralmente é a hipótese nula. Portanto, "Tudo vai funcionar como o coach previu" é sua hipótese nula, e "A receita será menor do que o previsto" é a hipótese alternativa. Desvios aleatórios são sempre possíveis. Você só pode dizer "a metodologia de Tobbinson não funcionou!" se sua receita for _significativamente_ menor que o valor proposto.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

from scipy import stats as st

import numpy as np

import pandas as pd

  

revenue \= pd.Series(\[727, 678, 685, 669, 661, 705, 701, 717,

655,643, 660, 709, 701, 681, 716, 655,

716, 695, 684, 687, 669,647, 721, 681,

674, 641, 704, 717, 656, 725, 684, 665\])

  

interested\_value \= 800\# quanto Robby Tobbinson prometeu?

  

alpha \= 0.05\# indique o nível de significância estatística

  

results \= st.ttest\_1samp(revenue,interested\_value)\# use a função st.ttest\_1samp()

  

print('valor-p:',results.pvalue / 2) \# imprima o valor-p para um teste unilateral)

  

if (results.pvalue / 2 < alpha) and (revenue.mean() < interested\_value):\# compare o valor obtido e o nível crítico de significância estatística

\# e verifique se a média da amostra está no lado correto de interested\_value):

print(

"Rejeitamos a hipótese nula: a receita foi significativamente menor do que 800 dólares"

)

else:

print(

"Não podemos rejeitar a hipótese nula: a receita não foi significativamente menor"

)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-20-34-771Z.md
### Última modificação: 2025-05-28 19:20:35

# Hipótese sobre a igualdade das médias de duas populações - TripleTen

Capítulo 4/6

Testes de hipóteses

# Hipótese sobre a igualdade das médias de duas populações

Às vezes, é necessário comparar as médias de duas populações estatísticas diferentes.

Se você deseja determinar se o gasto médio varia para clientes provenientes de diferentes canais, não basta apenas comparar os números de um determinado período. Até que você faça o teste correto, não vai poder dizer que a diferença entre as médias é aleatória, assim como também não vai poder dizer que a diferença é grande o suficiente para postular que o gasto entre as duas populações realmente varia (ou seja, que a diferença é significativa).

Digamos que o valor médio de compra do primeiro canal seja US$ 20 e a média do segundo canal, US$ 25.

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_PT_16_1692886384.png)

A diferença entre esses números é significativa? Isso vai depender da variância das amostras a partir das quais os valores são calculados. Em vez de basear sua comparação apenas nas médias, use os conjuntos de dados para realizar um teste estatístico.

Para testar a hipótese de que as médias das duas populações estatísticas são iguais com base nas amostras retiradas delas, aplique a função `scipy.stats.ttest_ind(array1, array2, equal_var)`.

Passamos estes parâmetros para a função:

-   `array1` e `array2` são vetores contendo as amostras
-   `equal_var` (variância igual) é um parâmetro que especifica se as variâncias das populações devem ou não ser consideradas iguais. Ele é passado como `equal_var = True` ou `equal_var = False` (`True` significa que consideramos as variâncias iguais, `False` significa que não). Sabemos que as variâncias podem ser diferentes. Para ter uma ideia de duas populações com variâncias (e valores das médias) diferentes, veja este exemplo com 2 populações aleatórias:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_38_1692704372.png)

Existe alguma razão para acreditar que as amostras são retiradas de populações com parâmetros semelhantes? Em caso afirmativo, defina esse parâmetro como `True`, e a variância de cada amostra será estimada a partir do conjunto de dados combinado das duas amostras, e não separadamente para cada uma.

Fazemos isso para obter resultados mais precisos. No entanto, isso só deve ser feito quando as variâncias das populações estatísticas das quais as amostras foram retiradas apresentarem um grande grau de similaridade. Caso contrário, precisamos definir o parâmetro como `False`. Ele é `True` por padrão (se você não o definir).

Vamos considerar um exemplo com dois conjuntos de dados: o valor gasto em compras feitas em um mês por visitantes provenientes de dois canais diferentes. Você tem uma amostra aleatória de 30 compras de cada canal.

```
from scipy import stats as st
import numpy as np

sample_1 = [3071, 3636, 3454, 3151, 2185, 3259, 1727, 2263, 2015, 
            2582, 4815, 633, 3186, 887, 2028, 3589, 2564, 1422, 1785, 
            3180, 1770, 2716, 2546, 1848, 4644, 3134, 475, 2686, 
            1838, 3352]

sample_2 = [1211, 1228, 2157, 3699, 600, 1898, 1688, 1420, 5048, 3007, 
            509, 3777, 5583, 3949, 121, 1674, 4300, 1338, 3066, 
            3562, 1010, 2311, 462, 863, 2021, 528, 1849, 255, 
            1740, 2596]

alpha = 0.05  # nível crítico de significância estatística
# se o valor-p for menor que alfa, rejeitamos a hipótese nula

results = st.ttest_ind(sample_1, sample_2) # executando um teste

print('valor-p: ', results.pvalue) # extraindo o valor-p

if results.pvalue < alpha: # comparando o valor-p com o limiar
    print("Rejeitamos a hipótese nula")
else:
    print("Não podemos rejeitar a hipótese nula")
```

```
p-value:  0.1912450522572209
Não podemos rejeitar a hipótese nula
```

O valor-_p_ nos diz que, embora os valores médios para os dois canais sejam diferentes, há uma probabilidade de 19,12% de obter aleatoriamente uma diferença desse tamanho ou maior. Essa probabilidade é claramente muito alta para concluir que há uma diferença significativa entre os valores médios gastos.

### Tarefa 1

Há dois conjuntos de dados: o tempo médio gasto em um site 1) de usuários que fazem login com nomes de usuário e senhas e 2) de usuários que fazem login por meio de alguma rede social. Teste a hipótese de que ambos os grupos passam a mesma quantidade de tempo no site.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

from scipy import stats as st

import numpy as np

  

\# tempo gasto no site por pessoas com nome de usuário e senha

time\_on\_site\_logpass \= \[368, 113, 328, 447, 1, 156, 335, 233,

308, 181, 271, 239, 411, 293, 303,

206, 196, 203, 311, 205, 297, 529,

373, 217, 416, 206, 1, 128, 16, 214\]

  

\# tempo gasto no site por pessoas que fazem login por redes sociais

time\_on\_site\_social \= \[451, 182, 469, 546, 396, 630, 206,

130, 45, 569, 434, 321, 374, 149,

721, 350, 347, 446, 406, 365, 203,

405, 631, 545, 584, 248, 171, 309,

338, 505\]

  

  

\# seu código vai abaixo

  

alpha \= 0.05\# seu código: defina um nível crítico de significância estatística

  

results \= st.ttest\_ind(time\_on\_site\_logpass, time\_on\_site\_social)\# seu código: teste a hipótese de que as médias das duas populações independentes são iguais

  

print('valor-p:',results.pvalue) \# seu código: imprima o valor-p obtido)

  

if (results.pvalue < alpha):\# seu código: compare os valores-p obtidos com o nível de significância estatística):

print("Rejeitamos a hipótese nula")

else:

print("Não podemos rejeitar a hipótese nula")

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Temos dois conjuntos de dados: a profundidade de visitas ao site de diferentes grupos de usuários para os meses de verão e outono. Teste a hipótese de que as profundidades de visitas do site são iguais. Por exemplo, talvez no verão os visitantes não se aprofundem tanto no conteúdo, o que seria algo a ser considerado ao planejar uma campanha publicitária para esses meses. O nível de significância deve ser 0,05.

Não esperamos que as variâncias sejam as mesmas, então defina o parâmetro `equal_var` como `False`. Você pode executar `np.var(pages_per_session_autumn)`, etc. para verificar a variação do conjunto.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

from scipy import stats as st

import numpy as np

  

pages\_per\_session\_autumn \= \[7.1, 7.3, 9.8, 7.3, 6.4, 10.5, 8.7,

17.5, 3.3, 15.5, 16.2, 0.4, 8.3,

8.1, 3.0, 6.1, 4.4, 18.8, 14.7, 16.4,

13.6, 4.4, 7.4, 12.4, 3.9, 13.6,

8.8, 8.1, 13.6, 12.2\]

pages\_per\_session\_summer \= \[12.1, 24.3, 6.4, 19.9, 19.7, 12.5, 17.6,

5.0, 22.4, 13.5, 10.8, 23.4, 9.4, 3.7,

2.5, 19.8, 4.8, 29.0, 1.7, 28.6, 16.7,

14.2, 10.6, 18.2, 14.7, 23.8, 15.9, 16.2,

12.1, 14.5\]

  

alpha \= 0.05 \# seu código: defina um nível crítico de significância estatística

  

results \= st.ttest\_ind(pages\_per\_session\_autumn, pages\_per\_session\_summer,equal\_var\=False)\# seu código: teste a hipótese de que as médias das duas populações independentes são iguais

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-20-37-684Z.md
### Última modificação: 2025-05-28 19:20:38

# Hipótese sobre a igualdade das médias de amostras pareadas - TripleTen

Capítulo 4/6

Testes de hipóteses

# Hipótese sobre a igualdade das médias de amostras pareadas

Ao trabalhar com uma população estatística, é útil saber se as mudanças afetam a média dela. Por exemplo, vamos ver se os pesos dos pacotes encomendados pelos clientes mudam ao alterar o método de cálculo dos custos de envio. Aqui, a alteração se refere ao novo método de cálculo dos custos de envio. A média da população representa o peso médio de um pacote encomendado.

Usaremos **amostras pareadas**, o que significa que estamos medindo uma variável duas vezes para cada cliente, antes e depois das alterações.

Para testar a hipótese de que as médias de duas populações são iguais para amostras dependentes (pareadas) em Python, precisamos usar esta função: `scipy.stats.ttest_rel()`.

Ela só precisa de dois parâmetros: vetores com dados de antes e depois. As matrizes precisam ser do mesmo tamanho.

Temos dois conjuntos de dados: peso do pacote, em gramas, antes de alterar o método de cálculo do frete e o peso depois (para os mesmos clientes repetidos). Vamos testar a hipótese de que o peso do pacote não mudou, embora o método de cálculo do frete tenha mudado.

```
from scipy import stats as st
import numpy as np

before = [157, 114, 152, 355, 155, 513, 299, 268, 164, 320, 
                    192, 262, 506, 240, 364, 179, 246, 427, 187, 431, 
                    320, 193, 313, 347, 312, 92, 177, 225, 242, 312]

after = [282, 220, 162, 226, 296, 479, 248, 322, 298, 418, 
                 552, 246, 251, 404, 368, 484, 358, 264, 359, 410, 
                 382, 350, 406, 416, 438, 364, 283, 314, 420, 218]

alpha = 0.05  # nível crítico de significância estatística

results = st.ttest_rel(before, after)

print('valor-p: ', results.pvalue)

if results.pvalue < alpha:
    print("Rejeitamos a hipótese nula")
else:
    print("Não podemos rejeitar a hipótese nula")
```

```
valor-p:  0.005825972457958989
Rejeitamos a hipótese nula
```

Os dados fornecem evidência suficiente, dado o nível de significância que selecionamos, para rejeitar a hipótese nula. Portanto, podemos concluir que houve uma mudança nos pesos dos pacotes.

### Tarefa 1

Temos dois conjuntos de dados: o tempo gasto por um conjunto de usuários nas áreas de conta pessoal em um site, registrado antes e depois que essa área foi reprojetada. Teste a hipótese de que o tempo gasto mudou (aumentou ou diminuiu) após a alteração.

Pense na frase "o tempo gasto lá mudou" na hipótese acima. Ela sugere a necessidade de um teste unilateral ou bilateral?

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

from scipy import stats as st

import numpy as np

  

time\_before \= \[1732, 1301, 1540, 2247, 1632, 1550, 754, 1946, 1889,

2748, 1349, 1648, 1665, 2416, 1470, 1681, 1868, 1629,

1271, 1633, 2131, 942, 1599, 1127, 2200, 661, 1207,

1737, 2410, 1486\]

  

time\_after \= \[955, 2577, 360, 139, 1618, 990, 644, 1796, 1487, 949, 472,

1906, 1758, 1258, 2554, 612, 309, 1864, 1294, 1487, 1164, 1559,

491, 2286, 1270, 2069, 1553, 1629, 1704, 1623\]

  

alpha \= 0.05\# seu código: defina um nível crítico de significância estatística

  

results \= st.ttest\_rel(time\_before, time\_after)\# seu código: faça o teste e calcule o valor-p

  

print('valor-p:', results\[1\])\# seu código: imprima o valor-p obtido)

  

if results.pvalue < alpha:\# seu código: compare o valor-p com o nível de significância estatística):

print("Rejeitamos a hipótese nula")

else:

print("Não podemos rejeitar a hipótese nula")

Dica

Mostrar a soluçãoValidar

### Tarefa 2

Temos dois conjuntos de dados: o número de balas compradas por jogadores ávidos de um jogo online, antes e depois da inclusão de uma mecânica que fornece incentivos para disparos em rajadas. Teste a hipótese de que os jogadores começaram a usar mais balas depois que o novo recurso foi incluído.

Pense na palavra "mais" na hipótese acima. Ela sugere a necessidade de um teste unilateral ou bilateral?

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

from scipy import stats as st

import numpy as np

import pandas as pd

  

bullets\_before \= \[821, 1164, 598, 854, 455, 1220, 161, 1400, 479, 215,

564, 159, 920, 173, 276, 444, 273, 711, 291, 880,

892, 712, 16, 476, 498, 9, 1251, 938, 389, 513\]

  

bullets\_after \= \[904, 220, 676, 459, 299, 659, 1698, 1120, 514, 1086, 1499,

1262, 829, 476, 1149, 996, 1247, 1117, 1324, 532, 1458, 898,

1837, 455, 1667, 898, 474, 558, 639, 1012\]

  

print('média antes:', pd.Series(bullets\_before).mean())

print('média depois:', pd.Series(bullets\_after).mean())

  

alpha \= 0.05

  

results \= st.ttest\_rel(

bullets\_before,

bullets\_after)

  

print('valor-p:', results.pvalue/2)\# seu código: imprima o valor-p obtido)

  

if (results.pvalue/2 < alpha):

print("Rejeitamos a hipótese nula")

else:

print("Não podemos rejeitar a hipótese nula")

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-20-38-966Z.md
### Última modificação: 2025-05-28 19:20:39

# Quiz do capítulo - TripleTen

Capítulo 4/6

Testes de hipóteses

# Quiz do capítulo

Pergunta

Em que casos a amostragem estratificada é preferível à amostragem aleatória? Talvez existam várias respostas corretas.

Escolha quantas quiser

Variável: consumo diário de sal. População: moradores de uma cidade grande.

Variável: venda de casacos. População: estatísticas ao longo do ano.

Variável: vendas de tênis. População: clientes online de diferentes idades e cidades.

Variável: tempo gasto consumindo notícias. População: um país inteiro.

Você conseguiu!

Pergunta

De que trata o teorema central do limite? Talvez existam várias respostas corretas.

Escolha quantas quiser

Se coletarmos amostras suficientes, suas médias serão normalmente distribuídas em torno da média de toda a população

A forma da distribuição das médias amostrais reflete a da população como um todo

Se aumentarmos o tamanho da amostra, a variância da distribuição das médias amostrais em torno da média populacional aumentará

Se aumentarmos o tamanho da amostra, a variância da distribuição das médias amostrais em torno da média populacional diminuirá

Trabalho maravilhoso!

Pergunta

Há cinco anos, os cidadãos da cidade de Hamgot liam uma média de quatro livros por ano. Queremos saber se esse número mudou desde que um empresário local começou a publicar sob a marca Penguin. Várias pesquisas foram realizadas, cada uma fornecendo uma estimativa do número médio de livros lidos por ano. Aqui está o gráfico resultante:

![](https://practicum-content.s3.amazonaws.com/resources/4.4.8PT_1_1693465741.png)

Qual afirmação é verdadeira? Talvez existam várias respostas corretas.

Escolha quantas quiser

O que vemos é uma função de densidade de probabilidade

A hipótese nula deve ser rejeitada

A hipótese nula não pode ser rejeitada

O nível de significância está acima de 50%

A hipótese nula é que o número médio de livros lidos é quatro

Os valores no gráfico parecem estar normalmente distribuídos

Excelente!

Pergunta

Qual afirmação sobre o erro padrão da estimativa (ESE) é verdadeira? Talvez existam várias respostas corretas.

Escolha quantas quiser

Quanto maior a amostra, mais próxima a média amostral está da média estatística da população

ESE\=SnESE = \\frac{S}{{n}}ESE\=nS​

ESE\=SnESE = \\frac{S}{\\sqrt{n}}ESE\=n​S​

Quando reduzimos o tamanho da amostra, o ESE diminui.

Trabalho maravilhoso!

Pergunta

Vamos voltar para os livros. Você quer ver se o número de livros lidos em um ano é quatro ou não. Qual abordagem é adequada?

Um teste bilateral para a hipótese nula de que o valor esperado de uma amostra de observações independentes será igual à média da população dada

Um teste unilateral para a hipótese nula de que o valor esperado de uma amostra de observações independentes será igual à média da população dada

Um teste t para as médias de duas amostras independentes

Um teste t para as médias de duas amostras pareadas

Fantástico!

Pergunta

Depois de executar um teste t bicaudal para estimar se o valor médio de sua amostra é quatro, você obtém o seguinte valor-p:

0,03011

Qual afirmação é verdadeira?

A probabilidade de que a hipótese nula seja verdadeira é de 0,03011%

Aproximadamente 3% dos valores na amostra são iguais a 4

Se a média de toda a população for de fato 4, então 3 de 100 amostras aleatórias teriam essa média

A probabilidade de que a média da população não seja 4 é de cerca de 3%

A hipótese nula deve ser rejeitada

Trabalho maravilhoso!

Pergunta

Agora você quer saber se podemos ter certeza de que o número médio de livros lidos **aumentou**. Você define alpha em 1%.

Que etapas precisam ser seguidas a seguir para dizer que a hipótese nula pode ser rejeitada?

Dividir o valor-p por dois e ver se é menor que alfa.

Garantir que a média da amostra seja maior que 4, dividir o valor-p por 2 e comparar com alfa. Nesse caso, a hipótese nula pode ser rejeitada.

Garantir que a média da amostra seja maior que 4, dividir o valor-p por 2 e comparar com alpha. Neste caso, a hipótese nula não pode ser rejeitada.

Garantir que a média amostral seja maior que 4 e verificar se seu valor-p é menor que alfa.

Fantástico!

Pergunta

Suponha que temos amostras com vendas de companhias aéreas para dois sites em um determinado dia: `sample_1` e `sample_2`. Há 200 valores em cada um, e espera-se que as distribuições tenham as mesmas propriedades. Definimos alfa em 0,05.

O que devemos fazer a seguir para determinar se devemos rejeitar ou manter a hipótese de que as médias das populações são as mesmas?

Executar `ttest_2samp(sample_1, sample_2)` do pacote `scipy.stats` e comparar o valor-p resultante com alfa

Executar `ttest_rel(sample_1, sample_2)` do pacote `scipy.stats` e comparar o valor-p resultante com alfa

Executar `ttest_1samp( sample_1, sample_2)` do pacote `scipy.stats` e comparar o valor-p resultante com alfa

Executar `ttest_ind(sample_1, sample_2)` do pacote `scipy.stats`, dividir o valor-p resultante por 2 e comparar com alfa

Executar `ttest_ind(sample_1, sample_2)` do pacote `scipy.stats` e comparar o valor-p resultante com alfa

Trabalho maravilhoso!

Pergunta

Vamos fazer uma última viagem para a cidade de Hamgot. Descobrimos de onde vem o antigo número de livros lidos: uma certa pesquisa. Você obtém uma cópia, entra em contato com as mesmas pessoas e obtém novos valores para elas. Você toma 0,05 como alfa.

O que você deve fazer para determinar se a hipótese nula (o valor não mudou) pode ser refutada?

Executar `ttest_rel( sample_1, sample_2)` do pacote `scipy.stats` e comparar o valor-p resultante com alfa

Executar `ttest_ind(sample_1, sample_2)` do pacote `scipy.stats` e comparar o valor-p resultante com alfa

Executar `ttest_ind(sample_1, sample_2, equal_var=True)` do pacote `scipy.stats`, dividir o valor-p resultante por 2 e comparar com alfa

Executar `ttest_2samp(sample_1, sample_2)` do pacote `scipy.stats` e comparar o valor-p resultante com alfa

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-20-40-245Z.md
### Última modificação: 2025-05-28 19:20:40

# Conclusão - TripleTen

Capítulo 4/6

Testes de hipóteses

# Conclusão

Agora você tem todas as ferramentas necessárias para testar uma hipótese. Sinta-se à vontade para realizar experimentos com produtos e testar os resultados, que serão precisos do ponto de vista estatístico.

Pergunta

Agora você é capaz de:

Escolha quantas quiser

Coletar amostras aleatórias e estimar a média populacional verdadeira

Formular hipóteses nulas e alternativas

Realizar testes unicaudais e bicaudais

Verificar a igualdade das médias de duas populações, incluindo médias pareadas

Você conseguiu!

### Leve isso com você

Faça download do sumário do capítulo e folha de conclusões para que você possa consultá-los quando necessário.

-   [Resumo do Capítulo: Testes de hipóteses](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_Sprint_3/PT/3.Resumo_do_Capítulo_Testes_de_hipóteses.pdf)
-   [Folha de Conclusões: Testes de hipóteses](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_Sprint_3/PT/3.Folha_de_concluses_Testes_de_hipteses.pdf)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-20-42-726Z.md
### Última modificação: 2025-05-28 19:20:43

# Sprint 4 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 5/6

Projeto

# Sprint 4 - Projeto

Parabéns! Você concluiu a seção sobre Análise estatística de dados. É hora de aplicar o conhecimento e as habilidades que você adquiriu em um projeto: um estudo de caso analítico da vida real que você vai concluir por conta própria.

Quando terminar, envie seu trabalho para o revisor do projeto para avaliação. Você vai receber feedback em até 48 horas. Use esse feedback para fazer alterações e envie a nova versão de volta ao revisor.

Você talvez receba mais feedback referente à nova versão. Isso é completamente normal. Não é incomum passar por vários ciclos de feedback e revisão.

Seu projeto será considerado concluído assim que o revisor o aprovar.

## Descrição do projeto

Você trabalha como analista para a empresa de telecomunicações Megaline. A empresa oferece aos clientes dois planos pré-pagos: Surf e Ultimate. O departamento comercial quer saber qual dos planos gera mais receita para ajustar o orçamento de publicidade.

Você vai realizar uma análise preliminar dos planos com base em uma pequena seleção de clientes. Você terá dados de 500 clientes da Megaline: que clientes são, de onde eles são, qual plano usam e o número de chamadas e mensagens realizadas em 2018. Seu trabalho é analisar o comportamento dos clientes e determinar qual plano pré-pago gera mais receita. Mais tarde, nas instruções do projeto, você verá exatamente quais aspectos do comportamento dos clientes precisa analisar. Determinar qual plano, em média, gera mais receita é uma tarefa que pode ser resolvida usando testes estatísticos. Você vai receber mais informação sobre isso na seção de instruções do projeto.

## Descrição dos planos

Observação: a Megaline arredonda segundos para minutos e megabytes para gigabytes. Para **chamadas**, cada chamada individual é arredondada para cima: mesmo que uma chamada tenha durado apenas um segundo, um minuto será contado. Para **tráfego da web**, sessões individuais da web não são arredondadas para cima. Ao invés disso, o total do mês é arredondado para cima. Se alguém usar 1.025 megabytes no mês, a cobrança será de 2 gigabytes.

Aqui está uma descrição dos planos:

**Surf**

1.  Preço mensal: $20
2.  500 minutos mensais, 50 mensagens de texto e 15 GB de dados
3.  Após exceder os limites do pacote:
    -   1 minuto: 3 centavos
    -   1 mensagem de texto: 3 centavos
    -   1 GB de dados: $10

**Ultimate**

1.  Preço mensal: $70
2.  3.000 minutos mensais, 1.000 mensagens de texto e 30 GB de dados
3.  Após exceder os limites do pacote:
    -   1 minuto: 1 centavo
    -   1 mensagem de texto: 1 centavo
    -   1 GB de dados: $7

## Dicionário de dados

Neste projeto, você vai trabalhar com cinco tabelas separadas.

1.  A tabela `users` (dados sobre usuários):
    
    -   _user\_id_ — identificador exclusivo do usuário
    -   _first\_name_ — nome do usuário
    -   _last\_name_ — sobrenome do usuário
    -   _age_ — idade do usuário (em anos)
    -   _reg\_date_ — data da inscrição (dd, mm, aa)
    -   _churn\_date_ — a data que o usuário parou de usar o serviço (se o valor estiver ausente, isso significa que o plano estava em uso quando o banco de dados foi extraído)
    -   _city_ — cidade de residência do usuário
    -   _plan_ — nome do plano
    
2.  A tabela `calls` (dados sobre as chamadas):
    
    -   _id_ — identificador de chamada exclusivo
    -   _call\_date_ — data da chamada
    -   _duration_ — duração da chamada (em minutos)
    -   _user\_id_ — identificador do usuário que faz a chamada
    
3.  A tabela `messages` (dados sobre mensagens de texto):
    
    -   _id_ — identificador exclusivo da mensagem de texto
    -   _message\_date_ — data da mensagem de texto
    -   _user\_id_ — identificador do usuário que envia a mensagem de texto
    
4.  A tabela `internet` (dados sobre sessões web):
    
    -   _id_ — identificador exclusivo da sessão
    -   _mb\_used_ — volume de dados gasto durante a sessão (em megabytes)
    -   _session\_date_ — data da sessão web
    -   _user\_id_ — identificador do usuário
    
5.  A tabela `plans` (dados sobre os planos):
    
    -   _plan\_name_ — nome do plano
    -   _usd\_monthly\_fee_ — preço mensal em dólares americanos
    -   _minutes\_included_ — pacote mensal de minutos
    -   _messages\_included_ — pacote mensal de mensagens de texto
    -   _mb\_per\_month\_included_ — volume do pacote de dados (em megabytes)
    -   _usd\_per\_minute_ — preço por minuto depois de exceder o limite do pacote (por exemplo, se o pacote inclui 100 minutos, o primeiro minuto excedente será cobrado)
    -   _usd\_per\_message_ — preço por mensagem de texto depois de exceder o limite do pacote
    -   _usd\_per\_gb_ — preço por gigabyte extra de dados após exceder o limite do pacote (1 GB = 1.024 megabytes)
    

## Instruções para concluir o projeto

Preparamos um modelo de notebook para você. O notebook contém orientações sobre que código escrever e também para a inclusão de explicações dos resultados ao longo do processo. Para concluir o projeto, você precisa preencher cada célula de código no modelo e editar as células Markdown nos casos em que for necessário explicar os resultados.

Certifique-se de incluir uma introdução que descreve brevemente seus objetivos e uma conclusão que traz um breve resumo dos resultados.

**Etapa 1**. **Abra o arquivo de dados e estude as informações gerais**

Abaixo estão os caminhos para os arquivos a serem lidos e links para baixá-los, se necessário.

_/datasets/megaline\_calls.csv_ [Baixar o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_Sprint_3/megaline_calls.csv)

_/datasets/megaline\_internet.csv_ [Baixar o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_Sprint_3/megaline_internet.csv)

_/datasets/megaline\_messages.csv_ [Baixar o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_Sprint_3/megaline_messages.csv)

_/datasets/megaline\_plans.csv_ [Baixar o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_Sprint_3/megaline_plans.csv)

_/datasets/megaline\_users.csv_ [Baixar o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_Refactored_Sprint_3/megaline_users.csv)

**Etapa 2. Prepare os dados**

-   Converta os dados para os tipos necessários.
-   Encontre e elimine erros nos dados. Certifique-se de explicar quais erros você encontrou e como os eliminou.

Para cada usuário, encontre:

-   O número de chamadas feitas e minutos usados por mês
-   O número de mensagens de texto enviadas por mês
-   O volume de dados por mês
-   A receita mensal gerada a partir de cada usuário. Para fazer isso, você precisa:
    -   Subtrair o limite gratuito do pacote do número total de chamadas, mensagens de texto e dados;
    -   Multiplicar o resultado pelo valor do plano;
    -   Adicionar o preço mensal dependendo do plano.

**Etapa 3. Analise os dados**

Descreva o comportamento do cliente:

-   Encontre os minutos, mensagens de texto e volume de dados que os usuários de cada plano necessitam por mês.
-   Calcule a média, a variância e o desvio padrão.
-   Construa histogramas. Descreva as distribuições.

**Etapa 4. Teste as hipóteses**

-   As receitas médias para os usuários dos planos Ultimate e Surf são diferentes.
-   A receita média dos usuários da área de NY-NJ é diferente da receita dos usuários de outras regiões.

Você decide quais valores alfa usar. Além disso, você precisa explicar:

-   Como você formulou as hipóteses alternativas e nulas.
-   Qual critério você usou para testar as hipóteses e por quê.

**Etapa 5. Escreva uma conclusão geral**

**Formato:** Conclua todas as tarefas no Jupyter Notebook. Armazene todo o código nas células `code` e explicações de texto nas células `markdown`. Adicione títulos e a formatação adequada quando necessário.

## Como meu projeto será avaliado?

Elaboramos alguns critérios de avaliação para o projeto. Leia-os com atenção antes de começar a trabalhar.

Aqui está o que os revisores de projeto vão buscar ao avaliar o seu projeto:

-   Como você explica os problemas identificados nos dados
-   Como você prepara os dados para análise
-   Quais gráficos você fez para as distribuições
-   Como você interpreta os gráficos resultantes
-   Como você calcula o desvio padrão e variância
-   Se você criou hipóteses alternativas e nulas
-   Quais métodos você usa para testar hipóteses
-   Se você interpreta os resultados dos seus testes de hipóteses
-   Se você interpreta os resultados dos seus testes de hipóteses
-   As conclusões que você tirou
-   Se você deixa comentários a cada etapa

No vídeo abaixo, abordamos algumas das dificuldades mais comuns que podem ser encontradas ao longo do projeto.

<iframe class="base-markdown-iframe__iframe" id="player-Eu51DpNQBhA" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="The SDA project NM" width="640" height="360" src="https://www.youtube.com/embed/Eu51DpNQBhA?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F081db06e-c3f8-43e6-91d6-7c1394cf7880%2Ftask%2Fbdcd19ad-4f9e-4254-a62f-0a7e2d57b552%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Boa sorte!

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-f67617d9-f743-492b-b126-98ca6730fb44.containerhub.tripleten-services.com/doc/tree/081db06e-c3f8-43e6-91d6-7c1394cf7880.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-20-50-208Z.md
### Última modificação: 2025-05-28 19:20:50

# Feedback do Sprint 4 - TripleTen

Capítulo 6/6

Conclusão

# Compartilhe sua opinião

**Olá!**

Você acabou de concluir um sprint e enviar seu projeto. Parabéns por alcançar este marco! 🚀

Agora é o momento perfeito para dar seu feedback sobre o sprint. Sua opinião vai nos ajudar a aprimorar ainda mais o processo de aprendizagem.

Este questionário curto vai levar apenas 2 minutos. Todas as respostas vão ser confidenciais: não vamos compartilhá-las de forma que identifique você.

Agradecemos por nos ajudar a evoluir com você!

Como você avalia a experiência com os materiais didáticos, as tarefas e os projetos neste sprint?

1

2

3

4

5

Respostas: 1 – Muito insatisfatória; 5 – Muito satisfatória.

Avançar

1 — 7

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-20-51-535Z.md
### Última modificação: 2025-05-28 19:20:51

# Conclusão - TripleTen

Capítulo 6/6

Conclusão

# Conclusão

Neste sprint, você começou sua jornada no estudo da estatística e da teoria da probabilidade. Agora você pode testar até as hipóteses mais ousadas, porque tem todas as ferramentas necessárias para isso!

Você resolveu tarefas de produtos simples, porém importantes na nossa plataforma online. Você conseguiu realizar uma análise preliminar do plano. Isso vai determinar a próxima estratégia da empresa de telecomunicações.

### Realizações educacionais

Vamos recapitular o que você aprendeu neste sprint:

-   Você aprendeu a escolher as métricas ideais para a descrição dos dados.
-   Você aprendeu a definir o tipo de histograma necessário para avaliar variáveis contínuas e discretas.
-   Você aprendeu a tirar conclusões sobre os dados a partir de métricas estatísticas.
-   Você aprendeu a entender as noções básicas da teoria da probabilidade.
-   Você aprendeu a definir e calcular distribuições normais e binomiais.
-   Você aprendeu a formular e testar hipóteses.

### O que vem a seguir?

Um sprint sobre as Ferramentas de desenvolvimento de software, em que você vai aprender como deixar sua vida de especialista de dados mais fácil. Você vai aprender a usar linhas de comando, criar seu primeiro projeto Git e se aprofundar em Python mais intermediário. Boa sorte, e nos veremos no próximo sprint!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-22-49-586Z.md
### Última modificação: 2025-05-28 19:22:50

# Introdução - TripleTen

Capítulo 1/8

Introdução

# Introdução

Olá, este é o sprint sobre ferramentas de desenvolvimento de software! Queremos destacar desde o início que este sprint terá um formato um pouco diferente dos anteriores. Você vai notar que os capítulos neste sprint não estão diretamente relacionados entre si, como eram antes. Em vez disso, cada capítulo abrange um conjunto de ferramentas de desenvolvimento independente e essencial para todo profissional de dados.

Aprender a utilizar todo o software usado por desenvolvedores profissionais pode parecer assustador. O objetivo deste sprint é deixar você mais confortável com as ferramentas de desenvolvimento mais usadas. Na prática, você vai aprender o seguinte:

-   A linha de comando no seu sistema operacional e os comandos básicos para aproveitá-la ao máximo
-   O sistema de versionamento Git e o ecossistema GitHub
-   As ferramentas e práticas para implementar seu código em um site ou servidor

Para finalizar, você vai trabalhar em um projeto no qual vai implantar seu próprio aplicativo web completo, que poderá ser usado por qualquer pessoa com um navegador web. Concluir este sprint vai levar de 25 a 30 horas aproximadamente.

Neste sprint, você vai se concentrar principalmente em:

![](https://practicum-content.s3.amazonaws.com/resources/Python_1713354922.png)

![](https://practicum-content.s3.amazonaws.com/resources/Ferramentas_de_Desenvolvimento_de_Software_1713354932.png)

As ferramentas aprendidas aqui serão úteis para o resto de sua carreira, pois são usadas como base para a criação de softwares com os quais você pode trabalhar.

![](https://practicum-content.s3.amazonaws.com/resources/S01_18_1_1707139503.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-22-52-212Z.md
### Última modificação: 2025-05-28 19:22:52

# Como abrir o Terminal no MacOS - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Como abrir o Terminal no MacOS

Durante este programa, vamos usar o sistema baseado em Linux, Ubuntu. Dependendo do sistema operacional (SO) que você tem instalado no seu computador (Windows, MacOS, Linux), acessar a linha de comando e executar comandos vai ser um pouco diferente.

**Usuários do MacOS** precisam abrir o aplicativo _Terminal_. Ele é pré-instalado em máquinas Mac, então você só precisa encontrá-lo e abri-lo.

Se você não conseguir encontrá-lo, aqui vai uma dica: abra a caixa de pesquisa do Spotlight pressionando simultaneamente Command e a barra de espaço. Depois, é só digitar "Terminal" e selecionar o aplicativo com esse nome.

Outra alternativa disponível é trabalhar em um Terminal remoto na plataforma indo à sandbox → Seu espaço pessoal de trabalho → Seu Jupyter Notebook.

Depois de abrir um caderno, clique no logotipo do jupyterhub no canto superior esquerdo.

![](https://practicum-content.s3.amazonaws.com/resources/1.1.2PT_1695289835.png)

Encontre o botão "New" (Novo), clique nele e selecione "Terminal" na lista de opções.

![](https://practicum-content.s3.amazonaws.com/resources/1.1.3PT_1707375509.png)

Fazer isso vai abrir um _Terminal_ no jupyterhub. Se não tiver certeza de qual opção é a melhor para você, recomendamos continuar trabalhando na plataforma usando o _Terminal_ da Sandbox.

![](https://practicum-content.s3.amazonaws.com/resources/7_1695289846.png)

Entretanto, se você tiver alguma preferência, escolha entre as opções fornecidas aquela que funciona melhor para você.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-22-55-525Z.md
### Última modificação: 2025-05-28 19:22:56

# Como abrir o Terminal no Windows - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Como abrir o Terminal no Windows

Durante este programa, vamos usar o sistema baseado em Linux, Ubuntu. Dependendo do sistema operacional (SO) que você tem instalado no seu computador (Windows, MacOS, Linux), acessar a linha de comando e executar comandos vai ser um pouco diferente.

É importante notar que a maioria dos comandos que você vai ver aqui são comuns para todos os sistemas operacionais, mas tenha em mente que alguns podem ser específicos para os sistemas com base no Linux/Unix. Então, dependendo do seu sistema operacional, você vai abrir a interface de linha de comando de maneiras diferentes.

**Usuários do Windows** têm várias opções de como acessar a CLI. CLIs padrão no Windows são dois aplicativos: um é chamado Prompt de Comando e o outro, PowerShell. O PowerShell é a melhor opção, mas vamos ensinar como abrir ambos. Vamos começar com o Prompt de Comando.

Dependendo da sua versão do Windows, você pode acessar o Prompt de Comando da seguinte maneira:

-   No **Windows 11**, pressione Win+R para abrir a caixa de diálogo Executar. Depois, digite `cmd` e pressione Enter (ou clique em OK) para abrir o prompt de comando com privilégios de usuário padrão.
-   No **Windows 10**, abra o menu iniciar e vá até "Sistema do Windows". Ao pressionar o menu suspenso, você vai ver um atalho para abrir o Prompt de Comando. Clique com o botão direito no atalho, selecione "Mais" e depois a opção "Executar como administrador".
-   No **Windows 8**, vá para o menu iniciar e clique em "Todos os Aplicativos". Depois role até o diretório "Sistema do Windows" aparecer. Procure o aplicativo Prompt de Comando nele.
-   No **Windows 7**, abra o menu iniciar e pressione "Todos os Programas". Vá até "Acessórios" e encontre o atalho Prompt de Comando. Clique com o botão direito no atalho e pressione "Executar como Administrador".

Como mencionamos, a melhor alternativa é o PowerShell. A melhor maneira de abri-lo é pelo menu Iniciar: é só abrir e digitar "PowerShell".

Embora o Prompt de Comando e o PowerShell venham pré-instalados com o Windows e abri-los seja bastante simples, recomendamos usar uma CLI baseada em Linux/Unix, já que aqui os comandos diferem dos do MacOS e Linux.

Para abrir um Terminal baseado em Linux/Unix no Windows, você tem esta opção:

Trabalhar em um Terminal remoto disponível na plataforma

Vá para a sandbox → Seu espaço pessoal de trabalho → Seu Jupyter notebook.

Depois de abrir um caderno, clique no logotipo do jupyterhub no canto superior esquerdo.

![](https://practicum-content.s3.amazonaws.com/resources/1.1.2PT_1695289911.png)

Encontre o botão "New" (Novo), clique nele e selecione "Terminal" na lista de opções.

![](https://practicum-content.s3.amazonaws.com/resources/1.1.3PT_1695289915.png)

Clicar no Terminal vai abri-lo.

![](https://practicum-content.s3.amazonaws.com/resources/Screenshot_from_2022-02-22_14-48-59_1695287039.png)

Se não tiver certeza de qual opção é a melhor para você, recomendamos continuar trabalhando na plataforma usando o _Terminal_ da Sandbox. Entretanto, se você tiver alguma preferência, escolha entre as opções fornecidas aquela que funciona melhor para você.

Se você fez tudo corretamente, vai ver a interface da linha de comando.

Dependendo do seu sistema operacional e o tema selecionado, esse aplicativo terá um fundo branco ou preto. O design geral também pode ser um pouco diferente.

Depois de abrir o aplicativo, você verá um nome de usuário na primeira linha da interface. Se você vir isso, então está com tudo pronto para começar!

Veja como a janela do Terminal deve ficar em um computador baseado em Linux:

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_NEW_4_sprint/4.2.3.4.png)

Quando um Terminal é iniciado, por padrão, ele é aberto no diretório _Home_. O diretório _Home_ é um lugar no computador (um diretório) em que são armazenados seus arquivos pessoais, diretórios e programas.

No Windows, o diretório _Home_ tem o nome do usuário:

![](https://practicum-content.s3.amazonaws.com/resources/1.2PT_1695289951.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-22-56-880Z.md
### Última modificação: 2025-05-28 19:22:57

# Como abrir o Terminal em um SO baseado em Linux - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Como abrir o Terminal em um SO baseado em Linux

Neste curso, vamos usar Ubuntu, um sistema operacional (SO) com base em Linux.

Se você utiliza **Linux**, é provável que já saiba abrir o Terminal. Para refrescar a memória, você pode abrir um _Terminal_ pressionando Ctrl, Alt e T ao mesmo tempo ou pode procurá-lo clicando no ícone "Dash" e digitando "terminal" no campo de busca.

Outra possibilidade é trabalhar em um Terminal remoto disponível na plataforma.

Vá para a sandbox → Seu espaço pessoal de trabalho → Seu Jupyter notebook.

Depois de abrir um caderno, clique no logotipo do jupyterhub no canto superior esquerdo.

![](https://practicum-content.s3.amazonaws.com/resources/1.1.2PT_1695289969.png)

Encontre o botão "New" (Novo), clique nele e selecione "Terminal" na lista de opções.

![](https://practicum-content.s3.amazonaws.com/resources/1.1.3PT_1695289973.png)

Clicar no Terminal vai abri-lo.

![](https://practicum-content.s3.amazonaws.com/resources/Screenshot_from_2022-02-22_14-48-59_1_1695287459.png)

Você vai ver a interface da linha de comando se fez tudo certo. Dependendo do seu sistema operacional e o tema selecionado, esse aplicativo terá um fundo branco ou preto. Então o design pode variar um pouco.

Quando abrir, você deve ver um nome de usuário seguido por um sinal de dólar na primeira linha da interface. Se você vir isso, então está com tudo pronto para começar!

Aqui está a aparência de uma janela do Terminal em um computador baseado em Linux:

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_NEW_4_sprint/4.2.3.4.png)

Quando um Terminal é iniciado, ele é aberto no diretório _Home_ por padrão. Um diretório _Home_ é um lugar no computador (um diretório) em que são armazenados seus arquivos pessoais, diretórios e programas.

Em sistemas com base Linux, o diretório _Home_ é uma pasta que pode ser facilmente encontrada aqui:

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_NEW_4_sprint/4.2.4.5.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-22-58-230Z.md
### Última modificação: 2025-05-28 19:22:58

# Como executar seu primeiro comando - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Como executar seu primeiro comando

Antes de inserir o nosso primeiro comando, primeiro precisamos entender o que é um comando.

Assista ao vídeo abaixo e depois continue lendo para aprender mais.

Video

<iframe class="base-markdown-iframe__iframe" id="player-N4k4m7jr3QM" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Running Your First Command NM" width="640" height="360" src="https://www.youtube.com/embed/N4k4m7jr3QM?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F59a612dd-8e0d-4795-99ae-e97871785882%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Cada comando tem uma estrutura específica: _`comando [opções...][argumentos...]`_.

Observação: opções entre colchetes (_`[opções...]`_) são uma forma de dizer que várias opções podem ser passadas com um único comando. De forma similar, argumentos entre colchetes (_`[argumentos...]`_) são usados para mostrar que vários argumentos são aceitos em uma única execução do comando.

### Comandos

Um **comando** é apenas um programa que diz ao sistema o que ele precisa fazer. É importante perceber que eles são sensíveis a maiúsculas e minúsculas, então _comando_ e _Comando_ são diferentes.

Para escrever um comando, o digitamos em uma janela do _Terminal_ e pressionamos _Enter_ para executá-lo.

É assim que fica quando rodamos um _Terminal_ e executamos um comando simples `ls`, que lista o conteúdo do diretório _Home_ do computador:

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_NEW_4_sprint/4.2.5.1.png)

Observe que esse resultado é específico para o nosso ambiente de computação. Executar esse comando em seu próprio computador ou em uma plataforma diferente pode produzir outro resultado.

O comando `ls` exibe o primeiro nível de subdiretórios e arquivos no diretório _Home_. Podemos ver muitos subdiretórios (em azul) e alguns arquivos (em branco). Você vai obter um resultado diferente se executar esse comando no seu próprio computador, já que tem diferentes arquivos e diretórios nele. Experimente!

**Opções:**

Se quisermos modificar levemente o comportamento de um comando, é aí que **opções** entram no jogo. Comandos são seguidos por opções. Aqui está um exemplo de inclusão de uma opção `r` no comando `ls` que já conhecemos:

```
~$ ls -r
```

A opção `-r` modifica o comando `ls` para imprimir a mesma informação, mas na ordem inversa. O comando `ls` lista o conteúdo em ordem alfabética, mas, às vezes, podemos querer listar de Z para A. O resultado de `ls -r` fica assim:

```
unsupervised-Learning    intro_to_ml
time-series-forecasting  integrated_project_2
supervised_learning      integrated_project_1
sql_project              eda
sda                      decision_trees_and_random_forest
numerical_methods        data_preprocessing
nlp                      computer_vision-ON_SERVER
ml4business              annot.csv
markup.py                annot_creation_for_lines_detector.ipynb
linear_algebra
```

Como você pode ver, podemos adicionar uma opção logo após o nome do comando, separado por um único espaço.

Em geral, opções são precedidas por um hífen (-) ou um hífen duplo (--). Vale mencionar que várias opções podem ser usadas ao mesmo tempo. Por exemplo, a opção `-a` para o comando `ls` permite imprimir não apenas arquivos e subdiretórios regulares, mas também os [ocultos](https://en.wikipedia.org/wiki/Hidden_file_and_hidden_directory) _(os materiais estão em inglês)_.

Veja como usar as opções `-r` e `-a` juntas:

```
~$ ls -r -a
```

Você também pode combinar duas ou mais opções juntas usando apenas um hífen: `ls -ra`

O resultado para ambas é exibido abaixo:

```
unsupervised-Learning    intro_to_ml
time-series-forecasting  integrated_project_2
supervised_learning      integrated_project_1
sql_project              eda
sda                      decision_trees_and_random_forest
numerical_methods        data_preprocessing
nlp                      computer_vision-ON_SERVER
ml4business              annot.csv
markup.py                annot_creation_for_lines_detector.ipynb
linear_algebra           ..
.ipynb_checkpoints       .
```

Agora, além dos subdiretórios em ordem inversa, há também três novos elementos:

-   _.ipynb\_checkpoints_ é um subdiretório oculto que agora é exibido. Algo que vale a pena observar é que nomes para todos os arquivos e diretórios ocultos sempre começam com `.`
-   Um único ponto (.) e o ponto duplo (..): um ponto único é uma meta-localização que descreve o diretório em que você está no momento, e o ponto duplo indica um diretório superior que você pode acessar.

Opções não são obrigatórias, o que significa que um comando pode ser executado sozinho sem elas: mais cedo, não tínhamos opções para o comando _ls_, e ele foi executado sem erros.

Também vale a pena mencionar que alguns comandos não aceitam opções.

**Argumentos:**

Vamos continuar trabalhando com `ls` e aprender mais sobre argumentos. Dê uma olhada no seguinte comando:

```
~$ ls -r sql_project
```

O nome do comando é `ls`, a opção `-r` é usada para a ordem inversa e `sql_project` é um argumento, mas o que `sql_project` faz?

Dos resultados anteriores, sabemos que existe um subdiretório no diretório _Home_ chamado _sql\_project_. Quando `sql_project` é usado como argumento, ele diz a `ls` para trabalhar com o subdiretório `sql_project`. Especificamente, ele pede para `ls` listar os arquivos e subdiretórios que estão dentro de `sql_project`. Este é o resultado de `ls -r sql_project`:

```
wmo_norms_db.py       wmo_norms.db         README.md  example_queries.sql
wmo_norms.db-journal  wmo_norms_1961-1990  LICENSE
```

Se fizermos um paralelo entre argumentos dos comandos da CLI e funções em Python, podemos dizer que ambos tratam argumentos como entradas.

Agora você sabe a sintaxe para os comandos da CLI. A seguir, vamos falar a respeito de um conjunto particular de comandos usados para a navegação em um sistema de arquivos.

### Como selecionar opções adequadas e passar argumentos válidos

Existem duas formas de verificar todas as possíveis opções e entender qual argumento usar para um dado comando.

-   `--help` é uma opção que pode ser usada para imprimir a mensagem sobre o uso do comando.

```
~$ ls --help
```

```
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      scale sizes by SIZE before printing them; e.g.,
                               '--block-size=M' prints sizes in units of
                               1,048,576 bytes; see SIZE format below
  -B, --ignore-backups       do not list implied entries ending with ~
  -c                         with -lt: sort by, and show, ctime (time of last
                               modification of file status information);
                               with -l: show ctime and sort by name;
                               otherwise: sort by ctime, newest first
  -C                         list entries by columns
      --color[=WHEN]         colorize the output; WHEN can be 'always' (default
                               if omitted), 'auto', or 'never'; more info below
  -d, --directory            list directories themselves, not their contents
  -D, --dired                generate output designed for Emacs dired mode
  -f                         do not sort, enable -aU, disable -ls --color
  -F, --classify             append indicator (one of */=>@|) to entries
      --file-type            likewise, except do not append '*'
      --format=WORD          across -x, commas -m, horizontal -x, long -l,
                               single-column -1, verbose -l, vertical -C
      --full-time            like -l --time-style=full-iso
  -g                         like -l, but do not list owner
      --group-directories-first
                             group directories before files;
                               can be augmented with a --sort option, but any
                               use of --sort=none (-U) disables grouping
  -G, --no-group             in a long listing, do not print group names
  -h, --human-readable       with -l and/or -s, print human readable sizes
                               (e.g., 1K 234M 2G)
      --si                   likewise, but use powers of 1000 not 1024
  -H, --dereference-command-line
                             follow symbolic links listed on the command line
      --dereference-command-line-symlink-to-dir
                             follow each command line symbolic link
                               that points to a directory
      --hide=PATTERN         do not list implied entries matching shell PATTERN
                               (overridden by -a or -A)
      --hyperlink[=WHEN]     hyperlink file names; WHEN can be 'always'
                               (default if omitted), 'auto', or 'never'
      --indicator-style=WORD  append indicator with style WORD to entry names:
                               none (default), slash (-p),
                               file-type (--file-type), classify (-F)
  -i, --inode                print the index number of each file
  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN
  -k, --kibibytes            default to 1024-byte blocks for disk usage
  -l                         use a long listing format
  -L, --dereference          when showing file information for a symbolic
                               link, show information for the file the link
                               references rather than for the link itself
  -m                         fill width with a comma separated list of entries
  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs
  -N, --literal              print entry names without quoting
  -o                         like -l, but do not list group information
  -p, --indicator-style=slash
                             append / indicator to directories
  -q, --hide-control-chars   print ? instead of nongraphic characters
      --show-control-chars   show nongraphic characters as-is (the default,
                               unless program is 'ls' and output is a terminal)
  -Q, --quote-name           enclose entry names in double quotes
      --quoting-style=WORD   use quoting style WORD for entry names:
                               literal, locale, shell, shell-always,
                               shell-escape, shell-escape-always, c, escape
  -r, --reverse              reverse order while sorting
  -R, --recursive            list subdirectories recursively
  -s, --size                 print the allocated size of each file, in blocks
  -S                         sort by file size, largest first
      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),
                               time (-t), version (-v), extension (-X)
      --time=WORD            with -l, show time as WORD instead of default
                               modification time: atime or access or use (-u);
                               ctime or status (-c); also use specified time
                               as sort key if --sort=time (newest first)
      --time-style=STYLE     with -l, show times using style STYLE:
                               full-iso, long-iso, iso, locale, or +FORMAT;
                               FORMAT is interpreted like in 'date'; if FORMAT
                               is FORMAT1<newline>FORMAT2, then FORMAT1 applies
                               to non-recent files and FORMAT2 to recent files;
                               if STYLE is prefixed with 'posix-', STYLE
                               takes effect only outside the POSIX locale
  -t                         sort by modification time, newest first
  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8
  -u                         with -lt: sort by, and show, access time;
                               with -l: show access time and sort by name;
                               otherwise: sort by access time, newest first
  -U                         do not sort; list entries in directory order
  -v                         natural sort of (version) numbers within text
  -w, --width=COLS           set output width to COLS.  0 means no limit
  -x                         list entries by lines instead of by columns
  -X                         sort alphabetically by entry extension
  -Z, --context              print any security context of each file
  -1                         list one file per line.  Avoid '\n' with -q or -b
      --help     display this help and exit
      --version  output version information and exit

The SIZE argument is an integer and optional unit (example: 10K is 10*1024).
Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).

Using color to distinguish file types is disabled both by default and
with --color=never.  With --color=auto, ls emits color codes only when
standard output is connected to a terminal.  The LS_COLORS environment
variable can change the settings.  Use the dircolors command to set it.

Exit status:
 0  if OK,
 1  if minor problems (e.g., cannot access subdirectory),
 2  if serious trouble (e.g., cannot access command-line argument).

GNU coreutils online help: <http://www.gnu.org/software/coreutils/>
Full documentation at: <http://www.gnu.org/software/coreutils/ls>
or available locally via: info '(coreutils) ls invocation'
```

No resultado podemos ver:

-   as informações de uso do comando (`List information about the FILEs (the current directory by default)`)
-   sua sintaxe (`ls [OPTION]... [FILE]...`)
-   opções disponíveis e uma breve descrição (por exemplo `-a, --all do not ignore entries starting with .`).

Perceba que a maioria das opções tem duas representações: uma com um hífen e outra com hífen duplo. Por exemplo, a opção `-r` é a mesma que `--reverse`.

-   O comando `man` pode ser usado para mostrar o manual do usuário de qualquer comando que podemos executar no _Terminal_. Então, se quisermos obter o manual para o comando `ls`, podemos fazer o seguinte:

```
~$ man ls
```

Aqui está o resultado:

```
NAME
       ls - list directory contents

SYNOPSIS
       ls [OPTION]... [FILE]...

DESCRIPTION
       List  information  about  the FILEs (the current directory by default).
       Sort entries alphabetically if none of -cftuvSUX nor --sort  is  speci‐
       fied.

       Mandatory  arguments  to  long  options are mandatory for short options
       too.

       -a, --all
              do not ignore entries starting with .

       -A, --almost-all
              do not list implied . and ..

       --author
              with -l, print the author of each file

       -b, --escape
              print C-style escapes for nongraphic characters

       --block-size=SIZE
              scale sizes by SIZE before printing them; e.g., '--block-size=M'
              prints sizes in units of 1,048,576 bytes; see SIZE format below

       -B, --ignore-backups
              do not list implied entries ending with ~

       -c     with -lt: sort by, and show, ctime (time of last modification of
              file status information); with -l: show ctime and sort by  name;
              otherwise: sort by ctime, newest first

       -C     list entries by columns

       --color[=WHEN]
              colorize  the output; WHEN can be 'always' (default if omitted),
              'auto', or 'never'; more info below

       -d, --directory
              list directories themselves, not their contents

       -D, --dired
              generate output designed for Emacs dired mode

       -f     do not sort, enable -aU, disable -ls --color

       -F, --classify
              append indicator (one of */=>@|) to entries

       --file-type
              likewise, except do not append '*'

       --format=WORD
              across -x, commas -m, horizontal -x, long -l, single-column  -1,
              verbose -l, vertical -C

       --full-time
              like -l --time-style=full-iso

       -g     like -l, but do not list owner

       --group-directories-first
              group directories before files;

              can   be  augmented  with  a  --sort  option,  but  any  use  of
              --sort=none (-U) disables grouping

       -G, --no-group
              in a long listing, do not print group names

       -h, --human-readable
              with -l and/or -s, print human readable sizes (e.g., 1K 234M 2G)

       --si   likewise, but use powers of 1000 not 1024

       -H, --dereference-command-line
              follow symbolic links listed on the command line

       --dereference-command-line-symlink-to-dir
              follow each command line symbolic link

              that points to a directory

       --hide=PATTERN
              do not list implied entries matching shell  PATTERN  (overridden
              by -a or -A)

       --hyperlink[=WHEN]
              hyperlink file names; WHEN can be 'always' (default if omitted),
              'auto', or 'never'

       --indicator-style=WORD
              append indicator with style WORD to entry names: none (default),
              slash (-p), file-type (--file-type), classify (-F)

       -i, --inode
              print the index number of each file

       -I, --ignore=PATTERN
              do not list implied entries matching shell PATTERN

       -k, --kibibytes
              default to 1024-byte blocks for disk usage

       -l     use a long listing format

       -L, --dereference
              when showing file information for a symbolic link, show informa‐
              tion for the file the link references rather than for  the  link
              itself

       -m     fill width with a comma separated list of entries

       -n, --numeric-uid-gid
              like -l, but list numeric user and group IDs

       -N, --literal
              print entry names without quoting

       -o     like -l, but do not list group information

       -p, --indicator-style=slash
              append / indicator to directories

       -q, --hide-control-chars
              print ? instead of nongraphic characters

       --show-control-chars
              show nongraphic characters as-is (the default, unless program is
              'ls' and output is a terminal)

       -Q, --quote-name
              enclose entry names in double quotes

       --quoting-style=WORD
              use quoting style WORD for entry names: literal, locale,  shell,
              shell-always, shell-escape, shell-escape-always, c, escape

       -r, --reverse
              reverse order while sorting
```

Observe que, ao executar esse comando na Sandbox, você talvez receba um feedback assim: "_This system has been minimized by removing packages and content that are not required on a system that users do not log into. To restore this content, including manpages, you can run the 'unminimize' command…_". Essa mensagem significa que "o sistema foi minimizado por meio de remoção de pacotes e conteúdo que não são obrigatórios em um sistema em que os usuários não fazem login. Para restaurar esse conteúdo, incluindo páginas de manual, você pode executar o comando 'unminimize'". Isso ocorre por causa da configuração da nossa plataforma, não se preocupe!

Pergunta

Quais sentenças estão incorretas?

Se eu abrir um _Terminal_ e executar o comando `ls`, vou listar o conteúdo do meu diretório Home.

Para listar o conteúdo do diretório Home em ordem inversa, abro um _Terminal_ e executo o comando `ls -r` ou `ls --reverse`.

Se você usa um hífen simples, apenas duas opções podem ser combinadas em um comando.

Mais do que 2 opções podem ser usadas simultaneamente com um hífen. Por exemplo, o comando `ls -ras` vai imprimir todo o conteúdo (inclusive arquivos e pastas ocultos) na ordem inversa. Ele também vai imprimir o tamanho alocado de cada arquivo, em blocos.

Você conseguiu!

Pergunta

Qual comando é **inválido**?

1

```
~$ ls -a sql_project
```

2

```
~$ ls -a --reverse
```

3

```
~$ ls --reverseall
```

Várias opções podem ser usadas juntas apenas depois de um hífen simples. Se passarmos duas opções juntas (como reverse e all) usando os nomes completos e um hífen duplo, teremos um erro:

```
ls: unrecognized option '--reverseall'
Try 'ls --help' for more information.
```

Seu entendimento sobre o material é impressionante!

Pergunta

Agora é sua vez de brilhar! Vá para a Sandbox e abra um _Terminal_ lá. Use um dos comandos que você aprendeu para verificar se o diretório Home contém algum subdiretório ou arquivo oculto. Selecione uma das respostas com base em qual comando você usou e qual foi o resultado.

Escolha quantas quiser

Usei `ls -q` e não encontrei nenhum subdiretório ou arquivo oculto.

Usei `ls --all` e notei vários subdiretórios/arquivos ocultos.

A opção `--all` é exatamente o que precisamos para listar arquivos e diretórios ocultos. Parece que há muitas pastas e arquivos ocultos.

Usei `ls -A`, e não havia nenhum arquivo ou subdiretório oculto.

A execução de `ls -a` revelou muitos diretórios e arquivos ocultos.

O comando `ls -a` é equivalente a `ls --all`, já que ele lista todos os arquivos e diretórios ocultos.

Trabalho maravilhoso!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-00-323Z.md
### Última modificação: 2025-05-28 19:23:01

# Navegação no sistema de arquivos - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Navegação no sistema de arquivos

Agora que você sabe o que é uma linha de comando e como uma sintaxe é estruturada, é hora de aprender alguns comandos usados para a navegação no sistema de arquivos.

Para dar o pontapé inicial, vamos voltar ao comando `ls`. Sabemos que ele lista conteúdos, mas qual tipo de conteúdo exatamente? Vamos descobrir isso com a ajuda do comando `pwd`.

### O comando **`pwd`**

`pwd` é uma sigla em inglês para "print working directory", que significa "imprimir o diretório atual". Ou seja, `pwd` pode nos ajudar a verificar em que diretório estamos.

```
~$ pwd
```

```
/home/anthony/edu_materials
```

Podemos ver que o resultado é o caminho para o diretório em que estamos.

Para resumir, sabemos que estamos no diretório `/home/anthony/edu_materials` (graças ao comando `pwd`) e que existem várias subpastas nele (graças ao comando `ls`).

### O comando **`cd`**

E se quisermos ir do diretório atual em que estamos para um diferente? Nesse caso, usamos o comando `cd`. `cd` é responsável por mudar de diretório. Ele aceita o caminho para um diretório como argumento, e isso permite que você passe o nome de uma subpasta em que você quiser entrar.

Usando o resultado do comando `ls`, vimos que há uma subpasta chamada `intro_to_ml`. Ir até lá é muito fácil:

```
~$ cd intro_to_ml/
```

Para verificar que conseguimos mudar de diretório, podemos chamar o comando `pwd` _de novo:_

```
~$ pwd
```

E aqui está o resultado:

```
/home/anthony/edu_materials/intro_to_ml/
```

Parece que saímos mesmo do lugar, viva!

Você também pode ir do diretório `/home/anthony/edu_materials` para a mesma subpasta `intro_to_ml` usando um ponto simples de meta-localização (`./`). Isso afirma explicitamente que estamos migrando do diretório atual para uma das subpastas:

```
~$ cd ./intro_to_ml/
```

Em outras palavras, `./` indica o diretório em que você está atualmente. Se você quiser migrar do diretório atual para uma das subpastas, pode fazer isso diretamente (por exemplo, `cd intro_to_ml/`) ou usando a meta-localização de ponto simples (`cd ./intro_to_ml/`). Os resultados serão idênticos.

Outra alternativa é passar um caminho absoluto para o diretório onde você gostaria de estar:

```
~$ cd /home/anthony/edu_materials/intro_to_ml/
```

Todas essas abordagens vão colocar você em `/home/anthony/edu_materials/intro_to_ml/`. A única diferença é qual argumento é passado.

Se você quiser ir para um diretório superior `/home/anthony/`, você pode passar um caminho absoluto:

```
~$ cd /home/anthony/
```

ou usar o ponto duplo como uma meta-localização para indicar o diretório superior:

```
~$ cd ../
```

Qualquer uma dessas abordagens vai nos deixar ir de `/home/anthony/edu_materials/` para `/home/anthony/`.

Vamos ver se tudo funcionou usando `pwd`_:_

```
~$ pwd
```

Nada surpreendente, aqui está o resultado:

```
/home/anthony/
```

Além disso, vale a pena conhecer o atalho `cd ~`, que nos leva ao diretório `/home/<usuário>`, onde `<usuário>` é o nome de usuário no seu computador. No nosso caso, `cd ~` vai nos levar a `/home/anthony/`.

Agora nosso arsenal inclui 3 comandos: `ls`, `pwd` e `cd`. Com eles, você nunca mais vai se perder no seu sistema de arquivos.

Pergunta

O que vai acontecer quando executarmos o seguinte comando:

```
~$ cd ./
```

Nada, vamos continuar no mesmo diretório.

A meta-localização de ponto simples `./` indica o diretório atual, então iremos do diretório atual para o diretório atual. Em outras palavras, nenhuma mudança.

Vamos ir do diretório atual para o diretório Home.

Vamos ir do diretório atual para o diretório superior.

Fantástico!

Pergunta

Se abrirmos um novo Terminal e executarmos o comando `pwd`, o que vamos ver?

O caminho absoluto para o diretório Home.

Exatamente! E isso é porque o novo Terminal abre no diretório `/home/<usuário>` por padrão.

O caminho absoluto para o diretório Desktop (Área de Trabalho) do computador.

O caminho absoluto para o diretório Downloads do computador.

Fantástico!

Pergunta

Hora de praticar! Vá para a Sandbox e abra um Terminal lá. Use um dos comandos que você aprendeu anteriormente para verificar em que diretório você está. Selecione uma das respostas com base no comando que você usou e no resultado que obteve.

Usei o comando `ls` com a opção `-r` para ver o diretório no qual estou. O resultado foi `work nltk_data`

Usei o comando `pwd`, e o resultado foi `/students_dir`

Usei o comando `pwd`, e o resultado foi `/home/jovyan`

`pwd` é exatamente o que você precisa usar para verificar o diretório atual.

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-01-645Z.md
### Última modificação: 2025-05-28 19:23:02

# Dicas e truques - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Dicas e truques

Neste ponto, você já sabe como é a sintaxe básica da CLI e como navegar pelo sistema de arquivos do seu computador. Agora, queremos ensinar a você algumas dicas e truques legais que podem facilitar sua vida.

### O comando `clear`

Você já deve ter notado que após alguns comandos a janela do _Terminal_ começa a ficar bagunçada, com muitas informações exibidas em uma única janela. Para limpá-la, podemos usar o comando _clear_.

Apenas digite _clear_ e, voilà, seu _Terminal_ está limpo e vazio como no exemplo abaixo.

```
~$ ls -r
unsupervised-Learning    nlp                   eda
time-series-forecasting  ml4business           decision_trees_and_random_forest
supervised_learning      linear_algebra        data_preprocessing
sql_project              intro_to_ml           computer_vision-ON_SERVER
sda                      integrated_project_2
numerical_methods        integrated_project_1
~$ clear
```

Quando _clear_ é executado, a janela fica vazia novamente:

```
~$ 
```

Isso não significa que todo o progresso anterior foi perdido. Você pode facilmente restaurar comandos anteriores.

### Extrair comandos anteriores

Para trazer de volta todos os comandos anteriores, basta pressionar o botão para cima (aquele com a seta apontando para cima).

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_SDT_refactored/Up_Down_key_in_Terminal.gif)

Você pode recuperar entradas anteriores apertando continuamente a seta para cima, e cada vez você vai ver os comandos anteriores surgindo na janela do _Terminal_. Muito legal, não é?

Também podemos usar a seta para baixo para obter o próximo comando na ordem.

É importante saber que se você fechar a janela do _Terminal_, todo progresso será perdido. As setas para cima e para baixo serão inúteis até que você comece a escrever algo novo.

### Preenchimento automático

O preenchimento automático é uma funcionalidade bem legal que nos ajuda a terminar o nome de um comando ou de um argumento após inserirmos algumas letras iniciais.

Para ver como isso funciona, comece a escrever o comando _`clear`_ e pare após 3 letras:

```
~$ cle
```

Agora pressione três vezes a tecla Tab no teclado e veja o resultado:

```
~$ clear
clear          clear_console
```

Aqui podemos ver alguns comandos sugeridos que começam com _`cle`_.

Vamos tentar outro comando já conhecido – o comando _pwd_:

```
~$ pw
```

Pressione duas vezes Tab, e isso é o que vamos ver:

```
pwck       pwconv     pwd        pwdecrypt  pwdx       pwunconv
~$ pw
```

Vemos que existem vários comandos que começam com _`pw`_. A CLI não sabe qual deles você quer, então imprime todos eles e espera até que você continue digitando o nome do nosso objetivo.

Com essas dicas, você acabou de tornar seu trabalho com a CLI muito mais eficiente e prático.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-02-959Z.md
### Última modificação: 2025-05-28 19:23:03

# Como trabalhar com arquivos e diretórios: parte 1 - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Como trabalhar com arquivos e diretórios: parte 1

Agora que sabemos como navegar pelo sistema de arquivos, é hora de ver o que podemos fazer com todos esses arquivos e diretórios.

### O comando `mkdir`

O primeiro comando que queremos apresentar a você é o `mkdir`, que usamos para criar novos diretórios. Ele usa a seguinte sintaxe: `mkdir [opções ...] [diretórios ...]`

O `mkdir` espera caminhos para os novos diretórios como argumentos. Normalmente, criamos um novo diretório dentro daquele em que já estamos. Para fazer isso, tudo o que precisamos fazer é passar o nome do novo diretório como um argumento.

Então, se queremos criar um único diretório chamado `my_new_directory`, precisamos executar o seguinte comando:

```
~$ mkdir my_new_directory
```

`mkdir` pode aceitar vários argumentos, o que significa que podemos listar vários nomes de diretórios que quisermos criar em apenas uma execução do comando.

Aqui está como podemos criar 3 diretórios ao mesmo tempo (`my_new_directory1`, `my_new_directory2`, `my_new_directory3`):

```
~$ mkdir my_new_directory1 my_new_directory2 my_new_directory3
```

Perceba que todos os argumentos (aqui os nomes dos novos diretórios) devem ser separados por um único espaço.

Vejamos outro exemplo.

Queremos criar um subdiretório chamado `my_sub_dir` e que ele esteja dentro do diretório superior `my_parent_dir`:

```
~$ mkdir my_parent_dir/my_sub_dir
```

Observe o caminho que passamos como argumento, `my_parent_dir/my_sub_dir`. Criamos o novo diretório passando o caminho, mas isso não funciona se o diretório superior não existir.

Se `my_parent_dir` não existir, teremos um erro:

```
mkdir: cannot create directory ‘my_parent_dir/my_sub_dir’: No such file or directory
```

O erro acima diz `No such file or directory` (Não existe tal arquivo ou diretório), o que significa que `my_parent_dir` não existe.

Para criar o subdiretório e também o diretório superior, tudo o que precisamos fazer é usar a opção `-p`:

```
~$ mkdir -p my_parent_dir/my_sub_dir
```

Nesse caso, `my_parent_dir` é criado primeiro, depois `my_sub_dir` é criado.

Para ilustrar esse processo, podemos incluir a opção `-v` (verbose), que mostra o que está sendo feito:

```
~$ mkdir -p -v my_parent_dir/my_sub_dir
```

e aqui está o resultado:

```
mkdir: created directory 'my_parent_dir'
mkdir: created directory 'my_parent_dir/my_sub_dir'
```

### O comando `rm`

Você pode usar o comando `rm` para remover arquivos e diretórios que não são mais necessários.

Vamos dizer que você criou um novo diretório chamado `my_new_directory` para um propósito educacional e agora não precisa mais dele. O código para isso é mais ou menos assim:

```
~$ rm my_new_directory
```

Espere aí, há um erro dizendo que:

```
rm: cannot remove 'my_new_directory/': Is a directory
```

O sistema identificou corretamente `my_new_directory` como um diretório, mas falhou em excluí-lo. Por quê?

Se usarmos `rm --help` para verificar todas as opções disponíveis, veremos que as opções `-d` ou `--dir` são usadas para remover diretórios vazios. Já que nosso diretório está vazio, precisamos usar a opção `-d` com o comando `rm`. Já que nosso diretório está vazio, precisamos usar a opção `-d` com o comando `rm`:

```
~$ rm -d my_new_directory
```

Não temos nenhum erro dessa vez, o que significa que `rm` fez seu trabalho e removeu o diretório que queríamos remover.

Se quisermos uma explicação mais "visual" do que foi feito, podemos adicionar a opção `-v` para imprimir uma instrução esclarecedora.

Para tentar tudo isso, vamos criar `my_new_directory` novamente e removê-lo usando o comando `rm` e aplicando a opção `-v`:

```
~$ mkdir my_new_directory
```

E agora remova o diretório recém-criado:

```
~$ rm -d -v my_new_directory
```

O resultado é o seguinte:

```
removed directory 'my_new_directory/'
```

Arquivos também podem ser removidos usando o comando `rm`. A sintaxe para esse comando permite que você exclua vários arquivos passando caminhos para eles como argumentos:

```
~$ rm file1 file2 file3
```

Você também pode usar a opção `-i` para solicitar uma mensagem interativa antes da remoção:

```
~$ rm -d -i my_new_directory
```

O que vai exibir esta mensagem antes de remover um diretório:

```
rm: remove directory 'my_new_directory/'?
```

Digitar `y` e pressionar _Enter_ executará o comando, e `n` reverterá o comando e nada será excluído.

Pergunta

Vá para a Sandbox e abra um Terminal lá. Use um dos comandos que você aprendeu até agora para criar um novo diretório chamado `my_first_dir`. Uma vez criado, escolha a resposta correta abaixo:

Eu criei o diretório `my_first_dir`, e agora há dois diretórios no total.

Eu criei o diretório `my_first_dir`, e agora há 2 diretórios e 2 arquivos no total.

Eu criei o diretório `my_first_dir`, e agora há 3 diretórios no total.

Legal! Depois que você criou `my_first_dir`, o número de diretórios foi para 3: `my_first_dir`, `nltk_data` e `work`.

Fantástico!

Pergunta

Agora queremos criar um subdiretório dentro do diretório `my_first_dir` recém-criado. Vamos chamar esse novo subdiretório `my_sub_dir`. Qual comando é o que devemos usar?

`mkdir -p my_first_dir/my_sub_dir`

`mkdir my_first_dir/my_sub_dir`

É exatamente assim que queremos criar um subdiretório!

`mkdir /my_first_dir/my_sub_dir`

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-05-740Z.md
### Última modificação: 2025-05-28 19:23:06

# Como trabalhar com arquivos e diretórios: parte 2 - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Como trabalhar com arquivos e diretórios: parte 2

Nesta lição, vamos continuar aprendendo novos comandos que costumam ser usados para trabalhar com arquivos e diretórios. Em particular, vamos dar uma olhada mais de perto nos comandos `echo`, `touch`, `mv` e `cp`.

### O comando `echo`

`echo` é um [comando integrado](https://www.computerhope.com/jargon/e/echo.htm) _(os materiais estão em inglês)_ usado para imprimir argumentos no _Terminal_ como resultado padrão.

Se executarmos:

```
echo Quero aprender sobre a CLI
```

O resultado vai ser:

```
Quero aprender sobre a CLI
```

Sem nenhuma opção, `echo` exibe a string de entrada exatamente como ela está.

Adicionar a opção `-e` nos permite usar [caracteres de escape](https://pt.wikipedia.org/wiki/Caractere_de_escape).

Vamos ver como mover uma sentença para uma nova linha. Precisamos adicionar a opção `-e` e colocar o caractere de escape `\n` no lugar onde queremos a quebra de linha:

```
echo -e 'Quero aprender sobre a CLI. \nTodo programador sabe usá-la!'
```

É importante perceber que a string que queremos imprimir está entre aspas agora. Se adicionarmos qualquer opção, precisamos colocar aspas.

Aqui está o resultado do comando acima:

```
Quero aprender sobre a CLI. 
Todo programador sabe usá-la!
```

Podemos ver que a quebra de linha está no mesmo lugar onde colocamos `\n`.

`echo` também pode ser usado para redirecionar seu resultado para um arquivo de texto. Por exemplo, se escrevermos `'Quero aprender sobre a CLI. \nTodo programador sabe usá-la!'` em `my_file.txt`, tudo o que precisamos fazer é usar `>` ou `>>`para direcionar a string para um arquivo:

```
echo -e 'I want to know CLI. \nTodo programador sabe usá-la!' > my_file.txt
```

Se o arquivo `my_file.txt` não existe, `echo` vai criá-lo.

A diferença entre `>` e `>>` está na maneira como eles gerenciam a substituição. Se o arquivo existe, `>` vai substituir seu conteúdo, enquanto `>>` vai apenas adicionar uma nova linha ao final do arquivo existente.

Podemos verificar o conteúdo do arquivo que acabamos de criar usando o comando `cat`:

```
cat my_file.txt
```

Como resultado, obtemos as duas linhas de texto que escrevemos anteriormente nesse arquivo:

```
Quero aprender sobre a CLI. 
Todo programador sabe usá-la!
```

### O comando `touch`

`touch` é um comando simples, mas útil, que nos permite criar arquivos dummy. Arquivos dummy não contêm dados e estão vazios.

Veja como criar um arquivo dummy com um único `touch`:

```
touch my_new_file
```

Agora, se executamos o comando `ls`, veremos o novo arquivo que acabamos de criar. Por exemplo, executar `ls` no nosso computador local vai mostrar o seguinte:

```
Desktop
Documents
Downloads
Music
my_new_file
Pictures
Videos
```

Vemos que `my_new_file` foi criado. É importante observar que executar `touch` em um arquivo já existente não faz nada.

Vamos usar o comando `touch` para criar arquivos dummy e mostrar como comandos da CLI funcionam.

### O comando `mv`

O comando `mv` é útil quando você quer mover um arquivo ou diretório de um lugar para outro.

Para mover um arquivo, `mv` espera dois argumentos: o caminho para um arquivo de origem e um caminho de destino. Enquanto estávamos aprendendo o comando `echo`, usamos um arquivo de texto chamado `my_file.txt`. Vamos criar um subdiretório chamado `sub_directory` e mover `my_file.txt` para lá:

```
mkdir sub_directory
mv my_file.txt sub_directory  
```

Podemos mover qualquer arquivo com o comando `mv`, não apenas arquivos `.txt`.

Se quisermos mover um arquivo do diretório atual para qualquer outro localizado fora do atual, só precisamos passar o caminho para o destino como segundo argumento.

Agora vamos mover `my_file.txt` do diretório atual para outro diretório localizado no diretório Home. Primeiro, vamos criar o diretório e depois mover o arquivo para lá:

```
mkdir ~/home_sub_directory
mv my_file.txt ~/home_sub_directory
```

Para refrescar a memória, usamos `~/` para o diretório Home.

Na primeira linha do trecho de código acima, criamos o diretório `home_sub_directory` dentro de `~/` e então, na segunda linha, movemos `my_file.txt` para lá usando o comando `mv`.

Um ou vários arquivos podem ser movidos usando uma única execução do comando. Para mover vários arquivos, precisamos listá-los separados por espaços.

Para ilustrar isso, vamos criar mais dois arquivos de texto, `my_file1.txt` e `my_file2.txt`, usando o comando `touch` e mover ambos os arquivos do diretório atual para `~/home_sub_directory`:

```
touch my_file1.txt
touch my_file2.txt
mv my_file1.txt my_file2.txt ~/home_sub_directory
```

Agora, se listarmos os conteúdos de `~/home_sub_directory`:

```
ls ~/home_sub_directory
```

Vamos ver todos os três arquivos ali:

```
my_file.txt
my_file1.txt
my_file2.txt
```

Diretórios também podem ser movidos. Tudo o que você precisa fazer é passar o diretório como o primeiro argumento e o destino como o segundo argumento.

Supondo que `sub_directory` esteja vazio, podemos movê-lo para `~/home_sub_directory`.

```
mv sub_directory ~/home_sub_directory
```

`mv` também pode ser usado para renomear arquivos.

Vamos criar um arquivo de texto chamado `old_file_name.txt` usando o comando `echo` e escrever uma única string lá. Que seja _testing content_ (conteúdo de teste):

```
echo testing content > old_file_name.txt
```

Para renomear `old_file_name.txt` para `new_file_name.txt`, fazemos isso:

```
mv old_file_name.txt new_file_name.txt
```

Se `new_file_name.txt` não existe, `old_file_name.txt` será renomeado. Mas se `new_file_name.txt` já existe, então `new_file_name.txt` será substituído. Quanto a `old_file_name.txt`, ele vai deixar de existir.

Vamos usar agora o comando `cat` para verificar o conteúdo de `new_file_name.txt`:

```
cat new_file_name.txt
```

Aqui está o resultado:

```
testing content
```

O conteúdo de `new_file_name.txt` é o mesmo que tínhamos escrito em `old_file_name.txt`, o que significa que `old_file_name.txt` foi renomeado com sucesso.

-   Pode ser uma boa ideia prevenir arquivos de serem substituídos enquanto você usar o comando `mv`. Para ignorar o comando `mv` se ele for substituir alguma coisa, podemos usar a opção `-n` (que vai prevenir a substituição). Você não verá nenhum erro ou mensagem, o comando `mv` será apenas ignorado e não será executado.
-   Como alternativa, a substituição pode ser evitada se usarmos a opção `-i` para avisar o usuário antes da substituição.

Supondo que já temos `new_file_name.txt` no diretório atual, vamos criar um novo arquivo de texto `my_text_file.txt` e renomeá-lo para `new_file_name.txt` usando o comando `mv` e a opção `-i`:

```
touch my_text_file.txt
mv -i my_text_file.txt new_file_name.txt
```

Como `new_file_name.txt` já existe, vamos ver a mensagem de aviso:

```
mv: overwrite 'new_file_name.txt'?
```

Para confirmar, temos que digitar `y` e pressionar Enter. Se não quisermos substituir o arquivo, digitamos `n`.

### O comando `cp`

Enquanto `mv` significa mover, `cp` significa copiar.

Assim como `mv`, `cp` também espera dois argumentos: a origem e o destino.

Vamos criar um arquivo `original_file.txt` e colocar sua cópia no mesmo diretório, mas com um nome diferente:

```
touch original_file.txt
cp original_file.txt copy_from_original.txt
```

No trecho de código acima, criamos `original_file.txt`, fizemos uma cópia dele e salvamos a cópia no mesmo diretório, mas com um novo nome: `copy_from_original.txt`.

É importante lembrar que, se o arquivo de destino já existe, por padrão, ele será substituído. Assim como para `mv`, precisamos usar a opção `-n` para prevenir `cp` de ser substituído se o nome do novo arquivo já existir.

A opção `-i` também pode ser usada para avisar caso haja possibilidade de substituir o arquivo.

`cp` pode funcionar com diretórios também, mas se um diretório que você quer copiar contém algum subdiretório ou arquivos nele, será necessário usar a opção `-r` para copiar não apenas o diretório em si, mas também seu conteúdo. Sem a opção `-r`, você vai ter um erro.

Vamos ver um exemplo.

Criamos um diretório e o chamamos de `my_files`:

```
mkdir my_files
```

Agora vamos entrar nesse diretório usando o comando `cd` e criar dois arquivos de textos nele: `file1.txt` e `file2.txt`:

```
cd my_files
touch file1.txt
touch file2.txt
```

Agora queremos voltar para o diretório superior:

```
cd ..
```

Lembre-se de que `..` é usado para indicar o diretório superior.

Supondo que temos `home_sub_directory` no diretório Home, podemos tentar copiar `my_files` sem a opção `-r`:

```
cp my_files ~/home_sub_directory
```

Parece que temos um erro:

```
cp: -r not specified; omitting directory 'my_files'
```

Entretanto, se adicionarmos a opção `-r`, o diretório `my_files` será copiado com seus conteúdos, o que significa que `file1.txt` e `file2.txt` também serão copiados:

```
cp -r my_files ~/home_sub_directory
```

Se listarmos o conteúdo de `~/home_sub_directory/my_files` usando o comando `ls`, veremos ambos os arquivos `file1.txt` e `file2.txt`:

```
ls ~/home_sub_directory/my_files
```

Aqui está o resultado:

```
file1.txt
file2.txt
```

Pergunta

Queremos criar um novo arquivo de texto vazio chamado `to_buy.txt`. Qual das opções listadas abaixo é inválida para esse propósito?

Esse arquivo pode ser criado por `touch to_buy.txt`

O melhor jeito de fazer isso é usando o comando `cat`. O comando será assim: `cat to_buy.txt`

Você entendeu certo! `cat` é usado para imprimir o conteúdo dos arquivos de texto existentes, mas não para a criação de novos arquivos.

`to_buy.txt` pode ser criado com a ajuda de `echo`. Assim: `echo > to_buy.txt`

Muito bem!

Pergunta

Se movermos um arquivo chamado `logs.txt` do diretório atual para o subdiretório `my_logs`, onde um arquivo com o mesmo nome já existe, o que nos ajudaria a prevenir que um arquivo existente seja substituído?

Escolha quantas quiser

Podemos prevenir `mv` de sobrescrever se adicionarmos a opção `-n`. O código vai ficar assim: `mv -n logs.txt my_logs`

A opção `-n` é exatamente o que precisamos para evitar a substituição.

Para evitar a substituição, precisamos adicionar a opção `-f`, que vai garantir que nenhuma mudança seja feita no arquivo existente. Para verificar se essa resposta está correta, execute `mv --help` e veja qual é a finalidade da opção `f`.

Podemos usar a opção `-i` para exibir um aviso e nos certificar de que seja solicitada uma confirmação se encontrarmos uma possível substituição.

`-i` deve nos avisar antes da substituição, então é isso que nos ajudará a evitar uma substituição.

Trabalho maravilhoso!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-07-098Z.md
### Última modificação: 2025-05-28 19:23:07

# Curingas - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Curingas

Às vezes, podemos querer aplicar comandos a vários arquivos cujos nomes seguem um padrão específico. Para fazer isso, precisamos usar curingas.

Assista ao vídeo abaixo e depois continue lendo para aprender mais.

Vídeo

<iframe class="base-markdown-iframe__iframe" id="player-hfk0jbkkpsQ" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Wildcards NM" width="640" height="360" src="https://www.youtube.com/embed/hfk0jbkkpsQ?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fc41eb24e-0b58-462b-9dd4-2de0bb25d81f%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Vamos considerar um cenário em que queremos listar todos os arquivos no diretório atual que comecem com `file_` e tenham a extensão `.txt` no final.

Podemos resolver esse problema em duas etapas:

1.  Primeiro, precisamos descobrir um padrão que vai identificar todos os nomes de arquivos do nosso interesse e excluir o resto.
2.  Segundo, precisamos aplicar o comando `ls` aos arquivos selecionados para listá-los.

Essa é uma situação em que curingas são úteis.

### Introdução aos curingas

Curingas são símbolos especiais usados para descrever padrões em uma string do nosso interesse.

### Principais caracteres curingas

Aqui estão alguns dos caracteres curingas mais usados nos sistemas Linux.

-   `*`
    -   asterisco
-   `?`
    -   ponto de interrogação
-   `[ ]`
    -   colchetes
-   `{ }`
    -   chaves

Vamos ver cada um deles individualmente e depois decidir qual devemos usar para acessar nosso padrão `file_` + `.txt`.

### Curinga `*`

`*` é usado para representar qualquer sequência de caracteres (letras, números e tudo mais), incluindo uma sequência vazia.

Então, se quisermos encontrar todas as strings que comecem com `a` e terminem com `e`, digitamos `a*e`. Esse padrão vai coletar casos como `ae`, `age` e `axe`, e também casos como `above`, `abominable` e `accumulative`.

Caracteres curingas são úteis quando usados em conjunto com outros comandos Linux.

Considere o comando `rm a*e.py`: ele vai remover todos os arquivos no diretório atual que começam com `a` e terminam com `e.py`. Arquivos como `agile.py` e `apple.py` serão deletados.

Antes de executar comandos como `rm`, é importante desenvolver o padrão correto usando um comando `ls` seguro. Um pré-teste de segurança vai prevenir que arquivos que gostaríamos de manter sejam acidentalmente removidos se um padrão que usamos em um curinga está incorreto. Também podemos verificar um padrão usando um serviço de terceiros como [regex101.com](https://regex101.com/).

Pergunta

Vamos ver se o curinga `*` pode nos ajudar a listar todos os arquivos no diretório atual que começam com `file_` e terminam com `.txt`. Selecione um comando que vai fazer o que queremos:

`ls file_*.txt` é exatamente o que precisamos

Oba! Parece que agora você conhece o poder do curinga `*`.

`file_*.txt`

`ls *file_*.txt`

Excelente!

### Curinga `?`

`?` é um curinga usado para representar exatamente um caractere, ao invés de um número de caracteres, como faz `*`.

Olhando para nosso exemplo anterior, se substituirmos `*` por `?`, então a string `a?e` vai coletar apenas os casos com um único caractere no meio. Essas são strings como `age` e `axe`.

Pergunta

Antes, nosso objetivo era listar todos os arquivos do diretório atual que começam com `file_` e terminam com `.txt`. Vamos ver como `ls`, junto com o curinga `?`, vai funcionar.

Selecione a afirmação correta entre as opções abaixo:

`ls file_?.txt` fará o mesmo trabalho que `ls file_*.txt`

`ls file_??.txt` vai funcionar se quisermos listar todos os arquivos, começando com `file_1.txt` e indo até `file_99.txt`.

`ls file_?.txt` vai listar todos os arquivos de texto que têm um único caractere no lugar de `?`. Então, arquivos como `file_1.txt` e `file_a.txt` serão incluídos, e `file_10.txt` e `file_aa.txt` serão excluídos.

Exatamente. Lembre-se de que `?` corresponde a apenas um caractere. `file_10.txt` tem `10`, e esses são dois caracteres.

Muito bem!

### Curinga `[ ]`

`[ ]` é usado quando queremos combinar caracteres específicos entre colchetes. Basicamente, especificamos um subconjunto de caracteres que temos interesse em ver no lugar do padrão `[ ]`.

Portanto, `a[abcdefg]e` vai coletar todas as strings que começam com `a`, terminam com `e` e têm no meio um dos caracteres listados entre `[` e `]`. Listamos os caracteres entre colchetes em ordem alfabética, mas isso não é necessário.

Por exemplo, `age` e `ace` vão satisfazer o padrão já que ambos `c` e `g` estão dentro de `[ ]`. `axe`, por outro lado, não está incluso, porque `x` não está em `[ ]`. Palavras como `adage` também não serão inclusas, apesar de todos os caracteres entre o primeiro e o último estarem entre os colchetes, já que `[ ]` corresponde apenas a um caractere por si só.

Em vez de listar todos os caracteres do nosso interesse, podemos usar um intervalo alfabético. O exemplo acima pode ser simplificado para `a[a-g]e` onde `[a-g]` corresponde ao intervalo de letras de `a` até `g`.

É importante notar que curingas diferenciam minúsculas de maiúsculas, então `[a-g]` não é o mesmo que `[A-G]`. Se você quiser coletar tanto letras minúsculas quanto minúsculas, pode usar um curinga `[a-gA-G]`, onde dois intervalos são especificados: de `a` até `g` e de `A` até `G`.

Pergunta

Vamos usar `[ ]` para resolver um problema um pouco diferente. Vamos precisar de um _Terminal_ real para esse quiz, então vá para a Sandbox e abra o _Terminal_ lá.

Como alternativa, você pode abrir um _Terminal_ localmente: no aplicativo para **usuários do MacOS**, usando o Prompt de Comando para **usuários do Windows** ou pressionando as teclas ctrl, alt e T juntas para **usuários do Linux**. Apenas lembre-se de que a execução de alguns comandos vai ser um pouco diferente dependendo do sistema operacional (SO) que você tem instalado no seu computador (Windows, MacOS, Linux).

Precisamos criar um novo diretório separado e alguns arquivos dummy. Veja como podemos fazer isso:

```
mkdir quiz_3_dir
cd quiz_3_dir
touch ere.txt eye.txt ewe.txt eve.txt
```

Primeiro, criamos um diretório chamado `quiz_3_dir`. Depois fomos até esse diretório recém-criado usando o comando `cd`. Por fim, criamos 4 arquivos dummy: `ere.txt`, `eye.txt`, `ewe.txt` e `eve.txt`.

Agora que o diretório e os arquivos foram criados, precisamos responder à pergunta: qual comando pode ser usado para listar apenas estes dois arquivos no diretório `quiz_3_dir`: `ere.txt` e `eye.txt`

`ls e[a-o]e.txt`

`ls e[ry]e.txt`

Exatamente! Só precisamos definir dois caracteres dentro dos colchetes: `r` e `y`. Isso significa que queremos ver `r` ou `y`.

`ls e[r][y]e.txt`

Fantástico!

### Curinga `{ }`

`{ }` é usado para várias correspondências.

Um bom exemplo de como usar esse curinga seria se quiséssemos remover todos os PDFs e arquivos de texto com a extensão `.txt` de um diretório. Com o curinga `{ }`, podemos fazer isso usando `rm {*.txt,*.pdf}`.

Observação: usamos dois curingas juntos, `*` e `{ }`. O asterisco `*` é necessário para selecionar todos os arquivos com uma extensão específica, enquanto `{ }` permite que especifiquemos várias extensões de arquivos.

Pergunta

Vamos usar `{ }` para resolver outro problema. Vamos precisar de um _Terminal_ real para esse quiz, então vá para a Sandbox e abra o _Terminal_ lá.

Novamente, precisamos criar um novo diretório separado e alguns arquivos dummy:

```
mkdir quiz_4_dir
cd quiz_4_dir
touch file1.txt file2.txt ere.txt eye.txt ewe.txt eve.txt
```

Começamos criando um diretório chamado `quiz_4_dir`. Então vamos até esse diretório recém-criado usando o comando `cd` e criamos 6 arquivos dummy: `file1.txt`, `file2.txt`, `ere.txt`, `eye.txt`, `ewe.txt` e `eve.txt`.

Queremos listar apenas estes 4 arquivos no diretório `quiz_4_dir`: `file1.txt`, `file2.txt`, `eve.txt` e `ewe.txt`. Qual comando é **inválido** para essa função?

`ls {file*.txt,e[vw]e.txt}`

`ls {file[0-2].txt,e[v-z]e.txt}`

Esse comando vai retornar `file1.txt`, `file2.txt`, `eve.txt` e `ewe.txt`, mas também `eye.txt`

`ls {file[0-2].txt,e[v-x]e.txt}`

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-08-441Z.md
### Última modificação: 2025-05-28 19:23:08

# Como visualizar e editar arquivos: parte 1 - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Como visualizar e editar arquivos: parte 1

Nesta lição, vamos apresentar novos comandos necessários para ver e editar arquivos. Em particular, você vai aprender sobre os comandos `cat`, `wc`, `head` e `tail`.

### `cat`

O `cat` é bastante versátil e usado com frequência no Linux. Apesar de o nome do comando `cat` significar concatenação (concatenation em inglês), `cat` também pode ser usado para visualizar o conteúdo e criar novos arquivos. Ele também pode copiar conteúdo de um arquivo para outro.

Vamos conferir alguns usos típicos do comando `cat`.

1.  `cat` pode ser usado para visualizar os conteúdos de um arquivo, e já usamos o comando `cat` para essa finalidade antes. Vamos ver como fazer isso com mais detalhes.

Podemos criar um simples arquivo de texto e usar o comando `cat` para exibir o conteúdo:

```
echo content to display > file2display.txt
cat file2display.txt
```

Criamos um arquivo de texto chamado `file2display.txt` e escrevemos uma única string nele. Com a ajuda do comando `cat`, exibimos o conteúdo do arquivo.

O resultado vai ser:

```
content to display
```

`cat` também pode exibir o conteúdo de vários arquivos:

```
echo other content to display > file2display_2.txt
```

Acabamos de criar outro arquivo de texto chamado `file2display_2.txt` e escrevemos outra string nele. Agora podemos usar `cat` para visualizar o conteúdo de ambos: `file2display.txt` e `file2display_2.txt`:

```
cat file2display.txt file2display_2.txt
```

Veja como fica o resultado do comando acima:

```
content to display
other content to display
```

2.  `cat` também pode ser usado para criar um arquivo ou para anexar o conteúdo de um arquivo ao final de outro arquivo:

Arquivos de texto podem ser criados com a ajuda do comando `cat`:

```
cat > my_text_file.txt
```

Após executarmos esse comando, entraremos no modo interativo. Vamos ver uma nova linha vazia no _Terminal_ onde podemos digitar o texto que gostaríamos de salvar em `my_text_file.txt`.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/data-eng/SDT/sdt-2-11-01.png?etag=1c0a6cc13e1f04899fcc66bb38f9de90)

Vamos começar a digitar:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/data-eng/SDT/sdt-2-11-02.png?etag=65c9b95da622bea517774d554cc30970)

Na captura de tela acima, você pode ver que digitamos `the first line of the text` (a primeira linha do texto). Ainda no modo interativo, podemos pressionar _Enter_ para ir para a próxima linha:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/data-eng/SDT/sdt-2-11-03.png?etag=f90084f6dd238e8fed7a8ad6bfe78af0)

Agora você pode ver que fomos para a próxima linha e digitamos `the second line of the text` (a segunda linha do texto).

Para terminar de escrever e salvar o texto no arquivo, só temos que pressionar _Enter_ (para confirmar o texto digitado) e então pressionar _Ctrl + D_. Depois disso, sairemos do modo interativo, e o arquivo será criado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/data-eng/SDT/sdt-2-11-04.png?etag=37e88ff0d2a9f617f5a7b9eb0699cc69)

Se `my_text_file.txt` já existe, ele será substituído. De maneira alternativa, podemos usar `>>` para prevenir o arquivo de ser substituído. `>>` vai apenas anexar o novo texto ao arquivo existente.

3.  `cat` pode copiar os conteúdos de um arquivo para outro:

Para explicar como isso funciona, vamos usar o comando `cat` duas vezes.

Primeiro, vamos copiar o conteúdo de `file2display.txt` criado anteriormente para o novo arquivo vazio chamado `another_text_file.txt`.

A seguir, vamos exibir o conteúdo resultante de `another_text_file.txt`:

```
cat file2display.txt > another_text_file.txt
cat another_text_file.txt
```

O conteúdo de `another_text_file.txt` está assim agora:

```

content to display
```

Como você pode ver, `another_text_file.txt` agora contém duas linhas: a primeira linha está vazia. Isso ocorre porque começamos com um arquivo dummy que contém apenas uma única linha vazia. A segunda linha é a mesma que em `file2display.txt`, o que significa que conseguimos fazer uma cópia usando o comando `cat`.

Há muitas outras maneiras de usar `cat`. Para aprender mais, você pode verificar a documentação via `cat --help`.

### `wc`

`wc` significa "word count": contagem de palavras. Como o nome sugere, podemos usar `wc` para contar o número de palavras em um arquivo. Tudo o que precisamos fazer é passar o caminho para um arquivo como um argumento.

Por exemplo, se executarmos:

```
echo lets practice the wc command > wc_file.txt
wc wc_file.txt
```

Vamos obter o seguinte resultado:

```
1  5 29 wc_file.txt
```

Lembre-se, esse arquivo contém uma única linha com o seguinte texto `lets practice the wc command` (vamos praticar com o comando wc).

Vamos dar uma olhada mais de perto no resultado de `wc wc_file.txt`:

-   `1` é o número de linhas no arquivo
-   `5` é o número de palavras
-   `29` é o número de bytes
-   `wc_file.txt` é o nome do arquivo

Da mesma forma que fizemos com `cat`, podemos passar mais de um arquivo para o comando `wc` para obter a contagem para cada arquivo, que devem ser separados por um espaço:

```
wc wc_file.txt file2display.txt
```

O resultado será assim:

```
1  5 29 wc_file.txt
1  3 20 file2display.txt
2  8 49 total
```

A última linha no resultado é para o `total`.

### `head` e `tail`

Tanto `head` quanto `tail` são usados para imprimir o conteúdo de arquivos.

Usamos o comando `head` para imprimir as primeiras linhas do arquivo, enquanto o comando `tail` imprime as últimas linhas. `head` e `tail` já devem ser familiares para você – são os métodos que usamos em Python ao trabalhar com DataFrames.

Por padrão, `head` e `tail` imprimem as primeiras e as últimas 10 linhas, respectivamente. Se quisermos imprimir qualquer outro número de linhas, só precisamos passar a opção `-n` e o número de linhas que queremos exibir.

Vamos dar uma olhada em um exemplo específico. Primeiro crie um arquivo de texto chamado `names.txt` contendo 7 linhas:

```
cat > names.txt
```

Executar o comando acima vai abrir o modo interativo. Nosso objetivo é escrever 7 nomes aleatórios. Aqui estão os nomes que digitamos:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/data-eng/SDT/sdt-2-11-05.png?etag=264c11e607a27a89729389fb712d4b69)

Agora vamos ilustrar como usar `head` para visualizar os 4 primeiros nomes:

```
head -n 4 names.txt
```

E aqui está o resultado:

```
anthony
alex
jeremy
zach
```

Podemos usar uma abordagem parecida para as últimas 4 linhas:

```
tail -n 4 names.txt
```

E aqui está o resultado:

```
zach
brian
robert
john
```

Pergunta

É hora de praticar os comandos que você acabou de aprender. Vá para o _Terminal_ na Sandbox ou abra um _Terminal_ localmente (usando o aplicativo de terminal para **usuários do MacOS**; usando o Prompt de Comando para **usuários do Windows** ou pressionando as teclas Ctrl, Alt e T juntas para **usuários do Linux**) e use `cat` para criar um arquivo de texto chamado `bike.txt` com o seguinte conteúdo:

```
I want to buy a new bike.
It costs around 500 dollars.
I definitely can afford it.
I will buy it next month.
Hopefully, I will get a discount.
```

Após criar o arquivo, conte o número de palavras nele usando o comando `wc` e selecione a resposta certa:

Conforme o resultado do comando `wc`, esse arquivo tem 27 palavras.

Conforme o resultado do comando `wc`, esse arquivo tem 29 palavras.

Exato!

Conforme o resultado do comando `wc`, esse arquivo tem 32 palavras.

Seu entendimento sobre o material é impressionante!

Pergunta

Agora queremos visualizar apenas as últimas 2 linhas desse arquivo. Qual comando pode nos ajudar com isso?

`tail bike.txt`

`head -n 2 bike.txt`

`tail -n 2 bike.txt`

Exatamente! Conseguimos selecionar as duas últimas linhas com a ajuda da opção `-n` e do comando `tail`.

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-11-069Z.md
### Última modificação: 2025-05-28 19:23:11

# Como visualizar e editar arquivos: parte 2 - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Como visualizar e editar arquivos: parte 2

Nesta lição, vamos falar a respeito do `vi` — o editor de texto padrão para sistemas operacionais UNIX. Podemos usar `vi` para ler e editar arquivos de texto já existentes ou criar e preencher um novo arquivo de texto do zero. `vi` significa "visual" e é um editor visual que permite editar qualquer texto diretamente na CLI.

Observação:

`vi` está indisponível no _Terminal_ na plataforma no momento. Se você usar o Terminal na plataforma, considere essa lição como opcional.

Se você trabalha no _Terminal_ disponível no seu computador, talvez note que `vi` não vem pré-instalado. Você pode instalá-lo seguindo [as instruções do site oficial](https://www.vim.org/download.php).

### `vi` para novos arquivos de texto

Para criar um novo arquivo de texto e abri-lo no `vi`, tudo o que precisamos fazer é executar isto:

```
vi my_new_text_file.txt
```

Se `my_new_text_file.txt` já existir, vamos ver o conteúdo do arquivo (supondo que não esteja vazio). No nosso caso, não existe esse arquivo, então tudo o que vemos são linhas vazias. Quando `vi my_new_text_file.txt` é executado, não estamos no ambiente shell, mas no ambiente `vi`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/data-eng/SDT/sdt-2-12-01.png?etag=6dae6de2d7f96463014336d27cbc6886)

Quando executamos `vi`, ele é aberto no _Modo de Comando_. Em geral, `vi` tem dois modos principais:

-   _Modo de Comando_
-   _Modo de Inserção_

_Modo de Comando_ é um modo em que todo caractere que digitarmos será tratado como parte de um comando. Em outras palavras, `vi` espera que apenas comandos sejam escritos nesse modo.

_Modo de Inserção_ é aquele que usamos para digitar o texto que queremos colocar no arquivo. Para entrar no _Modo de Inserção_ precisamos pressionar _i_ no teclado. É importante observar que nada muda na tela quando entramos no _Modo de Inserção_ a partir do _Modo de Comando_, mas podemos digitar textos quando estamos no _Modo de Inserção_.

Agora vamos pressionar _i_ e começar a trabalhar no nosso texto:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/data-eng/SDT/sdt-2-12-02.png?etag=9d3ddd8480b710935bdd76b0b629f918)

Escrevemos três linhas e agora queremos salvá-las. Para fazer isso, precisamos voltar para o _Modo de Comando_ pressionando o _Esc_ no teclado.

O _Modo de Comando_ do `vi` espera comandos como entradas. Dependendo do que queremos fazer, podemos usar um dos seguintes comandos:

-   `:wq` para salvar o texto no arquivo e sair.
-   `ZZ` para salvar alterações, se houve alguma, e sair. Funciona de modo parecido com `:wq`.
-   `:q` para sair. Esse comando vai funcionar apenas se o arquivo não foi modificado.
-   `:q!` para sair do arquivo sem salvar qualquer alteração.

Vamos salvar o texto e sair usando o comando `:wq`.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/data-eng/SDT/sdt-2-12-03.png?etag=20ec60ca1e5e928e34d9985aec62510c)

Na parte inferior da captura de tela acima, você pode ver o comando que digitamos. Para executá-lo, basta pressionar _Enter_. Quando o executarmos, vamos voltar para a janela normal do _Terminal_:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/data-eng/SDT/sdt-2-12-04.png?etag=8124b0dcea972b7f342408ad54cc181e)

Agora podemos usar `cat` para visualizar o conteúdo do novo arquivo:

```
cat my_new_text_file.txt
```

```
this is the text
that I want to save
into this file
```

### `vi` para arquivos existentes

Também podemos usar `vi` para editar um arquivo existente. Vamos abrir `my_new_text_file.txt` novamente e mostrar como `vi` funciona para edição de textos:

```
vi my_new_text_file.txt
```

O comando acima abre `my_new_text_file.txt` no _Modo de Comando:_

![image](https://practicum-content.s3.us-west-1.amazonaws.com/data-eng/SDT/sdt-2-12-05.png?etag=9d3ddd8480b710935bdd76b0b629f918)

Pressionando as setas no teclado, podemos mover o cursor para qualquer lugar do texto.

Os comandos mais comuns usados para editar textos no _Modo de Comando_ são os seguintes:

-   _dd_ para excluir uma linha selecionada;
-   _x_ para deletar um caractere selecionado;
-   _r_ para substituir um caractere selecionado.

Como alternativa, podemos pressionar _i_ para ativar o _Modo de Inserção_ e começar a editar o texto lá. Depois de editar o texto da maneira que queremos, saímos do _Modo de Inserção_ e salvamos o arquivo no _Modo de Comando_.

Pergunta

Abra um _Terminal_ localmente e execute `vi`.

Você precisa criar o mesmo arquivo de texto que pedimos para você criar na lição 11. O arquivo tinha o nome `bike.txt` e o seguinte conteúdo:

```
I want to buy a new bike.
It costs around 500 dollars.
I definitely can afford it.
I will buy it next month.
Hopefully, I will get a discount.
```

Quando você terminar, use `wc` para contar o número de linhas e palavras. Lembre-se de que `wc` também vai mostrar o tamanho do arquivo em bytes. Qual é o resultado de `wc`? Selecione a resposta correta:

O resultado é `4 28 141 bike.txt`

O resultado é `5 27 169 bike.txt`

Exatamente! O resultado é idêntico àquele que tivemos na lição anterior.

O resultado é `4 30 143 bike.txt`

Trabalho maravilhoso!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-12-432Z.md
### Última modificação: 2025-05-28 19:23:12

# Comandos de busca - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Comandos de busca

Nessa lição, você vai aprender sobre dois tipos de comandos de busca: `find` e `grep`.

### `find`

O comando `find` nos permite procurar arquivos e diretórios com base em critérios definidos. Isso faz com que essa seja uma boa alternativa à utilização do comando `ls` junto com caracteres curingas.

`find` espera vários argumentos e pode ter várias opções. Cada argumento é um caminho para um diretório, e esse deve ser o diretório em que queremos fazer a busca. Então, `find .` vai procurar arquivos no diretório atual, e `find ~` vai procurar arquivos no diretório _Home_. É importante também saber o que procurar.

O comando `find` nos permite definir o nome do arquivo que queremos encontrar. Para buscar um nome em particular, precisamos da opção `-name`.

Vamos conferir um exemplo em que criaremos um arquivo dummy e depois o encontraremos com `find`:

```
touch file2find.txt
find . -name file2find.txt
```

Temos o seguinte resultado:

```
./file2find.txt
```

Se quisermos encontrar nomes que tenham tanto letras minúsculas quanto minúsculas, devemos usar o sinalizador `-iname`, que não diferencia maiúsculas e minúsculas. Se `file2find.txt` não existir, nada será retornado.

Para ver como isso funciona, vamos criar outro arquivo chamado `File2find.txt` e tentar encontrar tanto `file2find.txt` quanto `File2find.txt` usando um único comando:

```
touch File2find.txt
find . -iname file2find.txt
```

Nosso resultado é:

```
./file2find.txt
./File2find.txt
```

Observe que, em alguns casos, o comando `find` mencionado acima pode produzir resultados diferentes para determinados usuários. Por exemplo, se você está usando um computador com Windows 10 com o sistema de arquivos NTFS. No entanto, você não precisa se preocupar com esses detalhes específicos agora. É importante entender a ideia geral de como isso funciona na maioria dos casos.

`find` nos permite especificar o tipo de objeto que queremos procurar com a opção `-type`. Se quisermos encontrar apenas diretórios, podemos usar `-type d` (`d` significa diretório).

Aqui está um exemplo:

```
mkdir to_find
touch To_find
find . -type d -iname to_find
```

Aqui está o resultado:

```
./to_find
```

Você também pode usar `-type f` para buscar apenas arquivos.

Curingas também podem ser usados com o comando `find`. Se um curinga for usado para um nome, o nome todo deve estar entre aspas:

```
find . -name "*o_find"
```

o que nos dará:

```
./to_find
./To_find
```

`find` também nos permite fazer buscas bem avançadas. Um bom exemplo seria um comando com o sinalizador `-mtime`, que pode ajudar a encontrar arquivos que modificamos nos últimos N dias, definindo N como o número que queremos. Então `-mtime -2` é para os arquivos ou diretórios que foram modificados nos últimos 2 dias.

```
find . -mtime -2
```

`-cmin`, por outro lado, ajuda a encontrar arquivos e diretórios que foram modificados nos últimos N minutos. Então, podemos usar `-cmin -120` para encontrar objetos que foram modificados nas últimas duas horas. Além de `-cmin`, existem outras opções. Para ler mais, clique [aqui](https://www.tutorialdba.com/2018/03/linux-ctimemtimeatimecminaminmmin.html) _(os materiais estão em inglês)_.

Por último, podemos usar `find` para realizar outra ação para todos os arquivos encontrados. Um dos usos mais comuns do comando é para excluir os resultados. Para fazer isso, só precisamos adicionar a opção `-delete`.

Então, se usarmos este comando:

```
 find . -name "*.txt" -delete 
```

Todos os arquivos `.txt` no diretório atual serão removidos.

Uma observação para os usuários do Windows:

O comando `find` no Windows tem uma funcionalidade diferente do que sua contraparte Linux. No Windows, ele é usado para procurar texto em arquivos ou pesquisar arquivos com base em determinados critérios. Entretanto, ele não suporta as opções `mtime` e `cmin` usadas para procurar arquivos por data de modificação.

### `grep`

`grep` nos deixa procurar padrões no conteúdo de um arquivo.

Para ver o que `grep` pode fazer, vamos criar um arquivo de texto:

```
cat > another_text_file.txt
```

```
Text files are great.
Here's some text.
What about the word "textile"?
```

Usando o comando `cat`, escrevemos as três linhas acima em `another_text_file.txt`. Agora podemos ilustrar como o comando `grep` é incrível.

Podemos exibir todas as linhas que contêm a palavra objetivo:

```
grep "text" another_text_file.txt
```

O comando acima procura a palavra `text` em `another_text_file.txt`, e esse é o resultado:

```
Here's some text.
What about the word "textile"?
```

Apesar de a primeira linha ter a palavra `Text`, ela não foi incluída porque, por padrão, `grep` diferencia maiúsculas e minúsculas. A segunda linha no resultado foi retornada por causa da palavra `text`. A terceira foi retornada porque tem a palavra `textile`.

Se quisermos procurar apenas combinações perfeitas para a palavra `text`, devemos adicionar a opção `-w`:

```
grep -w "text" another_text_file.txt
```

Com essa opção, apenas uma linha será retornada, porque é a única linha que contém exatamente `text`:

```
Here's some text.
```

Agora, se quisermos realizar uma pesquisa sem distinção de maiúsculas e minúsculas, precisamos adicionar a opção `-i`:

```
grep -iw "text" another_text_file.txt
```

O resultado contém 2 linhas:

```
Text files are great.
Here's some text.
```

Uma última observação: `grep` pode procurar todos os arquivos localizados no diretório atual. Então, se quisermos listar todas as linhas com os nomes de arquivos correspondentes no diretório atual que contenham a palavra `text` ou `Text`, executamos o seguinte comando:

```
grep -iw "text" *
```

`*` aqui significa todos os arquivos no diretório atual. Já que temos apenas um arquivo que contém uma das duas palavras-alvo, o resultado será assim:

```
another_text_file.txt:Text files are great.
another_text_file.txt:Here's some text.
```

Também podemos procurar um arquivo e contar quantas vezes uma determinada palavra aparece usando a opção `-c`:

```
grep -c "text" another_text_file.txt
```

Esse comando vai retornar:

```
2
```

Porque temos a palavra `textile` na terceira linha e a palavra `text` na segunda.

Também podemos usar `grep` para procurar um padrão nos arquivos e listar aqueles em que nenhuma correspondência for encontrada.

Para ilustrar esse caso, precisamos criar mais dois arquivos de texto. Vamos começar com o primeiro:

```
cat > text_file_1.txt
```

```
This file is needed to show
how the grep command works
```

E agora o segundo:

```
cat > text_file_2.txt
```

```
This file is also needed
to show how the grep command
can search for a particular text
```

Temos 3 arquivos de texto agora: o que foi criado antes, `another_text_file.txt` e dois novos arquivos: `text_file_1.txt` e `text_file_2.txt`.

Vamos usar `grep` para encontrar e listar apenas os arquivos que contenham a palavra `text`. Podemos fazer isso com a ajuda da opção `-l`.

```
grep -l "text" another_text_file.txt text_file_1.txt text_file_2.txt
```

Como esperado, o resultado é:

```
another_text_file.txt
text_file_2.txt
```

A palavra `text` não aparece em `text_file_1.txt`.

Pergunta

Seu objetivo é selecionar o comando abaixo que pode ser usado para encontrar todos os arquivos no diretório _Home_ que começam com `log_file_` e podem ter quaisquer caracteres e extensões.

Os arquivos que procuramos devem ser filtrados conforme a hora em que foram modificados. Queremos encontrar arquivos que foram modificados não mais do que uma semana atrás.

`find /Home/ -type f -mtime -7 -name "log_file_*"`

Você tem toda a razão! Procuramos arquivos no diretório Home modificados dentro dos últimos 7 dias e cujos nomes começam com `log_file_`.

`find . -mtime -7 -name "log_file_*"`

`find /Home/ -type f -mtime -7 -name log_file_*`

Você conseguiu!

Pergunta

Olhe para os 3 comandos abaixo e selecione um que retorne uma lista de arquivos no diretório atual que contenham a palavra `python` ou `Python`:

`grep -w "python" ./`

`grep -wi "python" *`

Esse comando não diferencia maiúsculas e minúsculas e procura tanto `python` quanto `Python`. `*` é necessário para procurar no diretório atual.

`grep -wi "Python" my_file.txt`

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-13-776Z.md
### Última modificação: 2025-05-28 19:23:14

# Alias - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Alias

Você deve ter começado a perceber que alguns comandos são usados com mais frequência do que outros. Se quisermos usar o mesmo comando novamente, podemos redigitá-lo ou copiar e colar, mas isso não é muito eficiente. Na ciência da computação, temos a tendência de automatizar a rotina e trabalho repetitivo para fazer com que sejam mais eficientes, e na CLI podemos fazer isso usando um alias.

Um alias é apenas uma string que digitamos na interface de linha de comando. Essa string é um atalho que faz referência a um comando. Enquanto um alias costuma ser curto, o comando que chamamos é normalmente longo ou repetitivo.

Para ver a lista dos aliases padrão, basta executar `alias` como um comando:

```
alias
```

E esta é a lista que obtemos:

```
alias alert='notify-send --urgency=low -i "$([ $? = 0 ] && echo terminal || echo error)" "$(history|tail -n1|sed -e '\''s/^\s*[0-9]\+\s*//;s/[;&|]\s*alert$//'\'')"'
alias egrep='egrep --color=auto'
alias fgrep='fgrep --color=auto'
alias grep='grep --color=auto'
alias l='ls -CF'
alias la='ls -A'
alias ll='ls -alF'
alias ls='ls --color=auto'
```

Aqui temos 8 aliases listados. Para entender como eles funcionam, vamos olhar para a linha `alias la='ls -A'`.

Nesta linha, vemos que o alias `la` é equivalente a uma execução do comando `ls` com a opção `-A`, o que significa que `la` produz o mesmo resultado que `ls -A`.

### Criar aliases temporários personalizados

Ter uma lista de aliases é ótimo, mas e se quisermos criar versões personalizadas? Isso é relativamente fácil. Tudo o que precisamos é executar um comando que salve um novo alias. A sintaxe fica assim:

```
alias name="value" 
```

onde:

-   `name` é o nome do alias que queremos definir.
-   `value` é o comando que queremos chamar. Como vemos a partir da sintaxe, o comando deve estar entre aspas.

Imagine que vamos trabalhar em um novo projeto e queremos criar um novo diretório para armazenar todos os arquivos relacionados a ele. Vamos chamar o diretório do projeto de `my_amazing_project` e criá-lo dentro do diretório. `/home/jovyan/work`. `/home/jovyan/work` é um diretório que já existe **na plataforma**.

```
mkdir /home/jovyan/work/my_amazing_project
```

Sabemos que vamos acessar esse diretório de projeto com muita frequência, então vamos querer ter um atalho em vez de digitar `cd /home/jovyan/work/my_amazing_project` toda vez. Portanto, precisamos de um alias para criar um atalho.

É importante dar nome aos aliases para manter o significado deles claro. Quanto ao nome do nosso alias, podemos usar `cdp`. Vamos usar `cdp` porque esse nome tem `cd` para o comando que queremos chamar e `p` para o projeto.

Para criar esse alias, precisamos executar um comando como este:

```
alias cdp="cd /home/jovyan/work/my_amazing_project"
```

Agora, se digitarmos e executarmos `cdp`, vamos para `/home/jovyan/work/my_amazing_project`.

Além disso, o nosso novo alias vai aparecer na lista de aliases. Se executarmos:

```
alias
```

Agora vamos ver

```
alias 
alias alert='notify-send --urgency=low -i "$([ $? = 0 ] && echo terminal || echo error)" "$(history|tail -n1|sed -e '\''s/^\s*[0-9]\+\s*//;s/[;&|]\s*alert$//'\'')"'
alias cdp="cd /home/jovyan/work/my_amazing_project" 
alias egrep='egrep --color=auto'
alias fgrep='fgrep --color=auto'
alias grep='grep --color=auto'
alias l='ls -CF'
alias la='ls -A'
alias ll='ls -alF'
alias ls='ls --color=auto'
```

Se quisermos remover um alias existente, podemos fazer isso com o comando `unalias`.

```
unalias cdp
```

Vamos olhar para outro exemplo da vida real.

É bem fácil substituir um arquivo usando o comando `mv`, já que se executarmos o comando `mv` sem a opção `-i`, ele vai substituir o arquivo que já existe (se ele tiver o mesmo nome). Para ter certeza de que não vamos substituir nenhum arquivo, podemos sempre executar o comando `mv` com a opção `-i` ou criar um alias.

Vamos chamar esse alias de `smv` (`mv` seguro) e defini-lo da seguinte forma:

```
alias smv="mv -i"
```

Agora, em vez de usar `mv`, vamos usar `smv` para realizar a mesma ação, mas com um lembrete que vai garantir que não vamos substituir acidentalmente nenhum arquivo.

Curiosamente, todos os novos aliases personalizados que criamos até agora são temporários, o que significa que eles só vão funcionar até o final da sessão atual do _Terminal_. Então, se fecharmos o _Terminal_ e o abrirmos de novo, vamos ter que criá-los de novo.

Precisamos seguir uma abordagem diferente para criar um alias permanente. Vale a pena mencionar que os aliases padrão que listamos com o comando `alias` estão sempre lá.

### Criar um alias personalizado permanente

Para manter um alias entre as sessões, precisamos adicioná-lo ao arquivo de configuração chamado `.bashrc`. Geralmente ele está no diretório _Home_, então o caminho completo para esse arquivo será `~./bashrc`.

`.bashrc` é um arquivo de texto que você pode abrir usando um editor de texto.

Observação:

`vi` está indisponível no _Terminal_ na plataforma no momento. Se você usar o Terminal na plataforma, considere essa parte do material como opcional.

Se você trabalha no _Terminal_ disponível no seu computador, talvez note que `vi` não vem pré-instalado. Você pode instalá-lo seguindo [as instruções do site oficial](https://www.vim.org/download.php).

Usando `vi` podemos abri-lo dessa forma:

```
vi ~./bashrc
```

Uma vez aberto, você vai ver que o conteúdo talvez seja assim:

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_NEW_4_sprint/Screenshot_from_2022-05-04_19-11-39.png)

Para criar um alias permanente, usamos a mesma sintaxe usada para criar os temporários. A única diferença é que fazemos tudo no arquivo de texto `.bashrc`.

Se quisermos criar um alias permanente, precisamos encontrar um lugar no arquivo `.bashrc` para ele. Ele pode ser armazenado perto de aliases já existentes ou em lugar diferente que escolhermos. Nossa recomendação é que você role para baixo até o fim do arquivo de texto `.bashrc` e comece a criar novos aliases ali.

Vamos criar os mesmos dois aliases de antes, mas permanentemente. Vamos precisar adicionar essas 3 novas linhas no final do arquivo `.bashrc`:

```
# meus aliases personalizados
alias cdp="cd /home/my_amazing_project"
alias smv="mv -i”
```

Você deve ter percebido que começamos com o comentário: `# meus aliases personalizados`. É uma boa prática comentar sobre o que as diferentes seções do arquivo `.bashrc` fazem.

A seguir, digitamos dois aliases, um por linha. Esses são os dois alias temporários que criamos antes.

Uma vez adicionados, precisamos salvar o arquivo atualizado `.bashrc` com os novos aliases. Para ter acesso a esses novos aliases, temos que fechar e reabrir o _Terminal_, tornando os novos aliases disponíveis.

Pergunta

Vá para o _Terminal_ na Sandbox e crie um novo alias temporário chamado `lsr` que vai listar o conteúdo em ordem alfabética inversa. Ele será um atalho para o comando `ls -r`.

Uma vez criado, use esse comando para listar o conteúdo do diretório _Home_. O que será exibido?

`jovyan`

Isso é precisamente o que obteremos se listarmos os conteúdos do diretório Home na ordem inversa.

`user-0-xxxxxxxxx`

Você conseguiu!

Pergunta

O que precisamos fazer para remover o alias que acabamos de criar?

Escolha quantas quiser

Basta usar `unalias lsr` para remover o alias.

Exatamente!

Podemos reiniciar o _Terminal_, e ele vai desaparecer.

Isso certamente funciona.

Basta usar o comando `clear` para limpar o histórico e os aliases criados.

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-15-822Z.md
### Última modificação: 2025-05-28 19:23:16

# Variáveis de ambiente - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Variáveis de ambiente

Em Python, você pode definir uma variável usando uma simples expressão como `name='brian'`, o que permite que nos refiramos ao valor de string `‘brian’` sempre que chamarmos a variável `name`.

Podemos fazer algo similar com variáveis na CLI. Cada variável na CLI começa com o sinal `$` e tem seu nome escrito em letras maiúsculas:

```
$HOME
```

Podemos ver o valor da variável com o comando `echo`:

```
echo $HOME
```

Isso gera o seguinte resultado quando o executamos no _Terminal_ aberto na Sandbox da TripleTen:

```
/home/jovyan
```

`$HOME` é o caminho absoluto para o diretório _Home_ e também é uma **variável ambiental**. Essas variáveis estão disponíveis em todo o sistema e são herdadas por todos os processos. Podemos executar o comando `env`:

```
env
```

para listar todas as variáveis ambientais:

```
SHELL=/bin/bash
JUPYTERHUB_ADMIN_ACCESS=1
MINIFORGE_VERSION=4.10.2-0
CONDA_EXE=/opt/conda/bin/conda
_CE_M=
HOSTNAME=1f95a5dc761d
LANGUAGE=en_US.UTF-8
JUPYTERHUB_API_TOKEN=0be01e8ff8fe43a488fc7b50cd8f4a87
JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
JUPYTERHUB_BASE_URL=/
NB_UID=1000
MEM_LIMIT=4194304000
PWD=/home/jovyan
CONDA_PREFIX=/opt/conda
JUPYTER_SERVER_URL=http://0.0.0.0:8888/user/user-0-1152896393/
MEM_GUARANTEE=209715200
JUPYTERHUB_SERVER_NAME=
LINES=41
HOME=/home/jovyan
LANG=en_US.UTF-8
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:
JPY_API_TOKEN=0be01e8ff8fe43a488fc7b50cd8f4a87
COLUMNS=127
NB_GID=100
CHOWN_HOME=yes
CONDA_PROMPT_MODIFIER=(base)
JUPYTERHUB_SERVICE_PREFIX=/user/user-0-1152896393/
JUPYTERHUB_OAUTH_CALLBACK_URL=/user/user-0-1152896393/oauth_callback
JUPYTER_SERVER_ROOT=/home/user-0-1152896393/work
TERM=xterm
_CE_CONDA=
CONDA_SHLVL=1
SHLVL=1
PYXTERM_DIMENSIONS=80x25
CONDA_DIR=/opt/conda
JUPYTERHUB_API_URL=http://jupyterhub:8081/hub/api
JUPYTERHUB_CLIENT_ID=jupyterhub-user-user-0-1152896393
JUPYTERHUB_HOST=
CONDA_PYTHON_EXE=/opt/conda/bin/python
MYSTEM3_PATH=/bins/mystem
CONDA_DEFAULT_ENV=base
CPU_GUARANTEE=0.1
CONDA_VERSION=4.10.2
CPU_LIMIT=1.0
NB_USER=user-0-1152896393
LC_ALL=en_US.UTF-8
PATH=/opt/conda/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin
JUPYTERHUB_USER=user-0-1152896393
JUPYTERHUB_ACTIVITY_URL=http://jupyterhub:8081/hub/api/users/user-0-1152896393/activity
DEBIAN_FRONTEND=noninteractive
_=/usr/bin/env
```

Como alternativa, podemos usar o comando `printenv` para obter os mesmos resultados. Também podemos usar `printenv` para verificar o valor de cada variável individualmente.

Então digitar:

```
printenv HOME
```

Também nos dará:

```
/home/jovyan
```

Olhando para o resultado de qualquer um dos comandos `env` e `printenv`, você verá que `$HOME` não é a única variável ambiental. Temos outras, como:

-   `$PATH` - uma lista de diretórios a ser pesquisada quando os comandos são executados. Quando você executa algum comando, o sistema procura nesses diretórios um arquivo executável para um determinado comando.
-   `$LANG` - determina a categoria local para a língua nativa.

Em contraste com variáveis ambientais, há outros tipos de variáveis que existem apenas na instância atual do _Terminal_. Nós as chamamos de **variáveis shell**.

A principal diferença entre as variáveis shell e as ambientais é que variáveis shell não podem ser acessadas em outros programas ou processos, incluindo o Jupyter Notebook. Já as variáveis ambientais podem ser acessadas pelo Jupyter Notebook.

### Como criar variáveis shell

Quando criamos variáveis shell, temos que as definir na CLI. Por exemplo, devemos digitar o seguinte para criar uma variável com o nome `MY_V` e o valor `my first variable` (minha primeira variável):

```
MY_V="my first variable"
```

Agora podemos verificar que a variável shell foi criada com sucesso:

```
echo $MY_V
```

E aqui está o resultado:

```
my first variable
```

Se verificarmos a existência dessa variável usando o comando `printenv`:

```
printenv MY_V
```

O resultado ficará vazio. Isso é devido ao fato que `$MY_V` não é uma variável ambiental, mas sim uma shell.

### Como criar variáveis ambientais

Para transformar `$MY_V` em uma variável ambiental, precisamos exportá-la usando o comando `export`:

```
export MY_V
```

Logo depois de ser exportada, `$MY_V` se torna uma variável ambiental. Podemos verificar isso usando o comando `printenv`:

```
printenv MY_V
```

Aqui está. E este é o resultado:

```
my first variable
```

Não precisamos digitar e executar 2 comandos (ou seja, criar uma variável shell primeiro) na CLI para criar uma variável ambiental; em vez disso, podemos fazer isso em uma única linha, da seguinte forma:

```
export MY_V="my first variable"
```

É importante lembrar que qualquer variável ambiental está disponível apenas na sessão atual do _Terminal_. Se cessarmos a sessão atual fechando o _Terminal_ e iniciando uma nova sessão, vamos perder todas as variáveis que criamos.

### Como criar variáveis ambientais permanentes

Para tornar uma variável ambiental permanente, precisamos:

1.  Abrir o _Terminal_.
2.  Executar `sudo -H gedit /etc/environment`.
3.  Digitar uma senha, se solicitado. A senha solicitada aqui é aquela que você usa para entrar em uma conta no seu computador.
4.  Editar o arquivo de texto que acabou de ser aberto. Por exemplo, se você quiser adicionar `MY_V="my first variable"`, escreva `MY_V="my first variable"` em uma nova linha.
5.  Salvar o arquivo de texto.

Não será possível fazer isso no _Terminal_ em nossa Sandbox, já que temos certas restrições, mas ao menos agora você sabe como fazer isso e pode tentar usando sua própria CLI.

### Como acessar variáveis ambientais a partir do Python

Para acessar variáveis ambientais a partir do Python, precisamos importar a biblioteca `os`:

```
import os
```

Quando importada, podemos extrair um valor de qualquer variável usando apenas `os.environ["name"]`, onde "name" é o nome da variável ambiental. Por exemplo, `os.environ["HOME"]` extrai o valor da variável `$HOME`, que é um caminho absoluto para o diretório _Home_.

Pergunta

Como criamos uma variável ambiental? Selecione todas as respostas corretas.

Escolha quantas quiser

Para criar uma variável ambiental, precisamos exportar uma variável shell existente usando `export`.

Correto! Se uma variável shell já está definida, só precisamos exportá-la com `export`.

Podemos usar uma expressão como `VAR="value"`.

Podemos executar um único comando que define a variável shell e a exporta.

Ótimo! Sim, podemos usar uma única linha para criar uma variável ambiental. Aqui está um exemplo da lição: `export MY_V="my first variable"`.

Você conseguiu!

Pergunta

O que é preciso para acessar uma variável ambiental personalizada a partir do _Python_?

Se definirmos uma variável ambiental no _Terminal_, podemos executar _Python_ (ou Jupyter Notebook) na mesma janela do _Terminal_ e ter acesso direto às variáveis ambientais em Python pelos nomes das variáveis.

Primeiro, definimos uma variável ambiental na CLI. Depois, executamos _Python_ no mesmo _Terminal_ e acessamos qualquer variável ambiental usando `os.environ`.

Isso é exatamente o que precisamos fazer! Lembre-se de que você pode transformar uma variável em uma constante e não precisa iniciá-la toda vez antes de executar _Python_.

Não existe uma maneira de acessar variáveis ambientais a partir do _Python_.

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-17-166Z.md
### Última modificação: 2025-05-28 19:23:17

# Pipes - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Pipes

Os pipes na CLI transferem o resultado de um comando para a entrada do outro. É mais ou menos como uma esteira transportadora move objetos em uma fábrica: ambas têm uma direção de fluxo e ambas conectam vários recursos (comandos) entre si.

Para mostrar como isso funciona, vamos olhar para o seguinte exemplo. Se executarmos:

```
ls
```

No _Terminal_ da plataforma na Sandbox teremos:

```
nltk_data  work
```

Imagine que queremos filtrar o resultado de `ls` e mostrar apenas o conteúdo que contenha a palavra `data`; como podemos passar o resultado de `ls` como entrada para `grep`? É aqui que usamos pipes.

```
ls | grep "data"
```

Listamos o conteúdo de um diretório usando `ls` e depois usamos um pipe (`|`) para passar o resultado de `ls` para o próximo comando (`grep`). Daqui, `grep` pega o resultado de `ls` como sua entrada e faz uma busca pela palavra `data`.

Isso nos dá:

```
nltk_data
```

Usar pipes nos permite combinar muitos comandos diferentes.

Por exemplo, podemos pegar o resultado do comando `ls` e apenas mostrar a última linha dele usando o comando `tail -1`:

```
ls | tail -1
```

Já que o resultado do comando `ls` tem apenas 2 linhas: `nltk_data` e `work`, o resultado do comando acima será:

```
work
```

Pergunta

Selecione a solução com o resultado correto para o seguinte comando:

```
cat super_text_file.txt | grep "python" | head -3
```

Ele pega o conteúdo de `super_text_file.txt`, seleciona todas as linhas com a palavra `python` e exibe apenas as últimas 3 linhas.

Ele olha o arquivo `super_text_file.txt`, pega todas as linhas que contêm `Python` ou `python` e imprime as primeiras 3 linhas do conteúdo filtrado do arquivo.

Esse comando abre `super_text_file.txt`, seleciona todas as linhas que contêm a palavra `python` e exibe as primeiras 3 linhas.

É exatamente para isso que serve esse comando!

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-18-455Z.md
### Última modificação: 2025-05-28 19:23:18

# Histórico - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Histórico

O comando `history` (histórico) nos permite verificar os comandos anteriores que executamos no Terminal.

Sem argumentos, `history` imprime os comandos que escrevemos antes.

Então, se abrirmos uma janela do Terminal e executarmos `history`:

```
 history
```

Vamos ver apenas uma única linha que mostra que executamos o comando `history`:

```
1  history
```

Se continuarmos a trabalhar na mesma sessão do Terminal, executarmos alguns outros comandos (como `ls` e `pwd`) e chamarmos history de novo, vamos ter o histórico completo de todos os comandos executados durante a sessão atual:

```
1  history
2  ls
3  pwd
4  history
```

Na primeira coluna, podemos ver números (de `1` até `4` nesse caso). Esses são índices que indicam a ordem dos comandos executados.

Como alternativa, podemos especificar exatamente o número de últimos comandos que queremos listar. Por exemplo, se executarmos:

```
history 3
```

vamos obter os últimos três comandos:

```
3  pwd
4  history
5  history 3
```

Às vezes, talvez seja do nosso interesse salvar o histórico em um arquivo de texto. Para fazer isso, basta executar:

```
history > history.txt
```

Isso salva tudo no arquivo `history.txt` localizado no diretório atual.

Se o histórico for muito longo, é uma boa ideia ser capaz de encontrar comandos específicos. Ao combinar `history` e `grep`, podemos obter exatamente o que queremos. Por exemplo, executar:

```
history | grep pwd
```

Nos dará:

```
3  pwd
7  history | grep pwd
```

Vamos dar uma olhada mais de perto no comando que acabamos de executar.

Começamos com o comando básico `history` seguido por um caractere pipe (`|`). Para refrescar a memória, o pipe nos permite pegar o resultado de um comando e enviá-lo como entrada para outro. Nesse exemplo, pegamos um resultado do comando `history` e mandamos como entrada para o comando `grep pwd`. Como sabemos, `grep` é usado para procurar, e `pwd` é seu argumento. Então, o comando acima pega o histórico completo e o envia para o comando `grep` para procurar `pwd`. Como resultado, recebemos todos os comandos de `history` que contêm `pwd`.

Outro exemplo seria se passarmos o resultado do comando `history` para o comando `head`. Como lembrete, `head` é usado para imprimir as primeiras 10 linhas de um arquivo. Então, o comando:

```
history | head -2
```

Vai imprimir os primeiros 2 comandos:

```
1  history
2  ls
```

Pergunta

Qual comando devemos executar para extrair apenas os comandos que contêm `python` dos últimos 10 comandos do histórico? Selecione todas as opções que são válidas.

Escolha quantas quiser

`history 10 | grep python`

Totalmente correto! Extraímos os últimos 10 comandos e os passamos para `grep` para extrair aqueles que contêm `python`.

`history | tail 10 | grep python`

Isso mesmo! Extraímos o histórico, o filtramos para exibir os últimos 10 comandos e procuramos pela palavra-objetivo.

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-20-541Z.md
### Última modificação: 2025-05-28 19:23:21

# Conclusão - TripleTen

Capítulo 2/8

Introdução à linha de comando

# Conclusão

Neste capítulo, você aprendeu bastante sobre a interface da linha de comando e seus comandos. Agora você já conhece bem o _Terminal_ e pode facilmente:

-   Navegar pelo sistema de arquivos com a ajuda de `pwd` e `cd`, listar conteúdo usando o comando `ls` e trabalhar com arquivos e diretórios usando comandos como `mv`, `cp`, `mkdir` e muitos outros.
-   Criar e visualizar arquivos de texto com `cat` e modificá-los no editor `vi`. Você pode olhar para o início do arquivo com `head` ou para o final com `tail`. Como alternativa, você pode obter informações do arquivo usando `wc`.
-   Por último, mas não menos importante, agora você sabe sobre aliases e variáveis de ambiente que fazem você trabalhar na CLI de maneira muito mais eficiente.

![](https://practicum-content.s3.amazonaws.com/resources/503c4480-0cab-11eb-94dc-b3bf98b1ad1b_1695039662.png)

Estar confortável com a CLI é uma habilidade importante para analistas e cientistas de dados. No próximo capítulo, vamos falar sobre ambientes de programação para expandir ainda mais suas habilidades como desenvolvedor de software.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-23-21-835Z.md
### Última modificação: 2025-05-28 19:23:22

# Home

Análise de Dados

83%

![](https://practicum-content.s3.amazonaws.com/resources/Final_Project_2_1700743144.svg)

### Sprint 14

## Projeto Final

### Capítulo 4

## Entregando Projetos

Projeto Final. Decomposição

[Avançar](/trainer/data-analyst/lesson/feaa4b83-c847-44c3-9187-c1a4ec45e577/)

## Conteúdo

1.  ![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/what_to_expect_1679489901_1681843066.svg)
    
    Módulo Adicional
    
    ### Curso demonstrativo
    
    5 capítulos
    
2.  ![](https://practicum-content.s3.amazonaws.com/resources/what_to_expect_1_1700743172.svg)
    
    Onboarding
    
    ### É bom ver você na TripleTen!
    
    14 aulas
    
3.  ![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1._Advanced_Spreadsheets_1676362144_1681842191.svg)
    
    Módulo Adicional
    
    ### Excel: Trabalhando com Dados
    
    3 capítulos restantes
    
4.  ![](https://practicum-content.s3.amazonaws.com/resources/5._Python_Fundamentals_1700742947.svg)
    
    Sprint 1
    
    ### Python básico
    
    9 capítulos
    
5.  ![](https://practicum-content.s3.amazonaws.com/resources/5._Python_Fundamentals_1700743842.svg)
    
    Sprint 2
    
    ### Continuação de Python básico
    
    7 capítulos
    
6.  ![](https://practicum-content.s3.amazonaws.com/resources/data_prep_1700742967.svg)
    
    Sprint 3
    
    ### Manipulação de dados
    
    11 capítulos
    
7.  ![](https://practicum-content.s3.amazonaws.com/resources/1._Advanced_Spreadsheets_1700742977.svg)
    
    Sprint 4
    
    ### Análise Estatística de Dados
    
    6 capítulos
    
8.  ![](https://practicum-content.s3.amazonaws.com/resources/6._Software_Development_Tools_1700742989.svg)
    
    Sprint 5
    
    ### Ferramentas de Desenvolvimento de Software
    
    8 capítulos
    
9.  ![](https://practicum-content.s3.amazonaws.com/resources/Final_Project_2_1700742997.svg)
    
    Sprint 6
    
    ### Projeto Integrado 1
    
    5 capítulos
    
10.  ![](https://practicum-content.s3.amazonaws.com/resources/sandbox_1679490946_1681843095_1682758313_1685704627_1687875109.svg)
    
    Módulo Adicional
    
    ### Sandbox
    
    2 capítulos restantes
    
11.  ![](https://practicum-content.s3.amazonaws.com/resources/2._SQL_Databases_1700743030.svg)
    
    Sprint 7
    
    ### Coleta e Armazenamento de Dados (SQL)
    
    8 capítulos
    
12.  ![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/10._Integrated_Project_2_1681844030.svg)
    
    Módulo Adicional
    
    ### Introdução ao HTML, CSS
    
    3 capítulos
    
13.  ![](https://practicum-content.s3.amazonaws.com/resources/9._Business_Analytics_1_1700743059.svg)
    
    Sprint 8
    
    ### Análise de Negócio
    
    9 capítulos
    
14.  ![](https://practicum-content.s3.amazonaws.com/resources/B_Testing_1700743072.svg)
    
    Sprint 9
    
    ### Tomando Decisões de Negócios Baseadas em Dados
    
    9 capítulos
    
15.  ![](https://practicum-content.s3.amazonaws.com/resources/4._Storytelling_with_Data_1_1700743084.svg)
    
    Sprint 10
    
    ### Como Relatar uma História Usando Dados
    
    6 capítulos
    
16.  ![](https://practicum-content.s3.amazonaws.com/resources/Final_Project_2_1700743102.svg)
    
    Sprint 11
    
    ### Projeto Integrado 2
    
    2 capítulos
    
17.  ![](https://practicum-content.s3.amazonaws.com/resources/11.__Intro_to_Machine_Learning_1700743112.svg)
    
    Sprint 12
    
    ### Automação
    
    6 capítulos
    
18.  ![](https://practicum-content.s3.amazonaws.com/resources/11.__Intro_to_Machine_Learning_1700743126.svg)
    
    Módulo Adicional
    
    ### Suplementos de Automação
    
    5 capítulos restantes
    
19.  ![](https://practicum-content.s3.amazonaws.com/resources/8.Data_Visualization_with_Python_1700743135.svg)
    
    Sprint 13
    
    ### Previsões e Predições
    
    6 capítulos
    
20.  ![](https://practicum-content.s3.amazonaws.com/resources/Final_Project_2_1700743144.svg)
    
    Sprint 14
    
    ### Projeto Final
    
    2 capítulos restantes
    

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-34-543Z.md
### Última modificação: 2025-05-28 19:26:35

# Introdução - TripleTen

Capítulo 3/8

Ambiente de programação

# Introdução

![](https://practicum-content.s3.amazonaws.com/resources/Artboard_1_copy_32x-100_1695044915.jpg)

Olá, este é o capítulo _Ambiente de programação_ do sprint _Ferramentas de desenvolvimento de software para profissionais de dados_! Neste capítulo, você vai continuar ampliando seu repertório de habilidades de desenvolvimento de software aprendendo como configurar um ambiente de programação no qual se sinta confortável para trabalhar.

Também vamos guiar você pelo processo de configuração do seu próprio ambiente de programação local, instalação do Python e pacotes de ciência de dados associados usando o [Anaconda](https://www.anaconda.com/products/individual), busca de diferentes maneiras de escrever e executar o código Python, uso do Jupyter Notebook e aprendizado de verificação e instalação de versões de pacotes.

É importante notar que, durante as aulas, você vai poder usar nossa plataforma para escrever e executar facilmente o código Python direto do navegador. No entanto, se você quiser trabalhar com Python localmente, será necessário instalar um interpretador Python e outro software para poder escrever e executar o código.

Isso é altamente recomendável, porque assim você vai se sentir mais confortável trabalhando com Python em diferentes ambientes, o que pode ser útil se você mudar de emprego ou trabalhar para organizações diferentes.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-35-838Z.md
### Última modificação: 2025-05-28 19:26:36

# Ambientes de programação - TripleTen

Capítulo 3/8

Ambiente de programação

# Ambientes de programação

Antes de começarmos a configurar um ambiente de programação para executar Python na sua máquina local, é importante saber o que é um ambiente de programação e por que precisamos usá-lo.

Basicamente, trata-se das ferramentas e aplicativos que você usa para escrever, organizar e executar o código.

Você precisa de pelo menos dois softwares na sua máquina para poder escrever e executar arquivos Python: um editor de texto e um interpretador Python. Nesta lição, vamos nos concentrar no componente de editor de texto.

### Escrever código em um editor de texto

Para escrever o código Python, você pode usar qualquer editor de texto. No entanto, alguns são mais adequados do que outros para escrever códigos com eficiência. Tente evitar processadores de texto como o Microsoft Word ou o Google Docs, já que esses programas fazem muita formatação e estilização de texto embaixo dos panos que podem atrapalhar a sintaxe adequada do Python e causar erros quando você executar o código.

Os editores de texto mais populares para escrever códigos incluem: [Atom](https://atom.io/), [Sublime Text](https://www.sublimetext.com/) e [Visual Studio Code](https://code.visualstudio.com/) (também conhecido como VS Code). Gostamos particularmente do VS Code, porque ele oferece uma boa funcionalidade sem complexidades desnecessárias. Discutiremos mais disso no capítulo Ambiente de programação integrado no final deste sprint.

Na imagem abaixo, você pode ver uma comparação entre um script Python escrito em um editor de texto comum (à esquerda) e um editor de texto para programação (à direita). O editor de texto para programação colore o script Python, permitindo identificar possíveis erros com muito mais facilidade.

![](https://practicum-content.s3.amazonaws.com/resources/text_editor_compare_1695044973.png)

Pergunta

Quais são as vantagens de escrever um script Python em um editor de texto para programação em comparação a um editor comum?

Escolha quantas quiser

Um editor de texto para programação colore o código, tornando todo o trabalho mais conveniente.

Isso é verdade.

Um editor de texto para programação permite encontrar bugs e erros com maior facilidade.

Isso mesmo.

Um editor de texto para programação permite escrever o código mais rapidamente.

Trabalho maravilhoso!

### Salvar scripts Python como arquivos .py

Por ora, abra qualquer editor de texto que quiser. Se estiver em dúvida sobre qual usar, escolha um editor básico de texto no seu computador, como Notepad no Windows ou TextEdit no Mac.

Agora vamos escrever um script Python simples que encontra o número de núcleos de CPU em seu computador e imprime o resultado. Para fazer isso, escreva as seguintes linhas de código Python em seu editor de texto:

```
import os # importando a biblioteca os

cpu_num = os.cpu_count() # obter o número de CPUs no sistema
print(f"Meu computador tem {cpu_num} núcleos de CPU") # imprimir o número de CPUs
```

O Python tem um módulo `os` integrado com métodos para interagir com o sistema operacional. Podemos importá-lo para obter o número de núcleos de CPU.

Se você não sabe o que é um núcleo de CPU, não se preocupe. Resumindo, quanto mais núcleos sua CPU (ou o processador de seu computador) tiver, mais rápido e eficiente será o computador.

Agora, após escrever as linhas no editor de texto, salve o arquivo e insira um nome. Escolha `cpus.py`. Observe que o nome do script termina com a extensão `.py`, o que significa que esse é um script Python.

A extensão `.py` é importante porque permite que o interpretador Python saiba que esse arquivo de texto contém código Python. Ela também permite que você, outras pessoas e editores de texto para programação saibam que esse arquivo de texto contém código Python. No entanto, o texto salvo em um arquivo com a extensão `.py` é idêntico ao mesmo texto salvo com a extensão `.txt`; ambos são apenas arquivos de texto que contêm as mesmas informações.

Lembre-se de onde você salvou seu arquivo. Você vai precisar dele mais tarde.

Ótimo! Agora você tem um autêntico script Python. Mas sem a outra parte do nosso ambiente de programação, ainda não podemos fazer nada com ele. Para ver o que nosso script faz, precisamos instalar um software que possa lê-lo, interpretá-lo e executá-lo.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-37-195Z.md
### Última modificação: 2025-05-28 19:26:37

# Instalação do Python por meio da distribuição Anaconda (Windows) - TripleTen

Capítulo 3/8

Ambiente de programação

# Instalação do Python por meio da distribuição Anaconda (Windows)

Na última lição, você usou um editor de texto para escrever um script Python, agora queremos executar esse script. Para fazer isso, precisamos do próximo componente do nosso ambiente de programação, que é um interpretador Python capaz de entender e executar o código Python.

Nesta lição, você vai aprender como instalar o Python no seu computador. O Python é uma linguagem de programação, mas não instalamos _linguagens_. Em vez disso, instalamos um compilador ou interpretador que converte instruções de texto legíveis por humanos em código binário que os computadores compreendem.

Existem várias maneiras de baixar e instalar o Python. Normalmente, esses aplicativos incluem um interpretador Python com vários pacotes Python. Um exemplo de um pacote já familiar para você é a _Pandas_. Quando escrevemos a linha `import os` em nosso script da última lição, estávamos dizendo ao Python para usar o pacote `os`. Os pacotes são uma parte importante do seu ambiente de programação, e vamos aprender mais sobre eles depois.

### A distribuição Anaconda Python

![](https://practicum-content.s3.amazonaws.com/resources/Artboard_1_copy2x-100_1695045052.jpg)

Se você ainda não tem Python instalado em seu computador, existem várias opções de como baixá-lo e instalá-lo. A maneira mais direta de fazer isso é no [site do Python](https://www.python.org/downloads/). Se você instalar Python assim, vai poder executar o código Python por meio da linha de comando do seu sistema operacional ao digitar `python`.

No entanto, sugerimos usar uma distribuição Python mais abrangente. Uma distribuição instala um interpretador com um conjunto de pacotes e outros programas que você pode usar para desenvolver projetos com eficiência e gerenciar pacotes Python.

A distribuição [Anaconda](https://www.anaconda.com/products/individual) foi desenvolvida especificamente para ciência e análise de dados. A instalação do Anaconda cria essencialmente um ambiente de programação pronto para uso.

Sinta-se à vontade para usar qualquer aplicativo ou distribuição do Python com o qual se sinta mais confortável. Apenas tenha em mente que vamos usar com frequência os pacotes incluídos na instalação do Anaconda, bem como outros aplicativos incluídos, como Jupyter Notebook e JupyterLab. Mesmo se você já tiver Python instalado, ainda poderá instalar o Anaconda e usar os aplicativos associados como parte do seu ambiente de programação.

A próxima seção vai mostrar como instalar o Anaconda no Windows. Para opções de instalação mais detalhadas, incluindo Linux, veja a documentação do Anaconda [aqui](https://docs.anaconda.com/anaconda/install/) _(os materiais estão em inglês)_.

### Instalação do Anaconda no Windows

Para instalar o Anaconda Individual Edition na sua máquina Windows:

-   Baixe o instalador do Anaconda [aqui](https://www.anaconda.com/products/individual#windows). Por exemplo, em meados de 2023, a página de download era assim:

![](https://practicum-content.s3.amazonaws.com/resources/Screenshot_from_2023-07-25_15-42-12_1695045139.png)

-   Após iniciar o download, você talvez veja uma janela pop-up solicitando que você se registre no Anaconda. Você pode pular isso se quiser, pois não precisa de uma conta Anaconda para usar o programa.
-   Abra o instalador baixado. O nome do arquivo deve se parecer com `Anaconda3-2023.07-1-Windows-x86_64.exe`. A versão específica pode variar dependendo da data de download e do seu sistema operacional.
-   Siga as instruções de instalação. Será necessário:
    
    1.  Concordar com os Termos de Serviço
    2.  Selecionar o tipo de instalação (recomendamos "Just Me")
    3.  Criar uma pasta onde você deseja instalar o Anaconda (qualquer lugar que você preferir)
    4.  Selecionar opções de instalação avançadas, se desejar. Sugerimos a instalação padrão, o que significa não selecionar nenhuma dessas opções avançadas

Capturas de tela de instalação do Windows 10

**Etapa 1:**

![](https://practicum-content.s3.amazonaws.com/resources/anaconda_installer_01_1695045273.png)

**Etapa 2:**

![](https://practicum-content.s3.amazonaws.com/resources/anaconda_installer_02_tos_1695045298.png)

**Etapa 3:**

![](https://practicum-content.s3.amazonaws.com/resources/anaconda_installer_03_type_1695045316.png)

**Etapa 4:**

![](https://practicum-content.s3.amazonaws.com/resources/anaconda_installer_04_location_1695045334.png)

**Etapa 5:**

![](https://practicum-content.s3.amazonaws.com/resources/anaconda_installer_05_options_1695045362.png)

-   O instalador deve informar quando a instalação estiver concluída. Ele vai ficar como na imagem abaixo.

![](https://practicum-content.s3.amazonaws.com/resources/anaconda_installer_06_complete_1696235577.png)

-   Após a instalação, você talvez veja outra janela perguntando se deseja instalar o PyCharm Pro. Você não precisa do PyCharm agora e vai poder instalá-lo mais tarde, se desejar, por isso recomendamos pular essa instalação.

Parabéns, você instalou o Anaconda em sua máquina Windows! Sinta-se à vontade para pular a seção de instalação para o Mac se não precisa dessa informação.

### Anaconda Navigator

Agora que você instalou o Anaconda, tem tudo o que precisa em seu ambiente de programação para escrever e executar código Python. Como parte da instalação, você deve ter um programa chamado Anaconda Navigator. Encontre o programa e o abra. Ele deve ser semelhante à imagem abaixo (Windows 10).

![](https://practicum-content.s3.amazonaws.com/resources/anaconda_navigator_homepage_1695045409.png)

Como você pode ver, o Anaconda vem com uma variedade de aplicativos para ajudar no desenvolvimento de projetos de ciência e análise de dados em Python.

Os aplicativos que já estão instalados têm um botão "Launch". Se desejar instalar aplicativos adicionais, clique no botão "Install" correspondente e siga as instruções. Sinta-se à vontade para explorar qualquer um dos aplicativos, mas nossas prioridades serão Jupyter Notebook, JupyterLab e Powershell Prompt.

Outras características importantes do Anaconda Navigator:

-   A aba "Connect" (Conectar), onde você pode entrar em sua conta do Anaconda, se tiver uma
-   A aba "Learning" (Aprendizagem), onde você tem amplo acesso à documentação e vídeos educacionais
-   A aba "Environments" (Ambientes), onde você pode configurar diferentes ambientes de projeto (o ambiente base é o suficiente para nossas necessidades)
-   O ícone de engrenagem de cada aplicativo, onde você pode instalar, remover ou atualizar aplicativos

Para detalhes sobre cada um deles e muito mais, consulte a documentação completa sobre o Anaconda Navigator [aqui](https://docs.anaconda.com/free/navigator/index.html).

Agora que configuramos um ambiente de programação completo, vamos finalmente executar o script Python que escrevemos na última lição!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-39-167Z.md
### Última modificação: 2025-05-28 19:26:39

# Instalação do Python por meio da distribuição Anaconda (Mac) - TripleTen

Capítulo 3/8

Ambiente de programação

# Instalação do Python por meio da distribuição Anaconda (Mac)

Na última lição, você usou um editor de texto para escrever um script Python, agora queremos executar esse script. Para fazer isso, precisamos do próximo componente do nosso ambiente de programação, que é um interpretador Python capaz de entender e executar o código Python.

Nesta lição, você vai aprender como instalar o Python no seu computador. O Python é uma linguagem de programação, mas não instalamos _linguagens_. Em vez disso, instalamos um compilador ou interpretador que converte instruções de texto legíveis por humanos em código binário que os computadores compreendem.

Existem várias maneiras de baixar e instalar o Python. Normalmente, esses aplicativos incluem um interpretador Python com vários pacotes Python. Os pacotes são basicamente um código Python adicional que você pode optar por usar para expandir os recursos do seu próprio código. Quando escrevemos a linha `import os` em nosso script da última lição, estávamos dizendo ao Python para usar o pacote `os`. Os pacotes são uma parte importante do seu ambiente de programação, e vamos aprender mais sobre eles depois.

### A distribuição Anaconda Python

![](https://practicum-content.s3.amazonaws.com/resources/Artboard_1_copy2x-100_1_1695045468.jpg)

Se você ainda não tem Python instalado em seu computador, existem várias opções de como baixá-lo e instalá-lo. A maneira mais direta de fazer isso é no [site do Python](https://www.python.org/downloads/). Se você instalar Python assim, vai poder executar o código Python por meio da linha de comando do seu sistema operacional ao digitar `python`.

No entanto, sugerimos usar uma distribuição Python mais abrangente. Uma distribuição instala um interpretador com um conjunto de pacotes e outros programas que você pode usar para desenvolver projetos com eficiência e gerenciar pacotes Python.

A distribuição [Anaconda](https://www.anaconda.com/products/individual) foi desenvolvida especificamente para ciência e análise de dados. A instalação do Anaconda cria essencialmente um ambiente de programação pronto para uso.

Sinta-se à vontade para usar qualquer aplicativo ou distribuição do Python com o qual se sinta mais confortável. Apenas tenha em mente que vamos usar com frequência os pacotes incluídos na instalação do Anaconda, bem como outros aplicativos incluídos, como Jupyter Notebook e JupyterLab. Mesmo se você já tiver Python instalado, ainda poderá instalar o Anaconda e usar os aplicativos associados como parte do seu ambiente de programação.

A próxima seção mostra como instalar o Anaconda para o Mac. Consulte a seção apropriada para você. Para opções de instalação mais detalhadas, incluindo Linux, veja a documentação do Anaconda [aqui](https://docs.anaconda.com/anaconda/install/) _(os materiais estão em inglês)_.

Capturas de tela do Mac

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_1695049547.png)

**Etapa 1:**

![](https://practicum-content.s3.amazonaws.com/resources/Screen_Shot_2021-11-15_at_21.52.25_1695049565.png)

**Etapa 2:**

![](https://practicum-content.s3.amazonaws.com/resources/Screen_Shot_2021-11-15_at_21.52.38_1695049586.png)

**Etapa 3:**

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_1_1695049601.png)

![](https://practicum-content.s3.amazonaws.com/resources/Screen_Shot_2021-11-15_at_21.52.45_1695049617.png)

**Etapa 4:**

![](https://practicum-content.s3.amazonaws.com/resources/Screen_Shot_2021-11-15_at_21.52.51_1695049638.png)

**Etapa 5:**

![](https://practicum-content.s3.amazonaws.com/resources/Screen_Shot_2021-11-15_at_21.52.53_1695049656.png)

**Etapa 6:**

![](https://practicum-content.s3.amazonaws.com/resources/Screen_Shot_2021-11-15_at_21.52.58_1695049674.png)

**Etapa 7:**

![](https://practicum-content.s3.amazonaws.com/resources/Screen_Shot_2021-11-15_at_22.00.11_1695049743.png)

![](https://practicum-content.s3.amazonaws.com/resources/Screen_Shot_2021-11-15_at_22.00.16_1695049757.png)

![](https://practicum-content.s3.amazonaws.com/resources/Screen_Shot_2021-11-15_at_22.00.21_1695049772.png)

### Anaconda Navigator

Agora que você instalou o Anaconda, tem tudo o que precisa em seu ambiente de programação para escrever e executar código Python. Como parte da instalação, você deve ter um programa chamado Anaconda Navigator. Encontre o programa e o abra. Ele deve ser semelhante à imagem abaixo.

![](https://practicum-content.s3.amazonaws.com/resources/anaconda_navigator_homepage_1695049803.png)

Como você pode ver, o Anaconda vem com uma variedade de aplicativos para ajudar no desenvolvimento de projetos de ciência e análise de dados em Python.

Os aplicativos que já estão instalados têm um botão "Launch". Se desejar instalar aplicativos adicionais, clique no botão "Install" correspondente e siga as instruções. Sinta-se à vontade para explorar qualquer um dos aplicativos, mas nossas prioridades serão Jupyter Notebook, JupyterLab e Powershell Prompt.

Outras características importantes do Anaconda Navigator:

-   A aba "Connect" (Conectar), onde você pode entrar em sua conta do Anaconda, se tiver uma
-   A aba "Learning" (Aprendizagem), onde você tem amplo acesso à documentação e vídeos educacionais
-   A aba "Environments" (Ambientes), onde você pode configurar diferentes ambientes de projeto (o ambiente base é o suficiente para nossas necessidades)
-   O ícone de engrenagem de cada aplicativo, onde você pode instalar, remover ou atualizar aplicativos

### Execução do Python

Agora que configuramos e instalamos o Python, podemos finalmente executar o código!

Você provavelmente costuma interagir com seu computador por uma Interface Gráfica de Usuário (GUI). Em uma GUI, você cria, modifica e abre pastas e arquivos clicando neles com o mouse. Como você aprendeu anteriormente neste sprint, também é possível fazer tudo isso na Linha de Comando (CL, na sigla em inglês) digitando comandos e pressionando Enter. O código Python é geralmente executado a partir da linha de comando, por isso é importante que você se familiarize com como isso funciona.

Basta abrir uma janela de _Terminal_.

![](https://practicum-content.s3.amazonaws.com/resources/anaconda_navigator_homepage_1_1695049826.png)

Após iniciar o aplicativo, podemos usar Python a partir da linha de comando, de forma interativa, digitando `python` no _Terminal_.

![](https://practicum-content.s3.amazonaws.com/resources/Screen_Shot_2021-11-15_at_22.06.53_1695049851.png)

![](https://practicum-content.s3.amazonaws.com/resources/Screen_Shot_2021-11-15_at_22.07.31_1695049862.png)

![](https://practicum-content.s3.amazonaws.com/resources/Screen_Shot_2021-11-15_at_22.08.17_1695049876.png)

![](https://practicum-content.s3.amazonaws.com/resources/Screen_Shot_2021-11-15_at_22.08.39_1695049887.png)

Você vai aprender mais sobre como usar Python interativamente a partir da linha de comando nas próximas lições.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-40-474Z.md
### Última modificação: 2025-05-28 19:26:40

# Como executar código Python na linha de comando - TripleTen

Capítulo 3/8

Ambiente de programação

# Como executar código Python na linha de comando

Agora que você tem as ferramentas necessárias para escrever e executar o código Python localmente em seu próprio computador, é hora de aprender mais sobre o componente de execução do seu ambiente de programação. Nesta lição, você vai aprender duas habilidades importantes:

1.  Como executar scripts Python salvos em arquivos `.py` a partir da linha de comando
2.  Como escrever e executar código Python em tempo real com o interpretador interativo

Vamos começar executando o script que você escreveu para imprimir o número de núcleos de CPU do seu computador.

### Como executar scripts Python da linha de comando

Se você usa uma máquina Windows, abra o Anaconda Navigator e inicie o aplicativo Powershell Prompt.

![](https://practicum-content.s3.amazonaws.com/resources/anaconda_navigator_homepage_1695288351.png)

Se você usa o Mac ou Linux, basta abrir uma janela de _Terminal_.

Para executar um script Python a partir da linha de comando, basta digitar o comando `python` seguido do caminho para o arquivo de script (`cpus.py`) que criamos anteriormente. Navegue até o diretório em que o seu arquivo `cpus.py` está localizado usando o comando `cd` que você já conhece. Em seguida, execute o arquivo usando o comando `python cpus.py`.

Abaixo está uma captura de tela do Powershell Prompt que mostra um exemplo disso para o script que você escreveu antes, que chamamos de `cpus.py`:

![](https://practicum-content.s3.amazonaws.com/resources/commandline_script_1693519466.png)

Vamos detalhar o que está acontecendo na captura de tela acima, que ilustra a execução do script.

A primeira linha é um prompt onde digitamos o comando `python cpus.py`. A linha começa com `(base)`, o que significa que estamos usando o ambiente base do Anaconda. O ambiente base significa que estamos usando a versão do Python que instalamos com o Anaconda e temos todos os pacotes disponíveis para nós também (qualquer versão do pacote que foi instalada). O ambiente base é o suficiente para tudo o que queremos fazer.

Além da `(base)`, o prompt inclui `PS` seguido por nosso diretório de trabalho atual (nesse caso, `D:\Documents\Python Scripts>`). O `PS` apenas indica que estamos no Powershell.

O comando real é `python cpus.py`. Como o arquivo `cpus.py` está no diretório `Python Scripts`, para executá-lo podemos apenas digitar seu nome.

A segunda linha é o resultado da execução do script. Nesse caso, o script imprime uma linha informando quantos núcleos de CPU existem no computador que executa esse script. Em geral, a execução de um script pode produzir um resultado de muitas linhas ou nenhum resultado.

A linha final é outro prompt onde podemos executar o script de novo ou executar um script diferente... ou podemos usar Python no modo interativo e escrever/executar o código em tempo real!

### Como executar código Python de forma interativa a partir da linha de comando

Também podemos usar o Python a partir da linha de comando de forma interativa, digitando `python` no Powershell Prompt.

A imagem do Powershell abaixo mostra o que acontece quando fazemos isso:

![](https://practicum-content.s3.amazonaws.com/resources/commandline_repl_1_1693519487.png)

A primeira linha é o prompt do Powershell onde inserimos o comando `python`. As linhas a seguir informam que agora estamos usando o interpretador interativo do Python. Esse modo interativo é conhecido como um ambiente **REPL** (sigla em inglês para "Read Evaluate Print Loop", que significa "Ler, Avaliar, Imprimir, Repetir).

O modo **REPL** começa exibindo a versão de Python que está sendo usada (nesse caso, é 3.8.10, mas sua versão pode ser diferente). Uma vez nesse modo, o prompt se torna `>>>`, e tudo que digitamos é interpretado como código Python, em vez de instruções de linha de comando.

Digite `print('Hello world')` no prompt, depois aperte Enter e veja o que acontece. Você deve ver uma linha sem o prompt `>>>` que exibe o resultado do seu código seguido por outra linha com um novo prompt. Esse é o processo **REPL** em ação! O interpretador interativo lê seu código, o executa, imprime o resultado e está pronto para você inserir mais código, para que ele possa fazer tudo de novo.

![](https://practicum-content.s3.amazonaws.com/resources/commandline_repl_2_1693519505.png)

Quaisquer variáveis que você criar no modo interativo ficam disponíveis até você encerrar a sessão. Por exemplo, a imagem abaixo mostra como você pode atribuir e modificar variáveis e usá-las em outras expressões.

![](https://practicum-content.s3.amazonaws.com/resources/commandline_repl_3_1693519522.png)

Sinta-se à vontade para brincar no modo interativo digitando o código que desejar. Na verdade, o modo interativo é geralmente usado para isso: verificar rapidamente como certas linhas de código funcionam sem ter que executar scripts ou programas inteiros. Na prática, você vai escrever a maioria do seu código em scripts e os executar em cadernos Jupyter (falaremos mais sobre isso na próxima lição).

Para sair do modo interativo **REPL** e retornar ao terminal Powershell, basta digitar `exit()` no prompt `>>>`. Apenas esteja ciente de que todas as suas variáveis vão desaparecer da memória quando você encerrar a sessão.

![](https://practicum-content.s3.amazonaws.com/resources/commandline_repl_4_1693519543.png)

Agora você tem as habilidades necessárias para escrever e executar o código Python da linha de comando em seu ambiente de programação. Em seguida, vamos explorar um dos aplicativos mais populares incluídos na instalação do Anaconda: o Jupyter Notebook.

Pergunta

O que podemos fazer na interface de linha de comando?

Escolha quantas quiser

Executar scripts Python.

Isso mesmo! Tudo o que você precisa fazer é digitar `python` seguido do caminho para um arquivo `.py`.

Digitar `print("Hello world")` diretamente na linha de comando e esperar que a interface de linha de comando entenda isso como um código Python.

Executar o interpretador interativo Python.

É fácil executá-lo digitando o comando `python`.

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-41-804Z.md
### Última modificação: 2025-05-28 19:26:42

# Como escrever e executar código no Jupyter Notebook - TripleTen

Capítulo 3/8

Ambiente de programação

# Como escrever e executar código no Jupyter Notebook

Muitos desenvolvedores trabalham com Python na linha de comando ou em um IDE, mas os notebooks (cadernos) Jupyter se tornaram o meio principal para cientistas e analistas de dados apresentarem e compartilharem seu trabalho com outras pessoas. Os cadernos são populares porque são simples de usar e permitem inserir texto e imagens com o código. Nesta lição, você vai aprender o básico do Jupyter Notebook, incluindo:

-   Como iniciar o Jupyter Notebook e navegar na interface do usuário
-   Como criar arquivos de caderno
-   Como escrever e executar código em um caderno
-   Como adicionar texto e imagens a um caderno

Vamos começar!

### Navegação no Jupyter Notebook

Você pode iniciar uma sessão do Jupyter Notebook iniciando o aplicativo no Anaconda Navigator:

![](https://practicum-content.s3.amazonaws.com/resources/anaconda_navigator_homepage_3_1695050118.png)

ou encontrando o Jupyter Notebook no seu computador e abrindo-o a partir de lá. Abrir o Jupyter Notebook vai iniciar uma sessão de servidor, abrindo uma janela em seu navegador web. O servidor será criado em sua máquina local, portanto, você não precisa de uma conexão com a internet para usar o programa.

Você verá algo semelhante à imagem abaixo (os arquivos e diretórios serão o que você tiver no seu computador):

![](https://practicum-content.s3.amazonaws.com/resources/jupyter_notebook_1_1695050132.png)

O endereço no navegador deve indicar `localhost:xxxx/tree`, onde `xxxx` é a porta 8888 por padrão, mas pode ser outra se você especificar uma diferente ou se várias sessões do Jupyter Notebook forem iniciadas ao mesmo tempo.

![](https://practicum-content.s3.amazonaws.com/resources/jupyter_notebook_1_1_1695050146.png)

A página inicial será um diretório em seu computador (você pode [alterar o padrão](https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/execute.html#change-jupyter-notebook-startup-folder-windows) _(os materiais estão em inglês)_ se desejar). A partir daí, você pode clicar em outras pastas para navegar pelo sistema de arquivos do seu computador.

Vamos criar um novo arquivo de caderno, então navegue até onde você deseja que o arquivo esteja em seu computador. Se você desejar criar uma nova pasta para ele, pode fazer isso pelo Jupyter Notebook ou do jeito como você costuma fazer isso no computador. Criamos uma pasta vazia chamada `Notebooks` para nossos exemplos aqui. Para criar uma pasta, clique em `New` (Novo):

![](https://practicum-content.s3.amazonaws.com/resources/jupyter_notebook_1_2_1695050216.png)

e selecione `Folder` (pasta). Uma nova pasta chamada `Untitled Folder` (pasta sem título) será criada:

![](https://practicum-content.s3.amazonaws.com/resources/Screenshot_from_2023-07-25_17-19-55_1696235748.png)

Você pode clicar na caixa vazia ao lado do nome da pasta e em `Rename` (Renomear):

![](https://practicum-content.s3.amazonaws.com/resources/Screenshot_from_2023-07-25_17-20-49_1696235711.png)

Para acessar o diretório renomeado, basta clicar nele.

Podemos criar um novo arquivo de caderno clicando na aba `New` e selecionando `Python 3` sob o título "Notebook":

![](https://practicum-content.s3.amazonaws.com/resources/jupyter_notebook_2_1695050247.png)

A criação do novo arquivo deve abrir automaticamente uma nova janela, na qual você pode começar a trabalhar.

Agora que você aprendeu como iniciar o Jupyter Notebook, navegar pelos diretórios usando a interface do usuário e criar um novo arquivo de caderno, vamos começar a escrever algum código Python.

### Trabalho em um arquivo de caderno

Sabemos que você já conhece os cadernos do Jupyter e como escrever código neles. Entretanto, acreditamos que agora é um bom momento para dar um passo atrás e revisar os detalhes básicos mais importantes.

Uma das primeiras coisas que você pode notar sobre seu novo caderno é que ele se chama `Untitled` (sem título). É uma boa prática começar dando ao caderno um nome mais exclusivo. Basta clicar em `Untitled` e digitar um nome diferente no prompt que aparecer. Depois, clique no botão `Rename` (renomear). Decidimos chamar o nosso caderno de `my_notebook` – meu caderno.

![](https://practicum-content.s3.amazonaws.com/resources/jupyter_notebook_3_1695050265.png)

Se você salvar o arquivo após mudar o nome dele, ele deve aparecer como um arquivo `.ipynb` no diretório em que você o criou (por exemplo, `.../Notebooks/my_notebook.ipynb`). A extensão do arquivo `.ipynb` significa "iPython notebook", porque o Jupyter é derivado do projeto [iPython](https://ipython.org/) mais antigo. Os arquivos `.ipynb` são apenas arquivos de texto simples com um formato [JSON](https://www.json.org/json-pt.html), que o Jupyter interpreta e transforma em cadernos visualmente bonitos. Entendemos que você pode ainda não conhecer o formato JSON, mas não se preocupe. Você terá a oportunidade de trabalhar com ele mais adiante no programa. Por ora, basta lembrar que um notebook Jupyter é salvo no formato JSON.

Agora que você renomeou seu caderno, vamos começar a programar. Você verá uma célula que começa com `In [ ]:`, seguida por uma caixa cinza onde você pode escrever o código. O `In` significa input, ou seja, entrada.

Comece com uma simples declaração de impressão: `print('Hello world')`. Para executar o código, verifique se o cursor está na caixa da célula e clique no botão Run (Executar) acima das células ou pressione o atalho de teclado "Ctrl + Enter" ou "Shift + Enter".

Seu caderno deve se parecer com esta imagem:

![](https://practicum-content.s3.amazonaws.com/resources/4.2.6PT_1695288548.png)

Você pode notar três mudanças significativas no caderno após executar o código:

1.  A célula que você executou agora é identificada como célula 1, indicada por `In [1]:` em vez de `In [ ]:`
2.  A célula 1 agora exibe a saída do seu código abaixo da caixa cinza
3.  Uma nova célula não rotulada apareceu abaixo da célula 1

Agora você pode digitar mais código na célula 1 ou na nova célula não rotulada. Vamos deixar a célula 1 como está por enquanto. Vá em frente e copie para a nova célula o código do seu script que exibe o número de CPUs no seu computador. Caso você não lembre, este é o código:

```
import os

cpu_num = os.cpu_count()
print(f"Meu computador tem {cpu_num} CPUs")
```

Execute o código na nova célula. Seu caderno agora deve ficar assim:

![](https://practicum-content.s3.amazonaws.com/resources/4.2.6.2PT_1695288577.png)

A célula que você acabou de executar agora está rotulada como célula 2, e há uma nova célula não rotulada abaixo dela. Em geral, as células são rotuladas sequencialmente cada vez que você executa uma célula, começando em 1. Agora execute novamente a célula 1 e veja o que acontece. Você deve descobrir que a célula 1 se tornou a célula 3. Isso porque essa é a terceira célula que você executou desde que iniciou o caderno. Para redefinir a rotulagem, vá para a aba "Kernel" e selecione "Restart & Run All" (Reiniciar e executar tudo). Seu caderno deve mais uma vez se parecer com a imagem acima.

Uma característica interessante do Jupyter Notebook é que você pode trabalhar na linha de comando dentro do próprio caderno usando o símbolo especial `!`. Para ilustrar, copie seu arquivo de script Python que conta CPUs para o mesmo diretório do seu caderno. Em seguida, na nova célula, escreva o seguinte código (substitua o nome do arquivo `cpus.py` pelo nome que você deu ao seu arquivo):

```
!python cpus.py
```

Ao executar a célula com esse código, você acabou de executar um script Python de um caderno sem precisar digitar o código do script em uma célula. Algo a ter em mente, se você fizer isso: quaisquer variáveis no script não estarão disponíveis em seu ambiente de caderno após a execução do script. Seu caderno agora deve estar assim:

![](https://practicum-content.s3.amazonaws.com/resources/4.2.6.3PT_1695288625.png)

Pergunta

Como executar uma célula em um caderno Jupyter?

Escolha quantas quiser

Clicar no botão `▷ Run` localizado no caderno

Essa opção certamente vai funcionar.

Pressionar "Ctrl + Enter" ou "Shift + Enter”

Esse é um atalho usado pela maioria dos profissionais de dados.

Clicar duas vezes na célula

Fantástico!

### Adição de texto e imagens

No Jupyter Notebook, você pode ter células que contêm texto ou imagens em vez de código. Esse é um dos principais benefícios do uso de cadernos. Você pode criar documentos independentes com código, explicações de texto e imagens para apresentar seu trabalho a outras pessoas em um único arquivo.

Para uma célula de texto, precisamos alterar o tipo de célula de "Code" para "Markdown". Isso pode ser feito selecionando a nova célula não rotulada e escolhendo "Markdown" no menu suspenso à direita do botão "Run".

![](https://practicum-content.s3.amazonaws.com/resources/Screenshot_from_2023-07-25_17-33-15_1695050406.png)

Markdown é uma "linguagem" de formatação de texto que adiciona formatação a símbolos de texto após a execução. Não entraremos em detalhes sobre todas as opções de formatação agora, mas você pode consultar [aqui](https://daringfireball.net/projects/markdown/) para qualquer dúvida sobre o Markdown.

Por enquanto, apenas copie e cole o seguinte texto para sua célula Markdown (mas não a execute ainda):

```
## Texto e imagens no caderno
---
Em um caderno Jupyter, você pode misturar:
* Código
* Texto
* Imagens
```

Seu caderno deve ficar assim:

![](https://practicum-content.s3.amazonaws.com/resources/4.2.6.5PT_2_1695288934.png)

Agora execute a célula Markdown para ver o que acontece. Você vai duas coisas:

1.  A célula Markdown se torna um texto bem formatado
2.  A célula Markdown não tem mais uma caixa de código cinza nem um rótulo de célula

Seu caderno deve ficar assim:

![](https://practicum-content.s3.amazonaws.com/resources/4.2.6.6PT_1695289102.png)

Observe que `##` transforma o texto em um cabeçalho e `*` é interpretado como um marcador.

É bem fácil adicionar uma imagem a um notebook Jupyter. Primeiro, você precisa converter uma célula para o tipo Markdown. Em seguida, selecione essa célula e arraste e solte uma imagem do seu computador.

![](https://practicum-content.s3.amazonaws.com/resources/Screenshot_from_2023-07-25_17-39-55_1693520931.png)

Após soltar uma imagem, vemos o seguinte:

![](https://practicum-content.s3.amazonaws.com/resources/Screenshot_from_2023-07-25_17-40-09_1693520976.png)

Então, execute a célula para incluir a imagem no notebook Jupyter:

![](https://practicum-content.s3.amazonaws.com/resources/Screenshot_from_2023-07-25_17-40-15_1693520992.png)

Esperamos que agora você veja como o Jupyter Notebook pode ser útil como um ambiente de programação. No entanto, o Jupyter não é perfeito quando se trata de práticas recomendadas para programação e desenvolvimento. Antes de passarmos para a lição final sobre pacotes Python, é importante discutirmos algumas das coisas a que devemos prestar atenção ao trabalhar no Jupyter Notebook.

Pergunta

Como adicionar um cabeçalho a um caderno Jupyter?

Criar uma nova célula → convertê-la para o tipo Mardown → digitar o cabeçalho desejado começando com o símbolo `*`.

Criar uma nova célula → convertê-la para o tipo Mardown → digitar o cabeçalho desejado começando com os símbolos `#`, `##` ou `###`.

Isso mesmo. Observe que quanto menos cerquilhas você adicionar, maior será o cabeçalho.

Você conseguiu!

### Armadilhas comuns do Jupyter Notebook

Há duas coisas muito importantes para ter em mente ao trabalhar no Jupyter Notebook:

1.  A ordem em que as células foram executadas
2.  A existência e os valores atuais de variáveis no seu ambiente

Quando você escreve todo o seu código em um script e o executa em uma nova sessão do Python, sabe que todo o código no script será executado na ordem em que foi escrito. No caderno, não há exigências para que as células sejam executadas em uma ordem específica. É por isso que células de código são rotuladas na ordem em que foram executadas, para que você possa tentar acompanhar tudo.

A execução de células fora de ordem geralmente acontece quando você está fazendo experimentos com o código, voltando e alterando algo em uma célula anterior para observar seu efeito na saída do código em uma célula posterior. Brincar com o código para ver o que acontece é uma ótima maneira de aprender! Em cadernos, no entanto, você deve ter um cuidado extra para que isso não altere os valores de maneiras que não seriam possíveis se o código fosse executado em ordem.

Obviamente, é uma boa prática organizar os cadernos de forma que todas as células sejam executadas em ordem de cima para baixo. Em caso de dúvida, você deve sempre limpar seu ambiente reiniciando o kernel e, em seguida, executar as células novamente na ordem em que aparecem.

Muitos elementos do Jupyter Notebook foram projetados para que ele seja intuitivo, especialmente se você já tem alguma experiência em programação. Se você tiver dúvidas sobre como fazer algo no caderno, fazer uma pesquisa no Stack Overflow geralmente pode ajudar a encontrar a resposta. O Jupyter Notebook também tem uma ótima documentação [aqui](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html) _(os materiais estão em inglês)_.

### Observações finais

Por que abordamos tudo isso? Não apenas para recapitular tudo o que sabemos sobre o Jupyter Notebook, mas também para que você possa trabalhar em um caderno Jupyter em seu próprio computador! Até agora, você só trabalhou em um caderno Jupyter que disponibilizamos na nossa plataforma de aprendizado. Agora que você também tem o Jupyter Notebook em sua máquina local, não há nada que te impeça de trabalhar em suas tarefas futuras da maneira mais conveniente. Aconselhamos que você continue trabalhando com seus projetos na plataforma, pois isso facilita verificar seu trabalho. Entretanto, para outras finalidades, você agora pode trabalhar no Jupyter Notebook no seu computador.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-44-053Z.md
### Última modificação: 2025-05-28 19:26:45

# Como instalar e usar módulos e pacotes - TripleTen

Capítulo 3/8

Ambiente de programação

# Como instalar e usar módulos e pacotes

Agora que você sabe como configurar e trabalhar em um ambiente de programação em sua máquina local, é hora de aprender mais sobre os pacotes Python e suas funções no ambiente.

![](https://practicum-content.s3.amazonaws.com/resources/Artboard_1_copy_52x-100_1695050566.jpg)

### Módulos, pacotes e bibliotecas

Os módulos, pacotes e bibliotecas são uma maneira hierárquica usada por várias pessoas para organizar o código Python.

Um módulo Python é basicamente um script Python (um arquivo de texto com a extensão `.py`) que contém definições de funções, classes e/ou variáveis que ajudam a resolver algum problema específico.

Basicamente, um pacote Python é apenas uma coleção de módulos relacionados entre si. Portanto, um pacote contém vários scripts.

Finalmente, uma biblioteca costuma ser uma coleção de pacotes e módulos Python.

Na internet, você verá esses três termos usados de forma intercambiável. Portanto, é melhor não perder muito tempo tentando memorizar as definições exatas desses termos. O importante é saber que módulos, pacotes e bibliotecas são códigos escritos por outras pessoas que podem ser importados para ajudar você a trabalhar em seus próprios projetos Python.

### Importação e uso de módulos e pacotes

A importação de pacotes é bastante simples. Você já fez uma importação neste capítulo quando criou seu script que conta CPUs. Vamos dar uma olhada no script novamente:

```
import os

cpu_num = os.cpu_count()
print(f"Meu computador tem {cpu_num} CPUs")
```

A primeira linha importa o módulo `os` (sistema operacional), um módulo padrão que acompanha qualquer instalação do Python. Usamos uma função do módulo chamada `cpu_count()`. Para acessar essa função, tivemos que prefaciá-la com o nome do módulo e um ponto (também chamado de notação de ponto).

Você também viu outros exemplos de importação no Jupyter Notebook. Por exemplo, estes:

```
import numpy as np
from matplotlib import pyplot as plt
```

Ambas as instruções de importação contêm a palavra reservada `as` (mais sobre palavras reservadas [aqui](https://www.w3schools.com/python/python_ref_keywords.asp) _(os materiais estão em inglês)_). Isso nos permite dar um apelido ao pacote importado. Por exemplo, podemos acessar a constante pi de NumPy usando `np.pi` em vez de `numpy.pi`. Você pode escolher o apelido que quiser, mas a maioria dos pacotes tem uma convenção padrão. Aderir à convenção padrão torna seu código muito mais fácil para outras pessoas lerem.

Pergunta

O que é uma biblioteca em Python?

É um script Python

É uma coleção de pacotes e módulos Python

Exatamente!

É uma coleção de scripts Python

Excelente!

### Verificar a versão de um pacote

A maioria dos pacotes, especialmente os populares, recebe atualizações frequentes. É possível que o código escrito usando uma versão de um pacote se comporte de maneira diferente quando executado usando outra versão do mesmo pacote. Isso pode causar erros que podem travar o código, ou o código pode ser executado, mas certas funções importadas podem se comportar de maneira diferente. Quando esses problemas surgirem, uma boa ideia é verificar as versões dos pacotes que você importou para o seu ambiente.

Para os pacotes padrão, como `os`, as versões devem ser iguais à versão do Python que você está usando. Você pode ver sua versão atual do Python na linha de comando simplesmente iniciando o interpretador interativo com o comando `python`. No Jupyter Notebook, você pode verificar a versão atual executando o seguinte código (sua saída vai variar dependendo de sua versão e de quando você executa o código):

```
import sys
sys.version
```

```
'3.8.10 (default, May 19 2021, 13:12:57) [MSC v.1916 64 bit (AMD64)]'
```

Para outros pacotes, geralmente você pode verificar a versão imprimindo o atributo `__version__` do pacote após importá-lo (há sublinhados duplos antes e depois de "version"). Por exemplo, vamos importar o pacote `pandas` (você vai usar esse pacote com bastante frequência na ciência de dados) e verificar a versão:

```
import pandas as pd
print(pd.__version__)
```

```
1.2.5
```

Observe que isso funciona um pouco diferente no caso de importar um pacote de uma biblioteca maior.

Vamos usar `pyplot` do Matplotlib como exemplo:

```
from matplotlib import pyplot as plt
print(plt.__version__)
```

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-15-8fe02450c2db> in <module>
      1 from matplotlib import pyplot as plt
----> 2 print(plt.__version__)

AttributeError: module 'matplotlib.pyplot' has no attribute '__version__'
```

Recebemos uma mensagem de erro porque `pyplot` não possui o atributo `__version__`. Em vez disso, toda a biblioteca Matplotlib possui o atributo `__version__`.

Mas se apenas importar o pacote `pyplot`, não podemos verificar a versão da biblioteca `matplotlib`:

```
from matplotlib import pyplot as plt
print(matplotlib.__version__)
```

```
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-16-32ac7390a56f> in <module>
      1 from matplotlib import pyplot as plt
----> 2 print(matplotlib.__version__)

NameError: name 'matplotlib' is not defined
```

Recebemos esse erro porque não temos um objeto `matplotlib` em nosso namespace, já que, na verdade, não importamos a biblioteca toda. Se quisermos verificar a versão do `pyplot`, precisamos importar toda a biblioteca `matplotlib`:

```
import matplotlib
print(matplotlib.__version__)
```

```
3.3.4
```

Em sua experiência anterior, todos os pacotes já estavam prontos para uso. No entanto, esse nem sempre é o caso, e às vezes precisamos instalar um pacote manualmente ou alterar a versão dele. Como podemos fazer isso? Vamos descobrir.

### Instalação de pacotes

O Anaconda (ou qualquer outra instalação do Python) não vem instalado com todos os pacotes Python, pois existem literalmente centenas de milhares deles. A maioria dos pacotes que você vai usar já estão instalados com o Anaconda, mas se você quiser usar um pacote que não foi instalado, vai precisar fazer isso por conta própria.

Usaremos o pacote Plotly como exemplo. [Plotly](https://plotly.com/python/) _(os materiais estão em inglês)_ é uma biblioteca gráfica que você pode usar como alternativa ao Matplotlib. Para instalar o Ploty, você precisa abrir o Powershell Prompt mais uma vez usando o Anaconda Navigator e digitar o seguinte na linha de comando:

```
conda install -c plotly plotly
```

Vamos analisar esse comando:

```
conda install -c channel_name package_name
```

`-c` significa "canal" e `channel_name`, que representa o nome do canal, vem em seguida. Um canal é apenas um local (uma URL) onde Conda vai procurar os pacotes.

Se o pacote for encontrado pelo Anaconda, você verá uma lista dos pacotes que serão instalados:

![](https://practicum-content.s3.amazonaws.com/resources/Screenshot_from_2023-07-26_10-53-18_1695050704.png)

Você talvez veja outros pacotes além do `plotly`, porque sempre que instalar um pacote, também deve instalar todos os outros pacotes dos quais o pacote inicial depende. Deve haver um prompt perguntando se você deseja iniciar a instalação. Insira `y` (de "Yes") para fazer isso:

![](https://practicum-content.s3.amazonaws.com/resources/Screenshot_from_2023-07-26_10-53-52_1695050725.png)

E é isso! Com o `plotly` instalado, agora você deve ser capaz de importá-lo e usá-lo. Vá em frente e tente usá-lo no caderno. Encontre mais informação sobre a instalação de pacotes [aqui](https://docs.anaconda.com/free/anaconda/packages/install-packages/) _(os materiais estão em inglês)_.

### Instalação de pacotes via pip

O [PIP](https://pip.pypa.io/en/stable/) é um gerenciador de pacotes do Python. Ele permite instalar e gerenciar pacotes que não são parte da [biblioteca padrão de Python](https://docs.python.org/pt-br/3/py-modindex.html). Em outras palavras, ele permite instalar qualquer pacote externo. Ele funciona de modo parecido com a instalação que fizemos com o Conda. Por exemplo, para instalar a Pandas, você faria o seguinte:

```
pip install pandas
```

O resultado vai ser o mesmo: um pacote vai ser instalado.

### Atualização de pacotes

Como acabamos de instalar a versão mais recente do Plotly, não precisamos atualizá-la. No entanto, se você quiser atualizar algum pacote no futuro usando o Conda, basta digitar o seguinte comando:

```
conda update package_name
```

Observe que `package_name` precisa ser substituído pelo nome real do pacote. Por exemplo, `plotly`.

Você também pode instalar, atualizar e remover pacotes na GUI do Anaconda Navigator. Para obter detalhes sobre isso, consulte a [documentação](https://docs.anaconda.com/anaconda/navigator/tutorials/manage-packages/).

Já no PIP, aqui está um exemplo de como fazer upgrade da biblioteca Pandas:

```
pip install --upgrade pandas
```

Observe que após a palavra `install` também temos o sinalizador `--upgrade`.

Pergunta

Como você começaria um comando para instalar um pacote em Python?

`conda install ...`

Exatamente!

`conda update ...`

`pd.__version__`

Fantástico!

### Ambientes virtuais

O que você faz se precisar executar um código que foi escrito para uma versão de um pacote diferente daquela que você instalou? Ou se você trabalhar em vários projetos e cada um usar uma versão diferente do mesmo pacote? Os ambientes virtuais nos permitem lidar com situações como essas.

Se você estiver no Windows, abra o Anaconda Prompt ou o Anaconda PowerShell Prompt no Anaconda Navigator. No MacOS ou Linux, basta abrir o Terminal. Use o comando `cd` para navegar até o diretório onde você quer criar seu ambiente virtual e use o seguinte comando para criá-lo:

```
conda create --name myenv
```

Nesse exemplo, `myenv` é o nome do ambiente que deve ser criado. Você pode substituir `myenv` por qualquer nome de ambiente de sua escolha. Em vez de `--name`, você também pode usar o `-n` abreviado.

Quando uma solicitação para continuar aparecer (`proceed ([y]/n)?`), digite `y`.

Para ativar o ambiente recém-criado, use o seguinte comando:

```
conda activate myenv
```

Nesse exemplo, `myenv` é o nome do ambiente que deve ser ativado. Para sair do ambiente, basta desativá-lo:

```
conda deactivate
```

Enquanto o ambiente está ativo, você pode instalar os pacotes, módulos ou bibliotecas que precisar. Por exemplo, para instalar a biblioteca plotly, use o seguinte comando:

```
conda install -c plotly plotly
```

Isso instala plotly diretamente no ambiente ativo.

Como alternativa, você pode especificar o ambiente no qual quer instalar um pacote indicando o nome após a opção `--name`. Por exemplo, para instalar o pacote SciPy em um ambiente chamado `myenv`, use o seguinte comando:

```
conda install --name myenv scipy
```

Você pode até instalar pacotes e bibliotecas ao criar um novo ambiente. Por exemplo, para criar um novo ambiente chamado `myenv` e instalar o pacote SciPy nele, use o seguinte comando:

```
 conda create --name myenv scipy
```

ou

```
conda install -n myenv scipy
```

Você também pode especificar uma versão particular de um pacote, ou uma versão particular de Python, conforme mostrado nos seguintes exemplos:

```
conda install --name myenv scipy=0.15.0
conda create --name myenv python=3.9
```

Por fim, você pode criar um ambiente a partir de um arquivo. Isso é útil quando você trabalha em um projeto em equipe e precisa reproduzir um ambiente específico. O arquivo normalmente tem a extensão `.yml` ou `.txt` file. Aqui está um exemplo de como importar um ambiente de um arquivo chamado `ENV.yml` e chamá-lo de `myenv`:

```
conda env create --name myenv --file ENV.yml
```

De forma parecida, podemos criar um ambiente a partir de um arquivo `.txt`:

```
conda env create --name myenv --file requirements.txt
```

Observe que um arquivo ambiental é comumente chamado de `requirements.txt`. Essa é uma das convenções de nomenclatura mais comuns para tais arquivos. Além disso, às vezes pode ser necessário se referir a esse arquivo por esse nome específico.

Após criar um ambiente, você pode abrir um caderno Jupyter e começar a trabalhar.

Para salvar seu ambiente atual em um arquivo, use este comando:

```
conda create --name myenv --file requirements.txt
```

O comando acima vai criar um ambiente chamado `myenv` com todos os pacotes listados no arquivo `requirements.txt`. Aqui está um exemplo do conteúdo de tal arquivo:

```
pandas==1.3.1
scipy==1.6.2
streamlit==1.12.2
```

Esse documento lista as bibliotecas e suas versões respectivas. Memorize o comando acima, ele será útil no próximo projeto.

Pergunta

Como criar um ambiente chamado `customers_analysis` com Python 3.7 e pandas 1.5.3?

Escolha quantas quiser

`conda create --name customers_analysis python=3.7 pandas=1.5.3`

Essa é uma das opções.

`conda install --name customers_analysis pandas=1.5.3 python=3.7`

Isso certamente vai funcionar.

`conda create --name customers_analysis python=3.7` e depois `conda install pandas=1.5.3`

Essa é uma abordagem de duas etapas que vai funcionar.

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-45-426Z.md
### Última modificação: 2025-05-28 19:26:45

# Conclusão - TripleTen

Capítulo 3/8

Ambiente de programação

# Conclusão

![](https://practicum-content.s3.amazonaws.com/resources/Artboard_12x-100_1695050926.jpg)

Parabéns por concluir o capítulo Ambiente de programação!

Neste capítulo, você aprendeu como configurar um ambiente de programação, instalar o Python e pacotes de ciência de dados associados usando o [Anaconda](https://www.anaconda.com/products/individual), a encontrar maneiras diferentes de escrever e executar o Python, usar o Jupyter Notebook e a verificar e instalar versões de pacotes.

Com todas essas novas ferramentas à sua disposição, você está com tudo pronto para dar os próximos passos no mundo da análise de dados!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-46-742Z.md
### Última modificação: 2025-05-28 19:26:47

# Introdução - TripleTen

Capítulo 4/8

Git e GitHub

# Introdução

![](https://practicum-content.s3.amazonaws.com/resources/Artboard_1_copy-100_1695108422.jpg)

Neste capítulo, você vai conhecer o Git e o GitHub. Essas são algumas das ferramentas mais usadas por programadores. Vamos explicar brevemente o motivo.

O **Git** é uma ferramenta usada para gerenciar o código, especificamente como um sistema de controle de versão em projetos de todos os tamanhos. O Git rastreia alterações no código, permitindo que vários desenvolvedores colaborem no mesmo projeto trabalhando independentemente de forma não-linear.

Após aprendermos o que é o Git, é importante falar sobre o conceito de um repositório, ou "repo". Um repositório é um espaço de armazenamento onde você pode manter todos os arquivos e recursos do seu projeto. Os desenvolvedores trabalhando em um projeto carregam as alterações que fazem em um repositório, contribuindo para o resultado final.

Por outro lado, o **GitHub** é um serviço de hospedagem que permite gerenciar repositórios Git. Entendemos que isso pode parecer abstrato, então vamos explicar cada etapa, fornecendo todos os detalhes necessários ao longo do caminho.

Com a cultura atual de código compartilhado abertamente, ser especialista no Git e no GitHub significa ter muito do código do mundo ao seu alcance! Com um pouco de prática, você vai conseguir encontrar, baixar e modificar rapidamente códigos abertos.

### O que você vai aprender

-   O que são os controles de versão, Git e GitHub.
-   Como configurar seu próprio repositório e confirmar alterações nele.
-   Como clonar repositórios com código aberto criado por outras pessoas.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-49-568Z.md
### Última modificação: 2025-05-28 19:26:50

# Controle de versão - TripleTen

Capítulo 4/8

Git e GitHub

# Controle de versão

Nesta lição, falaremos sobre o conceito de controle de versão, os motivos pelos quais os desenvolvedores o utilizam e um breve histórico do tema.

### O que é o controle de versão?

**Controle de versão** (também chamado de **controle de origem**) é qualquer método usado para rastrear, armazenar e gerenciar alterações em arquivos digitais pertencentes a algum projeto. Normalmente, o controle de versão é usado para projetos de software em que a maioria dos arquivos contém código de computador, como arquivos Python. No entanto, o controle de versão pode ser usado para gerenciar qualquer projeto digital.

A prática é implementada com um **software de controle de versão**. O mais antigo sistema de controle de versão difundido provavelmente foi o [Concurrent Versions System, ou Sistema de Versões Concorrentes](http://cvs.nongnu.org/) (CVS), lançado na década de 1990. Mais tarde, no início dos anos 2000, [Subversion](https://subversion.apache.org/) tornou-se popular. Hoje, ambos os sistemas continuam em uso, junto com softwares de controle de versão mais modernos, incluindo [Git](https://git-scm.com/), [Mercurial](https://www.mercurial-scm.org/) e [Helix Core](https://www.perforce.com/products/helix-core).

Em última análise, o software de controle de versão que você deve usar é aquele usado pela organização para a qual trabalha. No entanto, o software de controle de versão mais popular hoje é o **Git** e, por esse motivo, é esse que você vai aprender a usar neste programa. Mesmo se você acabar usando um software de controle de versão diferente no futuro, aprender o Git aqui vai incutir princípios básicos de controle de versão que você pode levar para outros sistemas.

### Por que usar o controle de versão?

Então, qual é a função do controle de versão? Por que tantas ferramentas foram criadas para resolver esse problema? Todos os sistemas de controle de versão nasceram de problemas comuns que surgem quando grupos de pessoas trabalham no mesmo projeto de desenvolvimento de software. Esses problemas incluem:

-   Trabalhar em diferentes partes do código simultaneamente
-   Trabalhar em diferentes versões do código
-   Acessar versões mais antigas do código

O controle de versão permite que equipes de pessoas trabalhem em diferentes partes do código ao mesmo tempo e, depois, combinem seu trabalho independente na mesma base sem quebrar nenhum código preexistente. E, se um bug ou outro problema for introduzido acidentalmente, o controle de versão permite que o código seja revertido para qualquer versão anterior, para que a equipe possa identificar quando o bug foi introduzido e corrigir o código.

### Um estudo de caso de controle de versão

Digamos que há uma startup trabalhando bastante para lançar um aplicativo e começar a ganhar muito dinheiro. A empresa tem várias equipes de desenvolvimento que trabalham em diferentes partes do aplicativo. O diagrama abaixo exibe como o controle de versão influencia o fluxo de trabalho:

![](https://practicum-content.s3.amazonaws.com/resources/4.3.2PT_1695382003.png)

A **ramificação principal** (também conhecida como **master branch**) representa a versão de trabalho do código-fonte da startup. Se o aplicativo deles fosse implementado, esse seria o código ativo que os clientes estariam usando e interagindo. Cada círculo representa uma versão diferente do código que foi armazenada no software de controle de versão.

Em algum momento, as equipes começam a trabalhar em características independentes do aplicativo ao mesmo tempo. O trabalho de cada equipe é representado pelas diferentes ramificações (A, B e C). Por exemplo, uma equipe trabalha em um novo código na ramificação A, enquanto outra equipe continua desenvolvendo código na ramificação principal. Eventualmente, o código na ramificação A chega a um ponto em que outra equipe pode usá-lo para desenvolver um novo recurso na ramificação B. Enquanto isso, outra equipe começa a trabalhar em um recurso na ramificação C que não depende de nenhum trabalho sendo feito na ramificação A ou B.

Mais tarde, os códigos nas ramificações A e B são unidos de volta na ramificação principal e se tornam parte do código-fonte pronto do produto. O trabalho na ramificação C continua em andamento. Dependendo de como as coisas avançarem, ele pode ou não se tornar parte do código-fonte na ramificação principal.

Mas espere um pouco! Uma das equipes encontra um grande bug na ramificação principal (representado pelo ícone do bug). Para corrigir o bug, é necessário encontrar a fonte. Usando o controle de versão, a equipe volta para versões mais antigas do código para ver se o bug também está presente nelas. Os círculos vermelhos representam as versões onde o bug é encontrado, e os círculos azuis significam que o bug não está lá. Parece que o bug foi introduzido em algum lugar na ramificação A antes que ela fosse juntada à ramificação principal.

Esperamos que esse exemplo tenha dado uma noção de por que o controle de versão é importante. Imagine trabalhar em uma base de código com décadas e na qual trabalharam milhares de pessoas. Sem o controle de versão, esse empreendimento não seria possível.

Pergunta

Quais são os riscos de não usar um sistema de controle de versão?

Escolha quantas quiser

Você pode acabar trabalhando com arquivos desatualizados ou até mesmo incorretos.

Esse é de fato um risco.

Você pode escrever um código ineficiente.

Não é possível restaurar versões anteriores do código.

Isso aí! Sem controle de versão, isso pode acontecer.

Fantástico!

Na próxima lição, vamos focar em um software de controle de versão específico: o Git.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-50-917Z.md
### Última modificação: 2025-05-28 19:26:51

# Git - TripleTen

Capítulo 4/8

Git e GitHub

# Git

O resto deste capítulo é dedicado à aprendizagem do [Git](https://git-scm.com/) para o controle de versão. O Git é gratuito, de código aberto e bastante popular. Por esses motivos, é bom ter um conhecimento prático dele. Além disso, devido à sua popularidade, há uma abundância de informação disponível em sites como o [Stack Overflow](https://stackoverflow.com/questions/tagged/git) _(os materiais estão em inglês)_ para solucionar quaisquer problemas que você possa encontrar ao usar o Git.

### História do Git

O Git é um sistema de controle de versão relativamente moderno, lançado em 2005 por Linus Torvalds como uma ferramenta para auxiliar no desenvolvimento do kernel Linux. Ele foi desenvolvido como uma alternativa gratuita, rápida e simples em relação a outras opções de software de controle de versão da época.

Desde então, o Git é mantido principalmente por Junio Hamano com contribuições de vários outros desenvolvedores ao longo dos anos. O Git explodiu em popularidade na última década, em parte devido a outros projetos criados como extensões do Git, como o GitHub.

Observe que afirmamos que o Git é simples e fácil de usar. Mas a verdade é que "fácil" é tanto relativo quanto subjetivo. O Git pode ter sido uma melhoria em relação a outros softwares de controle de versão da época, mas ele ainda tem uma alta curva de aprendizado para a maioria das pessoas. Pode levar anos de trabalho com o Git para entender todo o seu funcionamento interno e complexidades. Esses problemas não são exclusivos do Git, no entanto; controle de versão em geral é muito difícil.

Agora que você sabe um pouco sobre a origem do Git e por que o estamos usando, vamos garantir que você tenha um ambiente git que possa usar em seu computador local. Abordaremos como instalar o Git em máquinas Windows e Mac, então pule para a seção relevante para você.

### Instalação do Git no Windows

Você talvez já tenha o Git instalado em sua máquina Windows, e nesse caso não há necessidade de instalação. Para verificar se você tem o Git instalado, abra um terminal Powershell do Windows e digite o seguinte comando:

```
git --version
```

Se o Git já estiver instalado, você verá uma saída como esta (embora sua versão possa ser diferente):

```
git version 2.36.0.windows.1
```

Se o Git estiver instalado, parabéns! Você pode passar para a próxima lição. Caso contrário, você pode baixar o instalador do Git para Windows [aqui](https://git-scm.com/download/win). Basta baixar o instalador e seguir as instruções de instalação.

### Instalação do Git no Mac

Abra uma janela de terminal e digite o seguinte comando:

```
git --version
```

Se o Git já estiver instalado, você verá uma saída como esta (embora sua versão possa ser diferente):

```
git version 2.36.0
```

Se o Git não estiver instalado, você verá um prompt perguntando se deseja instalá-lo. Responda "Sim" e siga as instruções de instalação. Outra opção é seguir as instruções de download e instalação [aqui](https://git-scm.com/download/mac).

### Instalação do Git no Linux

Para instalar o Git em um sistema Linux como um pacote binário, você pode usar o gerenciador de pacotes da distribuição. Se você tiver o Fedora (ou uma distribuição parecida como RHEL ou CentOS), pode usar dnf:

```
sudo dnf install git-all
```

Se tiver uma distribuição baseada em Debian, como Ubuntu, use apt:

```
sudo apt install git
```

Após a instalação, abra uma janela do terminal e digite o seguinte comando:

```
git --version
```

Se o Git já estiver instalado, você verá uma saída como esta (embora sua versão possa ser diferente):

```
git version 2.36.0
```

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-52-239Z.md
### Última modificação: 2025-05-28 19:26:53

# GitHub - TripleTen

Capítulo 4/8

Git e GitHub

# GitHub

Se você já usou um mecanismo de pesquisa na internet para solucionar qualquer tipo de problema técnico relacionado a computadores ou programação, provavelmente já se deparou com o [GitHub](https://github.com/). O GitHub é uma plataforma onde as pessoas podem desenvolver, hospedar e compartilhar código usando o controle de versão do Git. Ele é usado extensivamente para hospedar software de código aberto e outros projetos, para que qualquer pessoa possa usar o código de outros ou até mesmo fazer as próprias contribuições. Outros serviços semelhantes incluem sites como [GitLab](https://about.gitlab.com/) e [GitBucket](https://gitbucket.github.io/), mas vamos focar no GitHub aqui, porque atualmente ele é a opção mais usada.

Nesta lição, você vai aprender algumas informações básicas sobre o GitHub, como ele difere do Git, como criar uma conta e alguns conceitos básicos sobre a segurança de conta na plataforma.

### História do GitHub

O GitHub foi totalmente lançado em 2008, não muito depois da introdução do Git em 2005. Na época, o Git não era tão popular quanto é hoje. Na verdade, foi o lançamento do GitHub e sua subsequente adoção como ferramenta pelos desenvolvedores que coincidiu com o rápido aumento da popularidade do Git.

Desde o lançamento há mais de 14 anos, o GitHub se tornou um de, se não _o_, maior host público de código-fonte do mundo. Muitos desenvolvedores individuais ou de pequenas equipes usam o GitHub de graça para gerenciar e compartilhar projetos com segurança. Organizações maiores podem pagar por diferentes planos de assinatura premium que oferecem vários recursos adicionais, que é uma maneira de o GitHub ganhar dinheiro.

### Git e GitHub

É fácil confundir o Git, o _software_ de controle de versão, com o GitHub, o _serviço web_ para hospedagem e compartilhamento de código-fonte que usa o controle de versão do Git. No entanto, como mencionamos antes, o Git e o GitHub não são a mesma coisa; eles nem foram desenvolvidos pelas mesmas pessoas. Você pode pensar no GitHub como uma espécie de extensão que interage com o Git para oferecer funcionalidades adicionais.

Você não precisa do GitHub para usar o Git: é possível instalar o Git no computador pessoal e usá-lo para gerenciar um projeto com controle de versão sem precisar se conectar à internet. O GitHub oferece um local para fazer backup e compartilhar seu projeto com outras pessoas, oferecendo um tipo de serviço "Git na nuvem".

Existem outras diferenças técnicas entre o Git e o GitHub, mas o importante para você saber é o seguinte: o Git é um software, o GitHub é um serviço, e os dois produtos são distintos um do outro.

### Criar uma conta no GitHub

Para começar a usar o GitHub, primeiro você precisa criar uma conta gratuita. Abaixo está um guia passo a passo sobre como criar e verificar sua conta. Você também verá uma breve visão geral do painel do GitHub após fazer login na sua conta. Observe que as imagens que você vê são do GitHub em junho de 2022. Se você já tem uma conta do GitHub, pode pular esta seção.

1.  Vá para a página inicial do GitHub [aqui](https://github.com/):

![](https://practicum-content.s3.amazonaws.com/resources/github_home_1695108718.png)

2.  Clique no botão "Sign up" no canto superior direito da janela para ir para esta página onde você pode inserir seu endereço de e-mail para se inscrever:

![](https://practicum-content.s3.amazonaws.com/resources/github_signup_1695108748.png)

3.  Verifique se há um link de verificação na sua caixa de entrada de e-mail e clique nele. Digite uma senha e nome de usuário, decida se deseja receber e-mails sobre os produtos do GitHub e confirme que você é humano:

![](https://practicum-content.s3.amazonaws.com/resources/github_verify_1695108778.png)

4.  Vá até às opções de personalização, se desejar, ou pule essa parte da criação da conta:

![](https://practicum-content.s3.amazonaws.com/resources/github_personalization_1695108806.png)

5.  Seu primeiro login na sua nova conta deve levar a um dashboard inicial parecido com este:

![](https://practicum-content.s3.amazonaws.com/resources/github_initial_dashboard_1695108846.png)

Nas próximas lições, vamos criar um repositório (essencialmente um projeto que usa o controle de versão do Git), usar comandos do Git para gerenciar o repositório e usar o GitHub para hospedar e compartilhar os arquivos que criamos.

### Dar acesso de segurança à conta

Recomendamos que você use a linha de comando para gerenciar o Git e o GitHub. Isso vai te ensinar algumas habilidades valiosas com o passar do tempo. Para instalar o Git para que ele possa ser gerenciado através da linha de comando, [siga as seguintes etapas](https://docs.github.com/pt/get-started/quickstart/set-up-git) da documentação oficial:

-   [Baixe e instale a última versão do Git](https://git-scm.com/downloads)
-   [Defina seu nome de usuário no Git](https://docs.github.com/pt/get-started/getting-started-with-git/setting-your-username-in-git)
-   [Defina seu endereço de e-mail de commit no Git](https://docs.github.com/pt/account-and-profile/setting-up-and-managing-your-personal-account-on-github/managing-email-preferences/setting-your-commit-email-address)

### Dar acesso de segurança à conta

Agora, precisamos ativar um token de segurança na instalação do Git para que o GitHub aceite as nossas alterações.

Para isso, vamos configurar o token pessoal. [Siga estas etapas](https://docs.github.com/pt/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens):

-   [Verifique seu endereço de e-mail](https://docs.github.com/pt/get-started/signing-up-for-github/verifying-your-email-address), se ainda não fez isso.
-   No canto superior direito de qualquer página do site do GitHub, clique na sua foto de perfil e depois clique em **Settings** (Configurações).
-   Na barra lateral esquerda, clique em **Developer settings** (Configurações do desenvolvedor) e em **Personal access tokens** (Tokens de acesso pessoal).
-   Clique em **Generate new token** (Gerar um novo token) e **give your token a name** (Dê um nome a seu token).
-   Dê ao token todas as permissões selecionando a opção **all permissions**, a menos que você tenha um motivo para limitar o acesso. Também indique uma data de validade com **Expiration date** para o token, se quiser.
-   Clique em **Generate token** (Gerar o token).

Você vai usar esse token como uma senha mais adiante quando trabalhar com o Git a partir da linha de comando. Por exemplo, na próxima lição, você vai aprender a clonar um repositório. Em poucas palavras, é bem fácil:

```
$ git clone https://github.com/username/repo
```

`username` se refere ao proprietário de um repositório, e `repo` é o nome do repositório que você vai clonar.

Após a execução, será solicitado que você digite seu nome de usuário e senha:

```
Username:seu_nome_de_usuário
Password:seu_token
```

Após o login, o Git vai começar o processo de clonagem.

### GitHub Desktop

Você vai aprender a usar a CLI para trabalhar com Git e GitHub. Entretanto, o GitHub também oferece uma boa ferramenta GUI chamada [GitHub Desktop](https://desktop.github.com/), que permite usar o Git e o GitHub para gerenciar projetos. Se preferir, baixe e explore essa ferramenta e use-a em vez da linha de comando.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-54-546Z.md
### Última modificação: 2025-05-28 19:26:55

# Repositórios - TripleTen

Capítulo 4/8

Git e GitHub

# Repositórios

### O que é um repositório

![](https://practicum-content.s3.amazonaws.com/resources/Artboard_2-100_1695109033.jpg)

Um repositório é o conceito de uma "pasta de projeto" no controle de versão: muitos arquivos pertencentes a um único projeto são agrupados em uma única pasta. Essa pasta é o **repositório**, e o git rastreia o histórico de todo o **repositório** no fluxo de trabalho.

Por exemplo, o código-fonte de todo o projeto linux é [um repositório que pode ser encontrado aqui](https://github.com/torvalds/linux). Da mesma forma, [o próprio git é um repositório git](https://github.com/git/git).

### Criar um repositório no GitHub

Para criar um novo repositório, basta pressionar o botão verde "new" (novo) no canto superior esquerdo da página inicial da sua conta do GitHub.

A partir daí, você terá que nomear o repositório. Recomendamos que você o inicie com um arquivo `README`:

![](https://practicum-content.s3.amazonaws.com/resources/4.5_1695289729.png)

Também recomendamos que você inclua o arquivo `.gitignore` padrão para projetos Python. Isso diz ao Git para ignorar certos tipos de arquivo, como arquivos de checkpoint de notebook python e arquivos de cache de código.

### Clonar um repositório localmente do GitHub

Clonar (`clone`) um repositório é fazer uma cópia local dele no seu computador. Você pode fazer isso na linha de comando digitando o comando `git clone` com a URL de qualquer repositório do GitHub. Por exemplo, aqui está como clonamos o repositório em que o código do nosso querido pacote NumPy está armazenado:

```
git clone https://github.com/numpy/numpy
```

Você já deve saber o que é um pacote. Após executar o comando acima, o repositório que agora temos localmente contém todos os módulos e arquivos que pertencem ao pacote NumPy.

Você pode então modificar a cópia local. No entanto, se você não possui o repositório no GitHub, não vai poder compartilhar com facilidade suas alterações com a cópia do proprietário. Por exemplo, imagine que você acha que encontrou um bug ao trabalhar com NumPy. Se você sabe como corrigi-lo e quer implementar essa alteração, siga as seguintes etapas.

Para isso, recomendamos primeiro bifurcar o repositório (`fork`) para criar sua própria cópia. Isso pode ser feito usando um botão no canto superior direito da [página do repositório](https://github.com/numpy/numpy):

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_2_1695109139.png)

Após clicar nesse botão, você vai ter uma cópia separada do código (que pode diferir a partir do momento em que você fez a bifurcação). Você verá isso pela tag "forked from" (bifurcado de) no canto superior esquerdo do repositório.

Ao bifurcar um repositório, pode usar a URL desse repositório bifurcado que você possui. Essa URL tem sua conta de usuário, que assume o seguinte formato:

```
https://github.com/YOUR_USERNAME/REPOSITORY_YOU_FORKED
```

Após a bifurcação, você pode usar git clone para clonar a cópia do código que você possui. E como agora você possui esse repositório no GitHub, vai poder enviar facilmente suas modificações de volta para sua versão no GitHub _confirmando_ e _enviando_ o código, como veremos na próxima lição.

Pergunta

Se você quiser contribuir para um repositório existente, qual é a sequência correta de etapas?

Primeiro, clonar o repositório e depois bifurcá-lo. Após bifurcar, podemos começar a trabalhar com ele.

Primeiro, precisamos bifurcar o repositório para criar nossa própria cópia, depois cloná-la para obter uma cópia local; feito isso, podemos começar a trabalhar com ela.

Isso mesmo! Essa é a ordem correta.

Não há necessidade de clonar e bifurcar para contribuir com um repositório já existente.

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-55-838Z.md
### Última modificação: 2025-05-28 19:26:56

# Confirmar alterações - TripleTen

Capítulo 4/8

Git e GitHub

# Confirmar alterações

O fluxo de trabalho com o Git é basicamente o seguinte:

1.  Bifurque o repositório do seu interesse.
2.  Baixe a versão mais recente do seu código usando `git clone [URL DO REPOSITÓRIO]` para um novo repositório. Se você já clonou um repositório, mas ele não está atualizado, você pode baixar as últimas alterações para seu computador usando o comando `git pull`.
3.  Em seguida, trabalhe e **edite** os arquivos localmente.
4.  Após editar os arquivos, você **prepara** as alterações usando o comando `git add`. Durante a **preparação**, é encontrada a diferença entre seu trabalho e o estado original do repositório, e apenas as partes que você modificou são extraídas. Essas alterações são "preparadas" para enviar apenas seu trabalho, e não o repositório inteiro, de volta para o servidor do Git.
5.  **Confirme** alterações no histórico do repositório com `git commit`. Isso grava suas alterações no histórico do repositório para sempre. Já que as confirmações são rastreadas no histórico, elas podem ser desfeitas ou empilhadas umas sobre as outras.
6.  **Envie** as alterações de volta para o GitHub com `git push`. Isso substitui o que está no servidor do GitHub com as alterações que você confirmou. Uma vez concluída essa etapa, seu computador e o servidor do GitHub vão estar sincronizados.

Visualmente, é assim:

![](https://practicum-content.s3.amazonaws.com/resources/26_07_DA_DS-10_1695382093.png)

Vamos seguir todas essas etapas fazendo uma pequena modificação no arquivo `README` do pacote [NumPy](https://github.com/numpy/numpy).

### Bifurcação e download

Primeiro, faça login na sua conta do GitHub e acesse o repositório oficial do NumPy neste endereço:

```
https://github.com/numpy/numpy
```

Faça uma bifurcação do repositório NumPy para sua conta usando o botão no canto superior direito.

Agora vamos clonar o repositório usando `clone`. Primeiro, abra um prompt de comando git (usando o Git CMD instalado anteriormente nesta seção do sprint_)_ e digite o seguinte:

```
git clone https://github.com/{MY_USER_NAME}/numpy
```

Aqui, `{MY_USER_NAME}` deve ser substituído pelo seu nome de usuário do GitHub. O URL geral deve corresponder ao que foi aberto no seu navegador quando você clicou no botão de bifurcação no repositório NumPy original.

Depois, altere o diretório na sua linha de comando para seu repositório NumPy com `cd`:

`cd numpy`

E agora que estamos trabalhando nesse repositório, vamos usar `pull` para acessar as últimas alterações e garantir que estamos atualizados:

`git pull`

Seu repositório deve estar atualizado, pois você acabou de baixá-lo. Nesse caso, o Git informaria `Already up to date.` ("Já está atualizado.")

### Editar arquivos

Abra o arquivo `README.md` no repositório NumPy que você acabou de clonar em seu computador com o editor de texto da sua preferência. Insira a seguinte linha na parte superior do arquivo:

```
Estou feliz por estar fazendo a minha primeira edição!
```

Salve o arquivo `README` e feche-o. Fique à vontade para fazer outras alterações no repositório (adicionar ou excluir arquivos, modificar outros arquivos, etc.). As mudanças não importam por enquanto, o que importa é que modifiquemos o repositório de alguma forma.

### Área de preparo

Agora que você alterou alguns arquivos no seu repositório, o Git precisa estar ciente de que os arquivos existentes foram alterados. A função `git add` adiciona os arquivos alterados ao "ambiente de preparação".

Você pode fazer isso usando o comando:

`git add .`

Execute esse comando na pasta raiz do seu repositório. O `.` implica "tudo aqui", então isso vai preparar tudo no repositório, porque usamos `cd numpy` antes para apontar nossa linha de comando para a pasta raiz do repositório.

### Confirmar as alterações

Agora que os arquivos alterados estão preparados, podemos confirmar (`commit`) permanentemente as alterações no histórico do repositório.

Cada confirmação registra um instantâneo do estado atual da sua versão do repositório, com o nome do confirmador, uma marca temporal e uma mensagem.

```
git commit -m "add notes file"
```

Confirmamos a nova versão do repositório.

**OBSERVAÇÃO:** Uma mensagem de confirmação é necessária! O sinalizador `-m` no comando adiciona a mensagem na linha de comando. Se você não incluir uma mensagem, o Git vai abrir um editor de texto na sua linha de comando para você inserir uma mensagem.

Observe também que o sinalizador `-am` adiciona e confirma ao mesmo tempo. Portanto, o comando `git commit -am "add notes file"` seria equivalente a:

```
git add .
git commit -m "add notes file"
```

Tente fazer várias confirmações pequenas e não confirmações grandes e infrequentes. Mudanças pequenas são mais fáceis de revisar e unir nas principais ramificações "upstream" online.

### Enviar alterações

![](https://practicum-content.s3.amazonaws.com/resources/Artboard_1_copy_3-100_1695109233.jpg)

Se você possui o repositório em que está trabalhando no GitHub, após confirmar corretamente as alterações, você vai poder iniciar o comando `git push`. Isso envia as alterações confirmadas do seu computador para a versão do repositório online no GitHub.

Você precisa se conectar à sua conta do GitHub em sua linha de comando e enviar para um repositório que sua conta possui para que o site aceite as alterações.

Caso ainda não tenha feito isso na [lição 4](https://tripleten.com/trainer/data-scientist/lesson/634090a2-f2d1-4c79-ba9c-d67b908ed3b3/), será necessário criar um [Token de Acesso Pessoal](https://docs.github.com/pt/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens) no seu computador vinculado à sua conta do GitHub. Isso garante acesso seguro para o GitHub aceitar as alterações que você está enviando.

Pergunta

Qual é o comando para informar ao Git que arquivos existentes foram modificados?

`git add .`

Exatamente! Isso é o que chamamos de preparação.

`git commit -m "change in the read.me file”`

`git push`

Trabalho maravilhoso!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-57-190Z.md
### Última modificação: 2025-05-28 19:26:57

# Conclusão - TripleTen

Capítulo 4/8

Git e GitHub

# Conclusão

![](https://practicum-content.s3.amazonaws.com/resources/Artboard_1_copy_5-100_1695109369.jpg)

O Git é uma ferramenta muito potente que nos permite colaborar e acompanhar todas as alterações feitas. Honestamente, não conseguimos pensar em alguma empresa que tenha vários desenvolvedores, mas não usa o Git em seu trabalho.

Agora você tem muito mais preparo para fazer parte de uma equipe e contribuir com qualquer projeto! Neste capítulo, você aprendeu como:

-   Usar o comando `clone` para clonar um repositório e começar a trabalhar em um projeto
-   Usar o comando `add` para adicionar mudanças no ambiente de preparação antes que elas sejam confirmadas no histórico do repositório
-   Usar o comando `commit` para confirmar alterações em sua ramificação local e enviá-las para o repositório remoto usando `push`
-   Usar o comando `pull` para enviar alterações remotas e integrá-las na sua ramificação atual

Em seguida, vamos nos aprofundar no Python e aprimorar bastante suas habilidades.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-26-59-676Z.md
### Última modificação: 2025-05-28 19:27:00

# Introdução - TripleTen

Capítulo 5/8

Python intermediário

# Introdução

![](https://practicum-content.s3.amazonaws.com/resources/Introduction_1695281740.jpg)

Neste capítulo, vamos apresentar algumas técnicas gerais de engenharia de software e do Python que não costumam ser abordadas em cursos introdutórios, mas que são usadas com frequência por profissionais.

A lista de tópicos inclui, mas não se limita, ao seguinte:

-   estrutura de projetos no Python
-   tratamento de erros
-   funções, argumentos nomeados e anotações de tipo
-   programação orientada a objetos
-   download, leitura e gravação de arquivos
-   APIs e aplicativos da web
-   qualidade de código e documentação

É importante se familiarizar com esses tópicos se você quiser ter mais eficácia e autonomia como analista ou cientista de dados, ou, pelo menos, se comunicar de maneira efetiva com seus futuros colegas de equipes de engenharia de software.

Aprender tudo isso não será fácil. Se fosse, todo mundo estaria fazendo isso, certo? Como de costume, este capítulo não será um recurso exaustivo para aprender sobre esses tópicos, mas vamos ajudar você a entender o essencial!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-27-00-996Z.md
### Última modificação: 2025-05-28 19:27:01

# Adição de flexibilidade a scripts Python - TripleTen

Capítulo 5/8

Python intermediário

# Adição de flexibilidade a scripts Python

Nesta lição, vamos aprender a criar aplicativos Python usando a interface de linha de comando (CLI), ou seja, aplicativos cujo comportamento pode ser modificado passando diferentes argumentos e opções de linha de comando.

![](https://practicum-content.s3.amazonaws.com/resources/lesson1_1695281786.jpg)

## Scripts

Existem várias maneiras de usar o Python. Executar scripts na linha de comando é uma das coisas que vai fazer com que seus amigos acreditem que você é expert em programação. Um programa Python é um script (ou um conjunto de scripts) salvo como um arquivo de texto separado para reutilização futura. Você já encontrou scripts antes, no [capítulo sobre ambientes de programação](https://tripleten.com/trainer/data-analyst/lesson/26635ad1-f779-42e1-b52c-954c47186233/), em que escrevemos um script simples para contar o número de núcleos na CPU.

Agora, vamos estudar outro script de exemplo chamado `image_rotator.py`. Neste script, vamos usar uma nova biblioteca com a qual você ainda não trabalhou: a `PIL`. `PIL` é uma abreviação para Python Imaging Library (biblioteca de imagens Python), e essa biblioteca é bastante usada por profissionais no trabalho com dados de imagem. No nosso exemplo, vamos usar o módulo `Image` da biblioteca `PIL` para ler um arquivo de imagem, imprimir seu tamanho, rodar a imagem e salvá-la rodada em um novo arquivo.

```
# image_rotator.py

from PIL import Image # importando o módulo Image da biblioteca PIL

# carregando uma imagem chamada 'tripleten_logo.jpeg'.
im = Image.open('tripleten_logo.jpeg')

# obtendo o tamanho da imagem usando o atributo .size e o imprimindo
print(im.size)

# rodando a imagem 90 graus em sentido anti-horário
rotated = im.rotate(90)

# salvando a imagem rodada
rotated.save('rotated.png')
```

Aqui está a imagem lida pelo script acima, se você quiser conferir sua representação visual. É o nosso logotipo:

![](https://practicum-content.s3.amazonaws.com/resources/tripleten_logo_1695281813.jpeg)

Você já sabe como criar um script Python. Se você salvar o código acima em um arquivo chamado `image_rotator.py` e o executar na interface de linha de comando, talvez receba uma mensagem de erro dizendo `ModuleNotFoundError: No module named 'PIL'`. Essa mensagem indica que a biblioteca `PIL` não está instalada no seu sistema. Se você quiser instalá-la, siga as [instruções de linha de comando](https://pillow.readthedocs.io/en/stable/installation.html#basic-installation) _(os materiais estão em inglês)_ fornecidas, que vão explicar o processo dependendo do seu sistema. Em poucas palavras, ela pode ser instalada executando estes dois comandos:

```
python3 -m pip install --upgrade pip
python3 -m pip install --upgrade Pillow
```

Após instalar a biblioteca, execute o script Python em um prompt de linha de comando. Se precisar refrescar a memória, confira [esta lição](https://tripleten.com/trainer/data-analyst/lesson/29a927cb-aa5d-4ed8-8f0d-310ecb30434d/). Se tanto o script quanto o arquivo de imagem estiverem localizados no diretório atual, execute o script da CLI assim:

```
python image_rotator.py
```

e aqui está o resultado exibindo o tamanho da imagem:

```
(1110, 694)
```

`1110` é a largura e `694` é a altura da imagem `'tripleten_logo.jpeg'`. Além disso, o nosso script Python salvou a imagem rodada em 90 graus.

Escrevemos um script Python e o executamos no computador usando a linha de comando. No entanto, esse programa é muito inflexível para uso geral. E se quisermos processar um arquivo de imagem diferente ou girar a imagem em outro ângulo? É aí que entram os argumentos e as opções.

### Argumentos

Argumentos de linha de comando são parecidos com as funções Python. É possível passá-los para o script _posicionalmente_ ao incluí-los na instrução de execução da linha de comando, e os valores dos argumentos que passamos são usados no script.

O nome do arquivo de entrada, o ângulo de rotação da imagem e o nome do arquivo de saída são ótimos candidatos para se tornarem argumentos no script. Podemos usar a biblioteca integrada de Python `argparse` para criar esses argumentos.

```
# image_rotator.py

from PIL import Image
import argparse # importar o módulo argparse

# iniciar o analisador
parser = argparse.ArgumentParser()

# adicionar os argumentos com os nomes correspondentes
parser.add_argument('input_file')
parser.add_argument('output_file')
parser.add_argument('angle', type=int)

# analisar os argumentos
args = parser.parse_args()

# carregar uma imagem do argumento input_file
im = Image.open(args.input_file)

# exibir tamanho da imagem
print(im.size)

# girar a imagem em um ângulo indicado como um dos argumentos
rotated = im.rotate(args.angle)

# salvar a imagem rodada usando o caminho de saída do argumento output_file
rotated.save(args.output_file)
```

Agora que o nosso script aceita argumentos, veja como os passamos para o script enquanto o executamos da linha de comando:

```
$ python image_rotator.py tripleten_logo.jpeg output.png 180
```

```
(694, 1110)
```

Começamos iniciando um analisador. Depois, criamos três argumentos: `input_file`, `output_file` e `angle`. Observe que tivemos que passar os argumentos na ordem em que os analisamos no script.

Por padrão, argumentos são passados para o script como strings. Se especificarmos `type=int` para o argumento do ângulo de rotação, isso nos obriga a passar um número inteiro como valor para esse argumento; caso contrário, receberemos um erro da linha de comando assim que tentarmos executar o script.

Por fim, `args` armazena todos os argumentos analisados como atributos: `args.input_file`, `args.output_file` e `args.angle`.

Com esses argumentos, podemos usar o mesmo script para qualquer arquivo de imagem e ângulo de rotação.

### Opções

Junto com argumentos, podemos passar opções para o script. Opções são uma espécie de argumentos opcionais que podemos incluir para mudar como o programa funciona. Assim como argumentos nomeados das funções Python, opções podem ser passadas em qualquer ordem.

Opções têm formas longas e curtas. Normalmente, para uma determinada opção, o nome completo é precedido por dois hifens, e a forma abreviada usa apenas a primeira letra e um único hífen: `--option` e `-o`.

Uma opção pode ser seguida por um valor separado por `=` ou por apenas um espaço: `--option=value` e `--option value` ou `-o=value` e `-o value`.

Uma opção pode ter um valor padrão, o que é útil quando não queremos tornar a opção obrigatória. Um valor padrão é especificado com o parâmetro `default=` em `add_argument()`.

A maneira como analisamos opções não é muito diferente dos argumentos posicionais. Seus valores podem ser obtidos como atributos do mesmo objeto `args`, e ambas as formas da mesma opção (longa e curta) compartilham o mesmo valor:

```
# image_rotator.py

from PIL import Image
import argparse

# iniciar o analisador
parser = argparse.ArgumentParser()

# adicionar os argumentos com os nomes correspondentes
parser.add_argument('input_file')
parser.add_argument('output_file')
# observe que abaixo usamos opções e o valor padrão para esta opção
parser.add_argument('--angle', '-a', type=int, default=90)

# analisar os argumentos
args = parser.parse_args()

# carregar uma imagem do argumento input_file 
im = Image.open(args.input_file)

# exibir tamanho da imagem
print(im.size)

# girar a imagem em um ângulo indicado como um dos argumentos
rotated = im.rotate(args.angle)

# salvar a imagem rodada usando o caminho de saída do argumento output_file
rotated.save(args.output_file)
```

Veja como passamos um valor para a opção no nosso script:

```
$ python image_rotator.py tripleten_logo.jpeg --angle 180 output.png
```

```
(694, 1110)
```

### Sinalizadores

As opções costumam ser usadas para introduzir sinalizadores, ou seja, opções booleanas especiais que só podem possuir valores `True`/`False` ("ativado" ou "desativado").

Podemos criar sinalizadores da mesma maneira que criamos outras opções, exceto que incluímos o parâmetro `action=` em `add_argument()`. Se definirmos `action='store_true'`, então o sinalizador será definido como `False` por padrão. Para ativar o sinalizador (definir como `True`), basta passá-lo para o script da linha de comando. Já que os sinalizadores só podem estar "ativados" ou "desativados", não precisamos indicar valores de sinalizadores quando os passamos para o script.

Aqui está um exemplo de um sinalizador `--info` que controla se exibimos a informação sobre o tamanho da imagem ou não:

```
# image_rotator.py

from PIL import Image
import argparse

# iniciar o analisador
parser = argparse.ArgumentParser()

# adicionar os argumentos com os nomes correspondentes
parser.add_argument('input_file')
parser.add_argument('output_file')
parser.add_argument('--angle', '-a', type=int, default=90)
# observe que adicionamos o sinalizador --info (ou apenas -i) abaixo
parser.add_argument('-i', '--info', action='store_true')

# analisar os argumentos
args = parser.parse_args()

# carregar uma imagem do argumento input_file
im = Image.open(args.input_file)

# só exibir o tamanho da imagem se o sinalizador --info for definido como True
if args.info:
    print('dimensões da imagem de entrada:', im.size)

# girar
rotated = im.rotate(args.angle)

# salvar
rotated.save(args.output_file)
```

Incluímos `action='store_true'`, então o sinalizador vai estar desativado, a menos que o passemos explicitamente para o script. Vamos executar o script com o sinalizador:

```
python image_rotator.py tripleten_logo.jpeg --angle 180 output.png -i
```

```
input image dimensions: (694, 1110)
```

### Mensagens de ajuda

Todo script Python executado da linha de comando tem uma opção integrada `--help` (ou `-h`). Podemos adicionar mensagens de ajuda úteis ao script usando o parâmetro `help=` no método `add_argument()` de `argparse`.

Vamos adicionar algumas mensagens úteis ao script:

```
# image_rotator.py

from PIL import Image
import argparse

# iniciar o analisador
parser = argparse.ArgumentParser()

# adicionar os argumentos com os nomes correspondentes
# observe que agora usamos o parâmetro help
parser.add_argument('input_file', help='input file path')
parser.add_argument('output_file', help='output file path')
parser.add_argument('--angle', '-a', type=int, default=90, help='rotação em sentido anti-horário (graus)')
parser.add_argument('-i', '--info', action='store_true', help='exibir tamanho da imagem')

# analisar os argumentos
args = parser.parse_args()

# carregar uma imagem do argumento input_file
im = Image.open(args.input_file)

# exibir o tamanho da imagem apenas se o sinalizador info estiver definido como True
if args.info:
    print('dimensões da imagem de entrada:', im.size)

# girar
rotated = im.rotate(args.angle)

# salvar
rotated.save(args.output_file)
```

Agora vamos visualizar as informações úteis usando a opção `-h`:

```
$ python image_rotator.py -h
```

```
usage: image_rotator.py [-h] [--angle ANGLE] [-i] input_file output_file

positional arguments:
  input_file   input file path
  output_file  output file path

options:
  -h, --help   show this help message and exit
  --angle ANGLE, -a ANGLE   
               counterclockwise rotation (degrees)
  -i, --info   display image size
```

### Combinação de opções curtas

Por fim, há mais um truque que queremos mencionar sobre como passar opções para um script. Se quisermos passar mais de uma opção, podemos combinar as formas curtas delas.

Para um script arbitrário chamado `myscript.py` e opções arbitrárias `-a` , `-b` e `-c`, você pode passá-las individualmente, da seguinte forma:

```
$ python myscript.py -a -b -c
```

Ou podemos combiná-las deste jeito:

```
$ python myscript.py -abc
```

Pergunta

Imagine que precisamos trabalhar com o seguinte script Python:

```
# resizer.py

from PIL import Image
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('input_file', help='input file path')

im = Image.open(args.input_file)
im.close()
```

À primeira vista, ele pode parecer inútil, já que apenas abre uma imagem e não faz nada. Vamos modificar esse script de maneira que ele não apenas leia uma imagem, mas também a redimensione e salve como um novo arquivo. Para isso, vamos usar a biblioteca `PIL`.

Nosso objetivo é modificar o script de forma que:

-   Ele use `width` e `height` como argumentos posicionais e os coloque no método `im.resize(new_size)` como uma tupla `(width, height)`;
-   Ele use `output_file` como um argumento posicional e o use como um caminho para salvar a imagem resultante.

O uso para o script será o seguinte: `$ python resizer.py input_image.png output_image.png 120 80`

Como vai ficar esse novo script modificado?

Opção #1:

```
# resizer.py

from PIL import Image
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('input_file', help='input file path')
parser.add_argument('output_file', help='output file path')
parser.add_argument('width', help='largura desejada', type=int)
parser.add_argument('height', help='altura desejada', type=int)
args = parser.parse_args()

im = Image.open(args.input_file)
new_size = (args.width, args.height)
resized = im.resize(new_size)
resized.save(args.output_file)
im.close()
```

Isso mesmo! Esse script faz exatamente o que queremos.

Opção #2:

```
# resizer.py

from PIL import Image
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('input_file', help='input file path')
parser.add_argument('width', help='largura desejada', type=int)
parser.add_argument('height', help='altura desejada', type=int)
args = parser.parse_args()

im = Image.open(args.input_file)
new_size = (args.width, args.height)
resized = im.resize(new_size)
im.close()
```

Opção #3:

```
# resizer.py

from PIL import Image
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('input_file', help='input file path')
parser.add_argument('output_file', help='output file path')
parser.add_argument('width', help='largura desejada', type=int)
parser.add_argument('height', help='altura desejada', type=int)
args = parser.parse_args()

im = Image.open(args.input_file)
im.save(args.output_file)
im.close()
```

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-27-02-300Z.md
### Última modificação: 2025-05-28 19:27:02

# Funções - TripleTen

Capítulo 5/8

Python intermediário

# Funções

Você já aprendeu sobre funções no sprint básico de Python. O objetivo desta lição é duplo:

1.  Fazer uma recapitulação dos principais conceitos relacionados a funções, como escopo de variáveis e diferentes tipos de argumentos.
2.  Apresentar o conceito de dicas de tipo, sua finalidade e uso.

![](https://practicum-content.s3.amazonaws.com/resources/4.4.2PT_1695290095.png)

# Recapitulação das funções

## Escopo de variáveis

O escopo de uma variável é a área (ou as áreas) de um programa em que essa variável é acessível. Precisamos estar cientes de três escopos ao programar em Python:

-   **Escopo local**: se você se referir a uma variável em uma função, o interpretador primeiro vai procurar o nome dela na definição da função.
-   **Escopo global**: se o interpretador não encontrar a variável no escopo local, ele a procura no escopo global (ou seja, nas partes do programa fora das funções).
-   **Escopo integrado**: por fim, se o interpretador não encontrar a variável no escopo local ou global, vai procurar no escopo integrado de Python.

Vamos demonstrar cada tipo de escopo com alguns exemplos.

### Escopo local

Vamos criar um script que inclui uma função e algumas variáveis:

```
# my_func.py
product_price = 12.0
product_qty = 5

def total_price():
    product_price = 11.0
    product_qty = 4
    print(f'Preço total: {product_price * product_qty}')

total_price()
```

```
$ python my_func.py
```

```
Preço total: 44.0
```

Observe que há variáveis `product_price` e `product_qty` tanto dentro da função quanto fora dela. Ao executar um script, o interpretador usa os valores de `product_price` e `product_qty` do escopo local (dentro da função), mesmo que essas variáveis existam também no escopo global (fora da função).

### Escopo global

```
# my_func.py
product_price = 12.0
product_qty = 5

def total_price():
    print(f'Preço total: {product_price * product_qty}')

total_price()
```

```
$ python my_func.py
```

```
Preço total: 60.0
```

As variáveis `product_price` e `product_qty` não estão definidas no escopo local da função, então o interpretador passa para o escopo global de `my_func.py` para encontrá-las.

### Escopo integrado

```
# my_func.py
product_price = 12.0
product_qty = 5

def total_price():
    print(f'Preço total: {price * qty}')

total_price()
```

```
$ python my_func.py
```

```
...

NameError: name 'price' is not defined
```

Nem `price` nem `qty` existem no escopo integrado, global ou local, então o interpretador gera um erro. A documentação do Python contém as listas completas de nomes para [funções integradas](https://docs.python.org/pt-br/3/library/functions.html) e [constantes integradas](https://docs.python.org/pt-br/3/library/constants.html).

Para evitar problemas com escopo no seu código, é uma prática recomendada escrever funções de forma que elas não dependam de variáveis definidas fora da função. Se sua função precisa usar uma variável que não é um parâmetro da função, tente criar uma variável dentro do corpo da função.

## Tipos de argumentos de funções

### Argumentos posicionais

Argumentos posicionais são argumentos de uma função que devem ser passados em uma ordem específica. Ao chamarmos uma função definida com argumentos posicionais, precisamos passar os valores desses argumentos exatamente na mesma ordem em que eles foram definidos.

```
# my_func.py
def trip_price(dist_miles, mpg, price, loc_from, loc_to):
        total_price = dist_miles * price / mpg
    print(f'Uma viagem de {loc_from} a {loc_to} custa ${total_price}')

trip_price(409, 35, 5.1, 'A', 'B')
```

```
$ python my_func.py
```

```
Uma viagem de A a B custa $ 59.59714285714285
```

Caso contrário, vamos obter um erro ou, dependendo da definição e uso da função, um valor de retorno incorreto. No exemplo acima, `409` é `dist_miles`, `35` é `mpg` e `5.1` é o preço `price`.

Se alterarmos a ordem, provavelmente teremos um erro:

```
# my_func.py
def trip_price(dist_miles, mpg, price, loc_from, loc_to):
    total_price = dist_miles * price / mpg
    print(f'Uma viagem de {loc_from} a {loc_to} custa ${total_price}')

trip_price('A', 'B', 409, 35, 5.1)
```

```
$ python my_func.py
```

```
...

TypeError: unsupported operand type(s) for /: 'str' and 'str'
```

### Argumentos nomeados

Podemos chamar uma função usando argumentos nomeados. Nesse caso, precisamos passar os nomes dos argumentos com seus valores. Isso exige escrever um pouco mais de código para chamar uma função, mas também oferece a flexibilidade de passar argumentos em qualquer ordem.

```
# my_func.py
def trip_price(dist_miles, mpg, price, loc_from, loc_to):
    total_price = dist_miles * price / mpg
    print(f'Uma viagem de {loc_from} a {loc_to} custa ${total_price}')

trip_price(dist_miles=409, loc_from='A', loc_to='B', mpg=35, price=5.1)
```

```
$ python my_func.py 
```

```
Uma viagem de A a B custa $ 59.60
```

Veja como chamamos a função:

```
trip_price(dist_miles=409, loc_from='A', loc_to='B', mpg=35, price=5.1)
```

Agora ela tem um argumento e seu valor correspondente ao lado dele.

### Misturar argumentos posicionais e nomeados

Ao chamar uma função, podemos misturar argumentos posicionais e nomeados desde que comecemos com os argumentos posicionais.

Colocar qualquer argumento nomeado antes de um posicional gera um erro de sintaxe `SyntaxError`:

```
# my_func.py
def trip_price(dist_miles, mpg, price, loc_from, loc_to):
    total_price = dist_miles * price / mpg
    print(f'Uma viagem de {loc_from} a {loc_to} custa ${total_price}')

trip_price(loc_from='Boston', loc_to='Nova York', price=5.1, 409, 35)
```

```
$ python my_func.py
```

```
...

SyntaxError: positional argument follows keyword argument
```

Se passarmos primeiro todos os argumentos posicionais, o script será executado sem erros:

```
# my_func.py
def trip_price(dist_miles, mpg, price, loc_from, loc_to):
    total_price = dist_miles * price / mpg
    print(f'Uma viagem de {loc_from} a {loc_to} custa ${total_price}')

trip_price(409, 35, loc_from='Boston', loc_to='Nova York', price=5.1)
```

```
$ python my_func.py
```

```
Uma viagem de Boston a Nova York custa US$ 59.60
```

### Argumentos padrão

Podemos definir valores padrão para qualquer argumento ao definir a função. Esses valores padrão serão usados se a função for chamada sem passar argumentos para aqueles parâmetros.

```
# my_func.py
def trip_price(dist_miles, mpg, price, loc_from='A', loc_to='B'):
    total_price = dist_miles * price / mpg
    print(f'Uma viagem de {loc_from} a {loc_to} custa ${total_price}')

trip_price(409, 35, price=3.8)
```

```
$ python my_func.py
```

```
Uma viagem de A a B custa $ 44.41
```

Observe que, quando definimos uma função com parâmetros com valores padrão, precisamos colocar todos esses parâmetros _depois_ dos parâmetros sem valores padrão na definição da função; caso contrário, teremos um erro:

```
# my_func.py
def trip_price(loc_to='B', dist_miles, mpg, price, loc_from='A'):
    total_price = dist_miles * price / mpg
    print(f'Uma viagem de {loc_from} a {loc_to} custa ${total_price}')

trip_price(409, 35, price=3.8)
```

```
...

SyntaxError: non-default argument follows default argument
```

Pergunta

O que o interpretador fará a seguir se a implementação de uma função usar variáveis que não estão presentes no escopo local?

Gerar um erro

Procurar os nomes dessas variáveis no escopo integrado

Procurar os nomes dessas variáveis no escopo global

Exatamente!

Muito bem!

Pergunta

Qual dessas ações na chamada de uma função em Python não levará a um erro?

Passar argumentos nomeados em qualquer ordem

Sim, isso não resulta em um erro. Podemos passar argumentos nomeados como quisermos.

Passar argumentos posicionais em qualquer ordem

Misturar argumentos posicionais e nomeados em qualquer ordem

Você conseguiu!

# Verificação e dicas de tipo

O Python é uma linguagem de tipagem dinâmica. Ou seja, o interpretador Python não verifica a validade das operações antes que elas precisem ser executadas. O Python não pode determinar com antecedência se uma operação é permitida, já que ele não sabe os tipos de dados dos valores antes de precisar usá-los.

Os exemplos abaixo mostram isso:

```
# subtract.py
def subtract(a=10, b='5'):
    return a - b
   
subtract()
print('Hello from subtract.py')
```

```
$ python subtract.py 
```

```
...

TypeError: unsupported operand type(s) for -: 'int' and 'str'
```

Como esperado, recebemos um erro porque não podemos subtrair uma string de um número. No entanto, se apenas definirmos a função sem chamá-la, o interpretador não terá nenhum problema com isso:

```
# subtract.py
def subtract(a=10, b='5'):
    return a - b

print('Hello from subtract.py')
```

```
$ python subtract.py
```

```
Hello from subtract.py
```

Em contraste com Python, as [linguagens de tipagem estática](https://developer.mozilla.org/en-US/docs/Glossary/Static_typing) _(os materiais estão em inglês)_ (como Java ou C++) exibem um comportamento diferente: se houver um problema com tipos de dados, o programa vai falhar em compilar (ou seja, produzir um arquivo executável) mesmo antes que você possa executá-lo.

Há prós e contras para cada paradigma. Confira [esta discussão](https://stackoverflow.com/questions/1517582/what-is-the-difference-between-statically-typed-and-dynamically-typed-languages) _(os materiais estão em inglês)_ no StackOverflow se quiser aprender mais.

## Dicas de tipo

Dicas de tipo são uma solução para indicar estaticamente o tipo de dados de um valor no seu código Python.

Você pode pensar nas dicas de tipo como uma maneira de obter algumas funcionalidades (e benefícios) de uma linguagem de tipagem estática em Python. As dicas de tipo foram especificadas no [PEP 484](https://peps.python.org/pep-0484/) _(os materiais estão em inglês)_ e introduzidas no Python 3.5.

Aqui está um exemplo de como usar dicas de tipo na nossa função. Anotamos os argumentos _e_ o valor de retorno:

```
def list_of_words(text: str, sep: str = " ") -> list:
    return text.split(sep)
```

A sintaxe `text: str` diz que o argumento `text` deve ser do tipo `str`. Da mesma forma, o argumento opcional `sep` também deve ter o tipo `str` (o valor padrão é `" "`). Por fim, a notação `-> list` especifica que `list_of_words()` vai retornar uma lista.

Em termos de estilo, [PEP 8](https://www.python.org/dev/peps/pep-0008/#other-recommendations) _(os materiais estão em inglês)_ recomenda o seguinte:

-   Use regras normais para dois pontos, ou seja, sem espaço antes e um espaço depois de dois pontos: `text: str`
-   Use espaços ao redor do sinal `=` ao combinar uma anotação de argumento com um valor padrão: `sep: str = " "`
-   Use espaços ao redor da seta `->`: `def list_of_words(...) -> list`

Observe que adicionar dicas de tipo a nossas funções não vai alterar o comportamento quando elas forem chamadas. No que diz respeito ao interpretador Python, as dicas de tipo não mudam nada. Isso significa que as duas versões da função abaixo têm comportamento idêntico durante a execução.

-   Sem dicas de tipo:

```
def list_of_words(text, sep):
    return text.split(sep)
```

-   Com dicas de tipo:

```
def list_of_words(text: str, sep: str = " ") -> list:
    return text.split(sep)
```

Vamos ver o que acontece quando passamos `None` para o argumento opcional `sep=` quando sua dica de tipo diz que deve ser `str`.

```
# list_of_words.py
def list_of_words(text: str, sep: str = " ") -> list:
    return text.split(sep)

print(list_of_words("Maria tinha um cordeirinho", sep=None))
```

```
$ python list_of_words.py 
```

```
['Maria', 'tinha', 'um', 'cordeirinho']
```

Mesmo que a nossa dica de tipo diga que `sep` deve ser do tipo `str`, o interpretador não se importa que passamos `None` para `sep`. E já que `None` é um argumento válido para o método `split()`, o programa é executado sem erros.

O importante aqui é que as dicas de tipo podem ser úteis para ajudar os programadores a acompanhar o que acontece no código, especialmente para equipes de desenvolvedores que estão trabalhando na mesma grande base de código. Mas as dicas de tipo não mudam o comportamento real do interpretador Python.

### Verificação de tipo

Embora as dicas de tipo não sejam impostas pelo próprio interpretador Python, existem ferramentas cujo único objetivo é realizar a verificação de tipo. A ferramenta [`mypy`](http://mypy-lang.org/) _(os materiais estão em inglês)_ é a mais conhecida.

Você pode instalá-la com `pip`:

```
$ pip install mypy
```

Depois use-a para verificar se o código contém erros de tipo:

```
$ mypy list_of_words.py 
```

```
list_of_words.py:4: error: Argument "sep" to "list_of_words" has incompatible type "None"; expected "str"
Found 1 error in 1 file (checked 1 source file)
```

E `mypy` diz exatamente qual é o problema do código.

Agora podemos corrigir o erro:

```
# list_of_words.py
def list_of_words(text: str, sep: str = " ") -> list:
    return text.split(sep)

list_of_words("Maria tinha um cordeirinho", sep="a")
print("Hello from list_of_words.py")
```

E agora execute `mypy` de novo:

```
$ mypy list_of_words.py 
```

```
Success: no issues found in 1 source file
```

Agora o nosso código parece ótimo! Observe que `mypy` apenas verificou se o script tem violações de tipo de dados, mas não o executou. Isso fica evidente porque não vemos nenhum resultado da instrução `print()` no script.

Pergunta

Qual saída você verá no _Terminal_ ao executar `python greetings.py`?

```
# greetings.py
def greetings(name: int) -> str:
    return f"Hello {name}!"

print(greetings(10))
```

`Hello 10!`

Sim, é exatamente assim que será o resultado.

`list_of_words.py:4: error: Argument 1 to "greetings" has incompatible type "int"; expected "str" Found 1 error in 1 file (checked 1 source file)`

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-27-05-244Z.md
### Última modificação: 2025-05-28 19:27:05

# Tratamento de erros - TripleTen

Capítulo 5/8

Python intermediário

# Tratamento de erros

Nesta lição, vamos abordar dois tópicos:

1.  Como lidar com erros.
2.  Exceções que podem ocorrer se um usuário (um humano ou outro aplicativo upstream) fornecer valores de entrada inesperados ao nosso programa.

![](https://practicum-content.s3.amazonaws.com/resources/lesson3_1695282952.jpg)

## Erros

Programas de computador não são tão flexíveis quanto os humanos; se algo inesperado acontecer, o fluxo é interrompido e um _erro é gerado_. No primeiro sprint do programa, já encontramos erros. Agora é hora de se aprofundar no tópico e investigá-lo em mais detalhes.

As mensagens de erro mostram (com sorte) em que parte do programa foi encontrado um erro e qual foi o problema. Em relação ao nosso programa que girava imagens da lição anterior, suponha que cometemos um erro de digitação ao escrever o nome do arquivo de imagem, por exemplo, `tripleten_logo.jpe` em vez de `tripleten_logo.jpeg`:

```
python image_rotator.py tripleten_logo.jpe --angle 180 output.png -i
```

```
FileNotFoundError: [Errno 2] No such file or directory: 'tripleten_logo.jpe'
```

Como podemos ver, uma exceção `FileNotFoundError` foi gerada. Em Python, há muitas exceções diferentes, ou tipos de erros. Todas as exceções pertencem ao tipo genérico `Exception`, mas cada tipo específico de exceção tem o próprio objeto exclusivo. Por exemplo, o objeto `FileNotFoundError` é um tipo diferente em comparação com `TypeError`, que recebemos quando, por exemplo, adicionamos uma string a um número. Confira todos os tipos diferentes de exceções [aqui](https://docs.python.org/3/library/exceptions.html#Exception) _(os materiais estão em inglês)_.

Se algum processo computacional enorme ou contínuo estiver em execução, não queremos ver tudo travar devido a um problema de digitação ou algum pequeno erro. _Detectar um erro_ significa localizar o problema sem encerrar o programa inteiro. Além disso, detectar erros pode servir para depurar ou tornar o programa mais intuitivo, fornecendo dicas e correções.

### Try-except

No primeiro sprint, aprendemos como um bloco try-except pode lidar com erros. Para recapitular: a parte do código suscetível a exceções pode ser colocada em um bloco try-except, o que ajuda o código a executar uma ação alternativa, em vez de travar o programa, se encontrar uma exceção. Vamos substituir a linha que abre um arquivo pelo seguinte código:

```
try:
    im = Image.open(args.input_file)

except FileNotFoundError:
    print('arquivo de entrada não encontrado, forneça o nome do arquivo correto:')
```

No bloco `try`, tentamos abrir um arquivo com o nome passado para `args.input_file`. No bloco `except`, dizemos ao programa o que fazer se ele não encontrar um arquivo com esse nome, ou seja, se o Python gerar uma exceção `FileNotFoundError`. Observe que embora não seja necessário especificar o tipo de exceção que queremos detectar, é uma boa prática sempre tentar fazer isso.

Mesmo que tenhamos capturado o erro no código acima, isso não resolve o problema de o que fazer a seguir se o arquivo não for encontrado. Para aprimorar a funcionalidade do programa, podemos solicitar que o usuário especifique outro nome de arquivo usando a função `input()`. Isso permite que o usuário digite o caminho para uma imagem no teclado, e ele será passado para o programa como uma string:

```
try:
    im = Image.open(args.input_file)

except FileNotFoundError:
    print('arquivo de entrada não encontrado, forneça o nome do arquivo correto:')
      im = Image.open(input())
```

Suponha que o arquivo seja encontrado, mas ainda assim não seja carregado direito porque não é o tipo certo ou porque está corrompido. Podemos detectar vários tipos de exceções adicionando vários blocos `except`. Vamos usar o tipo universal `Exception` para o segundo bloco `except`:

```
try:
    im = Image.open(args.input_file)

except FileNotFoundError:
    print('arquivo de entrada não encontrado, forneça o nome do arquivo correto:')
    im = Image.open(input())

except Exception:
      print('Parece ser o arquivo errado, tente outro:')
      im = Image.open(input())
```

Além disso, podemos receber a mensagem de erro que o Python teria exibido se o código fosse autorizado a travar. Fazemos isso usando um alias para o tipo da exceção e depois imprimindo esse alias no bloco `except`. Confira a variável `error_msg` no bloco `Exception` abaixo:

```
try:
      im = Image.open(args.input_file)

except FileNotFoundError:
      print('arquivo de entrada não encontrado, forneça o nome do arquivo correto:')
      im = Image.open(input())

except Exception as error_msg:
      print(error_msg)
      print('Parece ser o arquivo errado, tente outro:')
      im = Image.open(input())
```

```
$ python image_rotator.py abc.txt output.png 180
```

```
cannot identify image file 'abc.txt'
'Parece ser o arquivo errado, tente outro:'
_
```

### Try-except-else-finally

As instruções Try-except podem ir ainda mais longe. Podemos especificar o que acontece se nenhum erro for encontrado (a instrução `else`) ou o que acontece no final do bloco `try`, não importa o que acontecer (a instrução `finally`). Frequentemente usamos `finally` para tarefas como limpar a memória, terminar conexões de rede, etc.:

```
# image_rotator.py

from PIL import Image
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('input_file', help='input file path')
parser.add_argument('output_file', help='output file path')
parser.add_argument('angle', help='desired rotation', type=int)
parser.add_argument( '-i', '--info', help='display image size', action='store_true')
args = parser.parse_args()

angle = args.angle

try:
      im = Image.open(args.input_file)
    
except FileNotFoundError:
      print('arquivo de entrada não encontrado, forneça o nome do arquivo correto:')
      im = Image.open(input())

except Exception:
    print('Arquivo errado, tente outro:')
      im = Image.open(input())

else:
    print("Funcionando sem problemas")

finally:
      rotated = im.rotate(angle)
      im.close() # fechando o arquivo da imagem, removendo-o da memória
      rotated.save(args.output_file)

if args.info:
      print('dimensões da imagem de entrada:', im.size)
```

Tenha em mente que, nesse exemplo, prompts de entrada podem gerar mais erros, então essa solução não é perfeita, mas ela mostra como podemos incluir flexibilidade nos programas e lidar com erros de forma criativa.

## Recapitulação

Uma **exceção** é _gerada_ e a execução é encerrada quando um erro é encontrado no Python. Os blocos **Try-except** são usados para encapsular instruções propensas a erros e capturar exceções, ou seja, para tratá-las de um jeito que não gere problemas:

```
try:
      something

except FileNotFoundError:
      print('arquivo não encontrado')
      do this

except Exception as error_msg:
      print(error_msg)
      do that

else:
      print('funcionando sem problemas')

finally:
      clean up
```

Dependendo do programa, muitas coisas no código podem ser propensas a erros. Os exemplos mais comuns incluem as tarefas de abrir arquivos, converter tipos de dados e processar o que foi inserido pelo usuário. Nunca é uma má ideia implementar o tratamento de erros para códigos que realizam esses tipos de tarefas.

Pergunta

Suponha que estamos trabalhando em um script que espera três argumentos:

-   um caminho para uma imagem de entrada;
-   um caminho para uma imagem de saída;
-   um novo tamanho para uma imagem.

O script lê uma imagem de entrada, a redimensiona para o tamanho especificado e salva a imagem resultante usando o caminho de saída fornecido. O script é assim:

```
# resizer_2.py

from PIL import Image
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('input_file', help='input file path')
parser.add_argument('output_file', help='output file path')
parser.add_argument('new_size', help='image size', type=tuple)
args = parser.parse_args()

im = Image.open(args.input_file)
resized = im.resize(args.new_size)
resized.save(args.output_file)
im.close()
```

Hora de tentar lidar com os erros. Precisamos verificar se é possível abrir uma imagem de entrada. Se não, queremos:

1.  Imprimir uma mensagem de erro;
2.  Depois imprimir `"a imagem padrão foi usada"`;
3.  Ler o arquivo `default_input.png`.

Como lembrete, podemos usar `except Exception ...` para detectar qualquer erro ao envolver a parte `Image.open()`.

Como precisamos reescrever o script para que ele faça exatamente o que queremos?

Opção #1:

```
# resizer_2.py

from PIL import Image
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('input_file', help='input file path')
parser.add_argument('output_file', help='output file path')
parser.add_argument('new_size', help='image size', type=tuple)
args = parser.parse_args()

try:
    im = Image.open(args.input_file)

except Exception as error_msg:
    print(error_msg)
      print('the default image is used')
      im = Image.open('default_input.png')

resized = im.resize(args.new_size)
resized.save(args.output_file)
im.close()
```

Isso definitivamente vai funcionar!

Opção #2:

```
# resizer_2.py

from PIL import Image
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('input_file', help='input file path')
parser.add_argument('output_file', help='output file path')
parser.add_argument('new_size', help='image size', type=tuple)
args = parser.parse_args()

try:
    im = Image.open(args.input_file)

except:
    im = Image.open('default_input.png')

resized = im.resize(args.new_size)
resized.save(args.output_file)
im.close()
```

Opção #3:

```
# resizer_2.py

from PIL import Image
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('input_file', help='input file path')
parser.add_argument('output_file', help='output file path')
parser.add_argument('new_size', help='image size', type=tuple)
args = parser.parse_args()

try:
    im = Image.open(args.input_file)

except Exception as error_msg:
    print(error_msg)
      print('Indique um caminho para uma imagem válida e tente novamente')

im.close()
```

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-27-06-558Z.md
### Última modificação: 2025-05-28 19:27:06

# Módulos e estrutura do projeto - TripleTen

Capítulo 5/8

Python intermediário

# Módulos e estrutura do projeto

![](https://practicum-content.s3.amazonaws.com/resources/lesson4_1695283099.jpg)

Digamos que você trabalha com astronomia e escreve códigos Python para analisar os dados das suas observações. Após vários meses de trabalho, você começa a perceber que usa códigos muito parecidos entre si nos seus scripts. No início, você copiava e colava o código de um script para outro, mas depois pensou em uma solução melhor — você pode criar seu próprio módulo! Então você colocou todas as suas variáveis, funções e classes mais usadas no mesmo script e o chamou de `astro.py`. Agora, ao analisar um novo lote de dados, é só importar `astro.py` no início do seu código, da mesma forma que importamos Pandas. Então, você terá todas as ferramentas necessárias prontamente disponíveis. E melhor ainda: você pode compartilhar `astro.py` com todos os seus colegas para facilitar o trabalho de todo mundo também.

### Bibliotecas, pacotes e módulos

O arquivo `astro.py` do exemplo é chamado de módulo. Às vezes, os termos **biblioteca**, **pacote** ou **módulo** são usados de forma mais ou menos intercambiável, mas, estritamente falando, os módulos são os blocos de construção menores — arquivos `.py` separados. Um pacote contém vários módulos agrupados, e uma biblioteca pode significar o mesmo ou ainda um conjunto de pacotes mais amplo. Bibliotecas úteis geram comunidades, se espalham e estabelecem padrões em campos correspondentes. Neste curso, contamos muito com a Pandas, que também é uma biblioteca/pacote.

Adicionar módulos é tão fácil quanto construir coisas de LEGO. O Python espera ver a extensão `.py`, então esta parte é omitida:

```
# myscript.py

import astro # importando o nosso módulo teórico
```

Os módulos podem aumentar significativamente o conjunto de ferramentas de um programa, sem complicar demais o código, e permitem lidar com tarefas complexas usando apenas algumas linhas de código. Por exemplo, você já viu como a biblioteca `PIL` , que usamos antes, permite ler, girar e salvar uma imagem sem entrar em detalhes de pixels, bytes, etc.:

```
from PIL import Image

im = Image.open('myimage.png')
im.rotate(90)
im.save('output.png')
```

A modularidade é incentivada pelo Python e representa o mais alto nível de arquitetura de programa. Para usar um módulo, basta importá-lo usando `import`. A importação de um script o executa, deixando todos os objetos definidos naquele script disponíveis para o ambiente que o importou:

```
# module.py

print("Oi! Eu sou um módulo")
```

```
# myscript.py

import module

print("Executando o programa principal")
```

```
$ python myscript.py
```

```
Oi! Eu sou um módulo
Executando o programa principal
```

Na prática, módulos não costumam produzir nenhuma saída quando os importamos. Em vez disso, eles produzem várias variáveis, funções e classes que podemos usar no código.

É importante observar que no exemplo acima, `myscript.py` só pode importar o módulo `module` com sucesso se `module.py` estiver localizado no mesmo diretório que `myscript.py`.

### Busca por novos pacotes

O Python vem instalado com muitos pacotes integrados, mas se precisarmos de outros, também é fácil baixar e instalá-los. Existe um repositório de pacotes online chamado [PyPI](https://pypi.org/). Se um pacote estiver listado lá, ele pode ser instalado pelo gerenciador de pacotes integrado **Pip** ("PIP" significa "Pip instala pacotes").

Para buscar e instalar uma biblioteca para o usuário atual, use o seguinte modelo de comando:

```
$ pip install --user package_name
```

Aqui está um exemplo de como instalar a biblioteca `PIL` que já usamos antes:

```
pip install --user Pillow
```

### Namespaces

Agora, vamos fazer uma pausa e dar uma olhada em alguns fundamentos. Se atribuirmos uma variável `myvar = 123` em uma parte do código e depois a usarmos em outro lugar, o Python, de algum jeito, sabe o que queremos dizer. Isso acontece porque cada script em execução tem seu próprio **namespace** — uma espécie de dicionário oculto onde podemos encontrar todos os objetos criados pelo script que são acessíveis a partir do escopo atual. É claro, cada script individual inicialmente não tem ideia do conteúdo de outros arquivos. Importar módulos é como construir túneis até fragmentos de código adicionais. Objetos (funções, variáveis, etc.) que vivem no namespace de um módulo são chamados de **atributos**.

### Maneiras de importar coisas

Quando um módulo é importado, seu namespace (com todos os atributos) fica disponível. Para acessar os atributos do módulo, use a notação de ponto:

```
# module.py

import re

def check_email(string: str):
    '''
    use expressões regulares para verificar a formatação do endereço de e-mail
    o padrão é "algo@algo.algo"
    '''
    if re.match('[.\w]+@\w+[.]\w+', string):
        print('correto')
    else:
        print('verifique endereço de e-mail')

def check_age(age: int):
    if age >= 50:
        print('acesso concedido')
    else:
        print("você é muito jovem")
```

```
# myscript.py

import module

email = input()

module.check_email(email)
```

No exemplo acima, acessamos a função `check_email()` de `module.py` chamando-a com a notação de ponto `module.check_email(email)`.

Se precisamos de apenas um atributo e não quisermos usar a notação `module.attribute`, basta importar atributos individuais do módulo:

```
# myscript.py

from module import check_email

email = input()
check_email(email)
```

O perigo de importar atributos específicos é que tal atributo pode compartilhar o nome com um objeto que já existe no namespace atual ou que será criado mais tarde no script. Nesse caso, a última atribuição substitui a anterior, então tenha cuidado.

Se formos corajosos o suficiente para desempacotar _todos_ os atributos para o namespace atual, usamos `*`:

```
# myscript.py

from module import *

email = input()
check_email(email)
```

Tenha em mente que não queremos que nenhum dos nomes importados seja igual a outros objetos no programa. A dificuldade de saber os nomes de todos os objetos que importamos é a razão pela qual `import *` costuma ser considerado uma má prática na engenharia de software.

### Verificação if __main__

Às vezes, escrevemos um script que pode servir a dois propósitos: ele pode ser o script principal que executa o código do programa ou pode ser importado como um módulo para ser usado por outro script. Pode haver algum código no script que seja necessário para o executarmos como o principal script do programa, mas não queremos que esse código seja executado quando importarmos o script para outro lugar.

Por exemplo, digamos que `astro.py` contém código que processa e analisa um conjunto de dados, além de todas as suas definições de variáveis, funções e classes. Quando executamos o próprio `astro.py`, queremos executar esse código de análise. Mas quando importamos `astro.py` para outro script, queremos apenas executar o código que cria variáveis e define funções e classes.

Para garantir que alguns fragmentos de código em um módulo nunca sejam executados na importação, podemos colocá-los em um bloco especial `if` onde verificamos se a variável `__name__` tem o valor `'__main__'`. Esse bloco `if` costuma ser chamado de _ponto de entrada_. Aqui está um exemplo simples:

```
# module_1.py

def useful_function():
    print('funcionando')

if __name__ == '__main__':
    print("testando a função...")
    useful_function()
```

Você pode estar pensando, o que é essa variável `__name__` e de onde ela veio? Sempre que executarmos um script, o Python cria algumas variáveis especiais nos bastidores – para o script principal que executamos _e_ para todos os módulos importados pelo script principal; uma dessas variáveis é `__name__`. À variável `__name__` para o script principal que executamos é sempre atribuído o valor de string `'__main__'`, enquanto a variável `__name__` para os módulos importados pelo script principal sempre recebe valores de string que são apenas os nomes dos módulos correspondentes.

Para o nosso script de exemplo `module_1.py` acima, aqui está o que acontece se executarmos o próprio script a partir da linha de comando:

```
$ python module_1.py
```

```
testando a função...
funcionando
```

Vemos a saída do código no bloco de ponto de entrada, porque `module_1.py` era o script principal que executamos usando o comando `python`.

Agora vamos escrever outro script, `module_2.py`:

```
# module_2.py

import module_1

def double_check():
    print('Duas vezes:')
    module_1.useful_function()
    module_1.useful_function()

if __name__ == "__main__":
    print("testando a função...")
    double_check()
```

Se executarmos `module_2.py` como o script principal, então `module_2.py` terá `'__main__'` como o valor de `__name__`, e `module_1.py` terá `'module_1'` como valor de `__name__`:

```
$ python module_2.py
```

```
testando a função...
Duas vezes:
funcionando
funcionando
```

### Importar, importar e importar

Voltando para a estrutura do projeto, podemos importar tantos módulos quanto forem necessários. Muitos dos próprios módulos que importamos também vão importar outros módulos (por exemplo, a biblioteca Pandas importa da biblioteca NumPy). É uma boa prática de programação importar todos os módulos necessários no início do script. Os pacotes integrados da biblioteca padrão vêm primeiro, depois os pacotes de terceiros e, por fim, os módulos locais (se houver):

```
# myscript.py

# Pacotes integrados
import math
import os

# Pacotes de terceiros
import pandas
import numpy

# Meu próprio módulo
import mymodule

# O resto do programa segue
```

Pergunta

Suponha que você tenha o seguinte módulo:

```
# my_module.py

import re

def email_check(string: str):
      '''
      Use expressões regulares para verificar a formatação do endereço de e-mail:
      o padrão é "algo@algo.algo"
      '''
      if re.match('[.\w]+@\w+[.]\w+', string):
            print('correto')
      else:
            print('endereço de e-mail errado')

def age_check(age: int):
      '''
      Verifique se a idade está dentro do limite
      '''
      if age >= 50:
            print('acesso concedido')
      else:
            print("você é muito jovem")
```

Qual das seguintes opções vai funcionar corretamente?

```
import my_module

email_check('stewie@family.guy')
```

```
import my_module.py

email_check('stewie@family.guy')
```

```
import my_module

my_module.email_check('stewie@family.guy')
```

Correto!

```
import my_module.py

my_module.email_check('stewie@family.guy')
```

Muito bem!

Pergunta

Suponha que você tenha o mesmo módulo:

```
# my_module.py

import re

def email_check(string: str):
      '''
      Use expressões regulares para verificar a formatação do endereço de e-mail:
      o padrão é "algo@algo.algo"
      '''
      if re.match('[.\w]+@\w+[.]\w+', string):
            print('correto')
      else:
            print('endereço de e-mail errado')

def age_check(age: int):
      '''
      Verifique se a idade está dentro do limite
      '''
      if age >= 50:
            print('acesso concedido')
      else:
            print("você é muito jovem")
```

Qual das seguintes opções vai funcionar corretamente? Vamos supor que o módulo esteja localizado em algum lugar onde o script atual possa encontrá-lo.

Escolha quantas quiser

```
from my_module import email_check

email_check('stewie@family.guy')
```

Correto!

```
from my_module import *

email_check('stewie@family.guy')
```

Sim. Dessa forma, importamos tudo de `my_module` para o namespace atual. Mas tenha muito cuidado ao fazer isso!

```
from my_module import email_check, age_check

my_module('stewie@family.guy')
```

```
from my_module import email_check, age_check

email_check('stewie@family.guy')
```

Sim.

Trabalho maravilhoso!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-27-07-883Z.md
### Última modificação: 2025-05-28 19:27:08

# Programação orientada a objetos - TripleTen

Teoria

# Programação orientada a objetos

O Python é uma linguagem orientada a objetos. Tudo em Python são objetos, até mesmo as exceções. Objetos diferentes pertencem a **classes** diferentes, que definem o que podemos fazer com eles. As classes podem formar estruturas ramificadas, onde algumas classes específicas são construídas sobre outras mais genéricas, adicionando ou modificando funcionalidades.

### Classes

As classes servem de moldes para a criação de **instâncias**, que são objetos individuais do tipo de classe. Por exemplo, `'Hello, world!'` é uma _instância_ da _classe_ `str`.

Todas as instâncias da mesma classe têm o mesmo conjunto de atributos e métodos. Há muitas classes integradas, como strings, dicionários, listas, etc. Todas elas têm métodos específicos que definem seu uso e atributos específicos que contêm informações sobre elas. Por exemplo, o método `append()` pode ser aplicado a qualquer lista, porque é definido na classe `list`:

```
my_list **=** ['casaco', 'cabra']
my_list.append('barco')
print(my_list)
```

```
['casaco', 'cabra', 'barco']
```

### Criação de classes

Imagine que estamos criando um jogo de videogame e queremos criar alguns personagens com diferentes atributos e também definir ações que o jogador pode executar no jogo. Há tipos diferentes de personagens: magos, guerreiros, etc. Eles têm propriedades diferentes e podem fazer coisas diferentes.

Como podemos traduzir o processo de criação de um personagem para o código Python? Uma opção é usar dicionários para personagens e funções para ações do jogador, da seguinte maneira:

```
mage = {'health': 50, 'damage': 10, 'knowledge': 95}
knight = {'health': 100, 'damage': 25, 'knowledge': 20}

arthur = knight.copy() # fazendo uma cópia do dicionário original 'knight'
arthur['name'] = 'Arthur' # substituindo o nome dentro da cópia

richard = knight.copy() # fazendo mais uma cópia do dicionário original 'knight'
richard['name'] = 'Richard' # substituindo o nome dentro da nova cópia

def heal(character): # criando a função que altera a saúde (health)
    character['health'] += 20

heal(richard) # chamando a função para alterar a saúde de Richard
```

Podemos também criar personagens para o jogo mais facilmente usando classes:

```
class Knight: # criando a classe Knight
    def __init__(self, name):
        # configurando os parâmetros
        self.health = 100
        self.damage = 25
        self.knowledge = 20
        self.name = name

arthur = Knight('Arthur')
richard = Knight('Richard')
```

Criamos uma classe chamada `Knight()` (cavaleiro) usando a instrução `class`. Embora o Python não exija isso, a convenção para nomear classes determina que elas sempre comecem com uma letra maiúscula e usem CamelCase em vez de sublinhados se o nome da classe tiver mais de uma palavra.

Todas as classes Python têm um método especial `__init__()` que é chamado quando criamos uma instância da classe; por exemplo, quando usamos `Knight('Arthur')` para criar a variável `arthur`. O parâmetro especial `self` se refere à instância que estamos criando, então não precisamos passar nenhum argumento para ele. No entanto, precisamos especificar um argumento `name` cada vez que criamos uma instância de `Knight()`, porque não fornecemos nenhum valor padrão.

### Atributos

Cada instância de `Knight()` tem atributos que podemos acessar usando a notação de ponto:

```
class Knight:
    def __init__(self, name):
        self.health = 100
        self.damage = 25
        self.knowledge = 20
        self.name = name

arthur = Knight('Arthur')

print(arthur.health)
print(arthur.damage)
print(arthur.knowledge)
print(arthur.name)
```

```
100
25
20
Arthur
```

Também podemos alterar o valor de qualquer atributo simplesmente atribuindo um novo valor a ele:

```

class Knight:
    def __init__(self, name):
        self.health = 100
        self.damage = 25
        self.knowledge = 20
        self.name = name

arthur = Knight('Arthur')
print(arthur.health)

arthur.health = 150
print(arthur.health)
```

```
100
150
```

Por fim, podemos visualizar todos os atributos de uma instância como um dicionário acessando o atributo especial `__dict__`:

```

class Knight:
    def __init__(self, name):
        self.health = 100
        self.damage = 25
        self.knowledge = 20
        self.name = name

arthur = Knight('Arthur')

print(arthur.__dict__)
```

```
{'health': 100, 'damage': 25, 'knowledge': 20, 'name': 'Arthur'}
```

### Métodos

Os nossos cavaleiros têm alguns atributos muito úteis, mas e se quisermos que eles executem alguma ação? Podemos definir funções personalizadas em uma classe que estarão disponíveis para as instâncias da classe usarem. Essas funções específicas de classes são chamadas de **métodos**:

```
class Knight:
    def __init__(self, name):
        self.health = 100
        self.damage = 25
        self.knowledge = 20
        self.name = name
    
    def heal(self):
        self.health += 20
    
    def learn(self):
        self.knowledge += 5
```

Definimos métodos na classe exatamente da mesma maneira que definiríamos qualquer outra função. Ambos os métodos `heal()` e `learn()` têm apenas um parâmetro `self`, o que significa que uma chamada de tal método afeta a instância que os chamou; não precisamos passar nenhum argumento para eles. Chamamos os métodos usando a notação de ponto que você já conhece:

```
class Knight:
    def __init__(self, name):
        self.health = 100
        self.damage = 25
        self.knowledge = 20
        self.name = name
    
    def heal(self):
        self.health += 20
    
    def learn(self):
        self.knowledge += 5

arthur = Knight('Arthur')

arthur.heal()
arthur.learn()

print(arthur.health)
print(arthur.knowledge)
```

```
120
25
```

O atributo de saúde foi aumentado em 20, e o atributo de conhecimento `knowledge` – em 5, exatamente como especificamos em `heal()` (curar) e `learn()` (aprender) respetivamente. Se quisermos que os métodos sejam mais flexíveis, podemos adicionar outros parâmetros:

```
class Knight:
    def __init__(self, name):
        self.health = 100
        self.damage = 25
        self.knowledge = 20
        self.name = name
    
    def heal(self, amount):
        self.health += amount
    
    def learn(self, amount):
        self.knowledge += amount

arthur = Knight('Arthur')

arthur.heal(10)
arthur.learn(2)

print(arthur.health)
print(arthur.knowledge)
```

```
110
22
```

### Métodos estáticos

Todos os métodos que vimos até agora afetam a instância da classe que chama o método; até mesmo o método especial `__init__()` inclui o parâmetro `self`. Os **métodos estáticos** não interagem com uma instância diretamente e não usam o parâmetro `self`. Um método estático pode ser chamado sem um objeto para aquela classe. Além disso, ele não pode modificar o estado de um objeto, já que não está vinculado a ele.

Para criar um método estático, precisamos usar o **decorador** `@staticmethod`.

Aqui está um exemplo para uma classe `Stock()` que rastreia informações sobre cotações da bolsa. O método `show_current_price` serve para encontrar o preço atual das ações na internet e imprimi-lo.

```
class Stock:
      def __init__(self, ticker, amount):
            self.ticker = ticker
            self.amount = amount

      @staticmethod
    def show_current_price(ticker):
        current = # Aqui você implementaria o código para buscar o preço atual online
        print(current)
```

```
Stock.show_current_price('Apple')
```

Ao chamar `Stock.show_current_price('Apple')`, você está apenas invocando o método estático `show_current_price` da classe `Stock`. Nenhuma instancia da classe `Stock` é criada, então nenhum objeto é criado. O método estático pode ser chamado direto da classe sem criar uma instância da classe. Nesse caso, 'Apple' é passado como um argumento do método estático, mas nenhum objeto `Stock` é criado no processo.

### Métodos de classe

**Métodos de classe** podem ser usados para implementar formas alternativas de criar instâncias. Um método de classe aceita um objeto de classe como o primeiro argumento. Quando criamos um método de classe, precisamos usar o decorador `@classmethod`:

```
class Stock:
    def __init__(self, ticker, amount, price):
        self.ticker = ticker
        self.amount = amount
        self.price = price
        self.total = self.price * self.amount

    def buy(self, quantity):
        self.amount += quantity
        self.total = self.amount * self.price
       
      @staticmethod
    def show_current_price(ticker):
        current = # Aqui você implementaria o código para buscar o preço atual online
        print(current)

    @classmethod
    def from_string(cls, string): # criando um método de classe
        ticker, amount, price = string.split() 
        return cls(ticker, int(amount), float(price))
```

Veja como criamos uma instância de uma classe usando o método de classe `from_string()`:

```
abc = Stock.from_string('ABC 10 1.5')
```

A linha de código `abc = Stock.from_string('ABC 10 1.5')` cria uma instância da classe `Stock` chamada abc usando o método de classe `from_string` e a string `'ABC 10 1.5'` como argumentos.

### Herança

Vamos voltar para o código do videogame. Escrevemos uma ótima classe `Knight()`, mas agora queremos adicionar mais classes, como magos, camponeses e assim por diante.

No final das contas, o importante sobre as classes é que elas podem reutilizar código. Acontece que podemos usar **herança** para criar todas as classes de personagens que precisarmos a partir de uma classe pai, em vez de criar cada classe do zero.

Vamos criar uma classe pai `Character()` (personagem) e usá-la para criar as classes filhas, `Knight()` (cavaleiro) e `Peasant()` (camponês):

```
# os atributos e métodos da classe pai são herdados pelas classes filhas
class Character:
    def __init__(self, name):
        self.name = name
        self.health = 100
    
    def heal(self, value:int=20):
        self.health += value
    
    def learn(self, value:int=20):
        self.knowledge += value
        
# a classe filha adiciona atributos específicos
class Knight(Character):
    def __init__(self, name):
        Character.__init__(self, name) # herdando o construtor da classe pai
        self.damage = 25 # atributo adicionado
        self.knowledge = 20 # atributo adicionado

# mais uma classe
class Peasant(Character):
    def __init__(self, name):
        super().__init__(name) # outra maneira de herdar o construtor pai
        self.damage = 10
        self.knowledge = 36

arthur = Knight('Arthur')
arthur.heal() # valor padrão é usado
arthur.learn(100) # valor customizado
print(arthur.__dict__)
```

```
{'name': 'Arthur', 'health': 120, 'damage': 25, 'knowledge': 120}
```

Criamos duas classes filhas acima: `Knight` e `Peasant`. As duas herdam todos os métodos e propriedades da classe pai `Character`. Observe que a herança é implementada de maneiras diferentes para as classes `Knight` e `Peasant`: usamos a construção `Character` para a classe `Character.__init__(self, name)`. No entanto, para a classe `Knight` usamos a função `super()`. `super()` determina automaticamente a classe pai correta e chama o construtor, o que significa que você não precisa especificar o nome da classe pai como fizemos ao criar a classe `Knight`.

Usando a herança da classe `Character()`, a classe `Knight()` funciona. Podemos usar `Character()` de novo para criar outras classes que compartilhem características comuns com `Knight()` para quaisquer outros personagens que queremos no jogo. Por exemplo, todas as classes de personagens têm nomes e saúde; esses são atributos de `Character()`.

Agora você sabe como criar classes do zero e como usar herança e alguns métodos especiais. Mesmo que você não precise criar suas próprias classes agora, é importante entender como os objetos funcionam nos bastidores. As classes são muito potentes. Na verdade, as classes personalizadas estão no centro de muitas bibliotecas úteis.

Programação orientada a objetos

Tarefa5 / 5

1.

Crie uma classe `Account()` que represente uma conta bancária com os seguintes atributos:

-   `bank` para o nome do banco
-   `acc_id` para o id da conta
-   `holder_id` para o id do titular
-   `balance` para o saldo inicial da conta
-   `start_date` para a data e a hora em que a conta foi aberta

Certifique-se de que `balance` é do tipo `float` e tem um valor padrão de `0.0`. Para definir um valor padrão e um tipo padrão, passe a informação dentro de `__init__()`. Aqui está um exemplo em que definimos um valor inicial de `32` e o tipo `int` para `strength`:

```
def __init__(self, strength:int=32):
```

Quando uma instância da classe `Account()` é iniciada, ela deve registrar automaticamente a data e a hora em que a conta foi aberta e armazenar essas informações como `start_date`. Para alcançar isso, você precisa importar o módulo `datetime` do pacote `datetime`:

```
from datetime import datetime
```

Após importá-lo, você pode usar seu método de classe `now()` para criar uma marca temporal a partir da data e hora atual, da seguinte maneira:

```
from datetime import datetime

datetime.now()
```

2.

Adicione dois métodos de classe, `deposit()` e `withdraw()`; ambos aceitam números de ponto flutuante como argumentos e ajustam `balance` de acordo. O método `deposit()` aumenta o saldo `balance`, enquanto o método `withdraw()` o reduz.

Ambos os métodos esperam que a entrada `amount` seja do tipo `float`.

3.

Adicione um método que busca o número de telefone de um determinado banco. Por exemplo, esse método pode pesquisar na web, encontrar o número de telefone do banco e imprimi-lo. Para simplificar as coisas, pedimos que você implemente essa funcionalidade criando um método estático `bankphone`. Esse método deve aceitar um nome de banco e imprimir sempre `1-000-1234567` como saída.

Observe que no pré-código adicionamos uma descrição para o método estático que pedimos para você criar, com o seguinte texto: `print the bank number`.

4.

Suponha que precisemos de uma maneira rápida de adicionar uma conta usando uma string que armazena os IDs da conta e do titular. Por exemplo, temos uma string `"001/2406"`.

Para alcançar isso, pedimos que você crie o método de classe `quick()`. Esse método deve esperar uma string com os IDs da conta e do titular, dividi-la em duas partes e armazenar os valores correspondentes nas variáveis `acc_id` e `holder_id`. Em seguida, o método deve retornar a classe, em que `bank` é sempre `default_bank` e `balance` é `0.0`.

5.

Hora de usar a classe. Crie uma instância chamada `first` (primeira) usando a forma padrão com as seguintes informações: o nome do banco é `old_trusty`, o ID da conta é `001`, o ID do titular é `10043`, a soma inicial é 500. Em seguida, deposite mais 250 unidades e retire 400. Imprima o saldo.

Em seguida, crie outra instância chamada `second` (segunda) usando o método `quick()` e passando `'002/10123'` como entrada. Imprima o ano em que uma conta foi criada (para isso, lembre-se de que as instâncias `datetime` possuem um atributo `year` (ano)).

99

1

2

3

4

5

6

7

8

9

10

11

first \= Account("old\_trusty", '001', '10043', 500.0) \# crie a primeira conta

first.deposit(250) \# chame o método deposit

first.withdraw(400) \# chame o método withdraw

print(first.balance) \# imprima o saldo

  

second \= Account.quick("002/10123") \# crie a segunda conta

print(first.start\_date.year)

  

  

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-27-10-464Z.md
### Última modificação: 2025-05-28 19:27:10

# Entrada e saída de arquivos (File I/O) - TripleTen

Teoria

# Entrada e saída de arquivos (File I/O)

Se você já usou um computador, deve saber o que são arquivos. Basicamente, arquivos são pedaços de informação, ou de dados, que salvamos para que os possamos armazenar durante um longo prazo e reutilizar mais tarde. Seja qual for o tipo, todos os arquivos são codificados em bits. A estrutura de codificação depende do tipo do arquivo, e esses tipos diferentes são representados pelas extensões nos nomes de arquivos, como _.txt_, _.py, .jpeg_ \*\* ou alguma outra extensão.

Com frequência, precisamos que os programas em Python possam ler dados de arquivos externos, editar arquivos existentes ou criar novos. Já fizemos muita leitura de arquivos usando a função `read_csv()` da Pandas. Nesta lição, vamos aprender mais sobre como abrir arquivos e trabalhar com eles (entrada e saída de arquivos, ou só I/O: input/output em inglês) em Python.

Nossos exemplos nesta lição vão focar em arquivos de texto, já que eles são normalmente usados para o armazenamento e para a transferência de dados.

### Leitura de arquivos

O Python tem uma função integrada `open()` que pode abrir qualquer arquivo. Tudo o que precisamos fazer é passar o caminho de um arquivo para a função. Por padrão, arquivos são abertos no modo "leitura", o que significa que podemos obter informações deles, mas não podemos editá-los.

```
f = open('my_file.txt')
```

Também podemos passar explicitamente `mode='r'` para deixar claro no código que estamos abrindo um arquivo no modo "read" (leitura):

```
f = open('my_file.txt', mode='r')
```

Este é o conteúdo de `my_file.txt`:

```
"Do not go gentle into that good night"
[12, 13, 14]
100.42
Alice
```

mas veja o que obteremos quando imprimimos `f`:

```
f = open('my_file.txt')
print(f)
f.close()
```

```
<_io.TextIOWrapper name='my_file.txt' mode='r' encoding='cp1252'>
```

A saída atual pode ser diferente da esperada. Isso porque a variável `f`, que representa o arquivo aberto, é um objeto que precisa de código adicional para que seu conteúdo possa ser lido. A variável não pode fazer muito por si só. Portanto, precisamos escrever mais código para ler o conteúdo do arquivo.

Quando abrimos um arquivo, um objeto do tipo arquivo é criado:

```
f = open('my_file.txt')
print(type(f))
f.close()
```

```
<class '_io.TextIOWrapper'>
```

Observe que também usamos o método `close()` para fechar o arquivo após termos terminado todo o trabalho com ele. Fechar arquivos quando eles não são mais necessários é um bom hábito, caso contrário, eles ocupam memória e podem causar problemas não intencionais.

Agora vamos ler o arquivo e exibir o conteúdo. Uma maneira de fazer isso é percorrer todas as linhas no arquivo e imprimir cada uma delas:

```
f = open('my_file.txt')

for line in f:
    print(line)

f.close()
```

```
"Do not go gentle into that good night"

[12, 13, 14]

100.42

Alice
```

Imprimimos todo o conteúdo, mas temos aqui algumas linhas em branco que não existiam no arquivo original. Isso acontece porque cada linha em `my_file.txt` termina com o caractere de nova linha `\n`, _e_ `print()`, por padrão, também termina cada saída com um caractere de nova linha. Podemos corrigir isso usando o método `rstrip()` em cada linha, o que vai remover espaços em branco no final das linhas:

```
f = open('my_file.txt')

for line in f:
    print(line.rstrip())

f.close()
```

```
"Do not go gentle into that good night"
[12, 13, 14]
100.42
Alice
```

A abordagem acima funciona, mas também podemos usar os métodos `read()` ou `readlines()` para fazer isso. O método `read()` lê todo o arquivo como uma única string, enquanto o método `readlines()` cria uma lista de strings, em que cada elemento na lista é uma linha do arquivo:

```
f = open('my_file.txt')
print(f.read())
f.close()

print() # imprimindo uma linha vazia para separar as linhas

f = open('my_file.txt')
print(f.readlines())
f.close()
```

```
"Do not go gentle into that good night"
[12, 13, 14]
100.42
Alice

['"Do not go gentle into that good night"\n', '[12, 13, 14]\n', '100.42\n', 'Alice']
```

Com `readlines()`, podemos percorrer toda a lista para exibir cada linha ou processá-las separadamente.

Observe que tivemos que fechar o arquivo e abri-lo de novo para ler o conteúdo duas vezes. Isso acontece porque não podemos acessar linhas anteriores do arquivo depois de elas serem lidas a partir de um objeto do arquivo. Se precisarmos manter linhas anteriores, devemos armazená-las em uma lista ou em alguma outra estrutura de dados.

Por fim, também podemos abrir arquivos usando o gerenciador de contexto Python. Para fazer isso, criamos um bloco `with` e abrimos o arquivo no cabeçalho do bloco. A vantagem do gerenciador de contexto é que ele vai fechar automaticamente o arquivo assim que o nosso programa sair do bloco `with`:

```
with open('my_file.txt') as f:
    for line in f.readlines():    
        print(line.rstrip())
```

```
"Do not go gentle into that good night"
[12, 13, 14]
100.42
Alice
```

### Gravar em arquivos

Chega de falar da leitura: o que fazer se quisermos salvar nossos dados? Podemos criar novos arquivos ou editar existentes abrindo arquivos no modo "write" (gravação). Temos algumas opções que podemos passar como o argumento `mode=` para `open()` para gravar dados em arquivos:

-   `'w'`: abre um novo arquivo vazio em que os dados serão gravados.
-   `'a'`: abre um arquivo novo _ou_ existente e adiciona texto a ele.

Tenha cuidado ao usar `mode='w'` — se você abrir um arquivo com um caminho e um nome iguais aos de um arquivo já existente, os conteúdos do arquivo antigo serão completamente sobrescritos!

Após abrir o arquivo, vamos de fato gravar dados nele. Podemos fazer isso usando o método `write()`, que aceita um argumento de uma única string e grava essa string no arquivo a partir de onde estamos no arquivo.

Imagine que estamos escrevendo um diário. O seguinte código vai adicionar algumas linhas ao texto já existente sem remover nenhum conteúdo anterior do arquivo:

```
from datetime import datetime

# obter a hora atual
now = datetime.now() 

with open('my_journal.txt', 'a') as f:
    f.write('\n')     # começar uma nova linha
      f.write(str(now)) # marca temporal
    f.write('\n\n')   # inserir uma linha em branco
      f.write(' ')      # espaço vazio
      f.write(input())  # digitar uma entrada de diário no teclado
      f.write('\n\n')   # terminar com uma linha em branco
```

Se temos uma lista de strings e queremos gravar todas as strings no arquivo em uma determinada ordem, podemos usar o método `writelines()`:

```
titles = ['Planeta do futuro\n', 'Soluções para um mundo sustentável\n', "A cidade mais poluída do mundo\n"]

with open('output.txt', 'w') as f:
    f.writelines(titles)
```

Cada string na lista deve terminar com o caractere de nova linha se quisermos escrever cada string na sua própria linha.

### Coleções de arquivos

Coleções de arquivos são muito usadas para compactar ou embalar arquivos relacionados. Trabalhar com arquivos zip em Python é semelhante a arquivos simples. O pacote `zipfile` vai nos ajudar muito aqui:

```
from zipfile import ZipFile

with open("info.txt", "w") as f:
    f.write('alguns dados')
    
with ZipFile("archive.zip", mode="w") as archive:
    archive.write("info.txt")
```

Agora temos um arquivo zip com `info.txt` dentro. Podemos usar `'r'` e `'a'` para ler e adicionar arquivos a coleções de arquivos. O método `printdir()` é usado para listar os conteúdos de um arquivo:

```
from zipfile import ZipFile

with open("logs.txt", "w") as f:
    f.write('new_log')

with ZipFile("archive.zip", mode="a") as archive:
    archive.write("logs.txt")
    archive.printdir()
```

```
File Name                                             Modified             Size
info.txt                                       2022-04-04 10:10:10           13
logs.txt                                       2022-04-04 10:10:11            7
```

O texto é codificado em bytes quando o arquivamos, então precisamos usar o método `decode()` para que as strings sejam legíveis por humanos quando lemos diretamente de um arquivo zip:

```
with ZipFile("archive.zip") as archive:
    with archive.open("logs.txt") as f:
        print(f.read().decode())
```

```
new_log
```

### JSON

JSON é um formato de armazenamento e transferência de dados universal e legível por humanos muito usado para manipular dados em aplicativos web. Arquivos JSON se parecem muito com dicionários Python: são conjuntos de pares chave-valor que costumam têm muitas camadas aninhadas.

Podemos usar a biblioteca integrada `json` para ter facilidade em trabalhar com arquivos `.json`.

A convenção `dump/loads` é usada quando há uma necessidade de salvar/ler arquivos `.json`.

```
import json
import requests # biblioteca para procurar dados na internet

# baixando (obtendo) um arquivo json da internet
data = requests.get('https://dummyjson.com/products/1')

# extraindo o conteúdo do arquivo json baixado
text = data.text

# analisando o conteúdo usando a função loads
print(json.loads(text))
```

```
{'id': 1, 'title': 'Essence Mascara Lash Princess', 'description': 'The Essence Mascara Lash Princess is a popular mascara known for its volumizing and lengthening effects. Achieve dramatic lashes with this long-lasting and cruelty-free formula.', 'category': 'beauty', 'price': 9.99, 'discountPercentage': 7.17, 'rating': 4.94, 'stock': 5, 'tags': ['beauty', 'mascara'], 'brand': 'Essence', 'sku': 'RCH45Q1A', 'weight': 2, 'dimensions': {'width': 23.17, 'height': 14.43, 'depth': 28.01}, 'warrantyInformation': '1 month warranty', 'shippingInformation': 'Ships in 1 month', 'availabilityStatus': 'Low Stock', 'reviews': [{'rating': 2, 'comment': 'Very unhappy with my purchase!', 'date': '2024-05-23T08:56:21.618Z', 'reviewerName': 'John Doe', 'reviewerEmail': 'john.doe@x.dummyjson.com'}, {'rating': 2, 'comment': 'Not as described!', 'date': '2024-05-23T08:56:21.618Z', 'reviewerName': 'Nolan Gonzalez', 'reviewerEmail': 'nolan.gonzalez@x.dummyjson.com'}, {'rating': 5, 'comment': 'Very satisfied!', 'date': '2024-05-23T08:56:21.618Z', 'reviewerName': 'Scarlett Wright', 'reviewerEmail': 'scarlett.wright@x.dummyjson.com'}], 'returnPolicy': '30 days return policy', 'minimumOrderQuantity': 24, 'meta': {'createdAt': '2024-05-23T08:56:21.618Z', 'updatedAt': '2024-05-23T08:56:21.618Z', 'barcode': '9164035109868', 'qrCode': 'https://dummyjson.com/public/qr-code.png'}, 'images': ['https://cdn.dummyjson.com/products/images/beauty/Essence%20Mascara%20Lash%20Princess/1.png'], 'thumbnail': 'https://cdn.dummyjson.com/products/images/beauty/Essence%20Mascara%20Lash%20Princess/thumbnail.png'}
```

É importante observar como ficam os dados `data` quando usamos `requests.get()` para baixar um arquivo `'<https://dummyjson.com/products/1'`:

```
{
  "id": 1,
  "title": "Essence Mascara Lash Princess",
  "description": "The Essence Mascara Lash Princess is a popular mascara known for its volumizing and lengthening effects. Achieve dramatic lashes with this long-lasting and cruelty-free formula.",
  "category": "beauty",
  "price": 9.99,
  "discountPercentage": 7.17,
  "rating": 4.94,
  "stock": 5,
  "tags": [
    "beauty",
    "mascara"
  ],
  "brand": "Essence",
  "sku": "RCH45Q1A",
  "weight": 2,
  "dimensions": {
    "width": 23.17,
    "height": 14.43,
    "depth": 28.01
  },
  "warrantyInformation": "1 month warranty",
  "shippingInformation": "Ships in 1 month",
  "availabilityStatus": "Low Stock",
  "reviews": [
    {
      "rating": 2,
      "comment": "Very unhappy with my purchase!",
      "date": "2024-05-23T08:56:21.618Z",
      "reviewerName": "John Doe",
      "reviewerEmail": "john.doe@x.dummyjson.com"
    },
    {
      "rating": 2,
      "comment": "Not as described!",
      "date": "2024-05-23T08:56:21.618Z",
      "reviewerName": "Nolan Gonzalez",
      "reviewerEmail": "nolan.gonzalez@x.dummyjson.com"
    },
    {
      "rating": 5,
      "comment": "Very satisfied!",
      "date": "2024-05-23T08:56:21.618Z",
      "reviewerName": "Scarlett Wright",
      "reviewerEmail": "scarlett.wright@x.dummyjson.com"
    }
  ],
  "returnPolicy": "30 days return policy",
  "minimumOrderQuantity": 24,
  "meta": {
    "createdAt": "2024-05-23T08:56:21.618Z",
    "updatedAt": "2024-05-23T08:56:21.618Z",
    "barcode": "9164035109868",
    "qrCode": "https://dummyjson.com/public/qr-code.png"
  },
  "images": [
    "https://cdn.dummyjson.com/products/images/beauty/Essence%20Mascara%20Lash%20Princess/1.png"
  ],
  "thumbnail": "https://cdn.dummyjson.com/products/images/beauty/Essence%20Mascara%20Lash%20Princess/thumbnail.png"
}
```

O texto está na forma de uma string. Embora se pareça com um arquivo JSON, ele é baixado como uma string usando `requests.get()`. Portanto, a função `json.loads()` precisa convertê-la (ou analisá-la) em um verdadeiro arquivo JSON.

Agora vamos ver como gravar algo em um arquivo JSON:

```
import json

# aqui está um dicionário que queremos gravar em um arquivo JSON
data = dict(user_id=12, status='active', user_name='Rachel')

# criamos um arquivo json no modo de gravação e armazenamos os dados nele
with open ('output.json', 'w') as f:
    json.dump(data, f)
```

### YAML

Esse formato é adequado para transmitir definições de configuração e possui formatação específica, incluindo recuo no estilo Python. Ele é um superconjunto de JSON, o que significa que qualquer texto JSON também é um texto YAML adequado, mas YAML tende a ser mais legível por humanos. Dê uma olhada em como são um dicionário e uma lista quando traduzidos para arquivos YAML. Para fazer isso, precisaremos usar a biblioteca `yaml`:

```
import yaml

# criando uma lista de dicionários
data = [{'user_id': '1', 'age': 34},
         {'user_id': '2', 'age': 37}]

# salvando como yaml
print(yaml.dump(data))

# criando uma lista
var = [22, 23, 'root']

# salvando como yaml
print(yaml.dump(var))
```

```
- age: 34
  user_id: '1'
- age: 37
  user_id: '2'

- 22
- 23
- root
```

Entrada e saída de arquivos (File I/O)

Tarefa3 / 3

1.

Crie um arquivo chamado `lines.txt` e escreva nele o conteúdo da lista fornecida como linhas separadas. Use o método `writelines()`.

Lembre-se de que você pode gravar o conteúdo de uma lista fornecida como linhas separadas usando o método `.join()` aplicado a uma string que separa cada elemento da lista. O método `.join()` espera a lista como entrada.

2.

O pré-código contém o código que grava o conteúdo da lista fornecida como linhas separadas. Você precisa ler o arquivo `lines.txt` e imprimir as linhas uma por uma. Não se esqueça de que cada linha contém um caractere de nova linha — `\n'`. Você precisa removê-lo usando um dos métodos que aprendeu.

3.

Nesta tarefa, você vai trabalhar com arquivos JSON.

No pré-código, há dados que baixamos para você usando a biblioteca `requests`. Usamos este URL para baixar os dados: [https://dummyjson.com/products/category/smartphones](https://dummyjson.com/products/category/smartphones). Feito isso, extraímos o corpo principal da resposta usando o atributo `.text` da resposta.

Agora é sua vez de brilhar. Analise os dados baixados como um arquivo JSON; você pode usar a biblioteca `json` e a função apropriada. Armazene os dados analisados na variável `json_data`. Após armazenar os dados, você pode trabalhar com `json_data` como faria com um dicionário Python comum.

Agora, extraia a lista de produtos de `json_data` e armazene-a na variável `products`. A lista está armazenada como um valor. Você pode extraí-la usando a chave `'products'`.

Depois, você verá que criamos uma lista vazia chamada `items`. Além disso, inicializamos a variável `brand` para armazenar o valor `'Samsung'`.

Seu objetivo é percorrer a lista `products`. Cada entrada em `products` é um dicionário. Extraia o nome da marca usando a chave `'brand'` e, se ele corresponder ao valor na variável `brand`, anexe toda a entrada à lista `items`.

Por fim, crie um arquivo `'samsung_items.json'` e salve `items` nele.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

import json

import requests

  

\# busque dados na internet

response \= requests.get('https://dummyjson.com/products/category/smartphones')

text \= response.text

  

json\_data \= json.loads(text)\# escreva seu código aqui

products \= json\_data\["products"\]\# escreva seu código aqui. Use a chave 'products'

  

items \= \[\]

brand \= 'Samsung'

  

for entry in products:

if entry\['brand'\] \== brand:

items.append(entry)

  

with open('samsung\_items.json', 'w') as f:

json.dump(items, f)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-27-11-761Z.md
### Última modificação: 2025-05-28 19:27:12

# API Web - TripleTen

Teoria

# API Web

![](https://practicum-content.s3.amazonaws.com/resources/Montazhnaia_oblast_1-100_1695284174.jpg)

## HTTP

**HTTP** (Hypertext Transfer Protocol, ou Protocolo de Transferência de Hipertexto em português) é um protocolo da internet, um conjunto de regras para transferência de dados entre dois sistemas de computador; normalmente, entre um navegador e um site. Mas um navegador web não é o único tipo possível de cliente. Outros programas também são aceitos, incluindo um programa Python. Abaixo, vamos aprender como trabalhar com recursos web usando Python e ferramentas de linha de comando.

### APIs REST

Os recursos web são ótimos para compartilhar dados e fornecer serviços. Nós os usamos para realizar cálculos, ouvir música e assim por diante. Um usuário médio de um serviço não precisa ter nenhum conhecimento especial sobre programação. Geralmente, uma interface gráfica é aplicada para permitir a interação com o servidor (um navegador no caso de recursos web). Mas e se quisermos recuperar alguns dados web automaticamente a cada hora, 24 horas por dia, 7 dias por semana? Ou se precisarmos usar algum serviço como parte de um pipeline de dados maior? Abrir um navegador e fazer isso manualmente não parece ser a melhor opção. É aí que a API web entra em ação.

**API** é uma sigla em inglês para Application Programming Interface, que significa Interface de Programação de Aplicativos. Ela especifica como um programa (em contraste com uma pessoa) pode interagir com um serviço ou dados. **REST** (sigla em inglês para Representational State Transfer, que significa Transferência de Estado Representacional) é um conjunto de princípios (estilo de arquitetura de software) desenvolvido no ano 2000 e muito usado para aplicativos web atualmente. Conforme o REST, existem dois lados: um do **cliente** e um do **servidor**, que são bastante independentes.

O cliente usa o servidor para acessar **recursos**, o **URI** (sigla em inglês para Uniform Resource Identifier: Identificador Uniforme de Recursos) é tudo o que o cliente precisa saber, e toda a magia acontece no lado do servidor. O **URI** para recursos web é conhecido como **URL (Uniform Resource Locator — Localizador Uniforme de Recursos)**. Eles interagem mediante solicitações **HTTP** autossuficientes, que são leves e universais. Um cliente envia uma solicitação chamando uma URL (então, uma **chamada** é cada tentativa de receber uma resposta de um servidor), e o servidor retorna a resposta.

![](https://practicum-content.s3.amazonaws.com/resources/26_07_DA_DS-11_1695382244.png)

O formato JSON, legível e multilíngue, costuma ser usado para a troca de dados. O servidor não considera as interações anteriores ou a origem da solicitação e funciona de maneira uniforme, dando a mesma resposta para a mesma solicitação. O cliente não tenta alterar o estado do servidor. Além disso, URIs e mensagens de erro devem ser legíveis e autoexplicativos.

Existe uma variedade de métodos no padrão HTTP, e os mais úteis são chamados de **GET** e **POST**. Seus nomes se originam da ideia de que GET é usado para buscar algo em um servidor e POST é para alterar algo em um servidor.

Python é uma ferramenta confortável para trabalhar com recursos web. Vamos usar uma biblioteca muito popular chamada `requests` para aprender como obter dados de recursos web.

### Obter dados mediante uma API

Vamos revisar uma aplicação real do método GET. Imagine que você precise conhecer as taxas de câmbio do dia para calcular o valor de uma carteira de investimentos em USD. Como buscar essas taxas automaticamente? Primeiro, precisamos encontrar uma API web que forneça dados mediante solicitações adequadas. Vamos começar com uma que não requer autenticação.

Aqui está o **URL base:** `https://api.frankfurter.app`. Parece um link de internet normal, acessar [www.frankfurter.app](http://www.frankfurter.app) irá informá-lo sobre como usá-lo. Podemos aprender que `/latest` direciona para as taxas atuais. O URL final `https://api.frankfurter.app/latest` é chamado de **endpoint**. O uso básico é o seguinte:

```
import requests

response = requests.get("https://api.frankfurter.app/latest")
print(response)
```

```
<Response [200]>
```

Aqui usamos a função `get()` da biblioteca de solicitações `requests` integrada para buscar algo no recurso fornecido. O que recebemos foi um objeto `response` (documentado [aqui](https://requests.readthedocs.io/en/latest/api/#requests.Response) _(os materiais estão em inglês)_), que consiste em vários conjuntos de informações: código de status, cabeçalho, conteúdo e alguns outros. Quando tentamos examinar o objeto com `print()`, vemos o **código de status HTTP** por padrão. Ele mostra como a solicitação foi executada e consiste em três dígitos: 200 significa que está tudo bem, e 4XX e 5XX indicam problemas do lado do cliente e do servidor. Definitivamente queremos acessar mais informações, então vamos usar o método `json()`, que analisa a parte do conteúdo da resposta:

```
import requests

response = requests.get("https://api.frankfurter.app/latest")
print(response.json())
```

```
{'amount': 1.0, 'base': 'EUR', 'date': '2022-04-19', 'rates': {'AUD': 1.4663, 'BGN': 1.9558, 'BRL': 5.0261, 'CAD': 1.3631, 'CHF': 1.0208, 'CNY': 6.9008, 'CZK': 24.424, 'DKK': 7.4391, 'GBP': 0.82955, 'HKD': 8.4698, 'HRK': 7.562, 'HUF': 374.12, 'IDR': 15498, 'ILS': 3.5038, 'INR': 82.6, 'ISK': 139.8, 'JPY': 138.4, 'KRW': 1339.46, 'MXN': 21.472, 'MYR': 4.5961, 'NOK': 9.5228, 'NZD': 1.6016, 'PHP': 56.683, 'PLN': 4.6553, 'RON': 4.9411, 'SEK': 10.3408, 'SGD': 1.4763, 'THB': 36.466, 'TRY': 15.8416, 'USD': 
1.0803, 'ZAR': 16.0401}}
```

Isso é muito mais útil para o nosso propósito: vemos o valor, a moeda base e a data, seguidos de quanto das outras moedas podem ser compradas por esse valor. Vamos ajustar o pedido. Quantas libras podem ser compradas com 20 dólares? Ao ler a documentação no site [aqui](https://www.frankfurter.app/docs) _(os materiais estão em inglês)_, aprendemos que devemos usar os parâmetros `from` (de), `to` (para) e `amount` (quantia). Por conveniência, `requests` suporta a passagem de parâmetros para uma solicitação HTTP como um dicionário Python normal:

```
import requests

params = {"from":"USD", "to":"GBP", "amount":20}
res = requests.get("https://api.frankfurter.app/latest", params=params)
print(res.json())
```

```
{'amount': 20.0, 'base': 'USD', 'date': '2022-04-19', 'rates': {'GBP': 15.3578}}
```

Como alternativa, os parâmetros podem ir após o link conforme o esquema: `endpoint?param_1=value&param_2=value`. A solicitação a seguir vai terminar com o mesmo resultado visto acima:

```
import requests

res = requests.get("https://api.frankfurter.app/latest?from=USD&to=GBP&amount=20")
print(res.json())
```

```
{'amount': 20.0, 'base': 'USD', 'date': '2022-04-19', 'rates': {'GBP': 15.3578}}
```

O primeiro método de passagem de parâmetros (como um dicionário) é aconselhável porque nos permite evitar combinar todos os parâmetros em apenas uma string longa e leva a um código mais limpo.

Além do código de status e do conteúdo, as respostas contêm `headers` (cabeçalhos) com **metadados**, dados no nível do protocolo HTTP nesse caso. Lá podemos encontrar algumas informações técnicas sobre a resposta: versão do servidor, tipo de conteúdo, codificação, data, etc:

```
import requests

response = requests.get("https://api.frankfurter.app/latest")

print(response.headers['Server'])
print(response.headers['Content-Type'])
print(response.headers['Content-Length'])
```

```
nginx/1.21.3
application/json; charset=utf-8
457
```

Essas propriedades, especialmente o tipo de conteúdo, podem ser úteis para entender as especificidades de uma resposta. Por exemplo, um servidor web pode responder em um formato diferente de JSON (indicado no cabeçalho como `application/json`), o que seria uma dica (se não soubermos nada de API) para tratar os dados da resposta de maneira diferente (por exemplo, pode haver dados no formato de texto simples ou dados binários).

### Autenticação

O endpoint nos exemplos acima permite obter dados sem qualquer autenticação, como em um cliente que não apresenta e comprova sua identidade exclusiva. Isso é bastante fora do comum, porque muitos recursos web exigem alguma forma de autenticação para recuperar dados ou obter acesso a suas funcionalidades maiores, e muitos aplicativos também podem cobrar por suas funcionalidades completas. No entanto, a maioria das APIs permite um certo nível de busca de dados sem custo adicional.

Normalmente, a autenticação requer a existência de um segredo mútuo conhecido apenas pelo cliente e pelo recurso web. Pode ser o par clássico de nome de usuário e senha ou uma **chave de API** ou token. A chave de API é como uma senha, mas sem um nome de usuário. Ela costuma ser uma string criptografada como a seguinte:

```
0aebfa14099649068fcddcac9eabbed8
```

Ela é considerada um "segredo" devido à grande dificuldade de adivinhar seu valor exato; há muitas variantes a serem consideradas ao tentar adivinhá-la manualmente. É por isso que as chaves de API são compartilhadas apenas entre as pessoas ou sistemas estritamente obrigados a conhecê-las. Compartilhá-las publicamente quebraria sua segurança.

Vamos ver como podemos obter dados sobre as condições climáticas com a API HTTP de um recurso popular da web sobre previsão do tempo: [https://www.accuweather.com/](https://www.accuweather.com/). Esse recurso fornece a documentação de sua rica API localizada [aqui](https://developer.accuweather.com/apis). Ele também oferece pacotes diferentes listados [aqui](https://developer.accuweather.com/packages), mas não há problema em usar o gratuito (observe que ele é restrito a não mais de 50 solicitações por dia).

Na página [Getting Started](https://developer.accuweather.com/getting-started) _(os materiais estão em inglês)_, há uma explicação do processo pelo qual é necessário passar para começar a trabalhar com esses recursos web. As principais etapas:

-   Registrar-se no site clicando em "Register" no canto superior direito.
-   Preencher o formulário, confirmar seu endereço de e-mail fazendo login com o link do e-mail de confirmação, definir a senha permanente e concluir o perfil. É como um procedimento de registro padrão para todos os sites.
-   Agora que você se registrou, vá para a aba My Apps (meus aplicativos) e clique em "Add New App" (adicionar novo aplicativo), preencha o nome como 'test\_app' (ou qualquer outro de sua preferência), preencha o restante dos campos e crie o aplicativo.
-   Abaixo está uma captura de tela de exemplo de registro de um aplicativo no AccuWeather:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_1695284396.png)

-   Seu novo aplicativo vai aparecer na lista de aplicativos; clique no nome para expandi-lo e ver a chave de API secreta (algo como `zC5JOYIAeAgn92Z9CJKUQE1ns3dvHeyf`).

Agora que tem a chave para trabalhar com a API do AccuWeather, você pode obter algumas informações úteis por meio de sua API; por exemplo, a previsão do tempo de 5 dias para um determinado local.

Primeiro, precisamos obter o identificador de localização do AccuWeather chamando sua API de localização e obtendo os identificadores pelo nome do local (essa é apenas uma possibilidade). Vamos pesquisar pelo nome "New York".

```
import requests

api_key = 'zC5JOYIAeAgn92Z9CJKUQE1ns3dvHeyf'  # exemplo da chave de API, não é uma chave verdadeira
params = {'apikey': api_key, 'q': 'New York'}

aw_location_url = "https://dataservice.accuweather.com/locations/v1/cities/search"
aw_location_res = requests.get(aw_location_url, params=params)
```

O resultado retornado é uma lista de locais que o AccuWeather associa ao nome fornecido. Está no formato JSON e contém muitos campos (documentados [aqui](https://developer.accuweather.com/accuweather-locations-api/apis/get/locations/v1/cities/search) _(os materiais estão em inglês)_), que podemos auditar ao imprimir o resultado inteiro.

```
import pprint

pprint.pprint(aw_location_res.json())
```

El resultado

```
[{'AdministrativeArea': {'CountryID': 'US',
                         'EnglishName': 'New York',
                         'EnglishType': 'State',
                         'ID': 'NY',
                         'Level': 1,
                         'LocalizedName': 'New York',
                         'LocalizedType': 'State'},
  'Country': {'EnglishName': 'United States',
              'ID': 'US',
              'LocalizedName': 'United States'},
  'DataSets': ['AirQualityCurrentConditions',
               'AirQualityForecasts',
               'Alerts',
               'DailyAirQualityForecast',
               'DailyPollenForecast',
               'ForecastConfidence',
               'FutureRadar',
               'MinuteCast',
               'Radar'],
  'EnglishName': 'New York',
  'GeoPosition': {'Elevation': {'Imperial': {'Unit': 'ft',
                                             'UnitType': 0,
                                             'Value': 26.0},
                                'Metric': {'Unit': 'm',
                                           'UnitType': 5,
                                           'Value': 8.0}},
                  'Latitude': 40.779,
                  'Longitude': -73.969},
  'IsAlias': False,
  'Key': '349727',
  'LocalizedName': 'New York',
  'PrimaryPostalCode': '10021',
  'Rank': 15,
  'Region': {'EnglishName': 'North America',
             'ID': 'NAM',
             'LocalizedName': 'North America'},
  'SupplementalAdminAreas': [{'EnglishName': 'New York',
                              'Level': 2,
                              'LocalizedName': 'New York'}],
  'TimeZone': {'Code': 'EDT',
               'GmtOffset': -4.0,
               'IsDaylightSaving': True,
               'Name': 'America/New_York',
               'NextOffsetChange': '2022-11-06T06:00:00Z'},
  'Type': 'City',
  'Version': 1},
 {'AdministrativeArea': {'CountryID': 'GB',
                         'EnglishName': 'Lincolnshire',
                         'EnglishType': 'Non-Metropolitan County',
                         'ID': 'LIN',
                         'Level': 1,
                         'LocalizedName': 'Lincolnshire',
                         'LocalizedType': 'Non-Metropolitan County'},
  'Country': {'EnglishName': 'United Kingdom',
              'ID': 'GB',
              'LocalizedName': 'United Kingdom'},
  'DataSets': ['AirQualityCurrentConditions',
               'AirQualityForecasts',
               'Alerts',
               'DailyPollenForecast',
               'ForecastConfidence',
               'FutureRadar',
               'MinuteCast',
               'Radar'],
  'EnglishName': 'New York',
  'GeoPosition': {'Elevation': {'Imperial': {'Unit': 'ft',
                                             'UnitType': 0,
                                             'Value': 65.0},
                                'Metric': {'Unit': 'm',
                                           'UnitType': 5,
                                           'Value': 20.0}},
                  'Latitude': 53.078,
                  'Longitude': -0.137},
  'IsAlias': False,
  'Key': '710949',
  'LocalizedName': 'New York',
  'PrimaryPostalCode': 'LN4 4',
  'Rank': 85,
  'Region': {'EnglishName': 'Europe', 'ID': 'EUR', 'LocalizedName': 'Europe'},
  'SupplementalAdminAreas': [{'EnglishName': 'England',
                              'Level': 0,
                              'LocalizedName': 'England'},
                             {'EnglishName': 'East Lindsey',
                              'Level': 2,
                              'LocalizedName': 'East Lindsey'}],
  'TimeZone': {'Code': 'BST',
               'GmtOffset': 1.0,
               'IsDaylightSaving': True,
               'Name': 'Europe/London',
               'NextOffsetChange': '2022-10-30T01:00:00Z'},
  'Type': 'City',
  'Version': 1},
 {'AdministrativeArea': {'CountryID': 'GB',
                         'EnglishName': 'North Tyneside',
                         'EnglishType': 'Metropolitan Borough',
                         'ID': 'NTY',
                         'Level': 1,
                         'LocalizedName': 'North Tyneside',
                         'LocalizedType': 'Metropolitan Borough'},
  'Country': {'EnglishName': 'United Kingdom',
              'ID': 'GB',
              'LocalizedName': 'United Kingdom'},
  'DataSets': ['AirQualityCurrentConditions',
               'AirQualityForecasts',
               'Alerts',
               'DailyPollenForecast',
               'ForecastConfidence',
               'FutureRadar',
               'MinuteCast',
               'Radar'],
  'EnglishName': 'New York',
  'GeoPosition': {'Elevation': {'Imperial': {'Unit': 'ft',
                                             'UnitType': 0,
                                             'Value': 78.0},
                                'Metric': {'Unit': 'm',
                                           'UnitType': 5,
                                           'Value': 24.0}},
                  'Latitude': 55.026,
                  'Longitude': -1.486},
  'IsAlias': False,
  'Key': '2531279',
  'LocalizedName': 'New York',
  'PrimaryPostalCode': 'NE27 0',
  'Rank': 85,
  'Region': {'EnglishName': 'Europe', 'ID': 'EUR', 'LocalizedName': 'Europe'},
  'SupplementalAdminAreas': [{'EnglishName': 'England',
                              'Level': 0,
                              'LocalizedName': 'England'}],
  'TimeZone': {'Code': 'BST',
               'GmtOffset': 1.0,
               'IsDaylightSaving': True,
               'Name': 'Europe/London',
               'NextOffsetChange': '2022-10-30T01:00:00Z'},
  'Type': 'City',
  'Version': 1},
 {'AdministrativeArea': {'CountryID': 'US',
                         'EnglishName': 'Florida',
                         'EnglishType': 'State',
                         'ID': 'FL',
                         'Level': 1,
                         'LocalizedName': 'Florida',
                         'LocalizedType': 'State'},
  'Country': {'EnglishName': 'United States',
              'ID': 'US',
              'LocalizedName': 'United States'},
  'DataSets': ['AirQualityCurrentConditions',
               'AirQualityForecasts',
               'Alerts',
               'DailyAirQualityForecast',
               'DailyPollenForecast',
               'ForecastConfidence',
               'FutureRadar',
               'MinuteCast',
               'Radar'],
  'EnglishName': 'New York',
  'GeoPosition': {'Elevation': {'Imperial': {'Unit': 'ft',
                                             'UnitType': 0,
                                             'Value': 203.0},
                                'Metric': {'Unit': 'm',
                                           'UnitType': 5,
                                           'Value': 62.0}},
                  'Latitude': 30.839,
                  'Longitude': -87.201},
  'IsAlias': False,
  'Key': '2245721',
  'LocalizedName': 'New York',
  'PrimaryPostalCode': '32565',
  'Rank': 385,
  'Region': {'EnglishName': 'North America',
             'ID': 'NAM',
             'LocalizedName': 'North America'},
  'SupplementalAdminAreas': [{'EnglishName': 'Santa Rosa',
                              'Level': 2,
                              'LocalizedName': 'Santa Rosa'}],
  'TimeZone': {'Code': 'CDT',
               'GmtOffset': -5.0,
               'IsDaylightSaving': True,
               'Name': 'America/Chicago',
               'NextOffsetChange': '2022-11-06T07:00:00Z'},
  'Type': 'City',
  'Version': 1},
 {'AdministrativeArea': {'CountryID': 'US',
                         'EnglishName': 'Iowa',
                         'EnglishType': 'State',
                         'ID': 'IA',
                         'Level': 1,
                         'LocalizedName': 'Iowa',
                         'LocalizedType': 'State'},
  'Country': {'EnglishName': 'United States',
              'ID': 'US',
              'LocalizedName': 'United States'},
  'DataSets': ['AirQualityCurrentConditions',
               'AirQualityForecasts',
               'Alerts',
               'DailyAirQualityForecast',
               'DailyPollenForecast',
               'ForecastConfidence',
               'FutureRadar',
               'MinuteCast',
               'Radar'],
  'EnglishName': 'New York',
  'GeoPosition': {'Elevation': {'Imperial': {'Unit': 'ft',
                                             'UnitType': 0,
                                             'Value': 1052.0},
                                'Metric': {'Unit': 'm',
                                           'UnitType': 5,
                                           'Value': 321.0}},
                  'Latitude': 40.852,
                  'Longitude': -93.26},
  'IsAlias': False,
  'Key': '2213492',
  'LocalizedName': 'New York',
  'PrimaryPostalCode': '50238',
  'Rank': 385,
  'Region': {'EnglishName': 'North America',
             'ID': 'NAM',
             'LocalizedName': 'North America'},
  'SupplementalAdminAreas': [{'EnglishName': 'Wayne',
                              'Level': 2,
                              'LocalizedName': 'Wayne'}],
  'TimeZone': {'Code': 'CDT',
               'GmtOffset': -5.0,
               'IsDaylightSaving': True,
               'Name': 'America/Chicago',
               'NextOffsetChange': '2022-11-06T07:00:00Z'},
  'Type': 'City',
  'Version': 1},
 {'AdministrativeArea': {'CountryID': 'US',
                         'EnglishName': 'Kentucky',
                         'EnglishType': 'State',
                         'ID': 'KY',
                         'Level': 1,
                         'LocalizedName': 'Kentucky',
                         'LocalizedType': 'State'},
  'Country': {'EnglishName': 'United States',
              'ID': 'US',
              'LocalizedName': 'United States'},
  'DataSets': ['AirQualityCurrentConditions',
               'AirQualityForecasts',
               'Alerts',
               'DailyAirQualityForecast',
               'DailyPollenForecast',
               'ForecastConfidence',
               'FutureRadar',
               'MinuteCast',
               'Radar'],
  'EnglishName': 'New York',
  'GeoPosition': {'Elevation': {'Imperial': {'Unit': 'ft',
                                             'UnitType': 0,
                                             'Value': 455.0},
                                'Metric': {'Unit': 'm',
                                           'UnitType': 5,
                                           'Value': 139.0}},
                  'Latitude': 36.989,
                  'Longitude': -88.953},
  'IsAlias': False,
  'Key': '2179646',
  'LocalizedName': 'New York',
  'PrimaryPostalCode': '42087',
  'Rank': 385,
  'Region': {'EnglishName': 'North America',
             'ID': 'NAM',
             'LocalizedName': 'North America'},
  'SupplementalAdminAreas': [{'EnglishName': 'Ballard',
                              'Level': 2,
                              'LocalizedName': 'Ballard'}],
  'TimeZone': {'Code': 'CDT',
               'GmtOffset': -5.0,
               'IsDaylightSaving': True,
               'Name': 'America/Chicago',
               'NextOffsetChange': '2022-11-06T07:00:00Z'},
  'Type': 'City',
  'Version': 1},
 {'AdministrativeArea': {'CountryID': 'US',
                         'EnglishName': 'New Mexico',
                         'EnglishType': 'State',
                         'ID': 'NM',
                         'Level': 1,
                         'LocalizedName': 'New Mexico',
                         'LocalizedType': 'State'},
  'Country': {'EnglishName': 'United States',
              'ID': 'US',
              'LocalizedName': 'United States'},
  'DataSets': ['AirQualityCurrentConditions',
               'AirQualityForecasts',
               'Alerts',
               'DailyAirQualityForecast',
               'DailyPollenForecast',
               'ForecastConfidence',
               'FutureRadar',
               'MinuteCast',
               'Radar'],
  'EnglishName': 'New York',
  'GeoPosition': {'Elevation': {'Imperial': {'Unit': 'ft',
                                             'UnitType': 0,
                                             'Value': 6484.0},
                                'Metric': {'Unit': 'm',
                                           'UnitType': 5,
                                           'Value': 1976.0}},
                  'Latitude': 35.059,
                  'Longitude': -107.527},
  'IsAlias': False,
  'Key': '2212053',
  'LocalizedName': 'New York',
  'PrimaryPostalCode': '87007',
  'Rank': 385,
  'Region': {'EnglishName': 'North America',
             'ID': 'NAM',
             'LocalizedName': 'North America'},
  'SupplementalAdminAreas': [{'EnglishName': 'Cibola',
                              'Level': 2,
                              'LocalizedName': 'Cibola'}],
  'TimeZone': {'Code': 'MDT',
               'GmtOffset': -6.0,
               'IsDaylightSaving': True,
               'Name': 'America/Denver',
               'NextOffsetChange': '2022-11-06T08:00:00Z'},
  'Type': 'City',
  'Version': 1},
 {'AdministrativeArea': {'CountryID': 'US',
                         'EnglishName': 'Texas',
                         'EnglishType': 'State',
                         'ID': 'TX',
                         'Level': 1,
                         'LocalizedName': 'Texas',
                         'LocalizedType': 'State'},
  'Country': {'EnglishName': 'United States',
              'ID': 'US',
              'LocalizedName': 'United States'},
  'DataSets': ['AirQualityCurrentConditions',
               'AirQualityForecasts',
               'Alerts',
               'DailyAirQualityForecast',
               'DailyPollenForecast',
               'ForecastConfidence',
               'FutureRadar',
               'MinuteCast',
               'Radar'],
  'EnglishName': 'New York',
  'GeoPosition': {'Elevation': {'Imperial': {'Unit': 'ft',
                                             'UnitType': 0,
                                             'Value': 419.0},
                                'Metric': {'Unit': 'm',
                                           'UnitType': 5,
                                           'Value': 128.0}},
                  'Latitude': 32.168,
                  'Longitude': -95.669},
  'IsAlias': False,
  'Key': '2185062',
  'LocalizedName': 'New York',
  'PrimaryPostalCode': '75770',
  'Rank': 385,
  'Region': {'EnglishName': 'North America',
             'ID': 'NAM',
             'LocalizedName': 'North America'},
  'SupplementalAdminAreas': [{'EnglishName': 'Henderson',
                              'Level': 2,
                              'LocalizedName': 'Henderson'}],
  'TimeZone': {'Code': 'CDT',
               'GmtOffset': -5.0,
               'IsDaylightSaving': True,
               'Name': 'America/Chicago',
               'NextOffsetChange': '2022-11-06T07:00:00Z'},
  'Type': 'City',
  'Version': 1}]
```

Essa é uma estrutura longa; por isso, pode ser mais prático iterar sobre a lista e imprimir apenas o conjunto de campos selecionados para entender qual local é exatamente _aquela_ New York que é do nosso interesse.

```
for loc_info in aw_location_res.json():
    print('{:>8}   {:10}   {:16}    {:16}'.format(
        loc_info['Key'],
        loc_info['EnglishName'],
        loc_info['Country']['EnglishName'],
        loc_info['AdministrativeArea']['EnglishName']))
```

```
  349727   New York     United States       New York        
  710949   New York     United Kingdom      Lincolnshire    
 2531279   New York     United Kingdom      North Tyneside  
 2245721   New York     United States       Florida         
 2213492   New York     United States       Iowa            
 2179646   New York     United States       Kentucky        
 2212053   New York     United States       New Mexico      
 2185062   New York     United States       Texas
```

Uau, existem mesmo várias Novas Yorks por aí. Digamos que nosso interesse esteja na Nova York da primeira linha. Agora que sabemos a chave de localização, podemos consultar a previsão para esse local chamando um endpoint diferente, da API de previsão do AccuWeather.

```
import requests

api_key = 'zC5JOYIAeAgn92Z9CJKUQE1ns3dvHeyf'
params = {'apikey': api_key, 'metric': True}

location_id = 349727
aw_forecast_url = "https://dataservice.accuweather.com/forecasts/v1/daily/5day/" + str(location_id)
aw_forecast_res = requests.get(aw_forecast_url, params=params)
```

O resultado também é uma estrutura JSON longa (documentada [aqui](https://developer.accuweather.com/accuweather-forecast-api/apis/get/forecasts/v1/daily/5day/%7BlocationKey%7D) _(os materiais estão em inglês)_). Como acima, podemos estudá-lo imprimindo a coisa toda, mas vamos poupar essa parte aqui e apenas imprimir alguns campos nos quais temos mais interesse.

```
for daily_forecast in aw_forecast_res.json()['DailyForecasts']:
    print('{}   {:30} {}{}  {}{}'.format(
        daily_forecast['Date'], 
        daily_forecast['Day']['IconPhrase'], 
        daily_forecast['Temperature']['Minimum']['Value'],
        daily_forecast['Temperature']['Minimum']['Unit'], 
        daily_forecast['Temperature']['Maximum']['Value'],
        daily_forecast['Temperature']['Maximum']['Unit']))
```

```
2022-06-05T07:00:00-04:00   Mostly sunny                   15.6C  25.0C
2022-06-06T07:00:00-04:00   Partly sunny                   17.2C  26.0C
2022-06-07T07:00:00-04:00   Intermittent clouds            18.9C  25.6C
2022-06-08T07:00:00-04:00   Showers                        18.1C  25.6C
2022-06-09T07:00:00-04:00   Partly sunny w/ t-storms       16.4C  25.7C
```

Desse jeito, recebemos a previsão de 5 dias para New York usando a API. Isso é incrível, considerando que podemos integrar essas coisas nos nossos próprios aplicativos, sistemas domésticos inteligentes e em muitos outros aplicativos e dispositivos — nossa imaginação é o limite.

Um exemplo que todos já devem conhecer seria um programa meteorológico ou widget em um smartphone: ele liga internamente para um dos provedores de condições meteorológicas para mostrar a previsão de tempo na tela em um formato amigável.

## Conclusão

Sites, aplicativos e outros provedores de informações contam com muitos recursos/serviços específicos web nos bastidores, que são integrados entre aplicativos, comunicando entre eles para obter informações. E o conceito de APIs, especialmente da API HTTP, desempenha um papel importante neste processo, sendo um dos mecanismos que permite que diferentes serviços se "compreendam", dando-lhes a mesma "linguagem".

Para um profissional de dados, os recursos web são fontes valiosas de dados, e trabalhar com eles de forma programática nos permite integrar facilmente esses dados em nossos programas. Depois, podemos utilizar esses dados em um fluxo de trabalho específico de análise de dados, construção de um modelo e em muitas outras tarefas.

## Recursos adicionais

Lista de códigos de status HTTP: [https://pt.wikipedia.org/wiki/Lista\_de\_códigos\_de\_estado\_HTTP](https://pt.wikipedia.org/wiki/Lista_de_c%C3%B3digos_de_estado_HTTP). Pode ser útil verificar o que significa um código de status desconhecido.

API Web

Tarefa3 / 3

1.

API web é uma ferramenta versátil usada não apenas por empresas: há muitos museus, bibliotecas, projetos científicos e entusiastas que disponibilizam suas informações e serviços para o público. Use a biblioteca `requests` e o terminal fornecido no `url` para obter informações sobre uma pintura da coleção do Museu Metropolitano. Examine a URL: o link base é `https://collectionapi.metmuseum.org/`, então vamos para um recurso específico. Em algum momento, há uma parte `v1`.Essa é a versão da API. Versões diferentes podem agir de maneira diferente; por isso, é uma prática comum refletir a atual. Depois, o número `437133` é o id da pintura.

1.  Importe `requests`.
2.  Chame o endpoint usando o `url` fornecido. Recupere o resultado no formato JSON aplicando um método apropriado a ele. Salve o JSON resultante na variável `response`.
3.  Imprima `artistDisplayName` do JSON chamando-o pela chave.

2.

Para alcançar o endpoint `public/collection/v1/departments` usando o mesmo URL base, basta anexar `/public/collection/v1/departments` ao URL base. Armazene o URL resultante na variável `url`.

Em seguida, você verá uma linha no pré-código, em que guardamos a resposta do endpoint e a salvamos na variável `response`.

Agora seu objetivo é converter a resposta em um arquivo JSON. Depois, itere sobre seus `response.json()['departments']` e imprima aqueles que tenham `'Art'` em `dpt['displayName'].`.

3.

Nesta tarefa, você vai usar parâmetros de solicitações. O URL para esta tarefa foi salvo na variável `url`. Seu objetivo é definir o parâmetro `params = {'limit': 3}` ao usar o método `get()`. Isso vai nos permitir recuperar apenas as três primeiras entradas. Armazene a resposta na variável `response`. E então a converta para um JSON e imprima

9

1

2

3

4

5

6

7

8

9

import requests

  

url \= 'https://dummyjson.com/products'

  

params \= {'limit': 3}

response \= requests.get(url, params\=params)

  

print(response.json())

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-27-13-094Z.md
### Última modificação: 2025-05-28 19:27:13

# Aplicativos web - TripleTen

Capítulo 5/8

Python intermediário

# Aplicativos web

## Aplicativos web e onde eles vivem

Se você já se perguntou o que está por trás de um site, saiba que é um aplicativo executado em um servidor ou em um grupo de servidores, que são computadores potentes que costumam estar hospedados em centros de dados. Quanto maior for o site, mais sofisticado será o aplicativo ou mesmo o conjunto de aplicativos.

A interface do usuário de grandes sites como Amazon, Facebook ou Google representa a "ponta" do vasto iceberg de muitos aplicativos que implementam muitas ideias brilhantes em execução em muitos servidores potentes. Sites especializados menores podem ser apresentados por apenas um aplicativo executado em um servidor básico.

Aprenderemos como construir seu próprio aplicativo e hospedá-lo nos servidores de um provedor de armazenamento na nuvem para que todos possam acessar a interface de usuário do seu aplicativo, sem que você precise se preocupar com o lado da infraestrutura (o que pode ser muito complicado).

## Python e desenvolvimento web

O Python se popularizou nos últimos anos por desenvolver aplicativos web conquistando considerável participação na área entre outras linguagens de programação populares como PHP, Perl e Ruby. Existem dois **frameworks** genéricos populares (um **framework**, em programação, é uma biblioteca de componentes específicos prontos que auxiliam o desenvolvimento para uma tarefa específica) para desenvolvimento web em Python: Django e Flask, e alguns frameworks específicos como o [Streamlit](https://streamlit.io/) especialmente desenvolvido para Cientistas de Dados (exemplos de aplicativos web feitos com ele podem ser encontrados [aqui](https://streamlit.io/gallery)).

Aprenderemos como construir um aplicativo web com o Streamlit e executá-lo mundialmente com o [Render](https://render.com/), um serviço popular que fornece infraestrutura de hospedagem para aplicativos em execução. Vamos construir um aplicativo simples que vai simular jogadas aleatórias de moeda.

## Render: configuração inicial

Os pré-requisitos para trabalhar com o Render são os seguintes:

-   Uma conta GitHub para hospedar um repositório de projeto;
-   Uma conta Render, que você pode obter gratuitamente registrando-se em [render.com](https://render.com/). Você pode vincular sua conta GitHub diretamente à sua conta Render clicando na opção "GitHub" no momento de inscrição;
-   Um repositório Git local, para hospedar arquivos de origem e implantá-los no Render;
-   Instalação local do `git` para manter o repositório local.

## Render: criar um novo aplicativo

Vá para o GitHub e [crie um novo repositório Git](https://docs.github.com/pt/get-started/quickstart/create-a-repo) para esta lição. Adicione o `.gitignore` padrão de Python e um `README`

O Render requer pelo menos três arquivos para um aplicativo baseado em Python:

-   Um arquivo `requirements.txt` listando todos os pacotes de Python necessários para executar o aplicativo. Essencialmente, esses são todos os pacotes que o programa importa explicitamente. Crie `requirements.txt` no diretório raiz do projeto (ao lado de `README.md`) com as linhas:

```
pandas==1.3.1
scipy==1.6.2
streamlit==1.12.2
```

Você poderia usar outros números das versões para esses 3 pacotes, mas sabe-se que essas versões funcionam bem juntas.

Aqui, você precisa copiar seu repositório para sua máquina local usando `git clone` para trabalhar nele mais tarde.

-   Precisamos adicionar um arquivo especial de configuração `streamlit`. Ele deve estar em `.streamlit/config.toml`. Primeiro, crie um novo diretório no seu repositório Git chamado `.streamlit`. Esse diretório diz a `streamlit` para procurar informações sobre o aplicativo. Depois, adicione um arquivo `config.toml` nesse diretório com o seguinte conteúdo:

```
[server]
headless = true
port = 10000

[browser]
serverAddress = "0.0.0.0"
serverPort = 10000
```

Isso diz ao aplicativo para ser executado no modo de servidor (headless). Após ser iniciado, ele estará esperando por alguém para se conectar no endereço URL 0.0.0.0 na porta 10000 para responder com o aplicativo.

Um host é um local de rede onde um servidor pode ser encontrado, e uma porta corresponde a um serviço particular naquele servidor. Isso é semelhante à forma como uma agência de correios pode ser localizada em um endereço e ter muitas caixas postais para serem usadas por pessoas diferentes.

O Render espera que aplicativos sirvam na porta 10000, então precisamos configurar o arquivo de configuração `.streamlit/config.toml` acima para ser compatível.

-   Um arquivo `app.py` (na pasta raiz do projeto) que vai determinar a lógica do aplicativo em Python

Agora é um bom momento para especificar o que nosso aplicativo fará. Vamos construir um aplicativo simples que emula o lançamento de uma moeda quantas vezes forem solicitadas.

Um aplicativo básico baseado em Streamlit pode ser muito simples:

```
import streamlit as st

st.header('Jogando uma moeda')

st.write('Ainda não é um aplicativo funcional. Em construção.')
```

Salve essas linhas em `app.py` para que haja os seguintes arquivos em seu diretório local:

```
$ ls -a
```

```
.git .gitignore .streamlit app.py README.md requirements.txt
```

Vamos confirmar as alterações para garantir que o repositório esteja atualizado:

```
$ git commit -am "initial commit"
$ git push
```

## Testes do aplicativo

Vamos executar o aplicativo Streamlit localmente. Primeiro, certifique-se de que `streamlit` está instalado:

```
pip install streamlit
```

O nosso aplicativo Streamlit está definido em `app.py`. Para executá-lo localmente, use o comando `streamlit run` da raiz da pasta do repositório de seu projeto:

```
streamlit run app.py
```

A saída deve ligar a um URL que vai hospedar uma página web com o aplicativo vazio. Se você definir o endereço de servidor `serverAddress` como `"0.0.0.0"` na configuração, será necessário digitar `http://0.0.0.0:10000/` no seu navegador para ver a saída.

## Implantação no Render

Para implantar seu aplicativo web no Render, primeiro crie uma conta do Render vinculada a sua conta do GitHub. Quando estiver criando uma conta no Render, selecione a opção "GitHub" e siga os passos de inscrição. Depois crie um novo serviço web:

![](https://practicum-content.s3.amazonaws.com/resources/render_dashboard_1695285046.png)

A seguir, vincule o repositório do GitHub com os arquivos que acabamos de criar de sua conta vinculada do GitHub ao Render. Para fazer isso, selecione-o na página "make new web service" (criar novo serviço web):

![](https://practicum-content.s3.amazonaws.com/resources/render_new_web_service_1695285070.png)

**Observação:** você precisa vincular sua conta do GitHub a sua conta do Render para ver seus repositórios na página acima.

Agora vamos configurar o serviço. Para que o serviço web do Render seja compatível com um aplicativo streamlit, precisamos de algumas configurações. Já adicionamos a configuração para `streamlit` no arquivo `.streamlit/config.toml` no repositório. Para o Render, precisamos adicionar:

-   Na seção **Build Command**, adicionamos `pip install --upgrade pip && pip install -r requirements.txt`
-   Na seção **Start Command**, adicionamos `streamlit run app.py`

A página de configuração será assim:

![](https://practicum-content.s3.amazonaws.com/resources/9_1695797642.png)

Certifique-se de que o mesmo texto está presente nas caixas vermelhas destacadas.

Depois disso, clique no botão para começar o seu primeiro lançamento do aplicativo, e ele iniciará a implantação:

![](https://practicum-content.s3.amazonaws.com/resources/render_build_success_1695285103.png)

Aguarde até que a compilação seja concluída. A implantação leva algum tempo, cerca de 3 a 10 minutos, para construir o aplicativo do zero. Depois de concluído, você pode abrir a URL do aplicativo em `https://{APP_NAME}.onrender.com` (a URL exata está listada no canto superior esquerdo da página de implantação) e ver a interface do aplicativo.

O compilador pode falhar por vários motivos. **Aqui estão algumas dicas de depuração:**

1.  **Você consegue implantar seu aplicativo localmente no seu computador?** Navegue até seu repositório local usando a linha de comando e execute `streamlit run app.py` **Verifique se isso funciona em seu computador**.
2.  **O compilador está concluído no Render?** Seu compilador em [render.com](http://render.com) deve chegar à etapa "Build successful" (destacado em vermelho na imagem acima). Se não, **reimplante-o manualmente (botão azul no canto superior direito)** — às vezes, o compilador pode travar, falhar aleatoriamente ou expirar em uma conta gratuita.
3.  Se o Render diz que sua implantação foi bem-sucedida, **você ainda pode não ver seu aplicativo** no endereço `onrender.com`. Isso ocorre porque o [nível gratuito do Render](https://render.com/docs/free) desliga serviços inativos, e leva 30-60 segundos para eles acordarem. **Atualize a página algumas vezes** durante um período de cerca de um minuto, e ele deve carregar.

Ao trabalhar com um projeto, recomendamos que você execute seu aplicativo streamlit localmente muitas vezes (em seu computador usando o comando `streamlit run app.py` para procurar rapidamente por bugs nele. Essa é uma maneira mais rápida do que esperar minutos por uma implantação online cada vez que fizer commit.

## Render: aplicativo "expandido”

Agora, vamos tornar nosso aplicativo útil ao:

-   adicionar um elemento que permite aos usuários definir o número de lançamentos da moeda;
-   adicionar um elemento de botão para iniciar o teste;
-   calcular a média de dois resultados codificados como 0 e 1;
-   traçar o progresso atual;
-   mostrar uma tabela de resultados para todas as execuções.

Implantar tudo isso requer widgets. **Widgets** são "blocos de construção" para interface gráfica do usuário. Pense em uma caixa de texto para campos de entrada de texto, uma caixa de rolagem para navegar em uma página grande, um widget de reprodutor de vídeo, onde o vídeo está sendo exibido, etc.

Precisaremos tanto de widgets de entrada (onde os usuários inserem alguma informação), como dos de saída (onde o nosso aplicativo exibe resultados de cálculos).

Felizmente, Streamlit possibilita um bom conjunto de elementos descritos [aqui](https://docs.streamlit.io/library/api-reference) _(os materiais estão em inglês)_. Vamos pegar os seguintes elementos de interface:

-   [o controle deslizante](https://docs.streamlit.io/library/api-reference/widgets/st.slider)
-   [o botão](https://docs.streamlit.io/library/api-reference/widgets/st.button)
-   [o gráfico de linhas](https://docs.streamlit.io/library/api-reference/charts/st.line_chart)
-   [a exibição do dataframe](https://docs.streamlit.io/library/api-reference/data/st.dataframe)

Adicionando o controle deslizante e o botão ao programa:

```
import streamlit as st

st.header('Jogando uma moeda')

number_of_trials = st.slider('Número de tentativas?', 1, 1000, 10)
start_button = st.button('Executar')

if start_button:
    st.write(f'Executando o experimento de {number_of_trials} tentativas.')

st.write('Ainda não é um aplicativo funcional. Em construção.')
```

Implante o aplicativo localmente (ou no Render, fazendo commit para seu repositório do GitHub). O comportamento da interface deve ter o seguinte aspecto para permitir a interação do usuário com o aplicativo:

![](https://practicum-content.s3.amazonaws.com/resources/4.4.8PT_1695290222.png)

Agora, vamos adicionar os resultados das tentativas à interface do usuário, calcular a média e exibir como ela muda à medida que as tentativas progridem:

Primeiro, vamos adicionar a variável `chart` para o gráfico de linhas e a função `toss_coin` que emula o lançamento de uma moeda `n` vezes e calcula a média a cada nova iteração, que é adicionada a `chart` (como uma nova observação):

```
import scipy.stats
import streamlit as st
import time

st.header('Jogando uma moeda')

chart = st.line_chart([0.5])

def toss_coin(n): # função que emula o lançamento de uma moeda

    trial_outcomes = scipy.stats.bernoulli.rvs(p=0.5, size=n)

    mean = None
    outcome_no = 0
    outcome_1_count = 0

    for r in trial_outcomes:
        outcome_no +=1
        if r == 1:
            outcome_1_count += 1
        mean = outcome_1_count / outcome_no
        chart.add_rows([mean])
        time.sleep(0.05)

    return mean

number_of_trials = st.slider('Número de tentativas?', 1, 1000, 10)
start_button = st.button('Executar')

if start_button:
    st.write(f'Executando o experimento de {number_of_trials} tentativas.')
```

Não se preocupe se não entender completamente a função `toss_coin()`. É absolutamente normal – você não precisa entender todas as funções que usar. Por exemplo, quando você chama a função `read_csv()` de Pandas, você não precisa saber como ela funciona, mas consegue usá-la. O mesmo princípio funciona aqui.

Agora vamos fazer a chamada de `toss_coin` quando `start_button` é clicado (obtém o valor True).

```
import scipy.stats
import streamlit as st
import time

st.header('Jogando uma moeda')

chart = st.line_chart([0.5])

def toss_coin(n):

    trial_outcomes = scipy.stats.bernoulli.rvs(p=0.5, size=n)

    mean = None
    outcome_no = 0
    outcome_1_count = 0

    for r in trial_outcomes:
        outcome_no +=1
        if r == 1:
            outcome_1_count += 1
        mean = outcome_1_count / outcome_no
        chart.add_rows([mean])
        time.sleep(0.05)

    return mean

number_of_trials = st.slider('Número de tentativas?', 1, 1000, 10)
start_button = st.button('Executar')

if start_button:
    st.write(f'Executando o experimento de {number_of_trials} tentativas.')
    mean = toss_coin(number_of_trials)
```

Agora, podemos "jogar" uma moeda clicando em `run` com o aplicativo e ver um efeito interessante desse experimento — o valor médio calculado está indo em direção ao seu valor verdadeiro (0,5) à medida que o número de tentativas aumenta.

![](https://practicum-content.s3.amazonaws.com/resources/4.4.8.2PT_1695290248.png)

Agora, vamos adicionar uma saída a uma tabela com os resultados de todos os experimentos: Primeiro, precisamos adicionar duas variáveis de estado como as chaves de `st.session_state`. O estado da sessão é preservado ao longo de novas execuções do aplicativo Streamlit. Depois, adicionamos os resultados compilados dos experimentos ao DataFrame mantido como `st.session_state['df_experiment_results']`

```
import pandas as pd
import scipy.stats
import streamlit as st
import time

# estas são variáveis persistentes preservadas à medida que o Streamlin executa novamente esse script
if 'experiment_no' not in st.session_state:
    st.session_state['experiment_no'] = 0

if 'df_experiment_results' not in st.session_state:
    st.session_state['df_experiment_results'] = pd.DataFrame(columns=['no', 'iterations', 'mean'])

st.header('Jogando uma moeda')

chart = st.line_chart([0.5])

def toss_coin(n):

    trial_outcomes = scipy.stats.bernoulli.rvs(p=0.5, size=n)

    mean = None
    outcome_no = 0
    outcome_1_count = 0

    for r in trial_outcomes:
        outcome_no +=1
        if r == 1:
            outcome_1_count += 1
        mean = outcome_1_count / outcome_no
        chart.add_rows([mean])
        time.sleep(0.05)

    return mean

number_of_trials = st.slider('Número de tentativas?', 1, 1000, 10)
start_button = st.button('Executar')

if start_button:
    st.write(f'Executando o experimento de {number_of_trials} tentativas.')
    mean = toss_coin(number_of_trials)
```

Agora, usamos estas variáveis para exibir o DataFrame após cada execução do aplicativo:

```
import pandas as pd
import scipy.stats
import streamlit as st
import time

# estas são variáveis persistentes preservadas à medida que o Streamlin executa novamente esse script
if 'experiment_no' not in st.session_state:
    st.session_state['experiment_no'] = 0

if 'df_experiment_results' not in st.session_state:
    st.session_state['df_experiment_results'] = pd.DataFrame(columns=['no', 'iterations', 'mean'])

st.header('Jogando uma moeda')

chart = st.line_chart([0.5])

def toss_coin(n):

    trial_outcomes = scipy.stats.bernoulli.rvs(p=0.5, size=n)

    mean = None
    outcome_no = 0
    outcome_1_count = 0

    for r in trial_outcomes:
        outcome_no +=1
        if r == 1:
            outcome_1_count += 1
        mean = outcome_1_count / outcome_no
        chart.add_rows([mean])
        time.sleep(0.05)

    return mean

number_of_trials = st.slider('Número de tentativas?', 1, 1000, 10)
start_button = st.button('Executar')

if start_button:
    st.write(f'Executando o experimento de {number_of_trials} tentativas.')
    st.session_state['experiment_no'] += 1
    mean = toss_coin(number_of_trials)
    st.session_state['df_experiment_results'] = pd.concat([
        st.session_state['df_experiment_results'],
        pd.DataFrame(data=[[st.session_state['experiment_no'],
                            number_of_trials,
                            mean]],
                     columns=['no', 'iterations', 'mean'])
        ],
        axis=0)
    st.session_state['df_experiment_results'] = \
        st.session_state['df_experiment_results'].reset_index(drop=True)

st.write(st.session_state['df_experiment_results'])
```

![](https://practicum-content.s3.amazonaws.com/resources/4.4.8.3PT_1695290273.png)

Envie as últimas alterações ao repositório git

```
git add .
git commit -am 'version 1'
git push
```

Houve muito código novo para você nesta lição, não é? O nosso objetivo principal era guiar você pelo processo de criar e implementar um aplicativo web. Se você agora tem uma compreensão geral dos conceitos essenciais, isso é o que queríamos alcançar. Neste sprint, seu projeto se concentrará em como criar e implementar um aplicativo, então continue prestando atenção!

Pergunta

Quando as variáveis de estado do Streamlit são necessárias?

Quando precisamos manter valores constantes protegidos de qualquer mudança.

Quando precisamos manter valores em novas execuções de um aplicativo Streamlit.

Cada nova execução de um aplicativo Streamlit é como um novo começo, apenas as variáveis persistentes mantêm seus valores de execuções anteriores.

Excelente!

## Conclusão

Os aplicativos web nos permitem fornecer funcionalidades úteis com uma interface de usuário em um navegador, para um número praticamente ilimitado de pessoas. Existem vários frameworks que nos permitem criar aplicativos web com Python, alguns deles de forma bastante rápida devido ao seu prático conjunto de componentes. Streamlit é um framework desenvolvido com as necessidades de um profissional de dados em mente.

Os aplicativos web precisam ser hospedados em algum lugar, e o Render é um dos serviços em nuvem especializados em hospedar diferentes tipos de aplicativos, incluindo os web criados com Python.

A capacidade de desenvolver um aplicativo web pode ajudar um profissional de dados a compartilhar seus produtos analíticos com outras pessoas de forma prática, para que elas possam trabalhar com eles em um navegador.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-27-15-531Z.md
### Última modificação: 2025-05-28 19:27:16

# Qualidade do código - TripleTen

Capítulo 5/8

Python intermediário

# Qualidade do código

![](https://practicum-content.s3.amazonaws.com/resources/lesson9_1695285398.jpg)

Um código de boa qualidade é fácil de ler, entender e manter, funciona bem em condições extremas e usa os recursos de computação de forma eficiente. Fatores que afetam a qualidade do código:

-   Estilo de código
-   Confiabilidade
-   Atuação
-   Segurança

A qualidade do código é importante por vários motivos, e sua importância aumenta à medida que o número de usuários que usam produtos programados no código e o número de desenvolvedores trabalhando nele aumentam:

-   Comunicação; outras pessoas gastam menos tempo tentando entender o que o código faz.
-   Manutenção; fica mais fácil encontrar e corrigir bugs e expandir a funcionalidade do código.
-   Previsibilidade; funciona como esperado e minimiza surpresas desagradáveis quando as coisas dão problemas inesperadamente.

## Estilo de código

**Estilo de código** refere-se à aparência do código e não a como ele funciona. Assim como existem regras para uma comunicação clara e concisa em linguagens naturais, também temos regras de estilo de código para linguagens de programação. Aqui estão alguns exemplos de regras de estilo em Python:

-   O número de espaços para o recuo
-   O estilo de nomenclatura de variáveis, por exemplo, `book_title` vs `BookTitle`
-   A regra de colocar espaços ao redor de um operador de atribuição (`a = 1`) e omiti-los para argumentos ao chamar uma função (`my_function(a=1)`)

Um conjunto de tais regras é comumente conhecido como **guia de estilo**.

Diferentes pessoas ou organizações podem aderir a diferentes regras, formando efetivamente diferentes guias de estilo. Às vezes, mais de um guia de estilo se torna popular. Por exemplo, grandes empresas de tecnologia com muito desenvolvimento interno, como o Google, tendem a definir seus próprios [guias de estilo internos](https://developers.google.com/style). Para o Python, a maioria das pessoas adere amplamente ao guia de estilo [PEP 8](https://peps.python.org/pep-0008/) _(os materiais estão em inglês)_, então você deve consultá-lo se tiver dúvidas sobre como estilizar seu código.

## Aplicação de estilo de código

Mesmo com um guia de estilo abrangente, é comum que as pessoas se desviem dele aqui e ali. Existem ferramentas que podem ajudar a validar o código quanto à sua aderência a um estilo de código específico e avisar se houver erros. Essas ferramentas são chamadas de **linters**. Os linters podem verificar diferentes aspectos de estilo de código, alguns são mais restritivos, outros são mais adequados para grandes projetos. Para projetos pequenos, qualquer linter deve ser suficiente. Os linters populares são facilmente integrados aos IDEs.

[Pylint](https://pylint.pycqa.org/en/latest/) _(os materiais estão em inglês)_ é um linter bem estabelecido e respeitável para Python. Você pode instalá-lo com `pip` ou `conda` como uma ferramenta de linha de comando:

```
pip install pylint
```

Ou:

```
conda install pylint
```

E execute-o para verificar o estilo de código de um script com:

```
pylint my_script.py
```

O resultado do `pylint` indica onde o código não segue o estilo de código, que é PEP 8 por padrão:

```
- ************ Module my_script
my_script.py:6:8: C0303: Trailing whitespace (trailing-whitespace)
my_script.py:8:0: C0305: Trailing newlines (trailing-newlines)
my_script.py:1:0: C0114: Missing module docstring (missing-module-docstring)
my_script.py:3:0: C0103: Constant name "a" doesn't conform to UPPER_CASE naming style (invalid-name)

---

Your code has been rated at 0.00/10 (previous run: 0.00/10, +0.00)
```

O resultado inclui a lista de violações, com cada mensagem se referindo a uma linha no código e fornecendo uma breve descrição do que não está em conformidade com o guia de estilo. O código pode ser alterado para corrigir os problemas.

O conjunto completo dos verificadores do `pylint` está documentado [aqui](https://pylint.pycqa.org/en/latest/user_guide/checkers/features.html) _(os materiais estão em inglês)_. Podemos desabilitar certos verificadores se quisermos.

```
pylint --disable=C0114 my_script.py
```

## Documentação do código

Como parte do bom estilo, é importante manter o código devidamente documentado. A documentação é útil tanto para o autor do código quanto para outras pessoas que tenham que trabalhar com ele. O código é documentado com **comentários**, um texto que explica o que o código está fazendo (mas não é executado como código). Um bom comentário é conciso e ajuda as pessoas a entenderem o que o código está fazendo. Comentários ruins costumam ser irrelevantes, confusos ou muito longos.

Por exemplo, este comentário não oferece nenhuma informação adicional, então ele só gasta o tempo de qualquer pessoa que esteja lendo o código:

```
# atribui 1 a alguma variável
a = 1
```

Este comentário parece muito mais útil:

```
# inicializa o contador do ciclo com 1
a = 1
```

Um **comentário de bloco** precede o código para o qual fornece uma explicação e é indentado no mesmo nível desse código:

```
# Normaliza contatos de e-mail: corta-os, converte para letras minúsculas, etc.
# Ele não detecta se o contato não é um e-mail válido.
df['email'] = df['email'].str.strip().str.lower()
```

**Docstrings** são comentários que descrevem a finalidade e as especificidades de módulos, funções, classes e métodos. Elas são feitas para seguir um estilo específico em todo o seu código, para que **geradores de documentos** possam as encontrar e reunir em um arquivo de documentação consistente.

Aqui está um exemplo de uma docstring de várias linhas para a função `get_life_expectancy()` que segue a convenção de estilo [PEP 257](https://peps.python.org/pep-0257/) _(os materiais estão em inglês)_:

```
def get_life_expectancy(hamster_obj):

        """Prevê a expectativa de vida de um determinado hamster.
        
        Pode produzir uma pior precisão para o hamster anão Roborovski.
        """

        # desempacotando recursos do objeto hamster
    current_age = hamster_obj['current_age']
    gender = hamster_obj['gender']
    living_condition = hamster_obj['living_condition']
```

A docstring do ponto de vista da sintaxe é um comentário de várias strings.

O estilo docstring PEP 257 não é o único. Desenvolvedores de grandes bibliotecas podem estabelecer seus próprios padrões. Por exemplo:

-   O formato de docstrings para Pandas é descrito [aqui](https://pandas.pydata.org/docs/development/contributing_docstring.html) _(os materiais estão em inglês)_, e um exemplo ao vivo para o método `value_counts()` do DataFrame pode ser visto [aqui](https://github.com/pandas-dev/pandas/blob/9c241fed864e0c4badf00ce7996206464bf7bfdb/pandas/core/base.py#L894-L971). Você pode notar que se parece com [a descrição do método](https://pandas.pydata.org/pandas-docs/version/1.4/reference/api/pandas.Series.value_counts.html) _(os materiais estão em inglês)_ na documentação. Isso ocorre porque a documentação da API da Pandas é gerada a partir dessas docstrings.
-   scikit-learn, um exemplo ao vivo para a função `sklearn.metrics.silhouette_score()` pode ser visto [aqui](https://github.com/scikit-learn/scikit-learn/blob/a6574655478a24247189236ce5f824e65ad0d369/sklearn/metrics/cluster/_unsupervised.py#L42-L108) com sua [reflexão](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score) _(os materiais estão em inglês)_ na documentação.

Se você deseja começar a usar docstrings em suas próprias funções, seguir o estilo PEP 257 é um ótimo lugar para começar.

Pergunta

O código Python abaixo imprime os números primos inferiores a 100.

```
import math

def is_prime(n):

    if n <= 1:
        return False
    for i in range(2,int(math.sqrt(n) + 1)):
        if n % i == 0:
            return False
    return True

def main():

    """contém toda a lógica principal"""

    for i in range(100):
        if is_prime(i):
            print (i, end=' ')
    print();

if __name__ == '__main__':
    main()
```

Esse código funciona (ou seja, não gera nenhum erro de tempo de execução e calcula o resultado correto).

Aqui está o que você precisa fazer:

1.  Salve o código acima em um arquivo chamado `prime.py` em seu computador.
2.  Use `pylint` para verificar se o código segue as regras do guia de estilo PEP8 executando-o na interface de linha de comando em seu computador:
    
    ```
     pylint --disable=C0114 prime.py
     
    ```
    
3.  Corrija o código de modo que o pylint não levante nenhum problema sobre o estilo e classifique o código como 10/10. Após executar o pylint no seu código corrigido, você vai ver uma mensagem com a classificação 10/10. Copie a mensagem resultante do `pylint` na caixa de texto abaixo.
    

import math def is\_prime(n): """check if a number is prime or not""" if n <= 1: return False for i in range(2,int(math.sqrt(n) + 1)): if n % i == 0: return False return True def main(): """holds all the main logic""" for i in range(100): if is\_prime(i): print (i, end=' ') print() if \_\_name\_\_ == '\_\_main\_\_': main()

O código parece mais organizado agora!

Boa tentativa, mas não é a resposta corretaTente novamente

## Código de teste

Teste de produto é a prática de validar que um produto funciona conforme o esperado. A prática tem sido adotada em muitas indústrias para minimizar os riscos de erros inesperados. Aprenderemos técnicas comuns para testes em Python: **testes de unidade** com `pytest` e a ferramenta de linha de comando.

**Testes de unidade** são usados para testar partes individuais do nosso código. A biblioteca `pytest` é ótima para executar testes de unidade em Python e fornece as funcionalidades de:

-   Executar testes explicitamente
-   Filtrar quais casos de teste executar
-   Executar novamente apenas os testes que falharam
-   Reportar os resultados de testes

Você pode instalar `pytest` com `pip` ou `conda` como uma ferramenta de linha de comando:

```
pip install pytest
```

Ou:

```
conda install pytest
```

Em um cenário básico, `pytest` procura funções com nomes começando com `test_` em seu código e executa essas funções assim que as encontrar. Se nenhuma exceção `AssertionException` for gerada em uma função de teste, o teste será considerado aprovado:

```
# criando a função sign()
def sign(x):
    """Retorna o sinal de número."""
    if x == None:
        return None
    if x < 0:
        return -1
    return 1

# testando a função sign()
def test_sign():
    assert sign(-10) == -1
    assert sign(10) == 1
    assert sign(0) == 1
    assert sign(None) == None
```

`AssertionException` é um tipo de exceção gerada com instruções de asserção, que são instruções de verificação de prova real incorporadas no código. Elas devem sempre retornar `True`, a menos que haja um problema. A sintaxe é:

```
assert expression, "mensagem de asserção"
```

A `expression` deve gerar `True` se não houver problema, caso contrário, `False`. No último caso, a exceção `AssertionError` é gerada com um método de asserção, se for fornecido.

A função `test_sign()` acima é um teste de unidade que contém várias asserções, cada uma verificando seu próprio conjunto de condições.

Se salvarmos esse script como `sign.py`, podemos executar testes de unidade nele com `pytest`:

```
pytest sign.py
```

E obter um relatório:

```
========================== test session starts ================================
platform linux -- Python 3.9.12, pytest-6.2.5, py-1.10.0, pluggy-1.0.0
rootdir: /home/practicum/projects/
plugins: anyio-3.3.4
collected 1 item

sign.py .                                                                [100%]

========================== 1 passed in 0.03s ==================================
```

Se, de alguma forma, quebrarmos nossa pequena função útil (por exemplo, esquecemos de retornar 1 para o caso em que `x` não é negativo), `pytest` vai nos informar sobre os testes que falharam:

```
# faz algo útil
def sign(x):
    """Retorna o sinal de número."""
    if x == None:
        return None
    if x < 0:
        return -1

# testa a função de sinal
def test_sign():
    assert sign(-10) == -1
    assert sign(10) == 1
    assert sign(0) == 1
    assert sign(None) == None
```

```
pytest sign.py
```

```
$ pytest sign.py
========================== test session starts ================================
platform linux -- Python 3.9.12, pytest-6.2.5, py-1.10.0, pluggy-1.0.0
rootdir: /home/dpdonetskov/projects/dxp
plugins: anyio-3.3.4
collected 1 item

sign.py F                                                                [100%]

=========================== FAILURES ==========================================
___________________________ test_sign _________________________________________

    def test_sign():
        assert sign(-10) == -1
>       assert sign(10) == 1
E       assert None == 1
E        +  where None = sign(10)

sign.py:11: AssertionError
========================= short test summary info =============================
FAILED sign.py::test_sign - assert None == 1
========================== 1 failed in 0.28s ===================================
```

Escrever testes de unidade abrangentes pode ser um desafio, pois requer alguma imaginação para descobrir todas as maneiras possíveis de um usuário quebrar seu código, bem como testar essas situações. É prática padrão em muitos setores ter equipes de controle de qualidade (QA, na sigla em inglês) cujo trabalho é cobrir funcionalidades críticas de um programa com testes de unidade como os que você aprendeu nesta lição.

Pergunta

Seu colega do grupo de qualidade, seguindo os requisitos funcionais, criou um teste de unidade `test_get_age_group` para uma futura função `get_age_group`. Você precisa:

1.  Criar um arquivo de script chamado `age_group.py` e copiar este código para ele:
    
    ```
     def get_age_group(age):
     
         """
         Retorna a faixa etária com base na idade em anos dentro do intervalo 0..150
         caso contrário, retorna 'desconhecido'.
         """
     
         if 0 <= age <= 14:
             return 'crianças'
             # complete esta função para que ela passe no teste de unidade
                 return 'desconhecido'
     
     def test_get_age_group():
     
         """teste de unidade para get_age_group"""
     
         assert get_age_group(-1) == 'desconhecido'
         assert get_age_group(0) == 'crianças'
         assert get_age_group(14) == 'crianças'
         assert get_age_group(15) == 'jovens'
         assert get_age_group(24) == 'jovens'
         assert get_age_group(25) == 'adultos'
         assert get_age_group(64) == 'adultos'
         assert get_age_group(65) == 'idosos'
         assert get_age_group(80) == 'idosos'
         assert get_age_group(150) == 'desconhecido'
     
    ```
    
2.  Complete a função `get_age_group()` para que ela passe no teste de unidade
3.  Executar o teste de unidade (`-rA` significa que pedimos que pytest exiba todos os relatórios do teste resumidos)
    
    ```
     pytest -rA age_group.py
     
    ```
    
4.  Após executar o pytest, ele vai exibir uma mensagem. Copie a mensagem resultante do teste de unidade na caixa abaixo quando o teste for aprovado.
    

def get\_age\_group(age): """ Retorna a faixa etária com base na idade em anos dentro do intervalo 0..149. Caso contrário, retorna 'desconhecido'. """ if 0 <= age <= 14: return 'crianças' elif 15 <= age <= 24: return 'jovens' elif 25 <= age <= 64: return 'adultos' elif 65 <= age <= 149: return 'idosos' else: return 'desconhecido' def test\_get\_age\_group(): """teste de unidade para get\_age\_group""" assert get\_age\_group(-1) == 'desconhecido' assert get\_age\_group(0) == 'crianças' assert get\_age\_group(14) == 'crianças' assert get\_age\_group(15) == 'jovens' assert get\_age\_group(24) == 'jovens' assert get\_age\_group(25) == 'adultos' assert get\_age\_group(64) == 'adultos' assert get\_age\_group(65) == 'idosos' assert get\_age\_group(80) == 'idosos' assert get\_age\_group(150) == 'desconhecido'

Você pode colocar ambas as verificações em uma condição: `age >= 0 and age <= 150`

Ainda não, mas continue tentando!Tente novamente

## O Zen de Python

[PEP 20](https://peps.python.org/pep-0020/) _(os materiais estão em inglês)_ descreve o que é chamado de O Zen de Python, uma lista de 19 princípios para escrever um bom código Python: Essas não são regras rígidas, mas um guia de senso comum para escrever um código de qualidade. Elas podem ser chamadas de dentro do Python com a importação "easter egg":

```
import this
```

```
O Zen de Python, por Tim Peters

Bonito é melhor que feio.
Explícito é melhor que implícito.
Simples é melhor que complexo.
Complexo é melhor que complicado.
Plano é melhor que aninhado.
Disperso é melhor que denso.
A legibilidade conta.
Casos especiais não são especiais o suficiente para quebrar as regras.
Embora a praticidade supere a pureza.
Os erros nunca devem passar despercebidos.
A menos que explicitamente silenciados.
Diante da ambiguidade, recuse a tentação de adivinhar.
Deve haver uma — e de preferência apenas uma — maneira óbvia de fazer isso.
Embora essa maneira possa não ser óbvia no início, a menos que você seja holandês.
Agora é melhor que nunca.
Embora nunca frequentemente seja melhor que *exatamente* agora.
Se a implementação é difícil de explicar, é uma má ideia.
Se a implementação for fácil de explicar, pode ser uma boa ideia.
Namespaces são uma ótima ideia — vamos fazer mais desses!
```

## Conclusão

Manter a alta qualidade do código é um fator importante na criação de produtos de alta qualidade. Existem muitos aspectos na qualidade do código, e pode levar anos para um desenvolvedor dominá-los. Mas os básicos são bastante acessíveis e podem ser usados no início da carreira.

## Recursos _(os materiais estão em inglês)_

[PEP 8 – Guia de estilo para códigos Python](https://peps.python.org/pep-0008/)

[PEP 20 – O Zen do Python](https://peps.python.org/pep-0020/)

[PEP 257 – Convenções docstring](https://peps.python.org/pep-0257/)

[Guia de estilo de Python do Google](https://google.github.io/styleguide/pyguide.html)

[O guia do mochileiro para Python: Estilo de código](https://docs.python-guide.org/writing/style/)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-27-16-839Z.md
### Última modificação: 2025-05-28 19:27:17

# Conclusão - TripleTen

Capítulo 5/8

Python intermediário

# Conclusão

![](https://practicum-content.s3.amazonaws.com/resources/conclusion_1695285990.jpg)

Se conhecimento de Python subiu de nível.

Para começar, você não está mais com medo da extensão de arquivo `.py` e pode executar scripts Python em uma interface de linha de comando.

Você também sabe como adicionar uma construção try-except se encontrar um erro ao executar um script.

Além disso, as diferenças entre bibliotecas, pacotes e módulos devem estar mais claras agora, e você deve saber como importá-los.

Se precisar de algo realmente personalizado, você sabe como escrever sua própria classe com seus próprios atributos e métodos.

Abrir arquivos e gravar dados neles deve estar mais fácil agora.

Por fim, mas não menos importante, você aprendeu tudo sobre aplicativos web e como os criar usando o Render.

Você fez um ótimo trabalho – e boa sorte com o projeto em que você vai começar a trabalhar muito em breve!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-28-37-326Z.md
### Última modificação: 2025-05-28 19:28:37

# Introdução - TripleTen

Capítulo 6/8

Ambiente de programação individual

# Introdução

Olá, este é o capítulo sobre Ambientes de Programação Integrados (IDE, na sigla em inglês)!

Se você tentar escrever um programa Python que seja mais que apenas algumas linhas de código (ou até mais do que alguns arquivos Python separados), talvez comece a achar esse processo difícil de gerenciar:

-   o código pode ser difícil de ler sem realce de sintaxe
-   você alterna frequentemente entre o editor de texto e o navegador web para procurar até mesmo nomes simples de funções e métodos (é `my_list.delete()` ou `my_list.remove()`?)
-   você pode ter que encher seu código com muitas instruções `print()` para descobrir os valores de diferentes variáveis ao longo da execução do programa
-   o código pode apresentar erros devido a erros triviais de indentação ([uso de tab vs espaços, hein?](https://www.youtube.com/watch?v=SsoOG6ZeyUI))

Pode haver qualquer número de outras pequenas inconveniências capazes de tornar a sua experiência de escrever código Python em um editor de texto simples incômoda.

Nos próximos capítulos, você vai aprender como o "I" no IDE ajuda a aliviar muitos dos problemas mencionados acima.

Observe que a maioria das lições neste capítulo serão divididas em duas partes: uma para os usuários do macOS e outra para os usuários do Windows. Dependendo de seu sistema operacional, você só vai precisar estudar o que for relevante para você.

![](https://practicum-content.s3.amazonaws.com/resources/sdte-1-1-01_1695291900.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-28-38-611Z.md
### Última modificação: 2025-05-28 19:28:39

# O que é um IDE? - TripleTen

Capítulo 6/8

Ambiente de programação individual

# O que é um IDE?

Nesta lição, vamos falar sobre os IDEs comparando-os com editores de texto e cadernos Jupyter.

## IDE e editor de texto

Você pode pensar em um IDE como um editor de texto sob efeito de esteroides. Ele permite que você crie aplicativos combinando ferramentas de desenvolvimento usadas com frequência em uma única interface gráfica de usuário.

Que ferramentas são essas?

-   Editor de texto com muitos recursos: realce de sintaxe, preenchimento automático, formatação de código, inspeção de possíveis bugs e muito mais
-   Navegador de arquivos integrado
-   Ferramentas de teste e depuração de código
-   Ferramentas de refatoração de código

É certo que existem alguns editores de texto, como Notepad++, Sublime Text ou Atom, que possuem algumas das características mencionadas acima, portanto, em alguns casos, a linha entre um editor de texto e um IDE pode ficar embaçada.

## IDE e Jupyter Notebook

O software IDE também é diferente do Jupyter Notebook. Sim, você pode escrever código Python em ambos. E sim, ambos têm realce de sintaxe e um navegador de arquivos. Mas é aí que as semelhanças terminam.

Seja nos cadernos Jupyter em nossa plataforma ou em execuções locais, você provavelmente notou que todo o seu código, markdown e gráficos são salvos em um arquivo com extensão `.ipynb`. Esses não são arquivos de texto simples, como arquivos `.py`, que contêm apenas código Python. Se você abrir um deles em um editor de texto, verá um documento textual bastante complexo com uma estrutura hierárquica (chamada JSON).

Como regra geral, cadernos Jupyter são usados quando o problema em questão é muito amplo e exige a iteração de muitas ideias. Eles são especialmente potentes quando precisamos anotar nosso código com explicações textuais e visualizações de dados.

Mas o que acontece quando temos um resultado satisfatório ao trabalhar em cadernos Jupyter e agora precisamos incorporar esses resultados em um aplicativo maior que talvez precise se comunicar com um banco de dados ou até mesmo ter uma interface de usuário?

Nesses casos, cadernos Jupyter não podem ajudar muito.

É aí que entram os IDEs. Neles, você pode desenvolver um aplicativo completo de ponta a ponta (web, móvel ou outro) e até mesmo implementá-lo para executar em produção através da mesma interface de usuário.

Nas próximas lições, vamos mostrar como trabalhar com o Visual Studio Code (ou VS Code), que é um IDE desenvolvido pela Microsoft.

Existem algumas razões pelas quais o escolhemos:

-   É muito [popular](https://pypl.github.io/IDE.html) _(os materiais estão em inglês)_
-   É grátis
-   É leve e rápido (tanto que muitas vezes as pessoas discutem se é um IDE ou um editor de texto superpoderoso)
-   Ele suporta muitas linguagens de programação, portanto, no futuro, quando você decidir aprender outra linguagem de programação, não vai precisar aprender a usar outro IDE
-   Possui ótimo suporte para Jupyter Notebook, como você verá nas próximas lições

![](https://practicum-content.s3.amazonaws.com/resources/image_1_1693569329.jpg)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-28-39-945Z.md
### Última modificação: 2025-05-28 19:28:40

# Instalação do VS Code - TripleTen

Capítulo 6/8

Ambiente de programação individual

# Instalação do VS Code

Se você usa Windows

<iframe class="base-markdown-iframe__iframe" id="player-8m0Qy0N75AY" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[Windows] Install VS Code on Windows" width="640" height="360" src="https://www.youtube.com/embed/8m0Qy0N75AY?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F50988ef2-b4bb-4324-8448-258312f291c6%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Para instalar o VS Code:

-   Abra o seu navegador e acesse [https://code.visualstudio.com/download](https://code.visualstudio.com/download)
-   Faça o download do arquivo do instalador e inicie-o.
-   Aceite todas as configurações padrão durante o processo de instalação.

Depois de iniciar o VS Code, você verá uma página "Get Started" (Começar).

Os principais componentes da interface do usuário são:

-   Barra de atividade (A)
-   Barra lateral (B)
-   Grupos de editores (C)
-   Painel (D)
-   Barra de status (E)

![](https://practicum-content.s3.amazonaws.com/resources/6.3_1695296070.png)

Aqui está um guia completo da interface de usuário do VS Code: [https://code.visualstudio.com/docs/getstarted/userinterface](https://code.visualstudio.com/docs/getstarted/userinterface) _(os materiais estão em inglês)_

Em seguida, vamos aprender como escrever e executar o código Python a partir do VS Code.

Se você usa macOS

<iframe class="base-markdown-iframe__iframe" id="player-z1S-ogjemiI" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[macOS] Install VS Code on macOS." width="640" height="360" src="https://www.youtube.com/embed/z1S-ogjemiI?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=2&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F50988ef2-b4bb-4324-8448-258312f291c6%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Para instalar o VS Code:

-   Abra o seu navegador e acesse [https://code.visualstudio.com/download](https://code.visualstudio.com/download)
-   Faça o download do arquivo do instalador e inicie-o.
-   Aceite todas as configurações padrão durante o processo de instalação.

Depois de iniciar o VS Code, você verá uma página "Get Started" (Começar).

Os principais componentes da interface do usuário são:

-   Barra de atividade (A)
-   Barra lateral (B)
-   Grupos de editores (C)
-   Painel (D)
-   Barra de status (E)

![](https://practicum-content.s3.amazonaws.com/resources/6.3_1695296161.png)

Aqui está um guia completo da interface de usuário do VS Code: [https://code.visualstudio.com/docs/getstarted/userinterface](https://code.visualstudio.com/docs/getstarted/userinterface) _(os materiais estão em inglês)_

Em seguida, vamos aprender como escrever e executar o código Python a partir do VS Code.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-28-41-618Z.md
### Última modificação: 2025-05-28 19:28:41

# Trabalho no VS Code - TripleTen

Capítulo 6/8

Ambiente de programação individual

# Trabalho no VS Code

Se você usa Windows

<iframe class="base-markdown-iframe__iframe" id="player-r4K4rg1jiD4" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[Windows] Run python code in VS Code" width="640" height="360" src="https://www.youtube.com/embed/r4K4rg1jiD4?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fc5e37d52-4256-4a1a-a9a6-48ea6ee64faa%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Agora vamos criar e executar um script Python simples:

-   Clique em "New File" (Novo arquivo)
-   Salve o arquivo de texto em algum lugar localmente, dando um nome e a extensão `.py`
-   Se for necessário selecionar um interpretador Python, selecione o Anaconda. Este será seu ambiente virtual. Alternativamente, você pode selecionar um interpretador na barra lateral no canto inferior esquerdo.
-   Escreva algum código Python, por exemplo, uma simples instrução `print()`
-   Execute o script clicando no botão "Run" (Executar) no canto superior direito. Observação: o VS Code vai sugerir que você instale e configure o formatador de código [autopep8](https://pypi.org/project/autopep8/). Isso não é necessário agora, porque abordaremos a formatação de código em uma lição posterior. Se você quiser experimentá-lo agora, confira [esta documentação](https://code.visualstudio.com/docs/python/editing#_formatting) _(os materiais estão em inglês)_
-   Verifique a saída do script nas janelas do terminal.

Ao digitar as primeiras letras da instrução `print()`, você notou a janela flutuante abaixo do cursor com sugestões de preenchimento automático?

Toda vez que você começar a digitar algo, o VS Code fará algumas suposições inteligentes sobre como você pode querer terminar essa variável/função/nome do pacote.

O GitHub está tentando melhorar ainda mais isso com seu [GitHub Copilot](https://copilot.github.com/), uma extensão VS Code baseada em IA que é capaz de sugerir como completar automaticamente linhas ou até funções inteiras!

IDEs não são legais?

Se você usa macOS

<iframe class="base-markdown-iframe__iframe" id="player-Q0xUuYYkty8" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[macOS] Run python code in VS Code" width="640" height="360" src="https://www.youtube.com/embed/Q0xUuYYkty8?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=2&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fc5e37d52-4256-4a1a-a9a6-48ea6ee64faa%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Agora vamos criar e executar um script Python simples:

-   Clique em "New File" (Novo arquivo)
-   Salve o arquivo de texto em algum lugar localmente, dando um nome e a extensão `.py`
-   Se for necessário selecionar um interpretador Python, selecione o Anaconda. Este será seu ambiente virtual. Alternativamente, você pode selecionar um interpretador na barra lateral no canto inferior esquerdo.
-   Escreva algum código Python, por exemplo, uma simples instrução `print()`
-   Execute o script clicando no botão "Run" (Executar) no canto superior direito. Observação: o VS Code vai sugerir que você instale e configure o formatador de código [autopep8](https://pypi.org/project/autopep8/). Isso não é necessário agora, porque abordaremos a formatação de código em uma lição posterior. Se você quiser experimentá-lo agora, confira [esta documentação](https://code.visualstudio.com/docs/python/editing#_formatting) _(os materiais estão em inglês)_
-   Verifique a saída do script nas janelas do terminal.

Ao digitar as primeiras letras da instrução `print()`, você notou a janela flutuante abaixo do cursor com sugestões de preenchimento automático?

Toda vez que você começar a digitar algo, o VS Code fará algumas suposições inteligentes sobre como você pode querer terminar essa variável/função/nome do pacote.

O GitHub está tentando melhorar ainda mais isso com seu [GitHub Copilot](https://copilot.github.com/), uma extensão VS Code baseada em IA que é capaz de sugerir como completar automaticamente linhas ou até funções inteiras!

IDEs não são legais?

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-28-42-930Z.md
### Última modificação: 2025-05-28 19:28:43

# Clonar um repositório remoto Git - TripleTen

Capítulo 6/8

Ambiente de programação individual

# Clonar um repositório remoto Git

Se você usa Windows

<iframe class="base-markdown-iframe__iframe" id="player-PDgZJAqGek8" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[Windows] Create a GitHub repository and clone it with VS Code" width="640" height="360" src="https://www.youtube.com/embed/PDgZJAqGek8?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F862ef96d-4afe-4ba3-bca0-9b2f411170a5%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Então, como os profissionais trabalham em grandes projetos de software com muitos colaboradores?

Eles normalmente usam algum sistema de controle de origem como o Git, trabalham em uma cópia local do projeto em sua própria máquina e sincronizam as mudanças feitas de volta ao repositório central com acesso para todos.

Nesta e nas próximas lições, passaremos por uma versão simplificada desse processo.

Primeiro, vamos clonar um repositório GitHub remoto.

-   Vá para [github.com](http://github.com/) e crie um novo repositório.
-   Copie o URL do repositório e vá para o VS Code.
-   Clique em "Clone Git Repository" (Clonar o Repositório Git) e depois em "Clone from GitHub" (Clonar do GitHub).
-   Se solicitado, conceda permissões ao VS Code para acessar sua conta do GitHub.
-   Na barra de pesquisa do projeto, cole a URL do projeto que criamos antes.
-   Selecione uma pasta local onde você quer clonar o projeto.
-   Abra o projeto no VS Code.
-   Verifique se você vê os arquivos do projeto no VS Code (por exemplo, README e .gitignore).

Em seguida, vamos ver como enviar alterações de arquivos locais para o repositório remoto no GitHub.

Se você usa macOS

<iframe class="base-markdown-iframe__iframe" id="player-FchtTD3zsaA" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[macOS] Create a GitHub repository and clone it with VS Code" width="640" height="360" src="https://www.youtube.com/embed/FchtTD3zsaA?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=2&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F862ef96d-4afe-4ba3-bca0-9b2f411170a5%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Então, como os profissionais trabalham em grandes projetos de software com muitos colaboradores?

Eles normalmente usam algum sistema de controle de origem como o Git, trabalham em uma cópia local do projeto em sua própria máquina e sincronizam as mudanças feitas de volta ao repositório central com acesso para todos.

Nesta e nas próximas lições, passaremos por uma versão simplificada desse processo.

Primeiro, vamos clonar um repositório GitHub remoto.

-   Vá para [github.com](http://github.com/) e crie um novo repositório.
-   Copie o URL do repositório e vá para o VS Code.
-   Clique em "Clone Git Repository" (Clonar o Repositório Git) e depois em "Clone from GitHub" (Clonar do GitHub).
-   Se solicitado, conceda permissões ao VS Code para acessar sua conta do GitHub.
-   Na barra de pesquisa do projeto, cole a URL do projeto que criamos antes.
-   Selecione uma pasta local onde você quer clonar o projeto.
-   Abra o projeto no VS Code.
-   Verifique se você vê os arquivos do projeto no VS Code (por exemplo, README e .gitignore).

Em seguida, vamos ver como enviar alterações de arquivos locais para o repositório remoto no GitHub.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-28-44-255Z.md
### Última modificação: 2025-05-28 19:28:44

# Controle de origem no VS Code - TripleTen

Capítulo 6/8

Ambiente de programação individual

# Controle de origem no VS Code

Se você usa Windows

<iframe class="base-markdown-iframe__iframe" id="player-FRg9c_dG0yY" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[Windows] Using Source Control in VS Code" width="640" height="360" src="https://www.youtube.com/embed/FRg9c_dG0yY?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F8cae1085-8300-4c34-b9d2-6e5321616a2c%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Primeiro, vamos apresentar algumas alterações em nosso projeto:

-   Crie um script Python na pasta do projeto clonado recentemente. Se for necessário selecionar um interpretador, escolha o interpretador Python do Anaconda.
-   Escreva um script Python (ou modifique arquivos de projeto existentes). Para simplificar as coisas, você pode escrever uma simples chamada da função `print()`.
-   Verifique se o sprint é executado e produz o que é esperado.

Para enviar nossas alterações para o GitHub:

-   Vá até a seção "Source Control" (Controle de Origem). O VS Code observa todos os arquivos que foram alterados ou criados.
-   `settings.json` é onde o VS Code armazena suas configurações. Não o queremos em nosso repositório, então o adicionamos ao arquivo `.gitignore`.
    -   Primeiro, criaremos um commit com o arquivo `.gitignore` e enviaremos o commit para o GitHub
    -   Vamos verificar se o commit foi enviado com sucesso.
-   Agora vamos repetir o processo stage-commit-push para nosso script Python.
-   Como você pode ver, após enviar os commits para o GitHub, você pode vê-los imediatamente na página do repositório do projeto.

Se você usa macOS

<iframe class="base-markdown-iframe__iframe" id="player-ohS9Ia8_M9g" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[macOS] Using Source Control in VS Code" width="640" height="360" src="https://www.youtube.com/embed/ohS9Ia8_M9g?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=2&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F8cae1085-8300-4c34-b9d2-6e5321616a2c%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Primeiro, vamos apresentar algumas alterações em nosso projeto:

-   Crie um script Python na pasta do projeto clonado recentemente. Se for necessário selecionar um interpretador, escolha o interpretador Python do Anaconda.
-   Escreva um script Python (ou modifique arquivos de projeto existentes). Para simplificar as coisas, você pode escrever uma simples chamada da função `print()`.
-   Verifique se o sprint é executado e produz o que é esperado.

Para enviar nossas alterações para o GitHub:

-   Vá para a seção Source Control (Controle de Origem). O VS Code observa todos os arquivos que foram alterados ou criados.
-   Primeiro, preparamos os arquivos que queremos, depois criamos um commit com uma mensagem informativa.
-   Em seguida, enviamos o commit para o repositório remoto no GitHub.
-   Como você pode ver, após enviar os commits para o GitHub, você pode vê-los imediatamente na página do projeto.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-28-46-855Z.md
### Última modificação: 2025-05-28 19:28:47

# Jupyter Notebook no VS Code - TripleTen

Capítulo 6/8

Ambiente de programação individual

# Jupyter Notebook no VS Code

Se você usa Windows

<iframe class="base-markdown-iframe__iframe" id="player-Kb4fXZG-h7s" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[Windows] Jupyter Notebooks in VS Code" width="640" height="360" src="https://www.youtube.com/embed/Kb4fXZG-h7s?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F3665fabe-3321-45eb-bc91-99461c8d23b8%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

O VS Code também possui excelente suporte para cadernos Jupyter.

Para criar um caderno no VS Code:

-   Pressione \[Ctrl + Shift + P\], isso vai abrir a Paleta de Comandos.
-   Comece a digitar "Jupyter" e selecione "Jupyter: Create New Jupyter Notebook" (Jupyter: criar novo notebook Jupyter) nos resultados abaixo.
-   Trabalhe como faria normalmente em um caderno Jupyter. Por exemplo, escrevemos um código que exibe um gráfico de dispersão simples com a ajuda da biblioteca `matplotlib`.
-   Quando terminar, confirme e envie seu trabalho para o GitHub.
-   Verifique o repositório do GitHub e veja como ele exibe seu caderno em uma página web.

A [documentação do VS Code](https://code.visualstudio.com/docs/datascience/jupyter-notebooks) _(os materiais estão em inglês)_ tem uma descrição mais detalhada de quais outras características relacionadas ao Jupyter estão disponíveis no IDE.

Na próxima lição, você vai ver algumas características interessantes do VS Code que vão acelerar seu fluxo de trabalho de desenvolvimento, detectando automaticamente erros de sintaxe e formatando seu código para ele seguir as diretrizes para melhores práticas (como o [Guia de estilo Python](https://www.python.org/dev/peps/pep-0008/) oficial ou [Guia de estilo de Python do Google](https://google.github.io/styleguide/pyguide.html) _(os materiais estão em inglês)_).

Se você usa macOS

<iframe class="base-markdown-iframe__iframe" id="player-WoBqHEcgaMU" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[macOS] Jupyter Notebooks in VS Code" width="640" height="360" src="https://www.youtube.com/embed/WoBqHEcgaMU?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=2&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F3665fabe-3321-45eb-bc91-99461c8d23b8%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

O VS Code também possui excelente suporte para cadernos Jupyter.

Para criar um caderno no VS Code:

-   Pressione \[Ctrl + Shift + P\], isso vai abrir a Paleta de Comandos.
-   Comece a digitar "Jupyter" e selecione "Jupyter: Create New Jupyter Notebook" (Jupyter: criar novo notebook Jupyter) nos resultados abaixo.
-   Trabalhe como faria normalmente em um caderno Jupyter. Por exemplo, escrevemos um código que exibe um gráfico de dispersão simples com a ajuda da biblioteca `matplotlib`.
-   Quando terminar, confirme e envie seu trabalho para o GitHub.
-   Verifique o repositório do GitHub e veja como ele exibe seu caderno em uma página web.

A [documentação do VS Code](https://code.visualstudio.com/docs/datascience/jupyter-notebooks) _(os materiais estão em inglês)_ tem uma descrição mais detalhada de quais outras características relacionadas ao Jupyter estão disponíveis no IDE.

Na próxima lição, você vai ver algumas características interessantes do VS Code que vão acelerar seu fluxo de trabalho de desenvolvimento, detectando automaticamente erros de sintaxe e formatando seu código para ele seguir as diretrizes para melhores práticas (como o [Guia de estilo Python](https://www.python.org/dev/peps/pep-0008/) oficial ou [Guia de estilo de Python do Google](https://google.github.io/styleguide/pyguide.html) _(os materiais estão em inglês)_).

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-28-48-182Z.md
### Última modificação: 2025-05-28 19:28:48

# Linting e formatação de código - TripleTen

Capítulo 6/8

Ambiente de programação individual

# Linting e formatação de código

Se você usa Windows

## Linting

<iframe class="base-markdown-iframe__iframe" id="player-WoqUsxLSF0A" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[Windows] Linting in VS Code" width="640" height="360" src="https://www.youtube.com/embed/WoqUsxLSF0A?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F36769e8c-c72a-4590-b06b-f59e226eea2f%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Linting no VS Code

Linting é o processo automatizado de verificação de erros de sintaxe no código. Ele é feito usando uma ferramenta de linting (ou linter).

O VS Code possui um linter integrado que detecta automaticamente alguns erros básicos de sintaxe no Python.

Por exemplo, se chamarmos `print` sem os parênteses ao redor da entrada, o linter vai detectar isso e desenhar uma linha ondulada vermelha abaixo dela. Se você passar o mouse sobre essa linha ondulada, verá uma mensagem pop-up explicando o que o linter acha que está errado com o código. Às vezes, essas mensagens podem ser vagas ou abstratas.

![](https://practicum-content.s3.amazonaws.com/resources/image_1_1693593223.png)

Com o tempo, à medida que você se sentir mais confortável com o Python, raramente vai precisar ler essas mensagens, pois vai descobrir na hora por que o linter está reclamando.

## Formatação de código

Nem todo código que é executado é um bom código, então costumamos gastar mais tempo lendo (e relendo) código do que o escrevendo. Esse é o caso principalmente quando você está trabalhando com outras pessoas e todos precisam entender os códigos uns dos outros para fazer todo o sistema funcionar.

Isso ajuda a ter algumas diretrizes comuns sobre o estilo que a equipe vai usar para facilitar a vida de todos. Na maioria das vezes, um guia de estilo existente (como o [Guia de estilo do Python](https://www.python.org/dev/peps/pep-0008/) ou o [Guia de estilo de Python do Google](https://google.github.io/styleguide/pyguide.html) _(os materiais estão em inglês)_) será suficiente. Mas como garantir que todos sigam as diretrizes?

É aqui que as ferramentas automáticas de formatação de código podem ajudar.

O que elas fazem? Elas fazem pequenas alterações no código automaticamente para que ele fique conforme o estilo selecionado. Por pequenas alterações, queremos dizer remover/adicionar linhas vazias, reorganizar importações de pacotes, alinhar recuo, etc. Elas nunca fazem mudanças drásticas, como mudar o nome de uma variável ou implementar uma função.

<iframe class="base-markdown-iframe__iframe" id="player--i65OuOvcwA" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[Windows] Automatic code formatting in VS Code" width="640" height="360" src="https://www.youtube.com/embed/-i65OuOvcwA?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=2&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F36769e8c-c72a-4590-b06b-f59e226eea2f%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

No vídeo acima, adicionamos um bloco de código `if-else` ao arquivo `my_script.py`. O bloco apenas imprime mensagens diferentes dependendo se `x` foi um número par ou ímpar.

(A propósito, você notou como, após digitar `if`, selecionamos `ifelse` da lista de autossugestão e isso produziu um bloco de código multilinha? Eles são chamados de trechos de código, e você pode instalá-los com [esta extensão](https://marketplace.visualstudio.com/items?itemName=frhtylcn.pythonsnippets))

Podemos bagunçar de propósito esse trecho de código adicionando ou removendo espaços em branco aleatoriamente. Para corrigir tais erros automaticamente, primeiro precisamos configurar a formatação do código:

-   Instale autopep8 conforme descrito [aqui](https://marketplace.visualstudio.com/items?itemName=ms-python.autopep8)
-   Abra a "Command Pallet" (Paleta de comandos) ("View" → "Command Pallet").
-   Comece a digitar "Settings" (Configurações) e selecione a opção "Preferences: Open Settings (UI)" (Preferências: abrir configurações (IU)) na lista de sugestões.
-   No menu de configurações, procure "format provider" (provedor de formato). Isso mostra a configuração "Formatting: Provider" (Formatação: provedor).
-   Defina essa configuração como `autopep8` nas abas [User e Workspace](https://code.visualstudio.com/docs/getstarted/settings) _(os materiais estão em inglês)_ e retorne ao seu script com o código mal formatado.
-   Abra novamente "Command Pallet", procure a opção "Format Document" (Formatar documento) e clique nela.

Se você quiser que esse tipo de limpeza seja executada automaticamente toda vez que você salvar as alterações:

-   Abra as configurações de novo ("Command Pallet" → "Preferences: Open Settings (UI)" (Preferências: abrir configurações (UI)).
-   Procure "format on save" (formatar ao salvar) e habilite a configuração "Format On Save" (Formatar ao salvar)".
-   Agora tente estragar o código de novo, mas dessa vez apenas salve as alterações e observe o código ser formatado automaticamente na mesma hora.

Se você usa macOS

## Linting

<iframe class="base-markdown-iframe__iframe" id="player-qTFb0SoPM2M" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[macOS] Linting in VS Code" width="640" height="360" src="https://www.youtube.com/embed/qTFb0SoPM2M?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=3&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F36769e8c-c72a-4590-b06b-f59e226eea2f%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Linting é o processo automatizado de verificação de erros de sintaxe no código. Ele é feito usando uma ferramenta de linting (ou linter).

O VS Code possui um linter integrado que detecta automaticamente alguns erros básicos de sintaxe no Python.

Por exemplo, se chamarmos `print` sem os parênteses ao redor da entrada, o linter vai detectar isso e desenhar uma linha ondulada vermelha abaixo dela. Se você passar o mouse sobre essa linha ondulada, verá uma mensagem pop-up explicando o que o linter acha que está errado com o código. Às vezes, essas mensagens podem ser vagas ou abstratas.

![](https://practicum-content.s3.amazonaws.com/resources/image_1_1693594112.png)

Com o tempo, à medida que você se sentir mais confortável com o Python, raramente vai precisar ler essas mensagens, pois vai descobrir na hora por que o linter está reclamando.

## Formatação de código

Nem todo código que é executado é um bom código, então costumamos gastar mais tempo lendo (e relendo) código do que o escrevendo. Esse é o caso principalmente quando você está trabalhando com outras pessoas e todos precisam entender os códigos uns dos outros para fazer todo o sistema funcionar.

Isso ajuda a ter algumas diretrizes comuns sobre o estilo que a equipe vai usar para facilitar a vida de todos. Na maioria das vezes, um guia de estilo existente (como o [Guia de estilo do Python](https://www.python.org/dev/peps/pep-0008/) ou o [Guia de estilo de Python do Google](https://google.github.io/styleguide/pyguide.html) _(os materiais estão em inglês)_ será suficiente. Mas como garantir que todos sigam as diretrizes?

É aqui que as ferramentas automáticas de formatação de código podem ajudar.

O que elas fazem? Elas fazem pequenas alterações no código automaticamente para que ele fique conforme o estilo selecionado. Por pequenas alterações, queremos dizer remover/adicionar linhas vazias, reorganizar importações de pacotes, alinhar recuo, etc. Elas nunca fazem mudanças drásticas, como mudar o nome de uma variável ou implementar uma função.

<iframe class="base-markdown-iframe__iframe" id="player-Viy84iKS_iw" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[macOS] Automatic code formatting in VS Code" width="640" height="360" src="https://www.youtube.com/embed/Viy84iKS_iw?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=4&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F36769e8c-c72a-4590-b06b-f59e226eea2f%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

No vídeo acima, adicionamos um bloco de código `if-else` ao arquivo `my_script.py`. O bloco apenas imprime mensagens diferentes dependendo se `x` foi um número par ou ímpar.

(A propósito, você notou como, após digitar `if`, selecionamos `ifelse` da lista de autossugestão e isso produziu um bloco de código multilinha? Eles são chamados de trechos de código, e você pode instalá-los com [esta extensão](https://marketplace.visualstudio.com/items?itemName=frhtylcn.pythonsnippets))

Podemos bagunçar de propósito esse trecho de código adicionando ou removendo espaços em branco aleatoriamente. Para corrigir tais erros automaticamente, primeiro precisamos configurar a formatação do código:

-   Instale autopep8 conforme descrito [aqui](https://marketplace.visualstudio.com/items?itemName=ms-python.autopep8)
-   Abra a "Command Pallet" (Paleta de comandos) ("View" → "Command Pallet").
-   Comece a digitar "Settings" (Configurações) e selecione a opção "Preferences: Open Settings (UI)" (Preferências: abrir configurações (IU)) na lista de sugestões.
-   No menu de configurações, procure "format provider" (provedor de formato). Isso mostra a configuração "Formatting: Provider" (Formatação: provedor).
-   Defina essa configuração como `autopep8` nas abas [User e Workspace](https://code.visualstudio.com/docs/getstarted/settings) _(os materiais estão em inglês)_ e retorne ao seu script com o código mal formatado.
-   Abra novamente "Command Pallet", procure a opção "Format Document" (Formatar documento) e clique nela.

Se você quiser que esse tipo de limpeza seja executada automaticamente toda vez que você salvar as alterações:

-   Abra as configurações de novo ("Command Pallet" → "Preferences: Open Settings (UI)" (Preferências: abrir configurações (UI)).
-   Procure "format on save" (formatar ao salvar) e habilite a configuração "Format On Save" (Formatar ao salvar)".
-   Agora tente estragar o código de novo, mas dessa vez apenas salve as alterações e observe o código ser formatado automaticamente na mesma hora.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-28-49-499Z.md
### Última modificação: 2025-05-28 19:28:49

# JupyterLab - TripleTen

Capítulo 6/8

Ambiente de programação individual

# JupyterLab

Se você usa Windows

<iframe class="base-markdown-iframe__iframe" id="player-GJc2B_yDaYE" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="[Windows] JupyterLab Overview" width="640" height="360" src="https://www.youtube.com/embed/GJc2B_yDaYE?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fe6633f38-76e6-49a0-b285-b1cce8c3dcf5%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

## O que é o JupyterLab?

Se você planeja fazer a maioria do seu trabalho no Jupyter Notebook, mas não precisa de todas as funcionalidades adicionais que vêm com IDEs, como o VS Code, considere usar o JupyterLab.

Aqui está uma pequena explicação do que é o JupyterLab tirada do site oficial:

> O JupyterLab é um ambiente de desenvolvimento interativo baseado na Web para Jupyter Notebook, código e dados. O JupyterLab é flexível: configure e organize a interface do usuário para suportar uma ampla gama de fluxos de trabalho em ciência de dados, computação científica e aprendizado de máquina. O JupyterLab é extensível e modular: escreva plug-ins que adicionam componentes novos e fazem integração com os existentes.

Se isso ajudar, você pode pensar no JupyterLab como Jupyter Notebook+. Ele fornece algumas características interessantes, além de trabalhar em notebooks. Tal como:

-   Experiência unificada: navegador de arquivos, cadernos, editor de texto e terminal são todos consolidados em um aplicativo (e uma aba em seu navegador web).
-   Layouts flexíveis: organize e reorganize seu espaço de trabalho como quiser arrastando/soltando/redimensionando as abas. Você pode até mesmo olhar para diferentes partes do mesmo caderno em uma visão lado a lado.
-   Extensível: personalize temas e adicione funcionalidades adicionais com [Extensões JupyterLab](https://jupyterlab.readthedocs.io/en/stable/user/extensions.html) _(os materiais estão em inglês)_. Algumas extensões úteis permitem que você trabalhe bem com Git e GitHub, depure e formate automaticamente seu código, desenhe diagramas, crie apresentações e muito mais.

## Instalar e iniciar o JupyterLab

O JupyterLab vem pré-instalado com o Anaconda, e você pode iniciá-lo a partir da GUI do Anaconda Navigator. Se você tiver [miniconda](https://docs.conda.io/en/latest/miniconda.html) _(os materiais estão em inglês)_ e precisar instalar o JupyterLab com o gerenciador de pacotes `conda`, execute o seguinte comando: `conda install -c conda-forge jupyterlab`.

Você também pode instalá-lo com `pip`: `pip install jupyterlab`.

Para iniciar o JupyterLab a partir da linha de comando, basta executar: `jupyter lab`.

Isso vai iniciar um servidor web em `http://localhost:<PORT>/lab`, onde a porta padrão é 8888.

Se você quiser experimentá-lo antes de instalar, é possível fazer isso [aqui](https://mybinder.org/v2/gh/jupyterlab/jupyterlab-demo/HEAD?urlpath=lab/tree/demo). Esteja ciente que vai levar algum tempo para que tudo seja configurado e lançado.

## Jupyter Notebook vs JupyterLab vs VS Code?

Essa escolha é, em grande parte, uma questão de gosto e preferência pessoal. Dito isso, se você for trabalhar com poucos cadernos relativamente simples, talvez prefira a simplicidade do Jupyter Notebook.

Se, por outro lado, você planeja trabalhar com vários cadernos, arquivos de texto/markdown, imagens, etc., recomendamos que você experimente o JupyterLab.

Por fim, se você planeja iniciar seu trabalho exploratório no Jupyter Notebook, mas depois planeja incorporar os resultados desse trabalho em um aplicativo maior (como um aplicativo web), considere o VS Code: você vai poder fazer tudo no mesmo lugar.

Se você usa macOS

Se você planeja fazer a maioria do seu trabalho no Jupyter Notebook, mas não precisa de todas as funcionalidades adicionais que vêm com IDEs, como o VS Code, considere usar o JupyterLab.

Aqui está uma pequena explicação do que é o JupyterLab tirada do site oficial:

> O JupyterLab é um ambiente de desenvolvimento interativo baseado na Web para Jupyter Notebook, código e dados. O JupyterLab é flexível: configure e organize a interface do usuário para suportar uma ampla gama de fluxos de trabalho em ciência de dados, computação científica e aprendizado de máquina. O JupyterLab é extensível e modular: escreva plug-ins que adicionam componentes novos e fazem integração com os existentes.

Se isso ajudar, você pode pensar no JupyterLab como Jupyter Notebook+. Ele fornece algumas características interessantes, além de trabalhar em notebooks. Tal como:

-   Experiência unificada: navegador de arquivos, cadernos, editor de texto e terminal são todos consolidados em um aplicativo (e uma aba em seu navegador web).
-   Layouts flexíveis: organize e reorganize seu espaço de trabalho como quiser arrastando/soltando/redimensionando as abas. Você pode até mesmo olhar para diferentes partes do mesmo caderno em uma visão lado a lado.
-   Extensível: personalize temas e adicione funcionalidades adicionais com [Extensões JupyterLab](https://jupyterlab.readthedocs.io/en/stable/user/extensions.html) _(os materiais estão em inglês)_. Algumas extensões úteis permitem que você trabalhe bem com Git e GitHub, depure e formate automaticamente seu código, desenhe diagramas, crie apresentações e muito mais.

## Instalar e iniciar o JupyterLab

O JupyterLab vem pré-instalado com o Anaconda, e você pode iniciá-lo a partir da GUI do Anaconda Navigator. Se você tiver [miniconda](https://docs.conda.io/en/latest/miniconda.html) _(os materiais estão em inglês)_ e precisar instalar o JupyterLab com o gerenciador de pacotes `conda`, execute o seguinte comando: `conda install -c conda-forge jupyterlab`.

Você também pode instalá-lo com `pip`: `pip install jupyterlab`.

Para iniciar o JupyterLab a partir da linha de comando, basta executar: `jupyter lab`.

Isso vai iniciar um servidor web em `http://localhost:<PORT>/lab`, onde a porta padrão é 8888.

Se você quiser experimentá-lo antes de instalar, é possível fazer isso [aqui](https://mybinder.org/v2/gh/jupyterlab/jupyterlab-demo/HEAD?urlpath=lab/tree/demo). Esteja ciente que vai levar algum tempo para que tudo seja configurado e lançado.

## Jupyter Notebook vs JupyterLab vs VS Code?

Essa escolha é, em grande parte, uma questão de gosto e preferência pessoal. Dito isso, se você for trabalhar com poucos cadernos relativamente simples, talvez prefira a simplicidade do Jupyter Notebook.

Se, por outro lado, você planeja trabalhar com vários cadernos, arquivos de texto/markdown, imagens, etc., recomendamos que você experimente o JupyterLab.

Por fim, se você planeja iniciar seu trabalho exploratório no Jupyter Notebook, mas depois planeja incorporar os resultados desse trabalho em um aplicativo maior (como um aplicativo web), considere o VS Code: você vai poder fazer tudo no mesmo lugar.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-28-51-477Z.md
### Última modificação: 2025-05-28 19:28:51

# Conclusão - TripleTen

Capítulo 6/8

Ambiente de programação individual

# Conclusão

Neste capítulo, você aprendeu o que são IDEs e quais são suas funções, como realizar operações comuns no VS Code (escrever scripts Python, controlar a versão do seu código com o Git e o GitHub, trabalhar com o Jupyter Notebook e detectar e corrigir automaticamente erros de estilo) e os recursos do JupyterLab e como eles se comparam ao Jupyter Notebook e ao VS Code.

![](https://practicum-content.s3.us-west-1.amazonaws.com/data-eng/SDT/sdte-1-10-01.jpg?etag=49b7abdf4c71ee0b5518afdd4d5113dd)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-28-52-747Z.md
### Última modificação: 2025-05-28 19:28:53

# Sprint 5 - Projeto - TripleTen

Descrição

Revisar

História

# Sprint 5 - Projeto

Parabéns! Você concluiu a seção sobre ferramentas de desenvolvimento de software. Agora é hora de aplicar o conhecimento e as habilidades que você adquiriu em um projeto: vamos criar e implementar um dashboard de aplicativo web em um serviço de nuvem.

Quando terminar o projeto, lembre-se de enviar seu trabalho ao revisor para avaliação. Você vai receber feedback em até 24 horas. Use esse feedback para fazer alterações e envie a nova versão de volta ao revisor.

Você talvez receba mais feedback referente à nova versão. Isso é completamente normal. Não é incomum passar por vários ciclos de feedback e revisão.

Seu projeto será considerado concluído assim que o revisor o aprovar.

# Descrição do projeto

O foco desse projeto é que você tenha mais prática com tarefas comuns de engenharia de software. Isso vai aprimorar e complementar suas habilidades de dados e vai fazer com que você seja uma opção mais atrativa para possíveis empregadores.

As tarefas são: criar e gerenciar ambientes virtuais de Python, desenvolver um aplicativo web e implantá-lo em um serviço de nuvem que o tornará acessível ao público.

Neste projeto, vamos fornecer um conjunto de dados de anúncios de vendas de carros. No entanto, neste projeto, o foco não será no conjunto de dados nem na análise, então você está livre para escolher o conjunto de dados que desejar.

Veja como pode ficar sua solução final:

<iframe class="base-markdown-iframe__iframe" id="player-bna15Zj6jUI" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Final web application" width="640" height="360" src="https://www.youtube.com/embed/bna15Zj6jUI?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F17d030a7-7ac8-4dc6-a697-f3b761af168f%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

_Demonstração do aplicativo web que você vai criar neste projeto_

# Instruções para concluir o projeto

### Etapa 1. Configurar tudo

1.  Crie uma conta em [github.com](http://github.com). Se você já tem uma conta ou criou uma no [capítulo sobre o Git e GitHub](https://tripleten.com/trainer/data-analyst/lesson/b13bcde4-98a5-4fd4-9f17-bb1765e0ed9c/), pode pular essa etapa.
2.  Crie um novo repositório git com um arquivo `README.md` e um arquivo `.gitignore` (escolha um modelo de Python). Se precisar de uma recapitulação de como fazer isso, consulte [esta lição](https://tripleten.com/trainer/data-analyst/lesson/8e6f7469-dcb3-45f5-8603-cce8f1e51434/).
3.  Crie uma conta em [render.com](http://render.com/), se você ainda não fez isso. Falamos sobre o Render e aplicativos web [nesta lição](https://tripleten.com/trainer/data-analyst/lesson/060d909b-3466-468f-b8b7-245033126b61/). Quando estiver criando uma conta no Render, selecione a opção "GitHub" e siga os passos para fazer sua inscrição. Isso é exatamente o que você precisa para vincular sua conta no Render à sua conta no GitHub.
4.  Neste projeto, você vai realizar uma análise exploratória de dados. Para fazer isso, você precisa ter os pacotes `pandas` e `plotly-express` instalados. Já falamos sobre `plotly` [nesta lição](https://tripleten.com/trainer/data-analyst/lesson/7be99652-b08c-4bd1-8d16-febe87a99cac/). `plotly-express` é projetado com padrões razoáveis e opções configuráveis para tipos comuns de gráficos, o que o torna um ótimo ponto de entrada para iniciantes, em comparação com o próprio `plotly`. Se ainda não o conhece bem, não se preocupe — vamos guiar você por todo o processo! Além disso, você vai precisar do pacote `streamlit` para desenvolver um aplicativo web. Crie um novo ambiente virtual e escolha um nome significativo que seja relacionado ao conjunto de dados com o qual você vai trabalhar. Por exemplo, você poderia chamá-lo de `vehicles_env`. Certifique-se de que você instalou pelo menos os seguintes pacotes no ambiente: `pandas`, `streamlit` e `plotly-express`.

Se estiver com dificuldades para encontrá-los, aqui estão os comandos para instalar as bibliotecas:

-   Para `plotly-express`, confira esta [página de documentação](https://anaconda.org/plotly/plotly_express).
-   [Aqui está a página de documentação](https://anaconda.org/anaconda/pandas) da `pandas`, em que você encontra o comando que deve ser executado.
-   Por fim, [esta é a página](https://anaconda.org/conda-forge/streamlit) da `streamlit`.
-   Além disso, será necessário instalar mais uma biblioteca (`nbformat`) para ativar o plotly express. Faça a instalação com `conda install -n myenv nbformat`

Se precisar de uma recapitulação sobre como criar um ambiente virtual e instalar pacotes, consulte [a lição sobre esse tópico](https://tripleten.com/trainer/data-analyst/lesson/7be99652-b08c-4bd1-8d16-febe87a99cac/).

5.  [Instale o VS Code](https://code.visualstudio.com/download), se você ainda não fez isso. Clone seu repositório do projeto do GitHub e abra-o como um projeto no VS Code. Esse será o diretório do seu projeto. Defina o interpretador Python para aquele usado pelo ambiente virtual que você criou anteriormente.
6.  Para fins de simplicidade, em vez de salvar seu ambiente no arquivo `requirements.txt` no diretório do projeto, crie manualmente o arquivo `requirement.txt` e adicione três bibliotecas ali sem especificar as versões delas: `pandas plotly_express streamlit`

### Etapa 2. Baixar **o arquivo de dados**

1.  [Baixe o conjunto de dados de anúncios de carros](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_NEW_4_sprint/vehicles.csv) (`vehicles_us.csv`) ou encontre seu próprio conjunto de dados em formato CSV.
2.  Coloque o conjunto de dados no diretório do projeto.

### Etapa 3. Análise exploratória de dados

1.  Crie um diretório chamado `notebooks` no diretório do seu projeto.
2.  Crie um notebook Jupyter chamado `EDA.ipynb` no VS Code e salve-o no diretório `notebooks` do projeto. Lembre-se de que `.ipynb` é uma extensão de arquivo usada para notebooks Jupyter.
3.  Abra o notebook Jupyter `EDA.ipynb` e faça testes com `plotly-express` para criar visualizações para uma análise exploratória básica do conjunto de dados no notebook. Aqui estão alguns exemplos:

Como criar um histograma usando `plotly-express`.

```
import pandas as pd
import plotly.express as px

car_data = pd.read_csv('vehicles_us.csv') # lendo os dados
fig = px.histogram(car_data, x="odometer") # criar um histograma
fig.show() # exibindo
```

![](https://practicum-content.s3.amazonaws.com/resources/newplot_1693597197.png)

Como criar um gráfico de dispersão nessa biblioteca.

```
import pandas as pd
import plotly.express as px

car_data = pd.read_csv('vehicles_us.csv') # lendo os dados
fig = px.scatter(car_data, x="odometer", y="price") # criar um gráfico de dispersão
fig.show() # exibindo
```

![](https://practicum-content.s3.amazonaws.com/resources/newplot_1_1693597282.png)

Se você tiver interesse em construir outros tipos de gráficos usando `plotly-express`, confira exemplos dos gráficos básicos mais comuns [nesta página](https://plotly.com/python/basic-charts/). A página inclui tanto exemplos quanto o código necessário para implementá-los. Entretanto, não gaste muito tempo explorando o conjunto de dados, pois o foco deste projeto é diferente. O objetivo é criar um aplicativo web e, embora seja ótimo ter vários gráficos de tipos diferentes nele, isso não é necessário nesta etapa. Em vez disso, vamos nos concentrar no trabalho necessário para implementar tal aplicativo. Isso é exatamente o que faremos nas próximas etapas.

### Etapa 4. Desenvolver o dashboard do aplicativo web

1.  Crie um arquivo `app.py` no diretório raiz do projeto. Para criar um arquivo `.py`, clique em "New File" (Novo arquivo) no VS Code e armazene-o no diretório do projeto com o nome desejado e a extensão `.py`.
2.  Importe `streamlit` como `st`, `pandas` e `plotly_express` no início do arquivo.
3.  Leia o arquivo CSV do conjunto de dados em um DataFrame. O código será o mesmo que você tinha no Jupyter Notebook ao explorar o conjunto de dados.
4.  Agora, vamos criar o conteúdo do aplicativo baseado em Streamlit. Aqui está o que queremos que você inclua nele:
    -   Pelo menos um cabeçalho. Você pode usar \[`st.header()`\] para fazer isso. Na [lição sobre aplicativos web](https://tripleten.com/trainer/data-analyst/lesson/060d909b-3466-468f-b8b7-245033126b61/), mostramos como criar um cabeçalho.
    -   Um botão que, ao ser clicado, cria um histograma `plotly-express`. Para fazer isso, considere usar as funções [`st.write()`](https://docs.streamlit.io/library/api-reference/write-magic/st.write) e [`st.plotly_chart()`](https://docs.streamlit.io/library/api-reference/charts/st.plotly_chart) _(os materiais estão em inglês)_. Aqui está um exemplo de como você pode fazer isso:

```
        import pandas as pd
        import plotly.express as px
        import streamlit as st
        
        car_data = pd.read_csv('vehicles_us.csv') # lendo os dados
        hist_button = st.button('Criar histograma') # criar um botão
        
        if hist_button: # se o botão for clicado
            # escrever uma mensagem
            st.write('Criando um histograma para o conjunto de dados de anúncios de vendas de carros')
            
            # criar um histograma
            fig = px.histogram(car_data, x="odometer")
        
            # exibir um gráfico Plotly interativo
            st.plotly_chart(fig, use_container_width=True)
        
```

-   Adicione outro botão que, ao ser clicado, cria um gráfico de dispersão `plotly-express`. Сonsidere usar as funções [st.write()](https://docs.streamlit.io/library/api-reference/write-magic/st.write) e [st.plotly\_chart()](https://docs.streamlit.io/library/api-reference/charts/st.plotly_chart) _(os materiais estão em inglês)_.
    
    Esta etapa é opcional, mas, se você quiser um desafio extra, tente substituir botões por caixas de seleção, disponíveis em `streamlit` através de `st.checkbox()`. Você pode solicitar que o usuário selecione a caixa de seleção que corresponda a um histograma ou um gráfico de dispersão, e então um gráfico será criado dependendo da caixa selecionada. Aqui está um exemplo simples de como caixas de seleção funcionam em `streamlit`:
    
    ```
    import streamlit as st
    
    # criar uma caixa de seleção
    build_histogram = st.checkbox('Criar um histograma')
    
    if build_histogram: # se a caixa de seleção for selecionada
      st.write('Criando um histograma para a coluna odometer')
          ...
    ```
    

5.  Certifique-se de atualizar o arquivo `README` quando terminar. Ele deve incluir uma breve descrição do projeto, explicando para que serve o aplicativo web e quais funcionalidades ele oferece.
6.  Para tornar o Streamlit compatível com o Render, adicione um arquivo de configuração do Streamlit ao repositório do seu projeto em `streamlit/config.toml` com o seguinte conteúdo: `toml [server] headless = true port = 10000 [browser] serverAddress = "0.0.0.0" serverPort = 10000` Ele dirá ao Render para procurar no lugar certo para ouvir seu aplicativo Streamlit ao colocá-lo em seus servidores.
7.  Não se esqueça de confirmar e enviar todas as alterações de volta para o seu repositório após terminar o trabalho. Se você não fizer isso, nada vai funcionar direito.

Observações importantes:

-   À medida que você desenvolve o aplicativo adicionando um novo componente Streamlit, você pode executar o comando `streamlit run app.py` do terminal para ver como está o resultado.
-   À medida que você atinge alguns marcos no desenvolvimento do aplicativo (por exemplo, você adiciona um componente de trabalho e o aplicativo é executado sem erros), é uma boa prática fazer commit e enviar seu trabalho para um repositório remoto no GitHub. Portanto, não se esqueça de escrever uma mensagem de commit significativa!
    
-   Abra sua conta em [render.com](http://render.com) e crie um novo serviço web:
    

![](https://practicum-content.s3.amazonaws.com/resources/render_dashboard_1693598453.png)

2.  Crie um novo serviço web vinculado ao seu repositório Github:

![](https://practicum-content.s3.amazonaws.com/resources/render_new_web_service_1693598484.png)

1.  Configure o novo serviço web Render adicionando um comando de compilação **Build Command** que vai instalar tudo o que seja necessário para executar seu aplicativo, incluindo `streamlit` e todos os pacotes de `requirements.txt`. Use o seguinte comando:

```
pip install --upgrade pip && pip install -r requirements.txt
```

Adicione o seguinte ao seu **Start Command**: `streamlit run app.py`. Deve ficar assim:

![](https://practicum-content.s3.amazonaws.com/resources/render_configure_2_1693598507.png)

4.  Implemente o aplicativo no Render e aguarde até que a compilação seja bem-sucedida:

![](https://practicum-content.s3.amazonaws.com/resources/render_build_success_1693598525.png)

5.  Verifique se seu aplicativo está acessível no seguinte URL: `https://<APP_NAME>.onrender.com/`

**Observação:** como você está usando uma versão gratuita, pode levar vários minutos após uma implementação bem-sucedida para que o aplicativo fique disponível online. Observe também que os aplicativos ficam "adormecidos" após ficarem inativos por alguns minutos. Nesse caso, basta carregar e atualizar a página do aplicativo algumas vezes para que ele seja ativado.

**Se você atualizar seu repositório GitHub,** para implementar a versão mais recente no Render, clique em Manual Deploy → Latest Commit.

## Como enviar o projeto:

Você vai precisar enviar um link para seu repositório GitHub. Adicione também a URL do aplicativo no Render ao `README.md` do seu projeto

## Como meu projeto será avaliado?

Elaboramos alguns critérios de avaliação. Leia-os com atenção antes de fazer o envio.

Isto é o que os revisores procuram ao avaliar seu projeto:

-   O repositório do projeto contém pelo menos os seguintes arquivos?
    
    ```
      $ tree
      .
      ├── README.md
      ├── app.py
      ├── vehicles_us.csv
      ├── requirements.txt
      └── notebooks
          └── EDA.ipynb
      └── .streamlit
          └── config.toml
      
    ```
    
-   O aplicativo web pode ser acessado em um navegador?
-   O aplicativo web contém os itens a seguir?
    -   pelo menos um cabeçalho com texto
    -   pelo menos 1 histograma
    -   pelo menos 1 gráfico de dispersão
    -   pelo menos um botão ou uma caixa de seleção

[Avançar](/trainer/data-analyst/lesson/f1cbe360-ff76-4e99-8f26-f38cf8ed14b7/)

Passo 2/2

Sua atribuição foi revisada e aceita

A atribuição foi enviada

Revisão completa

Olá, Jonathas! Sou a Suelen e ficarei responsável pela revisão do seu projeto. Sua aplicação está **MUITO BOA!** Adorei os filtros que você utilizou, está bastante interativo. Também gostei da documentação do README.md, completa e bem explicativa. Espero que você se sinta orgulhoso do seu projeto!:) Está aprovado!

suelen.silvarevisor

Expandir

Como foi a avaliação?

## Seu projeto com feedback

[https://dashboard-web-5o3j.onrender.com/](https://dashboard-web-5o3j.onrender.com/)

Project link

Copy

[Avançar](/trainer/data-analyst/lesson/f1cbe360-ff76-4e99-8f26-f38cf8ed14b7/)

## Sprint 5 - Projeto

2025-02-07 02:00

O que você achou do projeto?

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-28-54-058Z.md
### Última modificação: 2025-05-28 19:28:54

# Feedback do Sprint 5 - TripleTen

Capítulo 8/8

Conclusão

# Compartilhe sua opinião

**Olá!**

Você acabou de concluir um sprint e enviar seu projeto. Parabéns por alcançar este marco! 🚀

Agora é o momento perfeito para dar seu feedback sobre o sprint. Sua opinião vai nos ajudar a aprimorar ainda mais o processo de aprendizagem.

Este questionário curto vai levar apenas 2 minutos. Todas as respostas vão ser confidenciais: não vamos compartilhá-las de forma que identifique você.

Agradecemos por nos ajudar a evoluir com você!

Como você avalia a experiência com os materiais didáticos, as tarefas e os projetos neste sprint?

1

2

3

4

5

Respostas: 1 – Muito insatisfatória; 5 – Muito satisfatória.

Avançar

1 — 9

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-28-55-694Z.md
### Última modificação: 2025-05-28 19:28:56

# Conclusão - TripleTen

Capítulo 8/8

Conclusão

# Conclusão

![](https://practicum-content.s3.amazonaws.com/resources/Artboard_1_copy2x-100_1695381148.jpg)

As habilidades de engenharia de software que você aprendeu neste sprint são fundamentais para trabalhar no campo de dados. Sem essas habilidades, você terá limitações severas e dificuldade em entender outros desenvolvedores da sua equipe.

O material que abordamos deve se tornar uma boa base para seu futuro progresso.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-30-40-055Z.md
### Última modificação: 2025-05-28 19:30:40

# Principais conclusões do primeiro módulo - TripleTen

Capítulo 1/5

Conclusões fundamentals do primeiro módulo

# Principais conclusões do primeiro módulo

Parabéns! Você concluiu oficialmente os quatro primeiros sprints do programa, concluiu seus primeiros projetos e chegou mais perto do seu objetivo.

Para descobrir quais aulas você deve revisar antes de concluir este projeto, responda às seguintes perguntas:

Pergunta

Você recebeu dados do usuário e descobriu lacunas nos campos "Idade" e "Renda". Como você vai preencher as lacunas?

Vou preencher "Idade" com o valor médio e "Renda" com a mediana.

Primeiro eu vou olhar para a variabilidade dos dados, e então eu vou tomar uma decisão.

Vou preencher "Idade" e "Renda" com a média.

Vou preencher "Idade" e "Renda" com a mediana.

Você conseguiu!

Pergunta

Que problemas você vai encontrar se tentar aplicar _mean()_ em uma coluna onde há strings e números?

Uma mensagem de erro de conversão de tipo aparecerá.

Não haverá erros; a média será calculada com base nos dados numéricos.

Não haverá uma mensagem de erro, mas os resultados não serão calculados.

Não haverá erros; a média será calculada com base em todos os dados.

Excelente!

Pergunta

Qual declaração sobre duplicados completos está correta?

Estas são linhas 100% idênticas.

Estes são dados com diferentes registros em uma coluna.

Estes são dados com diferentes formas de palavras em uma coluna.

Estes são dados que aparecem repetidamente em uma coluna.

Seu entendimento sobre o material é impressionante!

Pergunta

Escolha uma situação onde a categorização dos dados seja obrigatória.

Quando você precisa agrupar dados por cidade.

Quando há interesse em dados de notas de alunos.

Quando você precisa ordenar dados por classificação.

Quando há interesse em dados de faixa etária.

Excelente!

Pergunta

A correlação entre os dois valores é igual a 0,78. O que isso significa?

Há uma correlação forte positiva entre as duas variáveis.

Há uma correlação moderada positiva entre os dois valores.

Há uma forte correlação inversa entre os dois valores.

Há uma fraca correlação inversa entre os dois valores.

Nada pode ser declarado com confiança sobre o nível de conexão com base nestes dados.

Você conseguiu!

Pergunta

Quais métodos podem ser usados para obter uma fatia de dados?

O método _query()_ e o atributo _loc_.

O método _query()_ e uma fatia com uma condição lógica.

Uma fatia com uma condição lógica e o atributo _loc_.

O método _cut()_ e o método _query()_.

Você conseguiu!

Pergunta

Que tipo de hipóteses existem?

Primeira e segunda.

Nula e alternativa.

Principal e secundária.

Boa e ruim.

Preta e branca.

Excelente!

Pergunta

O que é um valor _p_?

A medição da disseminação de valores aleatórios de variáveis em relação ao valor esperado.

O valor médio de uma variável aleatória à medida que o número de medições se aproxima do infinito.

O nível de significância que separa a média do valor ao qual ela está sendo comparada сritério estatístico.

A probabilidade de ter um resultado que seja tão extremo quanto o que você está considerando, assumindo que a hipótese nula é verdadeira.

Nível de significância.

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-30-41-369Z.md
### Última modificação: 2025-05-28 19:30:41

# Como trabalhar com documentação - TripleTen

Capítulo 2/5

Uso de documentação

# Como trabalhar com documentação

Cada linguagem de programação tem o que é chamado de **documentação** (ou, no jargão da indústria, **docs**) — uma descrição de sua sintaxe e funções. Normalmente, os docs são de domínio público.

Se há um erro no seu código e você não entende qual é o problema, dê uma olhada nos docs.

Encontrar a documentação da linguagem de programação necessária é fácil: basta pesquisar, por exemplo, "documentação Python".

Se esse pedido de pesquisa for muito genérico, torne-o mais descritivo. Por exemplo, inclua o nome da função: "Python função len() documentação". O primeiro resultado provavelmente será um link para a documentação oficial da linguagem: [https://docs.python.org/pt-br/3/library/functions.html](https://docs.python.org/pt-br/3/library/functions.html).

Vamos dar uma olhada de perto nesta página.

![](https://practicum-content.s3.amazonaws.com/resources/5.2.1PT_1704786022.png)

Como determinar que o que você está vendo é a documentação oficial? É difícil dar diretrizes gerais, mas se você está vendo docs para o Python e a biblioteca pandas, note que seus domínios e a forma como as informações são estruturadas são semelhantes.

Tente usar sites com documentação oficial. Assim, você com certeza vai estar acessando informações corretas.

Tente encontrar informações sobre os métodos na documentação por conta própria.

Links para documentação oficial:

— [Linguagem de programação Python 3](https://docs.python.org/3/)

— [Biblioteca Pandas](https://pandas.pydata.org/pandas-docs/stable/)

— [Biblioteca Matplotlib](https://matplotlib.org/)

Pergunta

Encontre informações sobre o método _abs()_ de _pandas_ e selecione a resposta correta:

Esse método calcula a média e só pode ser usado em objetos _Series_.

Esse método encontra o valor absoluto e funciona apenas com um _DataFrame_.

Esse método encontra o valor absoluto e funciona com ambos os _DataFrame_ e _Series_.

Esse método encontra a soma na coluna _DataFrame_.

Você conseguiu!

Pergunta

Encontre informações sobre a função _sorted()_ e selecione a resposta correta:

A função _sorted()_ ordena listas.

A função _sorted()_ ordena dicionários.

A função _sorted()_ ordena qualquer estrutura de dados.

Excelente!

Nem sempre é fácil entender a documentação. Se você está tendo problemas, existem sites onde desenvolvedores e analistas fazem e respondem perguntas. O recurso mais popular desse tipo é o [Stack Overflow](https://stackoverflow.com/), que tem discussões em vários idiomas. As respostas geralmente incluem segmentos de código funcional. Pode ser tentador apenas copiar e colar no seu trabalho. No entanto, antes de qualquer coisa, verifique exatamente como o código que você achou resolverá sua tarefa e certifique-se de que entende como ele funciona. Se você não entendê-lo, inevitavelmente será necessário voltar até ele mais tarde.

Além do Stack Overflow e sites similares, você também pode consultar o [GitHub](https://github.com/), que é como um armazém para código. Aqui, desenvolvedores de todo o mundo postam soluções para seus problemas e, se tivermos sorte, também dão descrições detalhadas sobre por que seu código está escrito do jeito que está. Assim como nas soluções publicadas no Stack Overflow, você tem que ter cuidado e evitar copiar e colar códigos sem sentido.

-   Certifique-se que você entende todos os métodos que são usados no código
-   Verifique se o código resolve o seu problema corretamente

Aprender sobre o que outros desenvolvedores estão fazendo amplia seus horizontes e aguça suas habilidades. Se a documentação ou outros recursos não forem suficientes para encontrar uma solução, você sempre pode pedir ajuda a um colega ou supervisor.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-30-42-681Z.md
### Última modificação: 2025-05-28 19:30:43

# Como ler erros - TripleTen

Capítulo 2/5

Uso de documentação

# Como ler erros

Os erros em Python são chamados de **tracebacks**. Um traceback significa que um programa escrito é verificado em cada passo do caminho (é "rastreado de baixo para cima"). Se algo não se estiver bem, o Python produz uma mensagem de erro.

Vejamos um exemplo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_5/PT/moved_5.2.2PT.png)

Os erros geralmente contêm setas. A última seta indica a localização real do erro.

Não tenha medo de tracebacks longos. Eles, na verdade, são uma coisa boa, porque significam que o programa fez um monte de coisas antes de encontrar um erro. Dê uma olhada na última seta: essa é a raiz do problema.

Python é um cobrinha amigável que gosta de ajudar. Por isso, ao descrever o erro, ele sempre diz de que tipo é. No segundo exemplo, ele nos diz que temos um **IndexError**, e em seguida fornece uma explicação mais detalhada.

Em alguns casos, basta apenas descobrir onde você precisa fazer uma mudança, mas às vezes você não tem escolha a não ser analisar o problema em detalhes. A documentação sobre valores de erros está aqui: [https://docs.python.org/pt-br/3/library/exceptions.html](https://docs.python.org/pt-br/3/library/exceptions.html).

Vamos olhar para alguns outros erros e aprender como lidar com eles:

**SyntaxError: invalid syntax**

Esse erro significa que você esqueceu um parêntese, dois pontos ou vírgula. O _Python_ ajuda a detectar o problema rapidamente, indicando o local do erro e a linha onde ele aparece.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_5/PT/moved_5.2.2.2PT.png)

**IndentationError: unexpected indent**

Indentação... se acostume com esse erro, porque você vai vê-lo muitas vezes. As indentações desempenham um papel crucial no Python, e não há nada que você possa fazer sobre isso. Esse erro aparece quando há um número incorreto de espaços de indentação.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_5/PT/moved_5.2.2.3PT.png)

**NameError: name ‘c’ is not defined**

Quando você tenta se referir a uma variável antes de atribuir um valor a ela, você recebe um _NameError_.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_5/PT/moved_5.2.2.4PT.png)

Isso acontece, por exemplo, quando há um erro de digitação no nome da variável:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_5/PT/moved_5.2.2.5PT.png)

**IndexError: list/string index out of range**

Se você se referir a um elemento que não está na lista ou dicionário em questão, o _IndexError_ aparece para avisar. Verifique quantos elementos existem na lista/dicionário e certifique-se de que você está tentando obter o certo.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_5/PT/moved_5.2.2.6PT.png)

Resumo:

— As mensagens traceback podem parecer intimidadoras, mas elas nos fornecem informações úteis sobre o que deu errado. Por exemplo, elas mostram onde o erro ocorreu e que tipo de erro ele é.

— Os tipos mais comuns de erros são _SyntaxError_, _IndentationError_, _NameError_ e _IndexError_.

— _SyntaxError_: um erro com a sintaxe do programa.

— _IndentationError_: número incorreto de espaços de indentação.

— _NameError_: uma tentativa de se referir a uma variável que não foi definida ou cujo nome foi mal digitado.

— _IndexError_: uma tentativa de acessar um elemento inexistente em um dicionário ou lista.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-30-45-005Z.md
### Última modificação: 2025-05-28 19:30:45

# Introdução - TripleTen

Capítulo 3/5

Python em mais detalhes

# Introdução

Parabéns por continuar sua jornada de aprendizado em Python e se aprofundar no mundo empolgante de ciência de dados e desenvolvimento de software! Sua dedicação e seu progresso têm sido bem impressionantes, e está claro que você já construiu uma base sólida de conhecimento dos fundamentos de Python, técnicas de manipulação de dados e ferramentas essenciais de desenvolvimento de software.

Agora que você está se aproximando de um marco significativo, queremos fornecer a você a oportunidade de aprimorar suas habilidades em Python. Os seguintes materiais opcionais vão melhorar sua compreensão e também vão ensinar a lidar com desafios mais complexos:

Nesta etapa de sua jornada de aprendizagem, você vai explorar os seguintes tópicos:

-   **Uso de ciclos em dicionários:** entenda como percorrer elementos de um dicionário, que é uma estrutura de dados fundamental em Python.
-   **Estruturas de dados aninhadas com dicionários:** explore o poder de aninhamento de dicionários dentro de listas e veja as possibilidades de organização de dados que isso permite.
-   **Processamento de listas de dicionários:** aprenda como manipular listas contendo dicionários e extrair informações delas de forma eficiente.
-   **Parâmetros e valores padrão:** aprofunde-se no mundo dos parâmetros das funções e descubra como valores padrão podem aumentar a flexibilidade do seu código.
-   **Argumentos posicionais e nomeados:** compreenda a diferença entre os argumentos posicionais e os nomeados, o que vai adicionar mais controle e clareza a suas chamadas de funções.
-   **Retorno de valores:** aprenda como retornar valores para usar posteriormente em seus programas, pois essa habilidade vai tornar suas funções mais versáteis.
-   **O objeto Series:** conheça o objeto Series, um dos componentes principais da biblioteca Pandas usado para manipular dados.
-   **Estatística descritiva:** domine a arte de resumir e interpretar seus dados usando a estatística descritiva, uma habilidade crucial de análise de dados.

Esse currículo foi projetado para desafiar e enriquecer seu conhecimento de Python, fornecendo as ferramentas necessárias para lidar com tarefas de programação e desafios de manipulação de dados mais complexos. Não hesite em iniciar essa jornada e não se esqueça de que cada passo à frente é uma prova no seu desenvolvimento contínuo no campo de programação em Python.

Concluir este capítulo deve levar cerca de 2 a 3,5 horas. Então vamos começar!

![](https://practicum-content.s3.amazonaws.com/resources/4-1_1702290014.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-30-46-318Z.md
### Última modificação: 2025-05-28 19:30:46

# Dicionários: uso de ciclos em dicionários - TripleTen

Capítulo 3/5

Python em mais detalhes

# Dicionários: uso de ciclos em dicionários

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_1702290063.png)

Agora que aprendemos sobre as sutilezas dos dicionários em Python, você está prestes a passar para uma experiência de aprendizado ainda mais enriquecedora. Vamos levar nossas habilidades de dicionários ao próximo nível.

É importante mencionar que existem várias maneiras de acessar elementos de um dicionário. Além daquelas que já aprendemos, há outra maneira de fazer isso – percorrendo as chaves, valores ou itens do dicionário com um ciclo. Isso pode ser muito útil quando você precisa realizar uma operação em cada elemento. Por exemplo, você talvez queira atualizar os valores de determinadas chaves ou criar um novo dicionário com base em um subconjunto de chaves e valores.

Nesta lição, você vai aprender como usar os métodos `keys()`, `values()` e `items()` para percorrer elementos específicos de um dicionário.

### Percorrer chaves e valores

Você pode usar os métodos `keys()` e `values()` para visualizar as chaves e os valores de um dicionário, respectivamente. Esses métodos são úteis quando você quer realizar operações apenas nas chaves ou nos valores ou quando precisa verificar se uma determinada chave ou um valor específico está presente no dicionário.

Para percorrer as chaves de um dicionário, você pode usar o método `keys()`. Aqui está um exemplo:

```
financial_info = {
    'American Express': 93.23,
    'Boeing': 178.44,
    'Coca-Cola': 45.15,
    'Walt Disney': 119.34,
    'Nike': 97.99,
    'JPMorgan':96.27,
    'Walmart': 130.68 
}

for key in financial_info.keys():
    print(key)
```

O resultado será o seguinte:

```
American Express
Boeing
Coca-Cola
Walt Disney
Nike
JPMorgan
Walmart
```

Para percorrer os valores de um dicionário, vamos usar o método `values()`:

```
financial_info = {
    'American Express': 93.23,
    'Boeing': 178.44,
    'Coca-Cola': 45.15,
    'Walt Disney': 119.34,
    'Nike': 97.99,
    'JPMorgan':96.27,
    'Walmart': 130.68 
}

for value in financial_info.values():
    print(value)
```

O resultado será o seguinte:

```
93.23
178.44
45.15
119.34
97.99
96.27
130.68
```

Vamos nos certificar de que você entendeu bem esses novos métodos:

Pergunta

Selecione todas as afirmações corretas sobre dicionários.

Escolha quantas quiser

O método `keys()` permite extrair todas as chaves do dicionário. Podemos usar um ciclo `for` para percorrer as chaves da seguinte maneira:

```
for key in my_dictionary.keys():
    print(key)
```

Sim, isso está correto!

O método `values()` permite extrair todos os valores do dicionário. Podemos usar um ciclo `for` para percorrer os valores da seguinte maneira:

```
for value in my_dictionary.values():
    print(value)
```

Ambos os métodos `keys()` e `values()` podem ser aplicados a uma lista

Excelente!

Vamos criar um programa para percorrer o dicionário `financial_info` e calcular o valor total de todas as ações. Isso pode ajudar os investidores a ter uma visão geral rápida do desempenho de sua carteira de investimentos.

Para fazer isso, você precisa usar um ciclo `for` para percorrer os itens do dicionário `financial_info` , extrair o preço das ações (que são armazenados como valores) e atualizar a variável `total_value`.

Imprima a variável `total_value` quando terminar.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

financial\_info \= {

'American Express': 93,

'Boeing': 178,

'Coca-Cola': 45,

'Walt Disney': 119,

'Nike': 97,

'JPMorgan': 96,

'Walmart': 130

}

  

total\_value \= 0 \# o valor que você vai atualizar a cada iteração do ciclo

for value in financial\_info.values():

total\_value += value

\# escreva seu ciclo aqui. A cada iteração do ciclo, extraia o preço

\# e atualize a variável total\_value

  

print(total\_value)

Dica

Mostrar a soluçãoValidar

Acabamos de mostrar como percorrer os valores de um dicionário e realizar operações neles. Usamos o método `values()` para descobrir qual é o valor total de todas as ações no dicionário `financial_info`. Você pode usar esse truque para resolver vários problemas em que precisa fazer algo com cada elemento de um dicionário.

### Percorrer itens

Além de visualizar chaves e valores de um dicionário, você também pode obter uma visualização de seus pares (chave, valor) usando o método `items()`. Esse método pode ser útil quando você quer percorrer os pares e realizar uma operação em cada um deles.

Veja como podemos fazer isso:

```
financial_info = {
    'American Express': 93.23,
    'Boeing': 178.44,
    'Coca-Cola': 45.15,
    'Walt Disney': 119.34,
    'Nike': 97.99,
    'JPMorgan':96.27,
    'Walmart': 130.68 
}

for key, value in financial_info.items():
    print(key, value)
```

e aqui está o resultado:

```
American Express 93.23
Boeing 178.44
Coca-Cola 45.15
Walt Disney 119.34
Nike 97.99
JPMorgan 96.27
Walmart 130.68
```

Percorrer dicionários Python é uma habilidade muito útil no trabalho com essa estrutura de dados. Usando os métodos `keys()`, `values()` e `items()`, você pode facilmente percorrer determinados elementos de um dicionário e realizar neles todas as operações necessárias.

Agora vamos calcular novamente o valor total de todas as ações, mas usando o método `items()` desta vez:

Vamos mais uma vez criar um programa para percorrer o dicionário `financial_info` e calcular o valor total de todas as ações.

Desta vez, você precisa usar o método `items()` em vez de `values()`.

Imprima a variável `total_value` quando terminar.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

financial\_info \= {

'American Express': 93,

'Boeing': 178,

'Coca-Cola': 45,

'Walt Disney': 119,

'Nike': 97,

'JPMorgan': 96,

'Walmart': 130

}

  

total\_value \= 0 \# o valor que você vai atualizar a cada iteração do ciclo

for key, value in financial\_info.items():

total\_value += value

\# escreva seu ciclo aqui. A cada iteração do ciclo, extraia o preço

\# e atualize a variável total\_value

  

print(total\_value)

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-30-47-621Z.md
### Última modificação: 2025-05-28 19:30:48

# Dicionários: estruturas de dados aninhadas com dicionários - TripleTen

Capítulo 3/5

Python em mais detalhes

# Dicionários: estruturas de dados aninhadas com dicionários

Na lição anterior, você aprendeu sobre 3 novos métodos: `values()`, que extrai os valores de um dicionário, `keys()` que extrai suas chaves e `items()`, que extrai as chaves e os valores. Também mostramos como usar esses métodos em um ciclo `for`. Aqui está um exemplo:

```
financial_info = {
    'American Express': 93.23,
    'Boeing': 178.44,
    'Coca-Cola': 45.15,
    'Walt Disney': 119.34,
    'Nike': 97.99,
    'JPMorgan':96.27,
    'Walmart': 130.68 
}

for value in financial_info.values():
    print(value)
```

```
93.23
178.44
45.15
119.34
97.99
96.27
130.68
```

Nesta lição, vamos explorar estruturas de dados aninhadas que usam dicionários. Aprenderemos como combinar dicionários com listas, como criar essas estruturas e iterar sobre elas. Também vamos discutir como iterar sobre dicionários e como processar uma lista de dicionários.

Às vezes, os valores de um dicionário são estruturas aninhadas. No primeiro sprint, vimos um exemplo de uma lista de listas que representa uma estrutura aninhada. Aqui está:

```
movies_info = [
    ['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111],
    ['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730],
    ['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499],
    ["Schindler's List", 'USA', 1993, 'drama', 195, 8.818],
    ['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625],
    ['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619],
    ['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521],
    ['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644],
    ['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106],
    ['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077]
]
```

Dicionários também podem conter estruturas aninhadas. No final desta lição, você terá uma melhor compreensão de estruturas aninhadas e dicionários e de como eles podem ser usados para processar dados complexos.

## Dicionários de listas

Uma estrutura de dados útil em Python é um dicionário de listas, em que cada lista serve de valor para um registro ou uma entrada do dicionário. Isso facilita o processo de organizar dados de muitas maneiras diferentes. Cada dicionário pode ter muitos pares chave-valor que descrevem diferentes atributos ou propriedades do registro.

Vamos ver um exemplo de um itinerário de ônibus:

![](https://practicum-content.s3.amazonaws.com/resources/8.5PT_1702290637.png)

No dicionário acima, as chaves são os números dos ônibus, e os valores são listas com os horários de partida. Você pode acessar a grade horária de determinado itinerário utilizando a chave.

Veja como você pode iterar sobre um dicionário de listas. Primeiro, examine o código abaixo e o execute para ver o resultado.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

bus\_schedule \= {

'72': \['8:00', '12:00', '17:30'\],

'26': \['9:30', '15:00'\],

'17': \['7:30', '12:30', '15:30'\]

}

  

\# itere sobre as chaves e os valores do dicionário

for route, times in bus\_schedule.items():

\# itere sobre os valores da lista

for time in times:

\# imprima o itinerário e seus horários associados

print(f"Linha {route} - Partida {time}")

  

Mostrar a soluçãoExecutar

Usamos o método `items()` para percorrer as chaves e os valores do dicionário e, em seguida, usamos um ciclo `for` para iterar sobre a lista contida no valor e imprimir o itinerário e seus horários associados.

Agora, vamos praticar:

Pergunta

O que o seguinte código vai fazer?

```
bus_schedule = {
    '72': ['8:00', '12:00', '17:30'],
    '26': ['9:30', '15:00'],
    '17': ['7:30', '12:30', '15:30']
}

for route, times in bus_schedule.items():
    earliest_time = times[0]
    print(f'O horário da primeira partida do ônibus #{route} é {earliest_time}')
```

O código percorre os itens do dicionário `bus_schedule`, extrai o primeiro horário da lista e imprime a string que diz qual é o horário da primeira partida para uma certa linha.

Isso! Isso é exatamente o que esse código vai fazer. Aqui está o resultado que ele produz:

```
O horário da primeira partida do ônibus #72 é 8:00
O horário da primeira partida do ônibus #26 é 9:30
O horário da primeira partida do ônibus #17 é 7:30
```

O código percorre os itens do dicionário `bus_schedule`, extrai o último horário da lista e imprime a string que diz qual é o horário da última partida para uma certa linha.

Fantástico!

Dicionários de listas foram a primeira estrutura de dados aninhada que você aprendeu. Agora vamos passar de dicionários de listas para listas de dicionários e ver qual é a vantagem dessa estrutura aninhada.

## Listas de dicionários e tabelas

A última estrutura de dados que vamos examinar nesta lição é uma lista de dicionários.

Antes, você usou listas aninhadas para reproduzir tabelas. Aqui está um exemplo:

```
movies_table = [
    ['The Shawshank Redemption', 'USA', 'drama', 1994, 142, 9.111],
    ['The Godfather', 'USA', 'drama, crime', 1972, 175, 8.730],
    ['The Dark Knight', 'USA', 'fantasy, action, thriller', 2008, 152, 8.499]
]
```

O problema dessas "tabelas" é que suas linhas e colunas são acessadas por meio de índices. Por exemplo, para obter o ano de lançamento do filme da terceira linha, temos de acessar o item com índice 3 da sublista 2. Execute o código abaixo para ver como funciona:

CódigoPYTHON

9

1

2

3

4

5

6

7

movies\_table \= \[

\['The Shawshank Redemption', 'USA', 'drama', 1994, 142, 9.111\],

\['The Godfather', 'USA', 'drama, crime', 1972, 175, 8.730\],

\['The Dark Knight', 'USA', 'fantasy, action, thriller', 2008, 152, 8.499\]

\]

  

print(movies\_table\[2\]\[3\])

Mostrar a soluçãoExecutar

Se colocarmos essa lista de listas em formato de tabela, o resultado não será muito compreensível:

![](https://practicum-content.s3.amazonaws.com/resources/1FCC8BF0-635D-4698-806B-931420AE165C_1702290774.jpeg) _Tabela com índices em vez de nomes de colunas_

Um computador entenderia essa tabela, mas um analista poderia ficar confuso. Para obter os dados necessários, você teria que memorizar a ordem das colunas, qual índice corresponde à classificação, qual corresponde à duração e assim por diante.

Para possibilitar o uso desta tabela, precisaríamos dar nomes às colunas:

![](https://practicum-content.s3.amazonaws.com/resources/5B270EB2-06EE-4D2C-A674-70F6E3D40878_1702290795.jpeg) _Tabela com nomes de colunas_

As listas de dicionários nos possibilitam fazer a substituição dos números das colunas por seus nomes (chaves de dicionário). Abaixo, mostramos como os dados dos filmes aparecem em uma lista de dicionários. Examine o código abaixo e o execute:

CódigoPYTHON

9

1

2

3

4

5

6

7

8

movies\_table \= \[

{'movie\_name':'The Shawshank Redemption', 'country':'USA', 'genre':'drama', 'year':1994, 'duration':142, 'rating':9.111},

{'movie\_name':'The Godfather', 'country':'USA', 'genre':'drama, crime', 'year':1972, 'duration':175, 'rating':8.730},

{'movie\_name':'The Dark Knight', 'country':'USA', 'genre':'fantasy, action, thriller', 'year':2008, 'duration':152, 'rating':8.499}

\]

  

\# agora acesse a coluna pelo nome:

print(movies\_table\[2\]\['movie\_name'\])

Mostrar a soluçãoExecutar

As chaves do dicionário facilitam o acesso às colunas necessárias.

Conhecer diferentes estruturas de dados facilita o trabalho de qualquer analista. Esse domínio nos permite selecionar a estrutura ideal para qualquer conjunto de dados. Quando a ordem das colunas é menos informativa do que seus nomes, faz mais sentido usar uma estrutura com nomes.

As listas de dicionários combinam as vantagens das duas estruturas:

-   Dos dicionários — nomes
-   Das listas — a possibilidade de iterar sobre itens ordenados

Agora vamos praticar um pouco.

Pergunta

Aqui está a tabela `movies_table` que acabamos de ver no exemplo:

```
movies_table = [
    {'movie_name':'The Shawshank Redemption', 'country':'USA', 'genre':'drama', 'year':1994, 'duration':142, 'rating':9.111},
    {'movie_name':'The Godfather', 'country':'USA', 'genre':'drama, crime', 'year':1972, 'duration':175, 'rating':8.730},
    {'movie_name':'The Dark Knight', 'country':'USA', 'genre':'fantasy, action, thriller', 'year':2008, 'duration':152, 'rating':8.499}
]
```

Qual código não vai resultar em um erro após ser executado? Selecione a opção correta.

`print(movies_table[-1]['movie_name'])`

Sim, esse código não vai resultar em um erro. Ele vai retornar o nome do filme da última linha da tabela.

`print(movies_table['movie_name'][2])`

`print(movies_table[0]['wordlwide_gross'])`

Muito bem!

Legal! A seguir, vamos ver o que podemos fazer com uma lista de dicionários.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-30-49-932Z.md
### Última modificação: 2025-05-28 19:30:50

# Dicionários: processamento de listas de dicionários - TripleTen

Capítulo 3/5

Python em mais detalhes

# Dicionários: processamento de listas de dicionários

Agora sabemos sobre duas estruturas de dados aninhadas que usam dicionários:

-   Dicionários de listas. Aqui está um exemplo:
    
    ```
      bus_schedule = {
          '72': ['8:00', '12:00', '17:30'],
          '26': ['9:30', '15:00'],
          '17': ['7:30', '12:30', '15:30']
      }
      
    ```
    
-   Listas de dicionários, ótimas para representar tabelas.
    

Agora vamos dar uma olhada em um exemplo em que listas de dicionários são úteis.

Imagine que você está pedindo uma pizza e bebidas em um aplicativo móvel. Os dados do pedido são enviados para o servidor na forma de uma lista de dicionários em que:

-   Cada dicionário é um item no pedido.
-   Cada chave do dicionário é um parâmetro do pedido.

Examine a lista de dicionários `order` abaixo para ver como os dados do pedido são enviados:

```
order = [
    {
        'item': 'Margherita pizza',
        'category': 'pizza',
        'quantity': 2,
        'price': 9
    },
    {
        'item': 'Ham pizza',
        'category': 'pizza',
        'quantity': 1,
        'price': 12
    },
    {
        'item': 'Pepsi 1 l',
        'category': 'beverages',
        'quantity': 3,
        'price': 2
    }
]
```

Imagine que queremos saber o total do pedido. Para fazer isso, precisamos somar os valores numéricos dos dicionários. Sabemos que:

-   A lista permite que você percorra todos os itens do pedido utilizando um ciclo.
-   Os dicionários dão acesso ao preço e à quantidade de cada item.

Nosso objetivo é multiplicar a quantidade e o preço para cada item selecionado. O código abaixo faz exatamente isso. Examine-o e então o execute para ver o resultado. Preste atenção nos comentários que deixamos no código. Eles explicam o que está acontecendo.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

order \= \[

{

'item': 'Margherita pizza',

'category': 'pizza',

'quantity': 2,

'price': 9

},

{

'item': 'Ham pizza',

'category': 'pizza',

'quantity': 1,

'price': 12

},

{

'item': 'Pepsi 1 l',

'category': 'beverages',

'quantity': 3,

'price': 2

}

\]

  

\# variável para o preço total do pedido

total\_price \= 0

  

\# itera sobre cada dicionário da lista

for item in order:

\# na variável, soma o preço do item multiplicado pela quantidade

total\_price += item\['price'\] \* item\['quantity'\]

  

print(total\_price)

Mostrar a soluçãoExecutar

Como resultado, temos `36` como o total, mas e se quisermos calcular o total de uma categoria específica? Por exemplo, apenas as pizzas?

Quando trabalhamos com instruções condicionais e com a lista de listas aninhadas no primeiro sprint, aprendemos sobre filtros. Para aplicar um filtro, percorremos os itens de uma lista (por exemplo, `para movie em movies_info:`), verificamos uma condição (por exemplo, `if movie[4] > 180:`) e executamos uma ação se a condição retornou `True`. Aqui está o código completo para um filtro aplicado à lista de listas aninhadas:

```
movies_info = [
    ['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111],
    ['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730],
    ['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499],
    ["Schindler's List", 'USA', 1993, 'drama', 195, 8.818],
    ['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625],
    ['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619],
    ['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521],
    ['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644],
    ['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106],
    ['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077]
]

movies_filtered = [] # lista vazia para armazenar o resultado

for movie in movies_info: # iterando sobre as linhas da tabela original
    if movie[4] > 180: # se um filme dura mais do que 180 min
        movies_filtered.append(movie) # adiciona a linha à lista movie_filtered

for movie in movies_filtered: # imprimindo o conteúdo da lista movies_filtered
    print(movie) 
```

Agora vamos filtrar, por categoria, os itens em um dicionário aninhado. Encontre todas as pizzas!

-   O ciclo vai percorrer os itens da lista de dicionários e obter o valor para a chave `'category'`.
-   Se o item pertencer à categoria "pizza", o ciclo vai adicionar o dicionário inteiro à nova lista.

Examine o código abaixo e o execute para ver que resultado ele produz.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

order \= \[

{

'item': 'Margherita pizza',

'category': 'pizza',

'quantity': 2,

'price': 9

},

{

'item': 'Ham pizza',

'category': 'pizza',

'quantity': 1,

'price': 12

},

{

'item': 'Pepsi 1 l',

'category': 'beverages',

'quantity': 3,

'price': 2

}

\]

  

\# variável para armazenar o resultado

filtered\_order \= \[\]

  

for item in order: \# itera sobre cada dicionário da lista

if item\['category'\] \== 'pizza': \# se a categoria for pizza...

filtered\_order.append(item) \# adiciona o dicionário à lista filtered\_order

  

\# imprime a lista de dicionários filtrada

Mostrar a soluçãoExecutar

Agora temos uma lista de dicionários filtrada. Não é ótimo?

Agora é sua vez de brilhar! Confira a tarefa abaixo.

Vamos voltar para o exemplo do dicionário `order`. Encontre o preço total para todas as pizzas em `order` e o imprima. Use a mesma abordagem que usamos nos exemplos desta lição. Se tiver dificuldades, confira a dica.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

order \= \[

{

'item': 'Margherita pizza',

'category': 'pizza',

'quantity': 2,

'comment': 'Add extra cheese please!',

'price': 9 \# preço por item

},

{

'item': 'Ham pizza',

'category': 'pizza',

'quantity': 1,

'comment': '',

'price': 12

},

{

'item': 'Pepsi 1 l',

'category': 'beverages',

'quantity': 3,

'comment': '',

'price': 2

},

{

'item': 'Apple juice 0.5 l',

'category': 'beverages',

'quantity': 1,

'comment': '',

'price': 1

},

{

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-30-51-245Z.md
### Última modificação: 2025-05-28 19:30:51

# Funções: parâmetros e valores padrão - TripleTen

Capítulo 3/5

Python em mais detalhes

# Funções: parâmetros e valores padrão

Ok, você acabou de aprender como percorrer elementos de dicionários, criar estruturas aninhadas com dicionários e processá-las. É hora de se aprofundar nas funções e aprender sobre as técnicas mais empolgantes e poderosas que elas desbloqueiam.

No segundo sprint, você escreveu uma função de filtragem e imprimiu os resultados. Vamos recapitular e dar mais uma olhada nas funções que você escreveu:

```
# função que extrai o ano e o compara
def filter_by_year(data, year):
    filtered_result = []
    for row in data:
        if row[2] > year:
            filtered_result.append(row)
    return filtered_result

# função para imprimir apenas o nome do filme
def print_movie_info(data):
    for movie in data:
        print(movie)
```

A função `filter_by_year` espera dois parâmetros: `data` e `year`. No corpo da função, criamos uma lista vazia chamada `filtered_result`. O ciclo `for` percorre as listas aninhadas de filmes, e se o ano de um filme for maior do que o parâmetro `year`, toda a lista aninhada é adicionada a `filtered_result`. Em seguida, usamos a palavra-chave `return` para retornar os resultados da filtragem.

Agora é hora de ver como você pode usar valores padrão de parâmetros para melhorar o desempenho de suas funções. Lembre-se de que parâmetros são o que passamos entre os parênteses de uma função quando a criamos. Já mencionamos que a função `filter_by_year` tem dois parâmetros: `data` e `year`.

Começando com valores padrão de parâmetros, vamos tentar executar novamente a função da omelete que criamos em um dos sprints anteriores. Como sempre, primeiro examine o código e depois o execute.

CódigoPYTHON

9

1

2

3

4

5

def omelet(eggs\_number\= 0):

result \= 'A omelete está pronta! Ovos usados: ' + str(eggs\_number)

return result

  

print(omelet())

Mostrar a soluçãoExecutar

Erro! Podemos ver aqui que deveríamos ter passado um argumento para `eggs_number`. Sem ele, a função não pode converter `eggs_number` em uma string quando ela chama `str()`.

Se não quisermos que isso aconteça, precisamos fornecer um valor padrão para o parâmetro ao definir a função. Ao final desta lição, você vai conseguir fazer isso.

## Valores padrão de parâmetros

Assista ao vídeo abaixo, mas não se esqueça de continuar lendo para obter uma compreensão sólida deste tópico.

Vídeo

<iframe class="base-markdown-iframe__iframe" id="player-XKVGACnesyo" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Parameters and default values" width="640" height="360" src="https://www.youtube.com/embed/XKVGACnesyo?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Feaec5c5d-65c8-47be-be77-c0e247aaa29e%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

Imagine que você está em um café e pede uma omelete. Você acha que o garçom vai ficar confuso se você não especificar quantos ovos quer na sua omelete? O mais provável é que ele prepare seu pedido com um número "padrão" de ovos. Isso é o mesmo que um **parâmetro padrão**.

A seguir vamos mostrar como podemos adicionar um valor padrão a este parâmetro em nosso código:

CódigoPYTHON

9

1

2

3

4

5

6

\# adiciona um valor padrão ao parâmetro eggs\_number

def omelet(eggs\_number\=2):

result \= 'A omelete está pronta! Ovos usados: ' + str(eggs\_number)

return result

  

print(omelet())

Mostrar a soluçãoExecutar

Ao declarar uma função, você pode definir um valor padrão para qualquer parâmetro utilizando `=`. Neste exemplo, escrevemos `eggs_number=2`.

Parâmetros com valores padrão também são chamados de **parâmetros opcionais**. Se você não especificar o valor ao chamar a função, os valores padrão serão utilizados.

### Importante!

Ao definir uma função, parâmetros opcionais devem ser colocados após os obrigatórios, ou seja, aqueles que não têm valores padrão atribuídos a eles. Caso contrário, você vai obter um erro quando chamar a função.

Vamos ver como deve ficar no exemplo abaixo, em que há dois parâmetros:

-   `cheese`, que é um parâmetro obrigatório.
-   `eggs_number`, que é um parâmetro opcional. Ele tem um valor padrão 2.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

\# adiciona um valor padrão ao parâmetro eggs\_number

def omelet(cheese, eggs\_number\=2):

result \= 'A omelete está pronta! Ovos usados: ' + str(eggs\_number)

if cheese \== True:

result \= result + ', com queijo'

else:

result \= result + ', sem queijo'

return result

  

print(omelet(False))

Mostrar a soluçãoExecutar

Você sempre vai precisar dizer se quer queijo ou não, já que esse é um parâmetro obrigatório, mas se não especificar quantos ovos quer, receberá dois!

Agora é hora de praticar!

Vamos voltar para o exemplo de filmes. Agora seu objetivo é escrever uma função, `filter_by_genre()`, com dois parâmetros:

-   `genre=` — o nome do gênero, definido como `'drama'` por padrão
-   `data=` — dados sobre filmes, sem valor padrão

A função deve retornar uma lista de listas que contenha apenas os filmes do gênero passado no parâmetro `genre=`.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

\# essa função imprime tabelas filtradas. Não a modifique

def print\_movie\_info(data):

for movie in data:

print(movie)

  

  

\# defina sua função filter\_by\_genre() aqui

def filter\_by\_genre(data,genre\='drama'):

l \= \[\]

for i in data:

if genre in i\[3\]:

l.append(i)

return l

  

  

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

Dica

Mostrar a soluçãoValidar

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-30-52-558Z.md
### Última modificação: 2025-05-28 19:30:52

# Funções: retorno de valores - TripleTen

Capítulo 3/5

Python em mais detalhes

# Funções: retorno de valores

Agora sabemos sobre os parâmetros obrigatórios e opcionais (ou padrão) de uma função. Lembre-se de que parâmetros são o que passamos entre os parênteses de uma função quando a criamos. Parâmetros obrigatórios devem ser passados para que a função possa ser executada. Já os parâmetros opcionais, caso não sejam passados, serão substituídos por valores padrão.

## Como retornar resultados

Após aprendermos sobre argumentos posicionais e nomeados, vamos ver como podemos melhorar o uso da palavra-chave `return`.

Ao final desta lição, você será capaz de usar a palavra-chave `return` para atribuir o resultado de uma função com um ou mais valores para uma ou mais variáveis.

Assista ao vídeo abaixo, mas não se esqueça de continuar lendo para obter uma compreensão sólida deste tópico.

Vídeo

<iframe class="base-markdown-iframe__iframe" id="player-bh7AqRgLCW0" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Function Results" width="640" height="360" src="https://www.youtube.com/embed/bh7AqRgLCW0?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F7581bb25-83f3-4938-8017-c6d5109b4f0a%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Fhome%2Fdata-analyst%2F&amp;vf=1"></iframe>

## Término de uma função

A palavra-chave `return` pode aparecer mais de uma vez dentro do corpo de uma função, mas apenas a primeira chamada será executada. Assim que o Python vê essa palavra-chave, ele encerra a execução da função e retorna o resultado.

Aqui estamos nós, quebrando ovos novamente. Imagine se tentássemos fazer uma omelete sem ovos – isso seria bem estranho. Então precisamos fazer um teste para esse caso e retornar uma mensagem diferente.

Podemos usar palavras-chave `return` diferentes para isso. Vamos dar uma olhada:

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

def omelet(eggs\_number\=2):

if eggs\_number \> 0:

return 'A omelete está pronta! Ovos usados: ' + str(eggs\_number)

else:

return 'Como fazer uma omelete sem ovos?'

print('Eu nunca serei impressa :(')

  

  

print(omelet(3))

print(omelet(0))

Mostrar a soluçãoExecutar

A função `print` após a instrução `if` nunca será executada, porque o código será encerrado dentro do bloco `if` ou dentro do bloco `else`.

O uso de `return` em várias partes de uma função é uma prática comum que você vai encontrar durante seu trabalho. É importante garantir que qualquer código escrito após um bloco que contenha uma instrução `return` ainda seja executado.

---

## Múltiplos resultados

O que precisamos fazer se quisermos que uma função retorne mais do que um valor? Vamos escrever uma função que vai pegar dois lados de um retângulo e encontrar tanto a área quanto o perímetro da figura:

```
def area_and_perim(side_1, side_2):
    area = side_1 * side_2
    perimeter = 2 * (side_1 + side_2)
    return area, perimeter

print(area_and_perim(2, 3))
```

```
(6, 10)
```

Neste caso, a palavra-chave `return` é seguida por duas variáveis, fazendo com que a função retorne os dois valores. Você pode retornar qualquer tipo de valores se quiser. A única coisa para se lembrar é que, ao usar de várias variáveis após a palavra-chave `return`, essas devem ser separadas por vírgulas.

Tecnicamente, uma função sempre retorna um valor. Quando enumeramos várias variáveis após `return`, a função as reúne em uma única estrutura chamada `tupla`. Dali podemos desagrupá-las atribuindo várias variáveis ao resultado da função (neste caso, duas variáveis):

CódigoPYTHON

9

1

2

3

4

5

6

7

8

9

def find\_area\_and\_perim(side1, side2):

area \= side1 \* side2

perimeter \= 2 \* (side1 + side2)

return area, perimeter

  

\# desagrupa o resultado da função

rec\_area, rec\_area \= (find\_area\_and\_perim(7, 3))

  

print(f'A área do retângulo é {rec\_area}, o perímetro é {rec\_area}')

Mostrar a soluçãoExecutar

Aqui, atribuímos o resultado da função às variáveis globais `rec_area` e `rec_perimeter`. Observe que a ordem é importante. A variável global `rec_area` aparece primeiro, então ela recebe o primeiro valor que aparece após `return`, ou seja, o valor da variável local `area`.

Observe que evitamos dar o mesmo nome às variáveis locais e globais da área e do perímetro. Dar o mesmo nome a elas não vai gerar nenhum erro (tente você mesmo editando o código), mas isso não é uma prática recomendada, uma vez que dificulta a leitura do código e a depuração em caso de erro.

É hora de praticar um pouco!

Pergunta

Em funções Python, é possível retornar:

Um único valor

Não mais do que dois valores

Qualquer número de valores

Correto!

Trabalho maravilhoso!

E mais um, por favor!

Pergunta

Quando você quer que uma função retorne múltiplos resultados, você precisa listá-los após a palavra-chave `return` e separá-los com:

Ponto final

Ponto e vírgula

Vírgula

Exatamente! Por exemplo: `return 'a', 'lição', 'final'`.

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-30-54-644Z.md
### Última modificação: 2025-05-28 19:30:55

# Pandas: o objeto Series - TripleTen

Capítulo 3/5

Python em mais detalhes

# Pandas: o objeto Series

No segundo sprint, aprendemos como usar os métodos `count()`, `sum()` e `mean()` para que eles façam cálculos em nossos dados de um jeito simples e direto. Também aprendemos como criar e manipular DataFrames em Pandas para obter resultados incríveis de manipulação e análise de dados.

Nessa lição, apresentaremos uma nova estrutura de dados: o **objeto Series**. Um objeto Series é essencialmente uma coluna de um DataFrame. Por exemplo, quando filtramos um DataFrame e extraímos uma coluna que nos interessa, essa coluna se torna um objeto Series.

É importante saber diferenciar um DataFrame e um objeto Series, porque existem métodos e atributos que funcionam exclusivamente com um ou com outro. E alguns métodos, embora funcionem com ambos os tipos de objetos, se comportam de maneira diferente com um DataFrame ou com um objeto Series.

Ao final dessa lição, você será capaz de distinguir DataFrames de objetos Series e usar indexação com um objeto Series. A indexação é uma habilidade crucial para acessar os elementos de objeto Series, geralmente necessária para processar dados.

Vamos recapitular o que já sabemos até agora. Quando você obtém várias colunas ou linhas de uma tabela, obtém uma nova tabela, que é um DataFrame. Você pode verificar o tipo dela usando a função `type`. Vamos dar uma olhada:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

part_df = df[['genre', 'Artist']]
print(type(part_df))
```

```
<class 'pandas.core.frame.DataFrame'>
```

A tabela original tem o mesmo tipo. Veja isto:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')
print(type(df))
```

```
<class 'pandas.core.frame.DataFrame'>
```

Como você pode ver, elas de fato têm o mesmo tipo.

Agora vamos supor que você quer recuperar apenas uma coluna. Você pode fazer isso facilmente passando o nome da coluna entre colchetes. Isso retornará um **objeto Series**. Dê uma olhada:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

part_df = df['Artist']
print(type(part_df))
```

```
<class 'pandas.core.series.Series'>
```

Extraímos a coluna `Artist` e a obtemos como um objeto Series.

Os objetos Series são os blocos que compõem uma tabela. Vejamos como eles são estruturados.

## DataFrames e Series

![](https://practicum-content.s3.amazonaws.com/resources/Tema_5_structure_table_v1_1549308959_1549907576_1702364399.png)

Cada coluna em um DataFrame é um objeto Series. Como vimos, podemos obter objetos Series separados a partir de um DataFrame.

Os dados em um DataFrame são acessados usando duas coordenadas (nome da coluna e índice). Por outro lado, dados de tipo Series são acessados usando apenas valores de índice. Portanto, nos referimos a um objeto Series como uma estrutura unidimensional.

Além disso, objetos Series possuem um atributo chamado `name`:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

part_df = df['Artist']
print(part_df.name)
```

```
Artist
```

Quando recuperamos uma coluna de um DataFrame como um objeto Series, o atributo `name` assume o nome da coluna, como você pode ver acima. Além disso, quando você cria um DataFrame a partir de objetos Series individuais, o nome do objeto Series torna-se o nome da coluna.

Além do nome, o objeto Series tem determinado tamanho, ou seja, o número total de células. Ele pode ser acessado por meio do atributo `size`:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

part_df = df['Artist']
print(part_df.size)
```

```
67963
```

Esse objeto Series contém 67.963 elementos, ou células.

Claro, objetos Series possuem outros atributos, mas `name` e `size` são os que mais usamos.

## Indexação no objeto Series

A indexação de objetos Series é semelhante à indexação de uma lista. Para extrair um valor de uma célula com base em seu índice, basta passar o índice entre colchetes:

```
import pandas as pd
df = pd.read_csv('/datasets/music_log_chpt_11.csv')

# obtendo um objeto Series de um DataFrame
artist = df['Artist']

# obtendo uma célula de um objeto Series com apenas uma coordenada
print(artist[0])
```

```
Marina Rei
```

Como você pode ver, extraímos o primeiro elemento do objeto Series lidando com o índice de número 0. Isso significa que, assim como em listas, a indexação em objetos Series também começa em 0.

A indexação em objetos Series é parecida com a indexação em DataFrames no sentido que podemos usar tanto o atributo `loc` quando a notação abreviada. Confira uma lista completa na tabela abaixo:

**Tipo**

**Notação completa**

**Notação abreviada**

Um elemento

`total_play.loc[7]`

`total_play[7]`

Vários elementos

`total_play.loc[[5, 7, 10]]`

`total_play[[5, 7, 10]]`

Múltiplos elementos consecutivos (fatia)

`total_play.loc[5:10]` incluindo 10

`total_play[5:10]` não incluindo 10

Todos os elementos, começando pelo elemento determinado

`total_play.loc[1:]`

`total_play[1:]`

Todos os elementos até o elemento escolhido

`total_play.loc[:3]` incluindo 3

`total_play[:3]` não incluindo 3

Vamos agora praticar a indexação de um objeto Series.

Extraia a coluna `'track'` do DataFrame original e armazene-a na variável `tracks`. Esse será seu objeto Series. Em seguida, obtenha os primeiros 20 elementos desse objeto Series e armazene-os na variável `top20`. Imprima essa variável.

CódigoPYTHON

9

1

2

3

4

5

6

7

8

9

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

  

tracks \= df\['track'\]\# escreva seu código aqui

top20 \= tracks\[:20\]\# escreva seu código aqui

  

print(top20)

  

Dica

Mostrar a soluçãoValidar

### Filtragem de um objeto Series

A indexação lógica também funciona para um objeto Series e é mais simples que o equivalente do DataFrame.Para um objeto Series, basta uma condição lógica, não sendo necessário indicar a coluna de onde vêm os dados. Aqui está um exemplo de como verificamos se os valores em `'total play'` são menores que 20 usando a notação abreviada:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

total_play = df['total play']
lower_20 = total_play < 20

print(lower_20)
```

```
0        False
1        False
2        False
3         True
4        False
         ...  
67958    False
67959    False
67960    False
67961     True
67962     True
Name: total play, Length: 67963, dtype: bool
```

O resultado exibe o índice de cada música no objeto Series com o valor booleano que indica que a música foi reproduzida por menos de 20 segundos com o valor `True` – e com o `False` se ela foi reproduzida por mais de 20 segundos.

Se você quiser analisar apenas uma coluna de uma tabela, pode ser uma boa ideia armazenar a coluna em uma variável separada (como fizemos com a variável `total_play`). Dessa forma, você não vai precisar indicar o nome da coluna cada vez que quiser analisá-la.

É interessante que agora podemos usar esse novo objeto Series com valores booleanos para filtrar o DataFrame original. Veja como você pode fazer isso:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

total_play = df['total play']
lower_20 = total_play < 20

df_lower20_only = df[lower_20]

print(df_lower20_only)
```

```
                user_id  total play                                  Artist   
3      EF15C7BA    8.966000                                     NaN  \
5      4166D680    3.007000  Henry Hall & His Gleneagles Hotel Band   
6       F4F5677    0.100000                                     NaN   
8      A5E0D927    3.161000                   Andrew Paul Woodworth   
10     D3DD8D00    8.836000                          Steve Campbell   
...         ...         ...                                     ...   
67951  A6E13637    1.318000                             Julien Mier   
67953  A06381D8    2.502000                             Flip Grater   
67956  816FBC10    2.000000                                   89ers   
67961  DB0038A8   11.529112                            Less Chapell   
67962  FE8684F6    0.100000                                     NaN   

              genre                                 track  
3             dance                   Loving Every Minute  
5              jazz                                  Home  
6      classicmetal                                   NaN  
8               pop  The Name of This Next Song Is Called  
10             jazz                           Morning Dew  
...             ...                                   ...  
67951         dance                                Nearby  
67953          folk                          My Old Shoes  
67956         dance                              Go Go Go  
67961           pop                                  Home  
67962           NaN                                   NaN  

[33868 rows x 5 columns]
```

O resultado é uma tabela filtrada contendo apenas as linhas com valores na coluna `'total play'` inferiores a 20. E essas são 33.868 do total de 67.963 músicas que temos no conjunto de dados. Isso significa que cerca de metade de nossas músicas são quase intocadas.

Como alternativa, você pode filtrar o objeto Series e obter apenas os valores que atendem aos seus critérios. Por exemplo, se temos o objeto Series com valores booleanos, podemos usar esse objeto para filtrar o objeto Series `total_play`:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

total_play = df['total play']
lower_20 = total_play < 20

series_lower20_only = total_play[lower_20]

print(series_lower20_only)
```

```
3         8.966000
5         3.007000
6         0.100000
8         3.161000
10        8.836000
           ...    
67951     1.318000
67953     2.502000
67956     2.000000
67961    11.529112
67962     0.100000
Name: total play, Length: 33868, dtype: float64
```

Com esse processo de filtragem, obtemos os valores que atendem aos nossos critérios. Em outras palavras, obtemos um objeto Series em que todos os valores são menores que 20.

Agora, vamos praticar a filtragem de um objeto Series.

Seu objetivo é filtrar o DataFrame original extraindo apenas as músicas pop. Como fazer isso?

Primeiro, extraia a coluna `'genre'` do DataFrame original e armazene-a na variável `genre`. Isso criará um objeto Series. Em seguida, verifique se os valores no objeto Series são iguais a `'pop'`.

Essa verificação de igualdade é um pouco contraintuitiva, então vamos ver um exemplo complementar em que precisamos verificar se os valores dos nomes das músicas no objeto Series são iguais a `'Andrew Paul Woodworth'`. É assim que fazemos isso:

```
import pandas as pd

df = pd.read_csv('/datasets/music_log_chpt_11.csv')

tracks = df['track']
track_check = tracks == 'Andrew Paul Woodworth'
```

Esperamos que esse exemplo tenha sido bom o suficiente para ilustrar como verificamos a igualdade. Agora vamos voltar para a tarefa.

Em primeiro lugar, extraia a coluna `'genre'` do DataFrame original e armazena-a na variável `genre`.

Em seguida, com base no exemplo acima, verifique se os valores no objeto Series são iguais a `'pop'`. Armazene o resultado dessa verificação na variável `pop_genre_check`.

Por fim, use `pop_genre_check` para filtrar o DataFrame `df` original para incluir apenas músicas pop. Armazene esse DataFrame filtrado na variável `pop_df` e o imprima.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

import pandas as pd

  

df \= pd.read\_csv('/datasets/music\_log\_chpt\_11.csv')

  

genre \= df\['genre'\]

pop\_genre\_check \= genre \== 'pop'\# escreva seu código aqui

pop\_df \= df\[pop\_genre\_check\]\# escreva seu código aqui

  

print(pop\_df)

  

Dica

Mostrar a soluçãoValidar

E agora você é mestre em Pandas e tem todas as ferramentas necessárias para domar essa fera!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-30-57-257Z.md
### Última modificação: 2025-05-28 19:30:57

# Conclusão - TripleTen

Capítulo 3/5

Python em mais detalhes

# Conclusão

Parabéns por chegar ao ponto culminante da sua jornada de Python em mais detalhes! Seu compromisso e entusiasmo ajudaram você a passar por uma experiência de aprendizagem notável, e estamos felizes em celebrar seus resultados. Ao refletir sobre seu progresso, orgulhe-se das diversas habilidades que você desenvolveu ao longo do ano.

Vamos recapitular os conhecimentos incríveis que você adquiriu:

-   **Uso de ciclos em dicionários:** você dominou a arte de percorrer os elementos de um dicionário; essa é uma habilidade crucial para manipular dados de forma eficiente.
-   **Estruturas de dados aninhadas com dicionários:** você explorou a versatilidade dos dicionários aninhados, que permitem organizar e gerenciar estruturas de dados complexas sem problemas.
-   **Processamento de listas de dicionários:** você adquiriu proficiência na manipulação e extração de informações valiosas de listas contendo dicionários.
-   **Parâmetros e valores padrão:** você alcançou uma compreensão profunda dos parâmetros das funções, inclusive de como usar valores padrão de forma estratégica para aumentar a flexibilidade das funções.
-   **Retorno de valores:** você aumentou a versatilidade de suas funções adicionando a seu código os valores de retorno, que permitem integrar o código em programas maiores sem problemas.
-   **O objeto Series:** você aprendeu sobre o objeto Series da biblioteca Pandas, que é uma ferramenta poderosa para manipular e analisar dados de forma eficiente.
-   **Estatística descritiva:** você dominou a arte de resumir e interpretar dados usando a estatística descritiva, que é uma habilidade crucial na área de análise de dados.

Esse conjunto de habilidades abrangente faz com que você esteja com tudo que precisa para passar para seu primeiro projeto integrado. Seu compromisso permitiu que você adquirisse as ferramentas necessárias para lidar com desafios complexos, analisar dados de forma eficiente e criar soluções que tenham um impacto significativo.

Estamos muito empolgados com o início da próxima etapa da sua jornada, na qual você vai poder aplicar essas habilidades em um cenário do mundo real. Lembre-se: todo desafio é uma oportunidade de crescer, e sua capacidade de lidar com as complexidades de Python e da ciência de dados coloca você no caminho para o sucesso contínuo.

Um brinde a suas conquistas e aos projetos incríveis que estão à frente! Sua jornada não acaba aqui: ela evolui para um novo capítulo cheio de possibilidades ilimitadas. E esse capítulo começa com seu primeiro projeto integrado, no qual você vai aplicar todas as ferramentas que aprendeu até agora.

Você tem tudo para começar, então vamos lá!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-30-59-586Z.md
### Última modificação: 2025-05-28 19:31:00

# Sprint 6 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 4/5

Projeto integrado

# Sprint 6 - Projeto

Parabéns! Você concluiu oficialmente a primeira parte do programa na plataforma interativa. Agora é hora de reunir tudo o que você aprendeu até agora em seu primeiro projeto integrado, um estudo de caso analítico da vida real.

Este projeto vai abranger todas as habilidades que você desenvolveu até este momento:

![](https://practicum-content.s3.amazonaws.com/resources/Python_1713354953.png)

![](https://practicum-content.s3.amazonaws.com/resources/Pre-processamento_de_Dados_1713354962.png)

![](https://practicum-content.s3.amazonaws.com/resources/Analise_Exploratoria_1713355043.png)

![](https://practicum-content.s3.amazonaws.com/resources/Analise_Estatistica_1713355063.png)

Quando terminar o projeto, envie seu trabalho para ser avaliado na revisão. Você vai receber feedback dentro de 24 horas. Use o feedback para fazer alterações e, em seguida, envie a nova versão de volta ao revisor do projeto.

Você talvez receba mais feedback sobre a nova versão. Isso é completamente normal. Não é incomum passar por vários ciclos de feedback e revisão.

Seu projeto será considerado concluído assim que o revisor do projeto o aprovar.

## Descrição do projeto

Você trabalha para a loja online Ice, que vende videogames no mundo todo. As avaliações de usuários e especialistas, gêneros, plataformas (por exemplo, Xbox ou PlayStation) e dados históricos sobre vendas de jogos estão disponíveis em fontes abertas. Você precisa identificar padrões que determinam se um jogo tem sucesso ou não. Isso vai permitir que você identifique possíveis sucessos e planeje campanhas publicitárias.

Os dados disponibilizados remontam a 2016. Vamos imaginar que estamos em dezembro de 2016 e você está planejando uma campanha para 2017.

O importante é ganhar experiência trabalhando com dados. Não importa se você está prevendo as vendas de 2017 com base nos dados de 2016 ou as vendas de 2027 com base nos dados de 2026.

O conjunto de dados contém uma coluna de "rating" (classificação) que armazena a classificação ESRB de cada jogo. O Entertainment Software Rating Board avalia o conteúdo de um jogo e atribui uma classificação etária, como Teen (Adolescente) ou Mature (Adulto).

### Instruções para concluir o projeto

**Etapa 1. Abra o arquivo de dados e estude as informações gerais**

Caminho do arquivo:

_/datasets/games.csv_ . [Download dataset](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/games.csv)

**Etapa 2. Prepare os dados**

-   Substitua os nomes das colunas (transforme tudo em minúsculos).
-   Converta os dados para os tipos necessários.
-   Descreva as colunas onde os tipos de dados foram alterados e explique o motivo.
-   Se necessário, decida como lidar com valores ausentes: 1. Explique sua abordagem ao preencher valores ausentes ou ao deixá-los em branco. 2. Na sua opinião, por que os valores estão ausentes? Dê possíveis razões. 3. Preste atenção à abreviação TBD (que significa "to be determined", ou "a ser determinado"). Especifique como pretende lidar com esses casos.
-   Calcule o total de vendas (a soma das vendas em todas as regiões) para cada jogo e coloque esses valores em uma coluna separada.

**Etapa 3. Analise os dados**

-   Veja quantos jogos foram lançados a cada ano. Os dados de cada período são significativos?
-   Veja como as vendas variaram de plataforma para plataforma. Escolha as plataformas com as maiores vendas totais e construa uma distribuição com base nos dados para cada ano. Encontre as plataformas que costumavam ser populares, mas que agora não têm vendas. Quanto tempo leva para as novas plataformas aparecerem e as antigas desaparecerem?
-   Determine para qual período você deve pegar dados. Para fazer isso, olhe para suas respostas das perguntas anteriores. Os dados devem permitir que você construa um modelo para 2017.
-   Trabalhar apenas com os dados que você decidiu que são relevantes. Desconsidere os dados de anos anteriores.
-   Quais plataformas estão liderando em vendas? Quais estão crescendo ou diminuindo? Selecione várias plataformas potencialmente lucrativas.
-   Construa um diagrama de caixa para as vendas globais de todos os jogos, divididos por plataforma. As diferenças nas vendas são significativas? E quanto às vendas médias em várias plataformas? Descreva suas descobertas.
-   Veja como as avaliações de usuários e profissionais afetam as vendas de uma das plataformas populares (você escolhe). Construa um gráfico de dispersão e calcule a correlação entre avaliações e vendas.
-   Tendo suas conclusões em mente, compare as vendas dos mesmos jogos em outras plataformas.
-   Dê uma olhada na distribuição geral de jogos por gênero. O que podemos dizer sobre os gêneros mais lucrativos? Você pode generalizar sobre gêneros com vendas altas e baixas?

**Etapa 4. Crie um perfil de usuário para cada região**

Para cada região (AN, UE, JP), determine:

-   As cinco plataformas principais. Descreva as variações das suas quotas de mercado de região para região.
-   Os cinco principais gêneros. Explique a diferença.
-   As classificações do ESRB afetam as vendas em regiões individuais?

**Etapa 5. Teste as seguintes hipóteses:**

— As classificações médias dos usuários das plataformas Xbox One e PC são as mesmas.

— As classificações médias de usuários para os gêneros Action (ação) e Sports (esportes) são diferentes.

Defina por conta própria o valor do limiar _alfa_.

Explique:

— Como você formuoua as hipóteses alternativas e nulas.

— Qual o nível de significância que você escolheu para testar as hipóteses, e por quê.

**Etapa 6. Escreva uma conclusão geral**

**Formato:** conclua a tarefa no Jupyter Notebook. Insira o código nas células _code_ e textos de explicação nas células _markdown_. Aplique a formatação e adicione títulos.

### Descrição dos dados

—_Name_ (nome)

—_Platform_ (plataforma)

—_Year\_of\_Release_ (Ano de lançamento)

—_Genre_ (gênero)

—_NA\_sales_ (vendas norte-americanas em milhões de USD)

—_EU\_sales_ (vendas na Europa em milhões de USD)

—_JP\_sales_ (vendas no Japão em milhões de USD)

—_Other\_sales_ (vendas em outros países em milhões de USD)

—_Critic\_Score_ (Pontuação crítica) (máximo de 100)

—_User\_Score_ (Pontuação dos usuários) (máximo de 10)

—_Classificação_ (ESRB)

Os dados de 2016 podem estar incompletos.

## Como o meu projeto será avaliado?

Leia atentamente estes critérios de avaliação do projeto antes de começar a trabalhar.

Veja o que os revisores vão analisar ao avaliar seu projeto:

-   Como você descreve os problemas que identifica nos dados?
-   Como você prepara um conjunto de dados para análise?
-   Como você constrói gráficos de distribuição e como você os explica?
-   Como você calcula o desvio padrão e a variância?
-   Você formula hipóteses alternativas e nulas?
-   Que métodos você aplica ao testá-los?
-   Você explica os resultados dos testes de hipóteses?
-   Você segue a estrutura do projeto e mantém seu código limpo e compreensível?
-   Quais conclusões você tira?
-   Você deixou comentários claros e relevantes a cada etapa?

Tudo o que você precisa para concluir esse projeto está nas folhas de conclusões e resumos de capítulos anteriores. Boa sorte!

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-cd7e4017-1339-4e04-9b47-ddc13e6356c7.containerhub.tripleten-services.com/doc/tree/8b11ffbc-90a5-43bb-98d8-9bc190025f8d.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-31-05-322Z.md
### Última modificação: 2025-05-28 19:31:05

# Feedback do Sprint 6 - TripleTen

Capítulo 4/5

Projeto integrado

# Compartilhe sua opinião

**Olá!**

Você acabou de concluir um sprint e enviar seu projeto. Parabéns por alcançar este marco! 🚀

Agora é o momento perfeito para dar seu feedback sobre o sprint. Sua opinião vai nos ajudar a aprimorar ainda mais o processo de aprendizagem.

Este questionário curto vai levar apenas 2 minutos. Todas as respostas vão ser confidenciais: não vamos compartilhá-las de forma que identifique você.

Agradecemos por nos ajudar a evoluir com você!

Como você avalia a experiência com os materiais didáticos, as tarefas e os projetos neste sprint?

1

2

3

4

5

Respostas: 1 – Muito insatisfatória; 5 – Muito satisfatória.

Avançar

1 — 7

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-31-06-620Z.md
### Última modificação: 2025-05-28 19:31:06

# Agora você é um Aspirante a Analista de Dados! - TripleTen

Capítulo 5/5

Agora você é um Aspirante a Analista de Dados!

# Agora você é um Aspirante a Analista de Dados!

💡 Parabéns por alcançar o novo marco em sua jornada de aprendizagem! Você construiu uma base sólida para suas habilidades de análise de dados. Verifique seu progresso em sua [barra de progresso pessoal](https://tripleten.com/profile/da/#skillset)!

Neste ponto do curso, você já sabe como lidar com tarefas típicas de um analista de dados – resolver problemas com dados, como valores ausentes e duplicados, realizar análise exploratória, testar hipóteses usando métodos estatísticos e criar relatórios visualmente impactantes com base nos resultados obtidos.

### Compartilhe sua nova medalha com o mundo

Não se esqueça da importância do networking! Vá em frente e compartilhe sua medalha "Aspirante a Analista de Dados" com a comunidade do LinkedIn!

Fique à vontade para copiar a medalha e o texto do bloco abaixo para o seu post (claro, você pode personalizar o texto como quiser)! Antes de fazer isso, não se esqueça de tornar seus repositórios no GitHub públicos. Marque [@TripleTenBrasil](https://www.linkedin.com/company/tripleten-brasil/)) para que possamos compartilhar o seu post.

Jonathas Martins da Rocha

14 semanas estudando e eu já consigo lidar com tarefas típicas de um analista de dados! Analisar comportamentos e hábitos do usuário? Identificar sucessos de mercado para uma loja online? Eu já consigo fazer isso! #TripleTen #TripleTenBrasil #python #pandas #eda #statisticalanalysis

![](https://practicum-content.s3.amazonaws.com/resources/Aspiring_Data_Analyst_PT_1704787228.png)

Copiar textoSalvar imagem[Criar um post no LinkdIn](https://www.linkedin.com/feed/)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-31-53-428Z.md
### Última modificação: 2025-05-28 19:31:53

# O que é uma sandbox? - TripleTen

Capítulo 1/2

Seu espaço pessoal de trabalho

Opcional

# O que é uma sandbox?

Vamos pegar empestado um termo da cibersegurança, uma sandbox é um ambiente seguro de programação onde os códigos podem ser executados sem afetar recursos de rede ou aplicações locais. Com ajuda da sandbox, você pode inspecionar um código suspeito para ter certeza que ele não causará nenhum mal.

Mas o que isso significa para nós? Nós acreditamos que é importante ter esse tipo de espaço seguro quando você está aprendendo como programar. Linguagens de programação são as coisas inteligentes mais burras que existem - elas farão qualquer coisa que você pedir, mas elas farão _somente_ o que você pedir para elas fazerem. Se você já viu sua parcela de mensagens de erro, ou ciclos infinitos, então você sabe dessa luta. Se você ainda não viveu essas experiências, você precisa viver.

Cometer erros é uma parte natural do processo de aprendizado, e nós queremos que você se sinta confortável com isso. Pode ser frustrante, mas é assim que realmente se aprende. Você precisa de muita experiência vendo como o seu código funciona (ou não) em diferentes casos. Para deixar essa experiência menos frustrante, é importante trabalhar em um ambiente que não irá te tratar mal pelos seus erros. Nas duas páginas seguintes, você encontrará nosso IDE onde você completará a maioria das tarefas das lições, e um caderno Jupyter aberto. Sinta-se livre para usar qualquer um, ou os dois, alternar livremente entre eles usando a barra de navegação e, acima de tudo, sinta-se livre para cometer erros. Cada erro, cada tentativa de depuração, é uma oportunidade de aprender.

Apenas lembre-se, esse espaço é totalmente e verdadeiramente livre. Não há regras, não há ninguém checando ou dando feedback, e as horas não são contadas, então venha quando quiser. Isso também significa, por outro lado, que não há suporte - se o seu código não funciona, tente resolver sozinho primeiro. Se isso não funcionar, tente escrever sobre o problema no Discord. As chances de um outro aluno ajudar você são grandes. Se tudo isso não resolver, você pode tentar postar sua pergunta em [StackOverflow](https://pt.stackoverflow.com) .

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-31-54-799Z.md
### Última modificação: 2025-05-28 19:31:55

# IDE - TripleTen

Teoria

# IDE

Sinta-se livre para usar este IDE para praticar suas habilidades de programação. Clicando no divisor vertical e passando tudo para o lado esquerdo, você dará ao IDE o máximo de espaço de tela.

Não se esqueça de que em um IDE como esse é importante usar `print()` senão nada será mostrado nos resultados da seção de resultados.

Se você quiser brincar com os conjuntos de dados do programa, você pode encontrar os caminhos dos arquivo checando a aba na parte de cima da página ou executando o seguinte código:

```
import pandas as pd
dataset_list = pd.read_csv('/datasets/datasets.csv')
print(dataset_list)
```

Lembre-se que, assim como neste exemplo, para acessar um determinado conjunto de dados você precisa incluir `/datasets/` no nome do caminho. Além disso, você pode fazer uso da indexação para selecionar mais facilmente um arquivo para trabalhar. Por exemplo:

```
print(dataset_list['file'][3])

path = '/datasets/' + dataset_list['file'][3]
df = pd.read_csv(path)
print(df)
```

IDE

Tarefa

9

1

2

3

\# Execute qualquer código que você queira

\# Você também pode importar bibliotecas

\# E sinta-se livre para acessar qualquer conjunto de dados com o qual você está familiarizado

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-31-56-128Z.md
### Última modificação: 2025-05-28 19:31:56

# Seu próprio notebook Jupyter - TripleTen

Seu próprio notebook Jupyter

Tarefa

Assim como o IDE na seção anterior, você pode usar o Jupyter para praticar programação ou trabalhar nos seus projetos preferidos.

Se você quer brincar com os conjuntos de dados do programa, você pode encontrar os caminhos dos arquivos executando o seguinte código:

```
import pandas as pd
dataset_list = pd.read_csv('/datasets/datasets.csv')
dataset_list
```

Lembre-se que, assim como neste exemplo, para acessar um determinado conjunto de dados você precisa incluir `/datasets/` no nome do caminho. Além disso, você pode fazer uso da indexação para selecionar mais facilmente um arquivo para trabalhar. Por exemplo:

```
print(dataset_list['file'][3])

path = '/datasets/' + dataset_list['file'][3]
df = pd.read_csv(path)
df
```

<iframe class="page-lesson-jupyter__frame" title="Jupyter" src="https://jupyterhub.tripleten-services.com/user/user-3-f213ea5c-94ca-4f29-9c46-4771fbdad1ea/notebooks/0d12fac3-d2ad-4b8b-b87b-22a045384fc3.ipynb?token=438dd6ef548a4bdfaebb2a75013f6e57"></iframe>

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-35-00-684Z.md
### Última modificação: 2025-05-28 19:35:01

# Operações com strings - TripleTen

Capítulo 2/2 · Faltam 6 lições

Materiais adicionais deste sprint de Python Básico

Opcional

# Operações com strings

Agora que você sabe como acessar caracteres individuais ou partes de uma string usando a indexação, é hora de aprender como criar novas strings juntando strings.

No final desta lição, você será capaz de concatenar strings usando operadores de adição e multiplicação. A concatenação permite criar novas strings unindo strings já existentes em uma só, o que é útil para criar mensagens personalizadas.

## Concatenação

Mesmo que strings não sejam números, nós podemos somá-las. Mas a adição de strings é diferente da adição aritmética simples; o operador de adição se comporta de maneira diferente quando ele é usado com strings, em comparação com operações com números.

Pergunta

O que você acha que esse código irá imprimir?

```
print('Max ' + 'loves ' + 'sci' + 'fi')
```

Maxlovesscifi

Max loves sci fi

Max loves scifi

Isso mesmo!

Trabalho maravilhoso!

Este tipo de adição é chamado **concatenação**, e é bastante útil para juntar strings.

## Multiplicação de strings

Você não pode multiplicar uma string por uma outra string em Python:

```
print('sci' * 'fi')
```

```
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: can't multiply sequence by non-int of type 'str'
```

Este código retorna um erro `TypeError`.

Mas você pode multiplicar uma string por um número:

Pergunta

O que você acha que o código acima irá imprimir?

```
print('scifi' * 5)
```

Mostrar a resposta

## Tarefas

### Tarefa 1

O Maxwell quer aprontar e escrever `Vamos aprontar todas…` cinquenta vezes no computador da professora dele antes de ela entrar na sala; cada frase deve estar em uma linha separada. Como Max pode fazer isso rapidamente antes da professora voltar à sala?

CódigoPYTHON

9

1

print('Vamos aprontar todas...\\n' \* 50)

Legal! Será que ela vai mandar o Max para a direção depois da aula?

Dica

Mostrar a soluçãoValidar

Resultado

Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...
Vamos aprontar todas...

### Tarefa 2

Combine a string `start` (começo) com uma dessas variáveis: `first_choice` (primeira opção), `second_choice` (segunda opção), `third_choice` (terceira opção).

Salve o resultado na variável `result`. E depois a função `print()` irá mostrar o resultado.

CódigoPYTHON

9

1

2

3

4

5

6

start \= 'Max estuda '

first\_choice \= 'Teoria musical'

second\_choice \= 'Literatura'

third\_choice \= 'Medicina'

result \= start + third\_choice

print(result)

Nós geralmente realizamos operações com variáveis que armazenam valores, e não com os próprios valores.

Dica

Mostrar a soluçãoValidar

Resultado

Max estuda Medicina

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-35-01-977Z.md
### Última modificação: 2025-05-28 19:35:02

# A função range() - TripleTen

Capítulo 2/2 · Faltam 5 lições

Materiais adicionais deste sprint de Python Básico

Opcional

# A função range()

## Ciclos e sequências de números

Agora que você sabe como os ciclos `for` percorrem os elementos em uma sequência, é hora de ver como iterar sobre os índices da lista. Isso alcança o mesmo resultado que um ciclo usado para percorrer elementos, mas a diferença chave é que a opção de percorrer índices possibilita grande flexibilidade, especialmente ao lidarmos com múltiplas listas de uma vez.

No final desta lição, você será capaz de usar a função `range()` para percorrer os índices da lista.

Para fazer com que o ciclo `for` repita comandos um certo número de vezes, podemos chamar a função **`range()`**, que produz uma série de números de 0 até o valor que passamos, sem incluir ele mesmo:

```
for i in range(10):
    print('Executando agora o passo ', i) 
```

```
Executando agora o passo  0
Executando agora o passo  1
Executando agora o passo  2
Executando agora o passo  3
Executando agora o passo  4
Executando agora o passo  5
Executando agora o passo  6
Executando agora o passo  7
Executando agora o passo  8
Executando agora o passo  9
```

Como você pode ver, nossa variável do ciclo `i` pega cada número da sequência de 0 até 9.

Mesmo que não seja uma boa ideia nomear suas variáveis com nomes de uma letra só, é comum usar `i`, `j` ou `k` em combinação com a função `range()`.

## Índices e sequências de números

A função `range()` cria uma sequência de números, e não uma lista. Se nós passarmos o comprimento de uma lista para a função `range()`, o ciclo `for` irá percorrer os índices da lista, ao invés dos seus elementos.

```
movies = ['The Shawshank Redemption', 'The Godfather', 'The Dark Knight', 'Schindler\'s List']

for i in range(len(movies)): 
    print('Título original: ', movies[i]) 
```

```
Título original:  The Shawshank Redemption
Título original:  The Godfather
Título original:  The Dark Knight
Título original:  Schindler's List
```

O iterador se move pela sequência de números inteiros, de 0 até `len(movies) - 1`. Dentro do corpo, nós usamos o valor do iterador para conseguir um elemento com o índice `i`: `movies[i]`.

Um ciclo que itera sobre os elementos de uma lista em vez de seus índices seria mais conciso. Mas percorrer índices tem uma grande vantagem: **isso nos permite modificar os elementos da lista**.

Suponha que nós temos uma lista armazenando preços de bens, e a nossa tarefa é baixar cada preço em 10:

```
prices = [15, 29, 74, 32]

for i in range(len(prices)):
    prices[i] -= 10
print(prices)
```

```
[5, 19, 64, 22]
```

A cada passo, o ciclo `for`:

1.  Acessa o elemento da lista com o índice `i`
2.  Reduz este elemento em 10 usando o operador `-=`

### Tarefa 1

A lista `movies_duration` armazena a duração dos filmes em minutos. Mas o stream de filmes tem comerciais, então cada filme fica alguns minutos mais longo. Use um ciclo `for` com `range()` para adicionar 2 para cada elemento da nossa lista.

CódigoPYTHON

9

1

2

3

4

5

movies\_duration \= \[142, 175, 152, 195, 201\]

  

for i in range(len(movies\_duration)):

movies\_duration\[i\] += 2

print(movies\_duration)

Agora você sabe como modificar valores de lista de forma eficiente.

Dica

Mostrar a soluçãoValidar

Resultado

\[144, 177, 154, 197, 203\]

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-35-03-288Z.md
### Última modificação: 2025-05-28 19:35:03

# Expressões Lógicas Compostas - TripleTen

Teoria

# Expressões Lógicas Compostas

## Lógica formal

![](https://practicum-content.s3.amazonaws.com/resources/2__764x513_1_1688388694.jpg)

## Negando expressões lógicas

O operador `not` (não) funciona de um jeito um pouco diferente. Em vez de unir valores lógicos, ele substitui um valor pelo seu oposto. Vamos adicionar esse operador à nossa tabela de verdades:

Expressão A

Expressão B

A e B

A ou B

não A

True

True

True

True

False

True

False

False

True

False

False

True

False

True

True

False

False

False

False

True

Expressões Lógicas Compostas

Tarefa3 / 3

1.

Encontre filmes produzidos na Itália ou com classificação superior a 9.

2.

Anexe à lista `movies_filtered` os filmes de `movies_info` que foram produzidos nos EUA e que duram mais que 150 minutos.

3.

Crie a lista `movies_filtered` e selecione filmes que foram lançados em 1994 ou que têm avaliação menor do que 8.5.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

movies\_info \= \[

\['The Shawshank Redemption', 'USA', 1994, 'drama', 142, 9.111\],

\['The Godfather', 'USA', 1972, 'drama, crime', 175, 8.730\],

\['The Dark Knight', 'USA', 2008, 'fantasy, action, thriller', 152, 8.499\],

\["Schindler's List", 'USA', 1993, 'drama', 195, 8.818\],

\['The Lord of the Rings: The Return of the King', 'New Zealand', 2003, 'fantasy, adventure, drama', 201, 8.625\],

\['Pulp Fiction', 'USA', 1994, 'thriller, comedy, crime', 154, 8.619\],

\['The Good, the Bad and the Ugly', 'Italy', 1966, 'western', 178, 8.521\],

\['Fight Club', 'USA', 1999, 'thriller, drama, crime', 139, 8.644\],

\['Harakiri', 'Japan', 1962, 'drama, action, history', 133, 8.106\],

\['Good Will Hunting', 'USA', 1997, 'drama, romance', 126, 8.077\]

\]

  

movies\_filtered \= \[\]

  

for movie in movies\_info:

if movie\[2\] \== 1994 or movie\[5\] < 8.5:

movies\_filtered.append(movie)

  

for movie in movies\_filtered:

for elem in movie:

print(f'{elem:<46}', end\='')

print()

Resultado

The Shawshank Redemption                      USA                                           1994                                          drama                                         142                                           9.111
The Dark Knight                               USA                                           2008                                          fantasy, action, thriller                     152                                           8.499
Pulp Fiction                                  USA                                           1994                                          thriller, comedy, crime                       154                                           8.619
Harakiri                                      Japan                                         1962                                          drama, action, history                        133                                           8.106
Good Will Hunting                             USA                                           1997                                          drama, romance                                126                                           8.077

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-35-05-413Z.md
### Última modificação: 2025-05-28 19:35:05

# Listas de Dicionários - TripleTen

Teoria

# Listas de Dicionários

## Listas de dicionários e tabelas

A última estrutura de dados que iremos examinar antes de passarmos para a biblioteca pandas é a lista de dicionários. Precisamos dar uma olhada nela porque listas de dicionários são muito parecidas a DataFrames – estruturas bidimensionais usadas pela pandas para armazenar dados.

No final desta lição, você será capaz de criar listas de dicionários para trabalhar com tabelas e processar listas de dicionários.

Anteriormente você utilizou listas aninhadas para reproduzir tabelas:

```
movies_table = [
    ['The Shawshank Redemption', 'USA', 'drama', 1994, 142, 9.111],
    ['The Godfather', 'USA', 'drama, crime', 1972, 175, 8.730],
    ['The Dark Knight', 'USA', 'fantasy, action, thriller', 2008, 152, 8.499]
]
```

O problema destas "tabelas" é que suas linhas e colunas são acessadas por meio de índices. Por exemplo, para obter o ano de lançamento do filme da terceira linha, temos de acessar o item com índice 3 da sublista 2:

```
movies_table = [
    ['The Shawshank Redemption', 'USA', 'drama', 1994, 142, 9.111],
    ['The Godfather', 'USA', 'drama, crime', 1972, 175, 8.730],
    ['The Dark Knight', 'USA', 'fantasy, action, thriller', 2008, 152, 8.499]
]

print(movies_table[2][3])
```

```
2008
```

Se colocarmos esta lista de listas em formato de tabela, teremos um resultado incomum:

![](https://practicum-content.s3.amazonaws.com/resources/1FCC8BF0-635D-4698-806B-931420AE165C_1_1688557351.jpeg) _Tabela com índices em vez de nomes de colunas_

Um computador entenderia esta tabela, mas um analista poderia ficar confuso. Para obter os dados necessários, você teria que memorizar a ordem das colunas, qual índice corresponde à classificação, qual corresponde à duração e assim por diante.

Para possibilitar o uso desta tabela, precisaríamos dar nomes às colunas:

![](https://practicum-content.s3.amazonaws.com/resources/5B270EB2-06EE-4D2C-A674-70F6E3D40878_1_1688557371.jpeg) _Tabela com nomes de colunas_

As listas de dicionários nos possibilitam fazer a substituição dos números das colunas por seus nomes (chaves de dicionário). A seguir, mostramos como os dados dos filmes aparecem em uma lista de dicionários:

```
movies_table = [
    {'movie_name':'The Shawshank Redemption', 'country':'USA', 'genre':'drama', 'year':1994, 'duration':142, 'rating':9.111},
    {'movie_name':'The Godfather', 'country':'USA', 'genre':'drama, crime', 'year':1972, 'duration':175, 'rating':8.730},
    {'movie_name':'The Dark Knight', 'country':'USA', 'genre':'fantasy, action, thriller', 'year':2008, 'duration':152, 'rating':8.499}
]

# agora acesse a coluna pelo nome:
print(movies_table[2]['movie_name'])
```

```
The Dark Knight
```

As chaves do dicionário facilitam o acesso às colunas necessárias.

Conhecer diferentes estruturas de dados facilita o trabalho de qualquer analista. Esse domínio nos permite selecionar a estrutura ideal para qualquer conjunto de dados. Quando a ordem das colunas é menos informativa do que seus nomes, faz mais sentido usar uma estrutura com nomes.

As listas de dicionários combinam as vantagens das duas estruturas:

-   Dos dicionários — nomes
-   Das listas — a possibilidade de iterar sobre itens ordenados

Então, como a combinação desses dois recursos nos ajuda a processar dados complexos? Vamos descobrir.

## Processando uma lista de dicionários

Por exemplo, quando Max pede uma pizza e bebidas em um aplicativo móvel, os dados do pedido são enviados para o servidor na forma de uma lista de dicionários:

-   Cada dicionário é um item no pedido
-   Cada chave do dicionário é um parâmetro do pedido

```
order = [
    {
        'item': 'Margherita pizza',
        'category': 'pizza',
        'quantity': 2,
        'price': 9
    },
    {
        'item': 'Ham pizza',
        'category': 'pizza',
        'quantity': 1,
        'price': 12
    },
    {
        'item': 'Pepsi 1 l',
        'category': 'beverage',
        'quantity': 3,
        'price': 2
    }
]
```

Primeiro, vamos somar os valores numéricos dos dicionários para calcular o total do pedido do Max:

-   A lista permite que você percorra todos os itens do pedido utilizando um ciclo
-   Os dicionários te dão acesso ao preço e à quantidade de cada item

```
order = [
    {
        'item': 'Margherita pizza',
        'category': 'pizza',
        'quantity': 2,
        'price': 9
    },
    {
        'item': 'Ham pizza',
        'category': 'pizza',
        'quantity': 1,
        'price': 12
    },
    {
        'item': 'Pepsi 1 l',
        'category': 'beverages',
        'quantity': 3,
        'price': 2
    }
]

# variável para o preço total do pedido
total_price = 0 

# itere sobre cada dicionário da lista
for item in order: 
# na variável, some o preço do item multiplicado pela quantidade
    total_price += item['price'] * item['quantity'] 
print(total_price)
```

```
36
```

Agora vamos filtrar os itens por categoria. Encontre todas as pizzas!

-   O ciclo irá iterar sobre a lista de dicionários e obterá o valor da chave `'category'`
-   Se o item pertencer à categoria "pizza", o ciclo irá adicionar o dicionário inteiro à nova lista

```
order = [
    {
        'item': 'Margherita pizza',
        'category': 'pizza',
        'quantity': 2,
        'price': 9
    },
    {
        'item': 'Ham pizza',
        'category': 'pizza',
        'quantity': 1,
        'price': 12
    },
    {
        'item': 'Pepsi 1 l',
        'category': 'beverages',
        'quantity': 3,
        'price': 2
    }
]

# variável para armazenar o resultado
filtered_order = [] 

# itere sobre cada dicionário da lista
for item in order: 
# se a categoria for pizza...
    if item['category'] == 'pizza': 
# ...adicione o dicionário à lista filtered_order
        filtered_order.append(item) 

# imprima a lista de dicionários filtrada
print(filtered_order) 
```

```
[{'item': 'Margherita pizza', 'category': 'pizza', 'quantity': 2, 'price': 9}, {'item': 'Ham pizza', 'category': 'pizza', 'quantity': 1, 'price': 12}]
```

Agora nós temos uma lista de dicionários filtrada. Ela não é muito fácil de ler, mas como veremos no próximo capítulo, é aqui que a biblioteca pandas é útil.

Listas de Dicionários

Tarefa2 / 2

1.

A lista de dicionários `statues` descreve várias estátuas. Imprima a altura média das estátuas (as alturas estão em metros). Seu código deve incluir um ciclo e uma chamada a `len()`.

2.

Vamos voltar para o exemplo do dicionário `order`. Encontre o preço total de todas as pizzas e o imprima.

99

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

'category': 'pizza',

'quantity': 2,

'comment': 'Add extra cheeze please!',

\# preço por item

'price': 9

},

{

'item': 'Ham pizza',

'category': 'pizza',

'quantity': 1,

'comment': '',

'price': 12

},

{

'item': 'Pepsi 1 l',

'category': 'beverages',

'quantity': 3,

'comment': '',

'price': 2

},

{

'item': 'Apple juice 0.5 l',

'category': 'beverages',

'quantity': 1,

'comment': '',

'price': 1

},

{

'item': 'Croissant with cheese',

'category': 'baked foods',

'quantity': 2,

'comment': '',

'price': 1

}

\]

  

total\_pizza\_price \= 0

  

for item in order:

if item\['category'\] \== 'pizza':

total\_pizza\_price += item\['price'\] \* item\['quantity'\]

  

print(total\_pizza\_price)

Resultado

30

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-35-06-757Z.md
### Última modificação: 2025-05-28 19:35:07

# Combinando listas e funções - TripleTen

Capítulo 2/2

Materiais adicionais deste sprint de Python Básico

Opcional

# Combinando listas e funções

## Usando ciclos dentro de funções

Na lição anterior, nós definimos a função e a chamamos três vezes, já que haviam três itens no carrinho de compras.

Ao final desta lição, você será capaz de combinar listas e funções para deixar seu código mais elegante.

```
def calculate_total_price(price, quantity):
    total = price * quantity
    if total > 100:
        total -= total * 0.05
    return total

first_item_quantity = 3
first_item_price = 10.0

second_item_quantity = 2
second_item_price = 51.0

third_item_quantity = 10
third_item_price = 4.0

first_item_total = calculate_total_price(first_item_price, first_item_quantity)
second_item_total = calculate_total_price(second_item_price, second_item_quantity)
third_item_total = calculate_total_price(third_item_price, third_item_quantity)

print(first_item_total)
print(second_item_total)
print(third_item_total)
```

O código ainda não está muito elegante — e se tiverem 15 itens no carrinho? Fazer todo esse trabalho manualmente levaria muito tempo.

Felizmente, podemos utilizar listas para deixar o código mais flexível. Em vez de duas variáveis para cada item, vamos criar uma lista aninhada:

-   Cada sublista representa um item
-   Cada sublista inclui dois elementos, a quantidade e o preço do item

```
items_list = [
    [3, 10.0],
    [2, 51.0],
    [10, 4.0]
]
```

Dessa forma, não precisaremos utilizar tantas variáveis. Vamos simplesmente criar um ciclo que irá realizar as ações que precisamos executar.

CódigoPYTHON

99

1

2

3

4

5

6

7

8

9

10

11

items\_list \= \[

\[3, 10.0\],

\[2, 51.0\],

\[10, 4.0\]

\]

  

for item in items\_list:

total \= item\[0\] \* item\[1\]

if total \> 100:

total \-= total \* 0.05

print(total)

Mostrar a soluçãoExecutar

Resultado

30.0
96.9
40.0

Conseguimos deixar o código mais conciso sem utilizar uma função definida pelo usuário. No entanto, ficou um pouco mais difícil entender o código.

-   Em vez de variáveis nomeadas, temos uma lista aninhada
-   O ciclo no código está associado a uma declaração condicional e ao cálculo de desconto

Geralmente, é necessário escolher entre concisão e clareza. Uma maneira de balancear os dois é dividir o código em blocos lógicos que equivalem aos parágrafos de um texto impresso. Funções definidas pelo usuário nos ajudam a fazer exatamente isso.

## Lógica de negócio

A maneira como dividimos um texto em parágrafos ou um código em blocos é uma questão de estilo, mesmo que os profissionais de dados geralmente utilizem alguns princípios básicos.

Separar a lógica de negócio do restante do código é considerada uma boa prática de estilo. Quando falamos de lógica de negócio, estamos falando da parte do código relevante para a tarefa do negócio. No exemplo que apresentamos, a parte do código que calcula o desconto desempenha esse papel.

O restante do código pode assumir algumas formas diferentes. Parte dele pode ser formado pelos dados brutos com os quais temos que trabalhar (neste caso, a lista de itens e suas quantidades). Também pode ser um código que funciona como suporte, que reúne a parte da lógica de negócio e os dados propriamente ditos, como veremos a seguir.

## Dividindo nosso código de desconto em blocos

Aqui está uma maneira lógica de separar nosso código em blocos que serão facilmente lidos:

```
def calculate_total_price(price, quantity): 
    total = price * quantity 
    if total > 100: 
        total -= total * 0.05 
    print(total)

items_list = [
    [3, 10.0],
    [2, 51.0],
    [10, 4.0]
]

for item in items_list:
    calculate_total_price(item[0], item[1])
```

Agora podemos distinguir três blocos separados:

-   Lógica de negócio
-   Dados
-   O ciclo que chama a função e processa os dados

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-30-914Z.md
### Última modificação: 2025-05-28 19:36:32

# Introdução - TripleTen

Capítulo 1/8

Introdução à Coleção e Armazenamento de Dados (SQL)

# Introdução

Neste curso, você vai aprender o que é a estrutura de uma página web e como recuperar dados de fontes online. Você também vai ganhar alguma familiaridade com a linguagem SQL e aprender como extrair dados de bancos de dados.

### O que você vai aprender:

-   Como analistas obtêm dados
-   Como escrever consultas SQL de vários graus de complexidade
-   Para que servem expressões regulares e como escrevê-las
-   Como fatiar dados e escrever subconsultas
-   Como usar a documentação do SQL
-   Como utilizar as funções de agregação
-   Vários métodos de junção de tabelas

Então, neste sprint, você vai trabalhar no seguinte:

![](https://practicum-content.s3.amazonaws.com/resources/Pre-processamento_de_Dados_1713355084.png)

![](https://practicum-content.s3.amazonaws.com/resources/SQL_1713355117.png)

Você vai resolver uma tarefa interessante na nossa plataforma interativa. Sua missão será determinar se o Dia Internacional do Leite afeta as vendas e se existe uma ligação entre as compras online e o clima.

No final desta seção, você concluirá um projeto independente no qual você vai estudar padrões de uso de táxis em Chicago. Clique [aqui](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_6/moved_DA_DS_Sprint6_Projeto_do_Curso_prt.pdf) para ver a descrição do projeto e o que ele vai envolver.

### Quanto tempo isso vai levar?

Esta seção é de dificuldade média. Ter algum conhecimento sobre como fatiar dados vai te ajudar aqui. Espere usar entre 25 e 40 horas para completar esse material, dependendo do seu conhecimento prévio e hábitos de estudo. Se você sentir que está ficando para trás, não hesite em contactar a nossa Equipe de Orientação. Como sempre, estamos empenhados em ajudar você a cada passo do caminho.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-33-554Z.md
### Última modificação: 2025-05-28 19:36:33

# Introdução - TripleTen

Capítulo 2/8

Extraindo Dados de Recursos Online

# Introdução

Às vezes, analistas não têm dados suficientes para cumprir bem uma tarefa. Nesse capítulo, você irá desenvolver a habilidade de coletar dados de fontes online, para saber o que fazer quando você se encontrar em tais situações.

### O que você irá aprender:

-   A estrutura de páginas HTML e como funcionam as requisições GET
-   Como escrever expressões regulares simples
-   O básico das API e JSON
-   Como enviar requisições para sites e coletar dados
    

### Quanto vai demorar:

10 lições, de aproximadamente 20 minutos cada

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-36-678Z.md
### Última modificação: 2025-05-28 19:36:37

# O que é a Mineração de Dados na Web (Web Mining)? - TripleTen

Capítulo 2/8

Extraindo Dados de Recursos Online

# O que é a Mineração de Dados na Web (Web Mining)?

Às vezes, você pode não ter recebido dados suficientes para conduzir uma análise profunda. Nesses casos, você precisa fazer mais pesquisas. Isso permite que você leve em consideração mais fatores, identifique mais padrões e alcance conclusões inesperadas.

Por exemplo, os preços de uma loja online estrangeira dependem parcialmente das taxas de câmbio. Isso significa que uma análise relevante deve ter em conta dados históricos de taxa de câmbio recebidos de uma fonte externa.

![](https://practicum-content.s3.amazonaws.com/resources/PT6.2.2_1704891652.png)

Para testar a hipótese de que os usuários fazem mais compras online em dias chuvosos, você precisará de dados históricos de tempo. Qualquer aplicativo de clima (por exemplo, [Yahoo Weather](https://www.yahoo.com/news/weather) _(os materiais estão em inglês)_) fará o truque.

Os analistas podem enriquecer seus dados complementando-os com os dados da internet. Primeiro, eles encontram recursos que poderiam ser relevantes e então recuperam todos os dados necessários. Esse processo chama-se **mineração na web.**

![](https://practicum-content.s3.amazonaws.com/resources/PT6.2.2.2_1704891676.png)

Que tipo de conteúdo pode ser o objeto da mineração? Praticamente qualquer coisa:

-   _Conteúdo de um site_. Texto, imagens, arquivos e tabelas.
-   _Estruturas de um site_. Qualquer elemento da estrutura de um site (por exemplo, links e relacionamentos entre suas páginas).
-   _Informações do usuário._ A informação sobre a interação de usuários com um site, como os dados sobre seus visitantes recebidos de um serviço analítico.

Às vezes, a mineração na web também pode ser chamada de **parsing, ou seja, a análise sintática**. Os analistas usam essa palavra frequentemente dizendo que eles fazem **"parsing"** de sites. Outra maneira que os analistas chamam essa tarefa é “web scraping”.

Pergunta

O objetivo da mineração na web é:

analisar memes

obter informações adicionais do site

Certo! Com a mineração na web, você pode recuperar e um site praticamente qualquer informação que precisa

minerar bitcoin

Excelente!

Pergunta

Que tipos de dados podem ser coletados usando a análise sintática (parsing)?

imagens

arquivos de texto

tabelas

todas as opções acima

Certo! Você pode falhar em encontrar aquilo que está procurando na internet, mas você definitivamente encontrará algo interessante!

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-38-006Z.md
### Última modificação: 2025-05-28 19:36:38

# Algumas Coisas que um Analista Deve Saber sobre a Internet. - TripleTen

Capítulo 2/8

Extraindo Dados de Recursos Online

# Algumas Coisas que um Analista Deve Saber sobre a Internet. Navegadores. HTML. HTTP

Seus colegas compilaram uma lista de sites dos quais eles gostariam de receber informações sobre produtos. Sua primeira tarefa será fazer o download de uma página web de um servidor. No entanto, não nos apressemos para escrever consultas de servidor. Primeiro, precisamos dar uma olhada em como a internet funciona.

Os primeiros protótipos do que hoje chamamos de internet apareceram no início dos anos 1960, mas foram necessários mais 30 anos para que as coisas realmente começassem a funcionar. Naqueles primeiros dias, não havia regras gerais de utilização da internet. Em outras palavras, ela não tinha interface.

Em 1991, Tim Berners-Lee, um físico, estava trabalhando no laboratório CERN junto com a sua equipe. Eles queriam facilitar a interação com a internet criando regras que permitissem a comunicação de computadores dentro de uma rede. Foi assim que eles tiveram a ideia da World Wide Web.

O que tinha de acontecer para a Internet obter a forma que ela tem hoje?

Como a internet é uma rede de computadores que trocam dados entre si, esses dados precisam ser estruturados. Deve haver _uma maneira de representar essa informação na internet._

No entanto, apenas representar a informação não é o bastante; _é preciso ensinar computadores a exibi-la._ Havia uma necessidade de um software que permitisse à informação na internet ser lida e exibida para os usuários. Além disso, era preciso um conjunto de regras para governar como os computadores enviam e recebem essa informação.

Como resultado, Tim Berners-Lee e a sua equipe inventaram:

1.  Uma linguagem para criar documentos na internet: **HTML** (Linguagem de Marcação de Hipertexto)
2.  Um aplicativo de software para visualizar tais documentos: um **navegador web**
3.  Regras gerais de transferência de documentos: **HTTP** (Protocolo de Transferência de Hipertexto_)_

![](https://practicum-content.s3.amazonaws.com/resources/PT6.2.3_1705925160.png)

Um desenvolvedor preguiçoso não quer exportar os dados que você precisa. Mantenha a conversa fluindo dando respostas corretas à suas perguntas.

Pergunta

Eu não consigo abrir o site do seu banco! Você: Tente usar meu...

HTML

navegador

De fato, alguns sites não podem ser abertos em certos navegadores

HTTP

coffee

Muito bem!

Pergunta

Ok, ele abriu, mas parece estranho. Em que linguagem é escrito? Você: Em ...

HTML

Verdadeiro, qualquer página web é marcada com a formatação HTML, mesmo se seu conteúdo visível for renderizado apenas com scripts.

navegador web

HTTP

back end

Você conseguiu!

Pergunta

Você é genial! Você também sabe quem inventou a World Wide Web? Você: ...

Bill Gates

Steve Jobs

Bruce Lee

Tim Berners-Lee

Sim, Tim Berners-Lee mesmo! Embora ela seja frequentemente confundida com a internet, a World Wide Web proporcionou aos usuários uma maneira de acessar a informação online.

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-39-339Z.md
### Última modificação: 2025-05-28 19:36:39

# Protocolos de Transferência - TripleTen

Capítulo 2/8

Extraindo Dados de Recursos Online

# Protocolos de Transferência

A internet é uma rede através da qual os computadores trocam informações. Para que isso seja possível, devem haver regras que governam como cada computador envia dados uns aos outros.

A troca de dados na internet é baseada no princípio de "requisição-resposta": um navegador gera uma requisição, então o servidor analisa-a e envia uma resposta. As regras de formulação das requisições e respostas são determinadas pelo que conhecemos como um protocolo de transferência - nesse caso, HTTP. As letras "HT" significam "hipertexto", já que o protocolo foi originalmente desenvolvido para transferir documentos HTML. Entretanto, hoje ele é usado para transferir todos os tipos de dados na internet.

Aqui está uma analogia: quando você quer enviar uma carta, você precisa colocá-la dentro de um envelope, escrever o endereço do destinatário de acordo com certas regras, colocar um selo no envelope e levá-la até a agência dos correios. Computadores e servidores também trocam "cartas". O protocolo HTTP é, no fundo, o conjunto das regras de como criar essas mensagens e enviá-las.

### O protocolo HTTPS

A maioria dos sites hoje usam um protocolo aprimorado de transferência de dados chamado de **HTTPS**.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Screenshot_3_1654684183.png)

_Fonte: [https://transparencyreport.google.com/https/overview](https://transparencyreport.google.com/https/overview?hl=pt_BR)_

A letra "S" significa _seguro_: HTTPS é uma versão mais segura do HTTP. Esse protocolo garante que toda a comunicação entre seu navegador e um site é criptografada.

Sistemas bancários online e redes sociais mudaram para HTTPS há muito tempo. Navegadores modernos geralmente enviam um aviso de segurança quando você abre sites que usam HTTP. Ninguém quer que informações pessoais de usuários se tornem disponíveis para outros. Exceto os hackers, é claro.

### Navegador e requisições

Quando você acessa um site, seu navegador envia uma requisição HTTP para o servidor. O servidor, por sua vez, formula uma resposta: o código HTML para a página relevante.

![](https://practicum-content.s3.amazonaws.com/resources/PT6.2.4_1704892352.png)

### Formato da requisição

Uma requisição gerada por um navegador pode incluir o seguinte:

-   Método HTTP: ele determina a operação a ser realizada. Existem vários métodos, os mais populares são GET e POST. O primeiro solicita dados do servidor, enquanto o segundo envia-os.
-   Caminho: o segmento do endereço após o nome do site (por exemplo, em _[example.com/hello](http://example.com/hello)_ o caminho é /_hello)_.
-   Versão do HTTP: a versão do protocolo HTTP usada para enviar a requisição (por exemplo, HTTP/1.1).
-   Cabeçalhos de requisição: eles são usados para enviar ao servidor informações adicionais.
-   Corpo da requisição: por exemplo, o corpo de uma requisição POST são os dados enviados. Nem todas as requisições têm um corpo.

Dê uma olhada em uma requisição para o site do TripleTen.

![img](https://practicum-content.s3.us-west-1.amazonaws.com/tripleten/yango/data_collection_SQL/PT_AO/PT6.2.4.2.png)

### Formato de resposta

A resposta pode incluir:

-   A versão do HTTP.
-   O código e a mensagem da resposta (por exemplo, "200 OK" se tudo correr bem, ou "404 Not Found" se o caminho requisitado não puder ser encontrado).
-   Cabeçalhos que contêm informações adicionais para o navegador.
-   O corpo da resposta (por exemplo, quando você abrir o site, você verá o código HTML para esta página no corpo da resposta).

Quando uma requisição é enviada ao site, a resposta se parece com isso:

![img](https://practicum-content.s3.us-west-1.amazonaws.com/tripleten/yango/data_collection_SQL/PT_AO/PT6.2.4.3.png)

É assim que os navegadores usam requisições para se comunicar com os servidores. O seu trabalho é aprender a escrever tais requisições em Python.

Pergunta

Como se chama o protocolo de transferência de dados mais seguro?

HTTPS

Correto! HTTPS significa Hyper Text Transfer Protocol Secure (protocolo de transferência de hipertexto seguro)

HTML

HTTP

GET

POST

Fantástico!

Pergunta

Qual método é usado para obter dados de um servidor?

GET

Isso mesmo!

POST

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-41-245Z.md
### Última modificação: 2025-05-28 19:36:41

# Introdução a HTML - TripleTen

Capítulo 2/8

Extraindo Dados de Recursos Online

# Introdução a HTML

Para exportar uma lista de bens do site de uma loja online, primeiro você precisa obter o código da página e seu conteúdo.

HTML significa _Hypertext Markup Language (Linguagem de Marcação de Hipertexto)._ O próprio nome diz para que serve: cada objeto da página precisa ser marcado para exibir o conteúdo corretamente. Essa marcação envolve colocar blocos de informação dentro de comandos chamados "tags". Essas tags dizem aos navegadores como exibir a informação dentro elas.

Um elemento HTML é composto de _tags_ e o _conteúdo_ dentro delas. Uma tag HTML consiste de um nome cercado de parênteses angulares (< e >). Por exemplo, a tag do cabeçalho principal é `<h1>`.

Tags em um código são como sinais de trânsito. Por exemplo, quando você entra em uma cidade, você vê um sinal "Você chegou a São Francisco", e quando sai dela, você pode ver um sinal com as palavras "Você está saindo de São Francisco". De forma parecida, cada elemento HTML começa e termina com uma tag de abertura e uma de fechamento (a tag de fechamento começa com uma barra comum). O elemento resultante é normalmente referido pelo nome de sua tag.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/tripleten/yango/data_collection_SQL/PT_AO/PT6.2.5.png)

Os analistas agem como os detetives rastreando certas tags e extraindo seu conteúdo. Mas quais tags eles procuram?

Aqui está a estrutura típica de uma página HTML:

### 1\. <html> ... </html>

A tag **<html>** introduz cada documento HTML e indica o seu início, enquanto **</html>** marca o seu final. O cabeçalho e corpo do documento HTML são encontradas dentro das tags **<head>** e **<body>** .

### 2\. <head> ... </head>

Estas tags marcam o cabeçalho do documento. As tags que introduzem o título do documento (**<title>)** e a metainformação (adicional) (**<meta>**) são colocadas dentro destas tags.

### 3\. <body> ... </body>

A tag **<body>** marca o início do corpo de uma página HTML. Todos os conteúdos da página (cabeçalhos, parágrafos de texto, tabelas, imagens) são colocados dentro do corpo.

Para tornar a marcação mais clara, os desenvolvedores deixam comentários dentro de tags especiais `<!-- -->` no código da página. Isso ajuda muito os analistas, e nós também vamos deixar comentários no código dos nossos exemplos.

Os analistas frequentemente fazem análise sintática de tabelas. Elas são geralmente colocadas em elementos do tipo _tabela_, entre as tags `<table>` e `</table>`. A tag de abertura `<table>` marca o início da tabela, enquanto a tag de fechamento `</table>` marca seu final. Dentro deste elemento, os conteúdos da tabela são divididos em linhas pelas tags `<tr>` (table row, linha de tabela), e as linhas por sua vez são divididas em células pelas tags `<td>` **(**table data, dados da tabela). A primeira linha geralmente contém os cabeçalhos das colunas, em vez de células comuns. Eles são colocados entre as tags `<th>` (table headings, cabeçalhos da tabela).

Exemplo:

Sobrenome

Nome

Grupo

Smith

Alex

41-A

Stafford

John

13-C

Johnson

Mark

41-B

```
<!-- a tag <table> marca o início da tabela -->
<table id="student" class='Stanford'> 
    <!-- a tag <tr> marca o início da linha -->
    <tr> 
        <!-- a tag <th> marca o início da célula do cabeçalho e a tag </th> marca seu final -->
        <th>Sobrenome</th>
        <th>Nome</th> 
        <th>Grupo</th>
    </tr>
    <!-- a tag </tr> marca o final da linha -->
    <!-- ela é seguida pela tag <tr>, que define o início de uma nova linha -->
    <tr>
        <!-- a tag <td> tag marca o início de uma célula com dados e a tag </td> marca seu final -->
        <td>Smith</td> 
        <td>Alex</td> 
        <td>41-A</td> 
    </tr> 
    <tr> 
        <td>Stafford</td> 
        <td>John</td> 
        <td>13-C</td> 
    </tr>
    <tr> 
        <td>Johnson</td> 
        <td>Mark</td> 
        <td>41-B</td> 
</tr>
</table>
<!-- a tag </table> marca o final da tabela -->
```

O texto é frequentemente colocado dentro de um elemento _p_ (parágrafo). O início do parágrafo é marcado com a tag `<p>` e o final - com a tag `</p>`.

É muito comum a tag de bloco `<div>` (divisão), que pode agrupar vários elementos. _div_ é muito útil porque pode incorporar qualquer número de elementos, até mesmo de tipos diferentes (por exemplo, um cabeçalho com uma imagem e um par de parágrafos de texto), e atribuir a eles comportamento ou características comuns.

Você também pode incluir **atributos** dentro dos parênteses da tag de abertura para fornecer mais informações sobre como o elemento deve se comportar. Diferentes tipos de informação exigem diferentes atributos.

O formato para especificar atributos é `name=value`. O nome do atributo informa ao navegador a que característica se refere o atributo, enquanto o valor especifica o que deve acontecer com essa característica. Por exemplo, o valor do atributo _href_ especifica a que site vai o link:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/tripleten/yango/data_collection_SQL/PT_AO/6.2.5.2.png)

Ao procurar informações, os analistas quase sempre prestam atenção nos atributos, bem como nas tags.

Existe um grande número de tags e atributos HTML, e seria impossível aprender todos eles em um capítulo. Se você encontrar uma tag desconhecida, pesquise-a em um recurso como [este](https://developer.mozilla.org/pt-BR/docs/Web/HTML/Element).

Na maioria das vezes, você precisará trabalhar com os atributos `id` e `class`. Nós já os vimos no código acima:

```
<table id="student" class='Stanford'> 
```

O atributo _id_ fornece um identificador unívoco para um elemento. A página tem apenas um elemento `id` com o valor `"student"`. Essa página tem uma tabela que contém dados dos estudantes.

O valor do atributo _class_ é um nome que pode ser compartilhado entre vários elementos, assim como vários membros de uma família podem compartilhar o mesmo sobrenome. A classe `Stanford` pode ser atribuída a um conjunto de tabelas que contenham informação relacionada à Universidade de Stanford (por exemplo, os dados de seus estudantes, professores e departamentos).

Pergunta

Qual tag você usa para criar cabeçalhos de tabela?

`<table>`

`<td>`

`<th>`

Certo!

`<tr>`

Excelente!

Pergunta

Escolha a opção que contém apenas nomes de atributos

class, href, div

p, div, id

p, div, h2

href, class, id,

Muito bem! href, class, id são atributos!

Seu entendimento sobre o material é impressionante!

Pergunta

Usando [esse guia de HTML](http://www.w3schools.com/tags/default.asp), selecione a tag usada para exibir imagens na página.

<img>

Exatamente!

<ul>

<ol>

<form>

Você conseguiu!

Pergunta

Use [esse guia de HTML](http://www.w3schools.com/tags/default.asp), para selecionar a afirmação verdadeira sobre a tag <a>

A tag <a> serve para criar um clima festivo para os visitantes do site

<a> tem o atributo href cujo valor especifica o endereço do site pra onde o link vai

Perfeito!

<a> não tem uma tag de fechamento

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-42-556Z.md
### Última modificação: 2025-05-28 19:36:42

# Ferramentas de desenvolvedor - TripleTen

Capítulo 2/8

Extraindo Dados de Recursos Online

# Ferramentas de desenvolvedor

Todo navegador moderno tem uma _barra de ferramentas de desenvolvedor_, que é como um canivete suíço para os desenvolvedores. Aqui você pode dar uma olhada no código da página inteira ou de um elemento particular, ver o estilo de cada elemento da página e até mesmo alterar a visualização deles no seu computador.

Vamos abrir a [página](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_6/web-sites/most_famous_shipwrecks_ptbr_1.html?etag=9fcc222cac30e287d2b645aa88bcf13e) usando o Google Chrome. Passe o cursor sobre a primeira tabela, clique com botão direito e selecione "Inspecionar". Você também pode pressionar Control+Shift+i.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/Screenshot_2_1661968630.png)

Alguns fragmentos de código no painel de Elementos aparecem com cores diferentes. Agora podemos ver as tags que envolvem o nosso elemento. Para o cabeçalho "Ship's name" as tags são `<th>` e `</th>`.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1_1661968763.png)

Se você passar o cursor sobre um elemento (nesse caso, a primeira linha que diz <tr>\_</tr>), todo o conteúdo descrito por suas tags mudará de cor. Isso facilita a busca da informação que você precisa.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/2_1661968773.png)

Abra o seguinte site da lista fornecida pelos nossos colegas em uma nova aba. Explore os elementos desta [página](https://store.data-analyst.praktikum-services.ru/en/) _(os materiais estão em inglês)_ e suas tags usando as ferramentas de desenvolvedor.

Pergunta

Qual tag marca o início do corpo de uma página HTML?

<head>

<html>

<body>

Fantástico!

Pergunta

Dentro de qual elemento da página da loja online você encontra o slogan "High quality, local producers"?

p

div

style

a

Excelente!

Pergunta

Encontre o produto "Horizon Organic DHA Omega-3 Vanilla Lowfat Milk, 6 pk" na página da loja online.

Qual tag e quais atributos precedem seu nome? Encontre a opção mais próxima dentre as opções abaixo.

`<div class="t-clear t754__separator">`

`<div class="t754__title t-name t-name_md js-product-name" field="li_price__1564953783763" style="font-size:20px;font-weight:400;">`

`<div class="t754__col t-col t-col_4 t-align_center t-item t754__col_mobile-grid js-product" data-product-lid="1498486363994">`

Trabalho maravilhoso!

Pergunta

Encontre o produto "Nesquik Vanilla Lowfat Milk, 14 oz" na página da loja online.

Qual tag e quais atributos precedem seu preço? Encontre a opção mais próxima dentre as opções abaixo.

`<div class="t754__title t-name t-name_md js-product-name" field="li_title__1500469907722" style="font-size:20px;font-weight:400;">`

`<div class="t119__preface t-descr t-opacity_70" style="font-size:28px;opacity:1;" field="text">`

`<div class="t754__price-value js-product-price" field="li_price__1500469907722">`

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-43-881Z.md
### Última modificação: 2025-05-28 19:36:44

# Sua Primeira Requisição GET - TripleTen

Teoria

# Sua Primeira Requisição GET

Para obter dados do servidor, nós usaremos o método **get()**, e para enviar requisições HTTP, precisamos da biblioteca **Requests.** Importamos a biblioteca:

```
import requests
```

... e escreveremos a nossa primeira requisição GET!

Primeiro de tudo, precisamos de um link. Aqui está o [endereço](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_6/web-sites/most_famous_shipwrecks_ptbr_1.html?etag=9fcc222cac30e287d2b645aa88bcf13e) de uma página sobre os naufrágios mais famosos do século XX. Vamos salvar o link na variável URL. (Como você provavelmente sabe, URL é um endereço web. A sigla significa Uniform Resource Locator - Localizador Uniforme de Recursos).

```
URL = 'https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_6/web-sites/most_famous_shipwrecks_ptbr_1.html?etag=9fcc222cac30e287d2b645aa88bcf13e'
```

O método _get()_ da biblioteca _Requests_ age como um navegador. Vamos passar o link como um argumento para ele . O método enviará uma requisição GET para o servidor, processará os dados que ele receber do servidor e retornará uma **response,** (resposta) um objeto que contém a resposta do servidor para a requisição.

```
req = requests.get(URL) # salvando o objeto de resposta como a variável req
```

Um objeto _response_ contém a resposta do servidor: o código de status, o conteúdo da requisição e o código da própria página HTML. Os atributos de objetos _response_ permitem obter do servidor apenas dados relevantes. Por exemplo, um objeto _response_ com o atributo _text_ retornará apenas o conteúdo textual da requisição:

```
print(req.text) # o nome do atributo é colocado depois do objeto de resposta e separado dele por um ponto
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6.2.9PT_1661759156.png)

O atributo status\_code informa se o servidor respondeu ou se ocorreu um erro.

Vamos exibir o código de status da resposta:

```
print(req.status_code)
```

Resultado:

```
200
```

Infelizmente, nem todas as requisições retornaram com dados. Às vezes, as requisições retornam erros; cada erro tem um código especial dependendo do seu tipo. Aqui estão os erros mais comuns:

Error code

Name

Implication

200

OK

Tudo é fantástico

302

Found

A página foi movida

400

Bad Request

Erro na sintaxe do pedido

404

Not Found

A página não foi encontrada

500

Internal Server Error

Erro da parte do servidor

502

Bad Gateway

Erro na troca de dados entre servidores

503

Server Unvailable

O servidor está temporariamente indisponível para processar pedidos

Sua Primeira Requisição GET

Tarefa2 / 2

1.

Escreva uma requisição GET para a seguinte página: [https://tripleten-com.github.io/simple-shop\_pt-br/](https://tripleten-com.github.io/simple-shop_pt-br/). Salve o seu resultado na variável _req._

2.

Imprima os conteúdos textuais da requisição.

9

1

2

3

4

5

6

7

8

9

import requests

  

URL \= 'https://tripleten-com.github.io/simple-shop\_pt-br/'

req \= requests.get(URL)

print(req.text)

  

  

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-48-145Z.md
### Última modificação: 2025-05-28 19:36:48

# Expressões Regulares - TripleTen

Teoria

# Expressões Regulares

Os analistas trabalham com grandes conjuntos de dados e retiram deles informação, ignorando dados dúbios e procurando por padrões. É aqui que entra em ação uma ferramenta poderosa para buscar strings: expressões regulares.

Uma **expressão regular (regex)** é uma regra para procurar por substrings (fragmentos de texto dentro de strings). Por exemplo, se estivermos procurando pela combinação de letras "tion" na frase "Arrived at the station in total frustration", Python entenderá que estamos buscando a substring `"tion"`dentro da string `'Arrived at the station in total frustration'`.

Expressões regulares te permitem criar regras complexas para que uma expressão retorne várias substrings. Por exemplo, podemos encontrar todas as cartas de baralho mencionadas em _Alice no País das Maravilhas._

## Como expressões regulares funcionam

Para começar a trabalhar com expressões regulares em Python, precisamos importar o módulo `re` (expressões regulares). Seguem-se duas etapas.

Primeiro criamos o padrão da expressão regular. É um algoritmo para descrever o que deve ser procurado no texto (por exemplo, todas as letras maiúsculas).

Depois esse padrão é passado para métodos específicos do módulo _re_. Esses métodos procuram, substituem e removem símbolos. Em outras palavras, o padrão identifica o que deve ser procurado e como isso deve ser efetuado, enquanto o método define o que vai ser feito com as ocorrências encontradas.

### **Padrões básicos de expressões regulares**

A seguinte tabela contém os padrões mais simples de expressões regulares. Você pode criar expressões regulares mais complexas combinando esses padrões.

### **Sintaxe de expressão regular**

**Expressão regular**

**Descrição**

**Exemplo**

**Explicação**

\[\]

Um único caractere envolto entre colchetes

\[a-\]

a ou -

\[^…\]

Negação

\[^a\]

qualquer caractere exceto "a"

\-

Intervalo

\[0-9\]

intervalo: qualquer dígito de 0 a 9

.

Qualquer caractere único, exceto uma nova linha

a.

as, a1, a\_

\\d (see \[0-9\])

Qualquer dígito

a\\d

a\[0-9\]

a1, a2, a3

\\w

Qualquer letra, dígito ou _| a\\w | a_, a1, ab

\[A-z\]

Qualquer letra, incluindo \[, , \], ^, \_ e \`.

a\[A-z\]

ab

?

0 ou 1 instância

a?

a ou nenhum

+

1 ou mais instâncias

a+

a o aa, o aaa

\*

0 ou mais instâncias

a\*

nada ou a, ou aa

^

Início da string

^a

a1234, abcd

$

Fim da string

a$

1a, ba

Lembre-se de que às vezes os caracteres utilizados na sintaxe das expressões regulares (esses são chamados de caracteres especiais, ou caracteres reservados) precisam ser usados no seu sentido literal. Por exemplo, queremos encontrar uma substring que contenha um sinal de mais. Usar tais caracteres no seu sentido literal se chama "escapar caracteres especiais" e pode ser feito prefixando tais caracteres com uma contrabarra. Em outras palavras, temos que escrever `\+` para mostrar que precisamos de um sinal de mais literal ou `\.` para indicar que precisamos de um ponto literal. A própria contrabarra também é um caractere especial, então para obter uma contrabarra literal você precisa escrever `\\`.

**Criando uma expressão regular**

Vamos escrever uma expressão regular para verificar se a string começa com um número e, se sim, encontrar a correspondência do número. Vamos passo a passo.

`[0-9]` — este padrão encontra uma correspondência com um único dígito de 0 a 9 (apenas uma de cada vez!). Aplicado à string `155 plus 33`, ele retornará `1`, `5`, `5`, `3`, e `3`.

`[0-9]+` — esse padrão encontra uma correspondência com sequências de dígitos, ou seja, números. Aplicado à string `155 plus 33`, ele retornará `155` e `33`.

`^[0-9]+` — O `^` encontra uma correspondência no início de uma string. Já que uma determinada string pode ter apenas um início, esse padrão sempre irá retornar uma única correspondência para uma determinada string: o primeiro número (se a string começa com um número) ou nenhuma correspondência (se a string não começa com um número). Aplicado à string `155 plus 33`, esse padrão retornará uma única correspondência: `155`.

![](https://practicum-content.s3.amazonaws.com/resources/PT6.2.8_1687159316.png)

### Expressões regulares em **Python**

As tarefas mais comuns dos analistas incluem:

-   encontrar uma substring dentro de uma string
-   dividir strings em substrings
-   substituir partes de uma string por outras strings

Para completar essas tarefas, você precisará dos seguintes métodos do módulo `re`:

1.  **search(pattern, string)** procura por um `pattern` (padrão) em uma `string`_._ Embora `search()` percorra toda a string para encontrar o padrão, ele apenas retorna a primeira substring que encontrar:

```
import re
string = '"General Slocum" 15 June 1904 East River human factor'
print(re.search('\w+', string))
```

```
<re.Match object; span=(1, 8), match='General'>
```

O método `search()` retorna um objeto do tipo **match** (correspondência). O parâmetro `span` define um intervalo de índices que fazem match com o padrão. Em nosso caso, as aspas de abertura " não combina com a regra que ignora sinais de pontuação. É por isso que vemos on índices 1 a 8: de "G" a "l." (Não se esqueça de que o primeiro índice é 0.) O parâmetro `match` indica o valor da própria substring.

O padrão `'\w+'` encontra uma correspondência com qualquer substring que contenha um ou mais caracteres ou dígitos ou o símbolo "\_". Usando o método `search()`, determinamos que a primeira palavra da string corresponde ao padrão. Já que `'\w+'` não captura espaços, o método retornou todos os caracteres antes do primeiro espaço.

Se você precisar recuperar a primeira palavra da string, o exemplo acima será a maneira correta de fazer isso. Primeiro, você escreve um padrão que faz match com todas as palavras da string e depois o usa na função _search()_ para obter apenas a primeira correspondência. Embora seja frequentemente possível obter a primeira palavra usando somente o padrão `^\w+` (lembre-se de que o símbolo de acento circunflexo corresponde ao início da string), este método não é confiável.Ele supõe que a string começa com uma letra, um dígito ou um underscore, mas nem sempre esse é o caso, como demonstra o exemplo acima. O uso principal do símbolo de acento circunflexo é verificar se a string começa com um tipo específico de caractere ou com uma palavra. Se não, essa maneira de busca não irá percorrer mais a string e simplesmente não retornará nada.

Se não precisamos de qualquer informação sobre o intervalo, podemos retornar apenas a substring usando o método `group()`:

```
import re
string = '"General Slocum" 15 June 1904 East River human factor'
print(re.search('\w+', string).group())
```

```
'General'
```

Como identificamos palavras que ficam entre caracteres específicos? Vamos tentar encontrar uma correspondência de uma frase entre aspas assumindo que ela inclui apenas letras e espaços.

\[A-z\] - esse padrão encontra uma correspondência com qualquer letra única, seja ela maiúscula ou minúscula. Esse padrão encontraria cada letra única na string, mas como letras separadas, e não como palavras. Observe que `[`, `\\`, `]`, `^`, `_` e \`\`\` também serão incluídos nos padrões descritos abaixo.

\[A-z \] (observe o espaço no final) - esse padrão indica que o resultado também pode incluir espaços, mas ainda assim ele vai encontrar uma correspondência com apenas um caractere de cada vez, seja uma letra ou um espaço.

\[A-z \]+ — esse padrão encontra uma correspondência com uma sequência que contenha esses caracteres até algum outro caractere encontrado ou até o final da string. Então todos os caracteres não incluídos na lista (como dígitos ou sinais de pontuação) basicamente se tornam separadores. Se aplicarmos esse padrão à nossa string, receberemos quatro correspondências: `General Slocum`, um espaço único, `June` (observe os espaços ao redor da palavra) e `East River human factor`.

"\[A-z \]+" — esse padrão encontra uma correspondência com uma aspa dupla, uma sequência de letras e espaços (1 ou mais, sem qualquer limite superior) e mais uma aspa dupla. Então ele retorna `"General Slocum"` (incluindo, desta vez, as aspas) - é exatamente o que estávamos procurando.

```
import re

string = '"General Slocum" 15 June 1904 East River human factor'
print(re.search('"[A-z ]+"', string).group())
```

```
"General Slocum"
```

Observe que se os conteúdos entre as aspas incluíssem quaisquer caracteres que não estejam na lista, eles impediriam que o conteúdo correspondesse ao padrão. É por isso que adicionamos um espaço. Expressões regulares não são uma ciência exata.

2.  **split(pattern, string)** divide a `string` nos pontos onde aparecer o padrão `pattern`.

```
import re

string = '"General Slocum" 15 June 1904 East River human factor'
print(re.split('\d+', string))
```

```
['"General Slocum" ', ' June ', ' East River human factor']
```

Como você pode ver, a string foi dividida em três partes nos pontos onde o método encontrou o padrão especificado no argumento. Aqui o padrão `'\d+'`encontra a correspondência com um ou mais dígitos. Em todos os lugares onde `split()` encontrou um ou mais dígitos (nesse caso, 15 e 1904), ele dividiu a string em substrings. As substrings que correspondem ao padrão foram excluídas e não estão presentes no resultado.

O parâmetro **maxsplit** do método `split()` limita o número de vezes que você pode dividir a string.

```
import re

string = '"General Slocum" 15 June 1904 East River human factor'
print(re.split('\d+', string, maxsplit = 1))
```

```
['"General Slocum" ', ' June 1904 East River human factor']
```

Como você pode ver, a string foi dividida apenas uma vez.

3.  **sub(pattern, repl, string)** procura pela substring correspondente ao padrão `pattern` dentro da `string` e a substitui pela substring **repl** (replace, o que significa substituir).

```
import re

string = '"General Slocum" 15 June 1904 East River human factor'
print(
    re.sub('\d+', '', string)
)  # estamos procurando por sequências de 1 ou mais dígitos
# e removemos elas
```

```
'"General Slocum"  June  East River human factor'
```

Todas as substrings compostas de dígitos foram removidas (tecnicamente falando, foram substituídas com uma string vazia entre aspas simples: `''`).

4.  **findall(pattern, string)** retorna uma lista de todas as substrings na `string` que correspondem ao padrão `pattern`. Compare-o com o método `search()` que retorna apenas a primeira substring correspondente. Vamos tentar encontrar todas as palavras que terminam com `"tion"` :

```
import re

tion = "Arrived at the station in total frustration"
print(re.findall('[A-z]+tion', tion))
```

```
['station', 'frustration']
```

.. ou palavras com um hífen:

```
import re

string = 'sixty-seven drops of rain'
print(re.findall('\w+-\w+', string))
```

```
['sixty-seven']
```

O padrão encontra correspondências em palavras com hífen; neste caso, a única correspondência é "sixty-seven". O método **findall()** é muito útil quando usado junto com a função `len()` porque ele permite determinar quantas vezes uma substring é repetida.

```
import re

string = 'sixty-seven drops of rain'
print(len(re.findall('\w+', string)))
```

```
5
```

Você estava esperando que `len()` imprimisse 4? Ela retornou 5 porque contou "sixty-seven" como duas palavras; o padrão `'\w+'` não captura o hífen.

Vamos adicionar _`'-'`_ à expressão regular e contar "sixty-seven" como uma única palavra.

```
import re

string = 'sixty-seven drops of rain'
print(len(re.findall('[\w-]+', string)))
```

```
4
```

Agora que você está familiarizado com as expressões regulares, com os padrões mais comuns e suas combinações, você poderá encontrar tudo o que quiser em qualquer texto.

Vamos praticar um pouco neste site útil [regex101.com](https://regex101.com/)! Investigue os problemas deste [PDF](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_6/Data_6_sprint_Prtica_de_Expresso_Regular_ptbr.pdf?etag=1dde3cc3f816a00eeaccef7a5fb004d2).

### Padrões para diferentes idiomas

Diferentes idiomas usam diferentes conjuntos de símbolos. O inglês usa 26 letras, enquanto o russo usa 33. O chinês e o japonês utilizam caracteres em vez de letras.

Alguns idiomas usam diacríticos (acentos) para mostrar como palavras devem ser pronunciadas. O francês, o espanhol e o português utilizam diacríticos para diferenciar palavras que pareceriam iguais sem eles. No final das contas, o uso de diferentes símbolos em diferentes idiomas faz com que a comunicação entre as pessoas seja mais variada e interessante.

No entanto, isso torna a leitura desses idiomas mais difícil. Não podemos mais usar um padrão simples `[A-z]` para capturar todas as letras desses idiomas. Precisamos ter isso em mente quando trabalhamos com eles.

Aqui está uma expressão que captura todas as letras francesas, espanholas e portuguesas:

`[a-zA-Z àâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ]`

Como você pode ver, simplesmente adicionamos manualmente todos os caracteres necessários, e você pode fazer a mesma coisa se alguma vez se deparar com uma palavra que o seu padrão inicial não capture. Aqui está uma versão simplificada que não irá capturar todas as ligaduras do francês, mas vai funcionar para todos os outros casos.

`[a-zA-ZÀ-ÿ]`

Ou você pode adicionar as ligaduras que estão faltando e usar esta versão:

`[a-zA-Z àâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ]`

Aqui estão mais algumas fontes úteis para aprender mais sobre regex:

-   [https://regexone.com](https://regexone.com/) (lições curtas e prática)
-   [https://docs.python.org/3/howto/regex.html#regex-howto](https://docs.python.org/3/howto/regex.html#regex-howto) (introdução a regex, sem prática)
-   [https://github.com/ziishaned/learn-regex](https://github.com/ziishaned/learn-regex) (mais uma introdução)
-   [https://regexr.com](https://regexr.com/) (uma ferramenta para testar expressões regulares e ver como elas funcionam)
-   [https://www.debuggex.com/cheatsheet/regex/python](https://www.debuggex.com/cheatsheet/regex/python) (uma referência prática)

Expressões Regulares

Tarefa3 / 3

1.

Escreva uma requisição GET para a página web [https://tripleten-com.github.io/simple-shop\_pt-br/](https://tripleten-com.github.io/simple-shop_pt-br/) (os materiais estão em inglês). Salve o conteúdo textual da requisição na variável `req_text`. Depois escreva uma expressão regular para encontrar as tags <title> </title> e seus conteúdos. Imprima o resultado da seguinte forma: <title>Texto da tag</title>.

Observe que estamos procurando por uma única ocorrência, por isso desta vez vamos usar `re.search()`.

2.

Escreva uma expressão regular que retornará todos os nomes de produtos que incluem a palavra "Manteiga" (observe a letra maiúscula). "Manteiga" pode aparecer no meio do nome. Então tal nome _pode_ ter 0 ou mais letras ou espaços antes de "Manteiga" e _pode_ ter algumas letras ou espaços após "Manteiga". Não precisamos incluir o peso dos produtos em nossos nomes, então não inclua a vírgula em sua expressão regular. Imprima o que você encontrar.

Agora precisamos de múltiplas ocorrências, então vamos usar `re.findall()`

3.

Escreva uma expressão regular para encontrar todos os nomes de produtos que contenham a palavra "Horizon" (uma marca). Ela pode aparecer apenas no início de um nome. Não inclua o peso, apenas o nome do produto.

Os nomes dos produtos podem conter letras, dígitos, espaços, hifens e sinais de porcentagem. Salve o resultado na variável _found\_products_. Imprima o comprimento da lista (o número de produtos) e a própria lista em linhas separadas.

9

1

2

3

4

5

6

7

8

import requests

import re

  

URL \= 'https://tripleten-com.github.io/simple-shop\_pt-br/'

req\_text \= requests.get(URL).text

found\_products \= re.findall('Horizon\[ \\w\\-%\]+', req\_text)

print(len(found\_products))

print(found\_products)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-49-461Z.md
### Última modificação: 2025-05-28 19:36:49

# Análise sintática de HTML - TripleTen

Teoria

# Análise sintática de HTML

Você enviou uma requisição GET e obteve o seguinte código:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6.2.9PT_1661759306.png)

É um código bem legal, mas o problema é que extrair valores puros dos dados desta string será muito difícil. Para resolver o problema, precisamos pedir ajuda a... **BeautifulSoup!** Isso é uma biblioteca cujo nome é uma referência à sopa de tags (código desleixado e não estruturado).

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_____BeautifulSoup_1651843757.jpg)

Os métodos da BeautifulSoup transformam um arquivo HTML em uma estrutura de árvore. Então os conteúdos necessários podem ser encontrados através de tags e atributos.

![](https://practicum-content.s3.amazonaws.com/resources/PT6.2.8.2_1705569466.png)

Vamos importar a biblioteca e criar um objeto BeautifulSoup:

```
from bs4 import BeautifulSoup

soup=BeautifulSoup(req.text, 'lxml')
```

O primeiro argumento apresenta os dados que irão formar a estrutura de árvore. O segundo argumento é um analisador sintático. Ele define a maneira como uma página web será transformada em uma árvore. Existem vários analisadores sintáticos, e se você passar para eles o mesmo documento HTML, todos eles irão gerar diferentes estruturas. Nós escolhemos o analisador **lxml** por causa do seu alto desempenho, mas há outros, como html.parser, xml e html5lib.

### Navegando pela árvore

Agora que transformamos o código em uma árvore, vamos começar a recuperar dados!

O primeiro método de busca se chama **find()**. Ele percorre a árvore, encontra o primeiro elemento cujo nome foi passado como um argumento e retorna-o junto com as tags e o conteúdo do elemento. Vamos tentar encontrar o primeiro cabeçalho de segundo nível:

```
heading_2=soup.find("h2")
print(heading_2)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6.2.8.3PT_1661759684.png)

Para exibir o conteúdo sem tags você precisará do método **text**. Ele vai retornar uma string:

```
heading_2.text
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6.2.8.4PT_1661759433.png)

Existe um outro método de busca que se chama **find\_all.** Ao contrário do método anterior, `find_all()` encontra _todas_ as ocorrências de um determinado elemento na árvore e retorna uma lista.

```
paragraph = soup.find_all(
    'p'
) # lembrete: p significa parágrafo, o texto entre as tags <p> e </p>
print(paragraph)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6.2.8.5PT_1661759797.png)

Vamos usar o método text para extrair apenas o conteúdo destes parágrafos:

```
for paragraph in soup.find_all('p'):
    print(paragraph.text)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6.2.8.6PT_1661759757.png)

Os métodos `find()` e `find_all()` têm um filtro adicional para procurar por elementos de uma página. Ele é o parâmetro **attrs** (atributos).

Este parâmetro caça classes e identificadores. Você pode encontrar os nomes deles na barra de ferramentas de desenvolvedor:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT6.2.8.7_1661759488.png)

Você precisa passar para o parâmetro _attrs_ um dicionário com os nomes e valores dos atributos. Veja como podemos procurar por um elemento identificado como `"ten_years_first"`:

```
soup.find("table",attrs={"id": "ten_years_first"})
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT6.2.8.8_1661759516.png)

Agora vamos deixar a nossa tarefa um pouco mais complexa. Vamos tentar extrair a tabela que contém informações sobre os naufrágios que aconteceram desde a primeira década do século XX e transformá-la em um dataframe:

```
table = soup.find('table',attrs={"id": "ten_years_first"})
# aplicando o método find à tag table
# especificando o atributo da primeira tabela: ten_years_first
print(table)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6.2.8.9PT_1661759546.png)

Não se esqueça de que a tag de abertura `<table>` marca o início da tabela e a tag de fechamento `</table>` marca o final. Dentro dessas tags há também as tags das linhas (<tr>), das células (<td>) e dos cabeçalhos das colunas (<th>).

Vamos criar uma lista vazia, `heading_table`, e salvar nela os cabeçalhos das colunas. Depois usaremos o método `find_all()`para encontrar todos os elementos `th`. Ao usar o método `text`, vamos extrair o conteúdo e adicioná-lo à lista `heading_table`:

```
heading_table = []  # a lista na qual serão armazenados os dados da tabela
for row in table.find_all(
    'th'
):  # Os nomes das colunas estão entre os elementos <th>,
   # então vamos encontrar todos os elementos <th> na tabela e percorrê-los usando um ciclo
    heading_table.append(
        row.text
    )  # Adicione o conteúdo da tag <th> à lista heading_table usando append()
print(heading_table)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6.2.8.10PT_1661759582.png)

Agora criamos uma lista vazia chamada `content` e salvamos nela os dados da tabela. Percorremos todas as linhas encontrando todos os elementos `tr`.

Por favor, observe que nós não estamos interessados na primeira linha da tabela que possui os cabeçalhos entre as tags `<th> </th>`. Então precisamos especificar que ela não deve ser incluída na lista escrevendo `if not row.find_all('th')` no ciclo.

Agora aplicamos o método `find_all()` aos elementos `td` e usamos o método _text_ para remover tags das novas células. Adicionamos as células à lista `content`.

```
content = []  # a lista na qual serão armazenados os dados da tabela
for row in table.find_all('tr'):
    # Cada linha é colocada dentro das tags <tr>, então precisamos percorrer todas as linhas
    if not row.find_all('th'):
        # Precisamos desta condição para ignorar a primeira linha da tabela que contém os cabeçalhos
        content.append([element.text for element in row.find_all('td')])
        # Dentro de cada linha, o conteúdo das células é envolvido nas tags <td> </td>
        # Precisamos percorrer todos os elementos <td>, extrair os conteúdos das células e adicioná-los à lista
        # Depois adicione todas as listas à lista content
print(content)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6.2.8.10PT_1664867224.png)

Ficamos com duas listas. Guardamos os nomes das colunas na lista `heading_table`, enquanto a lista `content`contém o conteúdo da tabela na forma de um vetor bidimensional.

```
import pandas as pd

shipwrecks = pd.DataFrame(content, columns=heading_table)
# passe a lista bidimensional content como "data" e a lista heading_table como "headings"
shipwrecks.head()
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6.2.8.11_1664867291.png)

Transformamos a tabela do site em algo que você já conhece - um dataframe! Agora podemos trabalhar com ele da mesma maneira que trabalhamos com tabelas.

Análise sintática de HTML

Tarefa4 / 4

1.

Faça um objeto BeautifulSoup a partir da página HTML. Salve seus resultados na variável `soup`.

2.

Obtenha os nomes de produtos [https://tripleten-com.github.io/simple-shop\_pt-br/](https://tripleten-com.github.io/simple-shop_pt-br/) e adicione-os à lista `name_products`.

Identifique os nomes das tags e seus atributos na barra de ferramentas do desenvolvedor. Obtenha as tags necessárias usando `find_all()`.

Imprima a lista resultante.

3.

Obtenha os preços dos produtos de [https://tripleten-com.github.io/simple-shop\_pt-br/](https://tripleten-com.github.io/simple-shop_pt-br/) e adicione-os à lista `price`.

Identifique os nomes das tags e seus atributos na barra de ferramentas do desenvolvedor. Obtenha as tags necessárias usando `find_all()`.

Imprima a lista resultante.

4.

Adicione dados a um DataFrame vazio. Crie as colunas `'name'` e `'price'` e preencha-as com os dados das listas `name_products` e `price`. Imprima as primeiras 5 linhas do dataframe. As colunas devem aparecer nesta ordem: `name`, `price`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

import pandas as pd

import requests \# Importe a biblioteca de requisições enviadas ao servidor

from bs4 import BeautifulSoup \# Importe a biblioteca para análise sintática da página web

URL \= 'https://tripleten-com.github.io/simple-shop\_pt-br/'

req \= requests.get(URL) \# Requisição GET

soup \= BeautifulSoup(req.text, 'lxml')

  

name\_products \= \[\] \# A lista onde os nomes dos produtos estão armazenados

for row in soup.find\_all(

'p', attrs\={'class': 't754\_\_title t-name t-name\_md js-product-name'}

):

name\_products.append(row.text)

price \= \[\] \# A lista onde os preços dos produtos estão armazenados

for row in soup.find\_all(

'p', attrs\={'class': 't754\_\_price-value js-product-price'}

):

price.append(row.text)

products\_data \= (

pd.DataFrame()

) \# DataFrame com os dados sobre os nomes e preços dos produtos

products\_data\['name'\] \= name\_products

products\_data\['price'\] \= price

print(products\_data.head(5))

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-50-788Z.md
### Última modificação: 2025-05-28 19:36:51

# API - TripleTen

Teoria

# API

Você aprendeu sobre as requisições GET e até escreveu sua própria requisição.

Às vezes precisamos requisitar informação de fontes externas que possuem uma estrutura muito mais complexa de que uma página HTML simples. Para evitar a necessidade de estudar sua estrutura para obter dados mais rapidamente, os analistas enviam solicitações GET para aplicações de terceiros via uma interface especial de transferência de dados chamada **API** (do inglês "Application Programming Interface", que significa "interface de programação de aplicativos").

Primeiro, vamos nos certificar de que entendemos direito o que é uma interface. Em poucas palavras, é uma maneira simples de interagir com sistemas complexos. Se você quer aumentar ou diminuir o volume da música no seu carro, você pressiona um botão. Você não precisa compreender os detalhes de como os alto-falantes de seu carro funcionam para usá-los.

De forma parecida, uma API permite que os desenvolvedores interajam com sistemas mesmo sem compreender exatamente como eles funcionam. Para fazê-lo, a API te oferece "instruções", um conjunto de métodos específicos.

Se você não compreende como a eletricidade é produzida e fornecida, sua API de eletricidade é uma tomada. Para interagir com esta API, você precisa de um método: um plugue de tomada.

![](https://practicum-content.s3.amazonaws.com/resources/PT6.2.10_1709711217.png)

Grandes corporações como Facebook, Amazon ou Google desenvolvem APIs para seus clientes e também para uso interno. Por exemplo, entrar em uma conta de uma rede social envolve o uso de uma API.

**API de previsão do tempo**

Um serviço meteorológico tem uma API que fornece informações sobre o clima através da URL: [http://wttr.in/](http://wttr.in/). Normalmente, para trabalhar com uma API, você precisa primeiro se cadastrar em um site, mas essa API é aberta a todos. Você pode clicar no link e conferir tudo que a API pode fornecer, inclusive uma previsão completa de três dias para a localização do seu endereço de IP. Ao acessar essa URL, é importante incluir o nome de uma cidade no endpoint da URL para receber previsões de outras cidades ou locais. Por exemplo, a URL [https://wttr.in/Lisbon](https://wttr.in/Lisbon) vai exibir a previsão do tempo de Lisboa, em Portugal.

Você pode usar diferentes parâmetros para formatar como os resultados são apresentados. Por isso, vamos usar uma solicitação GET para acessar informações.

### Solicitações GET com parâmetros

A biblioteca **requests** te permite passar parâmetros para uma URL. Ao procurar determinados conteúdos em um site com múltiplas páginas, você pode passar o dicionário PARAM para a palavra-chave **params** (ou seja, "parâmetros"). Isso vai facilitar seu trabalho, já que a alternativa seria construir uma URL complexa com os parâmetros incorporados a ela. Por exemplo:

```
city = 'Lisbon'
URL = f'https://wttr.in/{city}'
PARAM={"format": 4, "M": ""}
req = requests.get(url = URL, params = PARAM)
```

Essa solicitação deve exibir a previsão atual para Lisboa, em Portugal, em um dos formatos padrões curtos (parâmetro `"format": 4`) e o vento em metros por segundo (parâmetro `"M": ""`).

Quando recuperamos dados usando uma API, passamos um dicionário com os parâmetros à palavra-chave `params` na requisição GET. Poderíamos passar esse parâmetro diretamente como uma parte da URL, adicionando o caractere `?` seguido pelo parâmetro e, no caso de vários parâmetros, podemos separar cada um usando o caractere `&`. O último exemplo ficaria assim: [https://wttr.in/Lisbon?M&format=4](https://wttr.in/Lisbon?M&format=4). Embora essa seja uma possibilidade, é muito melhor e mais fácil usar `params` e passar nele os parâmetros de forma organizada. Portanto, é isso que faremos.

Conforme a documentação da API do site acima, que pode ser consultada neste link [https://github.com/chubin/wttr.in](https://github.com/chubin/wttr.in), o resultado padrão é uma previsão completa de três dias. Alguns dos outros parâmetros para especificar outros tipos de previsão do tempo são os seguintes:

### Parâmetros para solicitar uma previsão do tempo

Nome do parâmetro

Descrição

Características

Valores

m

mostra os resultados em metric (SI)

opcional

“”

format

resposta com informações climáticas em vários formatos

opcional

de `1` a `4` para versões curtas, `j1` para resposta no formato JSON

`0`

apenas temperatura atual

opcional

“”

1

temperatura atual + previsão de hoje

opcional

“”

2

temperatura atual + previsão de hoje + de amanhã

opcional

“”

lang

código do idioma para tradução

opcional

veja [https://wttr.in/:translation](https://wttr.in/:translation)

Lembramos que o nome da cidade no endpoint é sempre necessária se quiser conferir a previsão do tempo para locais diferentes de onde você está. Sem isso, a API vai retornar dados padrões em vez da previsão desejada.

Vamos conferir a previsão para Paris agora no sistema métrico:

```
import requests

city = 'Paris'

BASE_URL = f'https://wttr.in/{city}'
# URL para o método get() 

PARAM={"format": 1, "m":""}

response = requests.get(BASE_URL, params = PARAM)
print(response.text)
```

```
☁️   +10°C
```

O servidor respondeu com uma previsão no formato de texto. Também há serviços que respondem no formato **JSON**. Nós vamos falar sobre isso na próxima lição. Por enquanto, pratique suas habilidade de obter dados com esse recurso climático.

API

Tarefa2 / 2

1.

Consiga uma previsão completa de três dias para Roma, na Itália, usando o sistema métrico.

2.

Obtenha uma previsão do tempo curta para Cancún, também usando o sistema métrico.

Imprima na tela os conteúdos textuais da requisição.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

import requests

  

city \= 'Cancún'

  

  

  

BASE\_URL \= f'https://wttr.in/{city}'

\# URL para o método get()

  

params \= {"m":"", "format":3}

  

  

response \= requests.get(BASE\_URL, params\=params)

print(response.text)

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-52-338Z.md
### Última modificação: 2025-05-28 19:36:52

# JSON - TripleTen

Teoria

Ao responder a sua requisição, o servidor retorna dados estruturados em um dos vários formatos especiais, sendo o mais comum entre eles o formato **JSON**. Ele se parece com uma mistura de dígitos, letras, dois pontos e chaves. A questão é: como podemos trabalhar com isso?

JSON significa JavaScript Object Notation (Notação de Objetos JavaScript)_._

Veja como ficam os dados neste formato:

```
[
  {
    "name": "General Slocum",
    "date": "June 15, 1904" 
  },
  {
    "name": "Camorta",
    "date": "May 6, 1902"
  },
  {
    "name": "Norge",
    "date": "June 28, 1904"
  }
]
```

Se o JSON incluir vários elementos, eles são escritos entre colchetes `[ ... ]`, assim como nas listas. Um objeto JSON individual se parece com um dicionário: é envolto em chaves e tem pares `key : value`.

A estrutura bem definida dos objetos JSON torna fácil pegar dados, transformá-los em uma string e passá-los para uma requisição HTTP. Essa é a essência de JSON: você junta dados dentro de um objeto (uma lista de pares `key : value`) e depois cria uma string a partir desse objeto para ser passada em uma requisição. O receptor transforma essa string novamente em um objeto.

Observe que as chaves são colocadas entre aspas duplas, o que é obrigatório em JSON. Seus valores podem ser strings, números, valores booleanos, valores nulos, vetores ou objetos:

```
{
  "name": "Camorta",
  "length": 86.9
  "decks": 3
  "date": "May 6, 1902",
  "details": {
    "place": "Bay of Bengal",
    "reason": "Force of nature"
  } 
}
```

O JSON começa com chaves (se contém um elemento) ou com colchetes (se contém uma lista de objetos). O JSON nunca contém funções, variáveis ou comentários.

## Modulo JSON

O Python tem um módulo embutido para trabalhar com dados no formato JSON:

```
import json
```

O método **json.loads()** converte as strings que estão no formato JSON:

```
x = '{"name": "General Slocum", "date": "June 15, 1904"}'
y = json.loads(x)

print('Name : {0}, date : {1}'.format(y['name'], y['date']))
```

```
# Resposta
Name : General Slocum, date : June 15, 1904
```

Nós imprimimos o nome do navio e a data do naufrágio. Mas como exatamente? Tínhamos uma string no formato JSON. O `json.loads()` transformou-a de modo que se tornou possível tratar os valores `name` e `date` com a função `print()`.

Nesse caso, o JSON continha informação sobre apenas um objeto. Na maioria das vezes é passada uma lista inteira:

```
x = '[{"name": "General Slocum", "date": "June 15, 1904"}, {"name": "Camorta", "date": "May 6, 1902"}]'
y = json.loads(x)
for i in y:
    print('Name : {0}, date : {1}'.format(i['name'], i['date']))
```

```
# Resposta
Name : General Slocum, date : June 15, 1904
Name : Camorta, date : May 6, 1902
```

Como você pode ver, há vários objetos aqui, e um ciclo foi chamado para listá-los.

O método **json.dumps()**, ao contrário, converte dados de Python para o formato JSON.

```
out = json.dumps(y)
print(out)
```

```
#Resposta
[{"name": "General Slocum", "date": "June 15, 1904"}, {"name": "Camorta", "date": "May 6, 1902"}]
```

Agora temos dados no formato JSON.

Resumo:

-   Use `json.loads()` para converter uma string JSON em um objeto Python (como um dicionário ou uma lista) para que você possa trabalhar com ele no seu código Python.
-   Use `json.dumps()` para converter um objeto Python em uma string JSON formatada.

JSON

Tarefa2 / 2

1.

Na lição anterior, você recebeu dados de um site sobre o clima, mas esses dados eram difíceis de manipular, porque havia muito texto se você não escolhesse as versões curtas. Extraia o clima completo de Paris de novo, mas, desta vez, solicite a resposta em **JSON** passando `j1` para o parâmetro `format`. Processe o conteúdo da resposta (`response.text`) com o método `json.loads()` e salve-o na variável `response_parsed`. Obtenha os dados com a **chave** `'current_condition'` e imprima os resultados.

2.

Obtenha os dados sobre o tempo atual e converta-os novamente no formato JSON. Processe os conteúdos textuais da resposta (`response.text`) com o método `json.loads()` e salve-os na variável `response_parsed`. Recupere os dados usando a chave `'current_condition'` e salve o resultado na variável `fact_weather`. Passe a variável para `json.dumps()` dentro do argumento da função `print()`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

import requests

import json

  

city \= 'Paris'

  

BASE\_URL \= f'https://wttr.in/{city}'

\# URL para o método get()

  

PARAM\={"format":"j1"}

  

  

response \= requests.get(BASE\_URL, params \= PARAM)

  

response\_parsed \= json.loads(response.text)

fact\_weather \= response\_parsed\['current\_condition'\]

print(json.dumps(fact\_weather))

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-53-648Z.md
### Última modificação: 2025-05-28 19:36:53

# Conclusão - TripleTen

Capítulo 2/8

Extraindo Dados de Recursos Online

# Conclusão

Tanta coisa feita! Você recuperou dados sobre produtos de uma loja online e também de uma previsão do tempo.

Você aprendeu

-   A estrutura das páginas HTML
-   Como funcionam requisições GET
-   As regras para escrever expressões regulares simples
-   Como processar dados com API e trabalhar com JSON

Nós salvamos os dados que você recuperou. Você vai trabalhar com eles nas lições a seguir.

### Leve isso com você

Baixe a [folha de conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_6/moved_Folha_de_Сoncluses_Extraindo_Dados_de_Recursos_Online.pdf) e o [sumário do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_6/moved_Resumo_do_Captulo_ExtraindoDados_Recursos_Online.pdf) para que você possa consultá-los sempre que precisar.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-54-981Z.md
### Última modificação: 2025-05-28 19:36:55

# Introdução - TripleTen

Capítulo 3/8

SQL como uma Ferramenta para Trabalhar com Dados

# Introdução

Neste capítulo, você aprenderá como os bancos de dados são estruturados, escreverá suas primeiras instruções SQL e as aprimorará com recursos úteis.

### O que você irá aprender:

-   Como escrever consultas SQL
-   Como obter fatias de dados usando o comando WHERE
-   Como você pode alterar os tipos de dados usando CAST
-   Como chamar as funções de agregação

### Quanto tempo vai demorar:

6 _aulas, com aproximadamente 15-20 minutos cada_

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-56-948Z.md
### Última modificação: 2025-05-28 19:36:57

# Bancos de Dados e Tabelas - TripleTen

Capítulo 3/8

SQL como uma Ferramenta para Trabalhar com Dados

# Bancos de Dados e Tabelas

Vamos falar sobre o que são os bancos de dados e como trabalhar com eles.

Imagine que um escritório tenha um armário com caixas marcadas da seguinte forma: “Funcionários”, “Relatórios” e “Fornecedores”. Cada caixa contém documentos. A caixa dos “Funcionários”, por exemplo, tem os perfis dos empregados, incluindo idade, sexo e cargo. Se você quiser saber a idade do Senhor Peterson, um engenheiro, não vai precisar de muito tempo. Os bancos de dados estão organizados do mesmo jeito.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/Data_sprint_6/PT/PT6.3.2.png)

O **banco de dados** é um local onde se armazenam dados de forma estruturada. Se você sabe como esse banco de dados é estruturado, então é fácil tirar informação dele.

Voltando para as nossas adoráveis caixas, observe que cada uma tem itens semelhantes. Os perfis de funcionários são guardados na caixa “funcionários”, enquanto a caixa “Fornecedores” tem os perfis de fornecedores, etc. Vamos chamá-los de entidades: grupos de objetos que possuem características comuns. Os objetos são instâncias individuais das entidades. Se a caixa “Funcionários” é uma entidade, então o perfil do Senhor Peterson é um objeto dela.

Os bancos de dados organizados como nossos armários são denominados **relacionais**. Assim como o armário armazenou os dados em caixas, os bancos de dados relacionais os armazenam em múltiplas tabelas.

Os funcionários trabalham com os fornecedores e podem ser mencionados em relatórios, fazendo com que os documentos de diferentes caixas estejam inter-relacionados. As tabelas em bancos de dados relacionais são entrelaçadas de maneira semelhante.

Mas a estante é inútil sem uma secretária. Você precisa de alguém que possa remover os perfis de ex-funcionários, adicionar os novos fornecedores, produzir relatórios e colocá-los no lugar certo. Para os bancos de dados, essa secretária é um **SGBD**, ou sistema de gerenciamento de banco de dados. Este é um conjunto de programas que permite criar um banco de dados, preenchê-lo com as novas tabelas, editar e exibir o conteúdo das tabelas existentes.

Você vai trabalhar com um dos SGBDs mais populares, **PostgreSQL.** Você pode encontrar sua documentação em [https://www.postgresql.org/docs/](https://www.postgresql.org/docs/) (a documentação está em inglês).

Pergunta

O que é um banco de dados?

um conjunto de programas que permite criar um banco de dados, preenchê-lo com novas tabelas, editar e exibir o conteúdo das tabelas existentes

um conjunto de linhas e colunas

um contêiner ou um armazenamento para os dados estruturados

Isso mesmo!

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-58-268Z.md
### Última modificação: 2025-05-28 19:36:58

# Tabelas - TripleTen

Capítulo 3/8

SQL como uma Ferramenta para Trabalhar com Dados

# Tabelas

A **tabela** é um conjunto de linhas e colunas. Vamos dar uma olhada na tabela a seguir: ela contém os nomes dos personagens do show _Silicon Valley_, funcionários da empresa Pied Piper.

As colunas são chamadas de **campos**. Eles contêm as características do objeto (nome, posição, pontos fortes e fracos):

![](https://practicum-content.s3.amazonaws.com/resources/PT6.3.3_1705925689.png)

Cada campo tem um nome unívoco e um tipo de dados específico.

As linhas da tabela são chamadas de **registros**. Cada linha contém informação sobre um objeto específico (no nosso caso, sobre um funcionário). Por exemplo, Donald Dunn é um gerente de operações, ele é bom em gerenciamento de processos e seu ponto fraco é que ele fala alemão enquanto dorme.

![](https://practicum-content.s3.amazonaws.com/resources/PT6.3.3.2_1705925796.png)

A **célula** é uma unidade na qual se intersectam um registro e um campo (linha e coluna). Por exemplo, Dinesh Chugtai é um desenvolvedor Java:

![](https://practicum-content.s3.amazonaws.com/resources/PT6.3.3.3_1705925733.png)

Neste exemplo, o número de cada linha serve como **chave primária**. Todas as tabelas de banco de dados precisam de pelo menos um campo de chave primária para definir inequivocamente cada registro.É por isso que todos os valores neste campo devem ser **unívocos**.

![](https://practicum-content.s3.amazonaws.com/resources/PT6.3.3.4_1705925760.png)

Algumas tabelas usam uma combinação de vários campos como chave primária, que é chamada de **chave primária composta**. Por exemplo, se a nossa tabela não tivesse um campo "Número", poderíamos usar uma combinação dos campos "Nome e sobrenome" e "Posição" como chave primária composta. Os valores de campos individuais podem não ser exclusivos, mas suas combinações são, o que nos permite identificar cada linha sem ambiguidade.

![](https://practicum-content.s3.amazonaws.com/resources/PT6.3.3.5_1705925778.png)

Pergunta

O registro de objeto na tabela de banco de dados é uma

coluna

célula

linha

chave primária

Excelente!

Pergunta

Qual é a característica mais importante de uma chave primária?

excepcionalidade

autenticidade

univocidade

Aham—as chaves têm que ser unívocas!

raridade

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-36-59-574Z.md
### Última modificação: 2025-05-28 19:36:59

# Sua primeira instrução SQL - TripleTen

Teoria

# Sua primeira instrução SQL

**SQL** (Structured Query Language) é uma linguagem de computação projetada para gerenciar dados em bancos de dados relacionais.

A sintaxe SQL é diferente da do Python. Aqui estão suas características básicas:

1) O início do comentário de linha única é marcado com dois hífens: `--`

```
-- um comentário de linha única em SQL
```

2) Um comentário de várias linhas é colocado entre `/*` e `*/`:

```
/* um comentário de várias linhas
em
várias 
linhas */
```

3) Idealmente, toda instrução (ou consulta) termina com um ponto e vírgula `;`. No entanto, isso não é obrigatório. Por exemplo, você pode fazer instruções simples sem usar um ponto e vírgula, mas é melhor ter o hábito de sempre usar um:

```
SELECT
    *
FROM
    table_name;

-- A instrução que solicita todos os dados da tabela termina com ";"
SELECT
    *
FROM
    table_name
WHERE
    column_name IN (1, 7, 9);

-- A instrução que seleciona dados por condição também termina com ";"
```

4) Os comandos são escritos em letras maiúsculas (habitualmente, mas não obrigatórios):

```
SELECT, WHERE, FROM 
```

5) Adicionam-se quebras de linha após cada palavra-chave (habitualmente, mas não obrigatória):

```
SELECT
    column_1,
    column_2,
    column_3,
    column_4
FROM
    table_name
WHERE
    column_1 = value_1
    AND column_2 = value_2
    AND column_4 = value_3;
```

### Instruções

Para selecionar os dados de tabelas, você precisa escrever uma **instrução** ou **consulta**. A **instrução** é um pedido escrito de acordo com a sintaxe SQL. Sua instrução deve especificar quais dados selecionar e como processá-los.

### SELECT

O operador **SELECT** faz a seleção que você precisa. As instruções SELECT são assim:

```
SELECT
    column_1,
    column_2,
    column_3...
FROM
    table_name;

--Selecionar as colunas da tabela
```

Temos duas palavras-chave em nossa instrução: **SELECT** e **FROM**. SELECT especifica as colunas necessárias da tabela do banco de dados. FROM especifica a tabela da qual os dados deveriam ser obtidos. Para manter as coisas organizadas e fáceis de ler, indentamos as linhas após os operadores.

Agora vamos ver como essa instrução funciona tentando obter os dados das colunas na tabela `books`:

name

book\_id

genre

author

date\_pub

pages

rating

publisher\_id

The City of Mirrors

1

Fantasy

Justin Cronin

2016/01/01

602

4.2

5

The October Country

2

Fiction

Ray Bradbury

1955/01/01

334

4.16

5

Saint Odd

3

Fiction

Dean Koontz

2015/01/01

352

4.14

8

From the Corner of His Eye

4

Thriller

Dean Koontz

2000/01/01

729

4.01

8

The Great and Secret Show

5

Fiction

Clive Barker

1990/01/01

658

4.05

15

World War Z: An Oral History of the Zombie War

6

Horror

Max Brooks

2006/01/01

342

4.01

19

Red Dragon

7

Thriller

Thomas Harris

1981/01/01

348

4.02

17

The Stand

8

Fantasy

Stephen King

1978/01/01

816

4.34

2

Lightning

9

Thriller

Dean Koontz

1988/01/01

384

4.06

14

The Exorcist

10

Classics

William Peter Blatty

1971/01/01

385

4.15

7

The Witching Hour

11

Fiction

Anne Rice

1990/01/01

1038

4.11

10

The Vampire Lestat

12

Paranormal

Anne Rice

1985/01/01

550

4.06

10

Intensity

13

Thriller

Dean Koontz

1995/01/01

436

4.04

10

The Terror

14

Historical Fiction

Dan Simmons

2007/01/01

769

4.04

16

House of Leaves

15

Fantasy

Mark Z. Danielewski

2000/01/01

705

4.12

9

Swan Song

16

Apocalyptic

Robert R. McCammon

1987/01/01

956

4.28

3

Boy's Life

17

Fantasy

Robert R. McCammon

1991/01/01

580

4.34

3

Summer of Night

18

Thriller

Dan Simmons

1991/01/01

600

4.02

18

Full Dark, No Stars

19

Short Stories

Stephen King

2010/01/01

368

4.05

11

Doctor Sleep

20

Thriller

Stephen King

2013/01/01

531

4.1

11

The Green Mile

21

Fantasy

Stephen King

1996/01/01

465

4.43

0

Rot & Ruin

22

Young Adult

Jonathan Maberry

2010/01/01

458

4.1

12

Let the Right One In

23

Paranormal

John Ajvide Lindqvist

2008/01/01

513

4.07

6

The Silence of the Lambs

24

Thriller

Thomas Harris

1988/01/01

367

4.17

6

It

25

Fantasy

Stephen King

1986/01/01

1116

4.22

1

Different Seasons

26

Short Stories

Stephen King

1982/01/01

560

4.35

1

Misery

27

Thriller

Stephen King

1987/01/01

320

4.13

1

NOS4A2

28

Fantasy

Joe Hill

2013/01/01

692

4.06

13

This Book Is Full of Spiders

29

Humor

David Wong

2012/01/01

406

4.26

End of Watch

30

Thriller

Stephen King

2016/01/01

432

4.09

```
SELECT
    name,
    author
FROM
    books;

--Obtendo as colunas contendo autores e títulos da tabela de livros
```

Aqui está o resultado da nossa consulta:

name

author

The City of Mirrors

Justin Cronin

The October Country

Ray Bradbury

Saint Odd

Dean Koontz

From the Corner of His Eye

Dean Koontz

The Great and Secret Show

Clive Barker

World War Z: An Oral History of the Zombie War

Max Brooks

Red Dragon

Thomas Harris

The Stand

Stephen King

Lightning

Dean Koontz

The Exorcist

William Peter Blatty

The Witching Hour

Anne Rice

The Vampire Lestat

Anne Rice

This Book Is Full of Spiders

David Wong

Intensity

Dean Koontz

End of Watch

Stephen King

The Terror

Dan Simmons

House of Leaves

Mark Z. Danielewski

Swan Song

Robert R. McCammon

Boy's Life

Robert R. McCammon

Summer of Night

Dan Simmons

Full Dark, No Stars

Stephen King

Doctor Sleep

Stephen King

The Green Mile

Stephen King

Rot & Ruin

Jonathan Maberry

Let the Right One In

John Ajvide Lindqvist

The Silence of the Lambs

Thomas Harris

It

Stephen King

Different Seasons

Stephen King

Misery

Stephen King

NOS4A2

Joe Hill

Para selecionar todas as colunas da tabela, adicione `*` ao operador `SELECT`:

```
SELECT
    *
FROM
    books;
```

O resultado da consulta será uma cópia completa da tabela `books`:

name

book\_id

genre

author

date\_pub

pages

rating

publisher\_id

The City of Mirrors

1

Fantasy

Justin Cronin

2016/01/01

602

4.2

5

The October Country

2

Fiction

Ray Bradbury

1955/01/01

334

4.16

5

Saint Odd

3

Fiction

Dean Koontz

2015/01/01

352

4.14

8

From the Corner of His Eye

4

Thriller

Dean Koontz

2000/01/01

729

4.01

8

The Great and Secret Show

5

Fiction

Clive Barker

1990/01/01

658

4.05

15

World War Z: An Oral History of the Zombie War

6

Horror

Max Brooks

2006/01/01

342

4.01

19

Red Dragon

7

Thriller

Thomas Harris

1981/01/01

348

4.02

17

The Stand

8

Fantasy

Stephen King

1978/01/01

816

4.34

2

Lightning

9

Thriller

Dean Koontz

1988/01/01

384

4.06

14

The Exorcist

10

Classics

William Peter Blatty

1971/01/01

385

4.15

7

The Witching Hour

11

Fiction

Anne Rice

1990/01/01

1038

4.11

10

The Vampire Lestat

12

Paranormal

Anne Rice

1985/01/01

550

4.06

10

Intensity

13

Thriller

Dean Koontz

1995/01/01

436

4.04

10

The Terror

14

Historical Fiction

Dan Simmons

2007/01/01

769

4.04

16

House of Leaves

15

Fantasy

Mark Z. Danielewski

2000/01/01

705

4.12

9

Swan Song

16

Apocalyptic

Robert R. McCammon

1987/01/01

956

4.28

3

Boy's Life

17

Fantasy

Robert R. McCammon

1991/01/01

580

4.34

3

Summer of Night

18

Thriller

Dan Simmons

1991/01/01

600

4.02

18

Full Dark, No Stars

19

Short Stories

Stephen King

2010/01/01

368

4.05

11

Doctor Sleep

20

Thriller

Stephen King

2013/01/01

531

4.1

11

The Green Mile

21

Fantasy

Stephen King

1996/01/01

465

4.43

0

Rot & Ruin

22

Young Adult

Jonathan Maberry

2010/01/01

458

4.1

12

Let the Right One In

23

Paranormal

John Ajvide Lindqvist

2008/01/01

513

4.07

6

The Silence of the Lambs

24

Thriller

Thomas Harris

1988/01/01

367

4.17

6

It

25

Fantasy

Stephen King

1986/01/01

1116

4.22

1

Different Seasons

26

Short Stories

Stephen King

1982/01/01

560

4.35

1

Misery

27

Thriller

Stephen King

1987/01/01

320

4.13

1

NOS4A2

28

Fantasy

Joe Hill

2013/01/01

692

4.06

13

This Book Is Full of Spiders

29

Humor

David Wong

2012/01/01

406

4.26

End of Watch

30

Thriller

Stephen King

2016/01/01

432

4.09

Você recebeu acesso ao banco de dados e pediu para estudar suas tabelas. Dê uma olhada no que está dentro.

Sua primeira instrução SQL

Tarefa4 / 4

1.

Selecione as seguintes colunas da tabela `products_data_all`: `id_product`, `name`, `category`, `name_store`, nessa ordem.

2.

Selecione todas as colunas da tabela `products_data_all`.

3.

Os clientes fazendo compras geralmente recebem um bônus quando apresentam os comprovantes de compras anteriores. Os dados dos comprovantes foram coletados da tabela `transactions`. Selecione todas as colunas da tabela e estude-as.

4.

Temos uma tabela com os dados sobre o clima. Chama-se — espere por isso! — `weather`_._ Escreva uma instrução para selecionar todos os dados desta tabela.

9

1

2

SELECT \* from weather \--escreva seu código aqui

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-37-01-898Z.md
### Última modificação: 2025-05-28 19:37:02

# Fatias de Dados em SQL - TripleTen

Teoria

# Fatias de Dados em SQL

Você trabalhou com fatias de dados na parte do curso sobre análise de gráficos e diagramas. Lembre-se de que fazer uma fatia permite filtrar dados irrelevantes, deixando apenas o que você precisa para sua análise. Você sabe como obter as fatias de dados usando o método _query()_ em Python. Agora é hora de aprender como fazer isso em SQL.

O início da condição usada para selecionar os dados é marcado com o comando **WHERE**. A condição é avaliada em cada linha da tabela:

```
SELECT
    column_1,
    column_2 --selecionando os nomes de colunas
FROM
    table_name --especificando a tabela
WHERE
    condition;

--definindo a condição de seleção de linha
```

A ordem dos operadores é estritamente definida:

1)`SELECT`

2)`FROM`

3)`WHERE`

Observe que o bloco WHERE é mais geral que SELECT: na verdade, ele é processado primeiro. WHERE filtra os dados da tabela como um todo, enquanto SELECT restringe a seleção.

O que as condições devem conter? Os operadores de comparação que você já conhece (mas observe que escrevemos =, não ==):

Name

Meaning

\=

igual a

<>, !=

diferente de

\>

maior que

<

menor que

\>=

maior ou igual a

<=

menor ou igual a

Agora vamos voltar para a tabela `books` e escrever uma consulta com uma condição.

name

book\_id

genre

author

date\_pub

pages

rating

publisher\_id

The City of Mirrors

1

Fantasy

Justin Cronin

2016/01/01

602

4.2

5

The October Country

2

Fiction

Ray Bradbury

1955/01/01

334

4.16

5

Saint Odd

3

Fiction

Dean Koontz

2015/01/01

352

4.14

8

From the Corner of His Eye

4

Thriller

Dean Koontz

2000/01/01

729

4.01

8

The Great and Secret Show

5

Fiction

Clive Barker

1990/01/01

658

4.05

15

World War Z: An Oral History of the Zombie War

6

Horror

Max Brooks

2006/01/01

342

4.01

19

Red Dragon

7

Thriller

Thomas Harris

1981/01/01

348

4.02

17

The Stand

8

Fantasy

Stephen King

1978/01/01

816

4.34

2

Lightning

9

Thriller

Dean Koontz

1988/01/01

384

4.06

14

The Exorcist

10

Classics

William Peter Blatty

1971/01/01

385

4.15

7

The Witching Hour

11

Fiction

Anne Rice

1990/01/01

1038

4.11

10

The Vampire Lestat

12

Paranormal

Anne Rice

1985/01/01

550

4.06

10

Intensity

13

Thriller

Dean Koontz

1995/01/01

436

4.04

10

The Terror

14

Historical Fiction

Dan Simmons

2007/01/01

769

4.04

16

House of Leaves

15

Fantasy

Mark Z. Danielewski

2000/01/01

705

4.12

9

Swan Song

16

Apocalyptic

Robert R. McCammon

1987/01/01

956

4.28

3

Boy's Life

17

Fantasy

Robert R. McCammon

1991/01/01

580

4.34

3

Summer of Night

18

Thriller

Dan Simmons

1991/01/01

600

4.02

18

Full Dark, No Stars

19

Short Stories

Stephen King

2010/01/01

368

4.05

11

Doctor Sleep

20

Thriller

Stephen King

2013/01/01

531

4.1

11

The Green Mile

21

Fantasy

Stephen King

1996/01/01

465

4.43

0

Rot & Ruin

22

Young Adult

Jonathan Maberry

2010/01/01

458

4.1

12

Let the Right One In

23

Paranormal

John Ajvide Lindqvist

2008/01/01

513

4.07

6

The Silence of the Lambs

24

Thriller

Thomas Harris

1988/01/01

367

4.17

6

It

25

Fantasy

Stephen King

1986/01/01

1116

4.22

1

Different Seasons

26

Short Stories

Stephen King

1982/01/01

560

4.35

1

Misery

27

Thriller

Stephen King

1987/01/01

320

4.13

1

NOS4A2

28

Fantasy

Joe Hill

2013/01/01

692

4.06

13

This Book Is Full of Spiders

29

Humor

David Wong

2012/01/01

406

4.26

End of Watch

30

Thriller

Stephen King

2016/01/01

432

4.09

Vamos selecionar todos os livros escritos por Stephen King:

```
SELECT
    name,
    author
FROM
    books
WHERE
    author = 'Stephen King';

--strings colocam-se entre aspas, como em Python
```

name

author

The Stand

Stephen King

End of Watch

Stephen King

Full Dark, No Stars

Stephen King

Doctor Sleep

Stephen King

The Green Mile

Stephen King

It

Stephen King

Different Seasons

Stephen King

Misery

Stephen King

... e agora todos os livros não escritos por Stephen King:

```
SELECT
    name,
    author
FROM
    books
WHERE
    author != 'Stephen King';
```

name

author

The City of Mirrors

Justin Cronin

The October Country

Ray Bradbury

Saint Odd

Dean Koontz

From the Corner of His Eye

Dean Koontz

The Great and Secret Show

Clive Barker

World War Z: An Oral History of the Zombie War

Max Brooks

Red Dragon

Thomas Harris

Lightning

Dean Koontz

The Exorcist

William Peter Blatty

The Witching Hour

Anne Rice

The Vampire Lestat

Anne Rice

This Book Is Full of Spiders

David Wong

Intensity

Dean Koontz

The Terror

Dan Simmons

House of Leaves

Mark Z. Danielewski

Swan Song

Robert R. McCammon

Boy's Life

Robert R. McCammon

Summer of Night

Dan Simmons

Rot & Ruin

Jonathan Maberry

Let the Right One In

John Ajvide Lindqvist

The Silence of the Lambs

Thomas Harris

NOS4A2

Joe Hill

Agora vamos encontrar todos os livros com mais de 700 páginas:

```
SELECT
    name,
    author,
    pages
FROM
    books
WHERE
    pages > 700;
```

name

author

pages

From the Corner of His Eye

Dean Koontz

729

The Stand

Stephen King

816

The Witching Hour

Anne Rice

1038

The Terror

Dan Simmons

769

House of Leaves

Mark Z. Danielewski

705

Swan Song

Robert R. McCammon

956

It

Stephen King

1116

Vamos aumentar a complexidade e tentar escrever uma condição complexa usando operadores lógicos. Assim como Python, SQL usa os operadores lógicos `AND`, `OR` e `NOT`. Lembre-se, é assim que eles diferem:

operator

description

AND

selects rows for which both conditions are true

OR

selects rows for which either or both conditions are true

NOT

selects rows for which the condition is false

Vamos restringir a nossa seleção introduzindo várias condições ao mesmo tempo:

```
SELECT
    *
FROM
    table_name
WHERE
    condition_1
    AND condition_2;

--Seleciona as linhas em que ambas as condições são verdadeiras
SELECT
    *
FROM
    table_name
WHERE
    condition_1
    OR condition_2;

--Seleciona as linhas em que uma ou ambas as condições são verdadeiras
SELECT
    *
FROM
    table_name
WHERE
    condition_1
    AND NOT condition_2;

--Seleciona as linhas em que condition_1 é verdadeira e condition_2 é falsa
```

Vamos recuperar os dados de livros escritos entre 1996 e 2000, incluindo seus autores e o número de páginas:

```
SELECT
    name,
    author,
    date_pub,
    pages
FROM
    books
WHERE
    date_pub > '1995-12-31'
    AND date_pub < '2001-01-01';


/* as datas de início e término não estão incluídas;
para selecionar todos os dias de 1996 a 2000,
tomamos o último dia de 1995
e o primeiro dia de 2001 */
```

name

author

date\_pub

pages

From the Corner of His Eye

Dean Koontz

2000/01/01

729

House of Leaves

Mark Z. Danielewski

2000/01/01

705

The Green Mile

Stephen King

1996/01/01

465

Podemos encurtar esta instrução introduzindo uma instrução de **BETWEEN**. Ao contrário do código anterior, `BETWEEN` inclui os limites (aqui, as datas de início e término) na seleção resultante:

```
SELECT
    name,
    author,
    date_pub,
    pages
FROM
    books
WHERE
    date_pub BETWEEN '1996-01-01'
    AND '2000-12-31';


/* as datas de início e término estão incluídas, então
selecionamos o primeiro dia do ano de 1996 
e o último dia de 2000 */
```

Os resultados das duas instruções serão os mesmos. Escolha a abordagem que você mais gosta. Alguns analistas consideram o método com duas desigualdades uma solução melhor, pois indica claramente se os limites do intervalo estão incluídos na seleção. Outros argumentam que as instruções com BETWEEN são um código melhor porque são mais concisas.

Agora vamos escrever uma declaração que selecione os nomes de todos os livros dos gêneros Humor, Fantasia ou Jovem Adulto.

```
SELECT
    name,
    genre
FROM
    books
WHERE
    genre = 'Humor'
    OR genre = 'Fantasy'
    OR genre = 'Young Adult';
```

name

genre

The City of Mirrors

Fantasy

The Stand

Fantasy

This Book Is Full of Spiders

Humor

House of Leaves

Fantasy

Boy's Life

Fantasy

The Green Mile

Fantasy

Rot & Ruin

Young Adult

It

Fantasy

NOS4A2

Fantasy

Mas e se quiséssemos todos os títulos de uma lista de 20 ou mais gêneros? Precisaríamos de uma solução mais concisa. Como esta:

```
SELECT
    name,
    genre
FROM
    books
WHERE
    genre IN ('Humor', 'Fantasy', 'Young Adult');
```

name

genre

The City of Mirrors

Fantasy

The Stand

Fantasy

This Book Is Full of Spiders

Humor

House of Leaves

Fantasy

Boy's Life

Fantasy

The Green Mile

Fantasy

Rot & Ruin

Young Adult

It

Fantasy

NOS4A2

Fantasy

Escrevemos um código com o operador **IN**. O operador `IN` é seguido por uma lista de valores a serem incluídos no resultado:

```
SELECT
    *
FROM
    table_name
WHERE
    column_name IN ('value_1', 'value_2', 'value_3');
```

Se os valores forem números, eles serão separados por vírgulas: `IN (3,7,9)`. Se forem strings, são colocadas entre aspas simples e, novamente, separadas por vírgulas: `IN ('value_1','value_2','value_3')`. A data e a hora são indicadas da seguinte forma: `IN ('aaaa-mm-dd','aaaa-mm-dd')`

Colocar `NOT` na frente do operador `IN` permite que você selecione todos os livros cujos gêneros **não** são Humor, Fantasia e Jovem Adulto:

```
SELECT
    name,
    genre
FROM
    books
WHERE
    genre NOT IN ('Humor', 'Fantasy', 'Young Adult');
```

name

genre

The October Country

Fiction

Saint Odd

Fiction

From the Corner of His Eye

Thriller

The Great and Secret Show

Fiction

World War Z: An Oral History of the Zombie War

Horror

Red Dragon

Thriller

Lightning

Thriller

The Exorcist

Classics

The Witching Hour

Fiction

The Vampire Lestat

Paranormal

Intensity

Thriller

End of Watch

Thriller

The Terror

Historical Fiction

Swan Song

Apocalyptic

Summer of Night

Thriller

Full Dark, No Stars

Short Stories

Doctor Sleep

Thriller

Let the Right One In

Paranormal

The Silence of the Lambs

Thriller

Different Seasons

Short Stories

Misery

Thriller

O chefe do grupo de analistas gostaria de estudar o comportamento das lojas e clientes em 1º de junho, Dia Mundial do Leite. Seu trabalho é recuperar dados em SQL. Preparar, apontar, fogo!

Fatias de Dados em SQL

Tarefa3 / 3

1.

Estude quais itens as lojas oferecem e seus números de vendas para 1º de junho. Você pode receber informação sobre as atualizações diárias do catálogo de produtos na tabela `products_data_all`.

Escreva uma consulta para selecionar os seguintes campos da tabela:

-   Nome do produto (`name`)
-   Preço (`price`)
-   Nome da loja (`name_store`)
-   Data (`date_upd`)

Faça uma fatia de dados com base na categoria (`category`) e data (`date_upd`). Você quer a categoria `milk` e a data do Dia Mundial do Leite — `'2019-06-01'`.Observe que especificaremos as condições no bloco WHERE, não no bloco SELECT.

2.

Faça o download dos campos `name, price, name_store, date_upd` (nessa ordem) para a categoria `leite` para os sábados restantes de junho (8, 15, 22 e 29 de junho).

3.

Precisamos da sua ajuda para testar a hipótese de que os clientes compram mais leite no Dia Mundial do Leite.Você precisará selecionar os dados da tabela `transactions`. Baixe todos os dados sobre as compras de leite de 1 de junho de 2019.

Observe que o campo `date` da tabela `transactions` especifica horas, minutos e segundos. Escreva uma condição que selecione compras feitas entre 1 de junho (inclusive) e 2 de junho (exclusive).

A tabela `transactions` não contém informação sobre categorias de produtos, mas preparamos uma lista de identificadores unívocos (`id_product`) para os produtos da categoria `leite`. Adicione uma instrução à cláusula WHERE para selecionar os produtos com os IDs a seguir:

```
( 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64,
65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,
78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90,
91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103,
104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,
117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,
130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,
143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,
156, 157, 158, 159, 160, 161)
```

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

SELECT

\*

FROM

transactions

WHERE

date \>= '2019-06-01'

AND date < '2019-06-02'

AND id\_product IN ( 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,

13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,

26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,

39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,

52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64,

65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,

78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90,

91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103,

104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,

117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,

130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,

143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,

156, 157, 158, 159, 160, 161);

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-37-03-293Z.md
### Última modificação: 2025-05-28 19:37:03

# Funções de agregação - TripleTen

Teoria

# Funções de agregação

Assim como Python, SQL possui funções específicas para calcular o número total de linhas, somas, médias e valores mínimos e máximos. São chamados de **funções de agregação**. Elas coletam, ou _agregam_, todos os objetos dentro de um grupo para calcular um único valor de resumo.

Aqui está a sintaxe de uma instrução com uma função de agregação:

```
SELECT
    AGGREGATE_FUNCTION (field) AS here_you_are
    --here_you_are - nome da coluna onde a saída (também chamado de alias para a coluna de saída) da função será armazenada
FROM
    TABLE;
```

Quando você chama uma função de agregação, por padrão, ela dá à coluna resultante um nome complicado. Para evitar isso, use o comando **AS** e digite um nome novo mais simples.

Vamos ver como as funções de agregação funcionam aplicando-as à nossa tabela `books` . Lembre-se, ela se parece com isto:

name

book\_id

genre

author

date\_pub

pages

rating

publisher\_id

The City of Mirrors

1

Fantasy

Justin Cronin

2016/01/01

602

4.2

5

The October Country

2

Fiction

Ray Bradbury

1955/01/01

334

4.16

5

Saint Odd

3

Fiction

Dean Koontz

2015/01/01

352

4.14

8

From the Corner of His Eye

4

Thriller

Dean Koontz

2000/01/01

729

4.01

8

The Great and Secret Show

5

Fiction

Clive Barker

1990/01/01

658

4.05

15

World War Z: An Oral History of the Zombie War

6

Horror

Max Brooks

2006/01/01

342

4.01

19

Red Dragon

7

Thriller

Thomas Harris

1981/01/01

348

4.02

17

The Stand

8

Fantasy

Stephen King

1978/01/01

816

4.34

2

Lightning

9

Thriller

Dean Koontz

1988/01/01

384

4.06

14

The Exorcist

10

Classics

William Peter Blatty

1971/01/01

385

4.15

7

The Witching Hour

11

Fiction

Anne Rice

1990/01/01

1038

4.11

10

The Vampire Lestat

12

Paranormal

Anne Rice

1985/01/01

550

4.06

10

Intensity

13

Thriller

Dean Koontz

1995/01/01

436

4.04

10

The Terror

14

Historical Fiction

Dan Simmons

2007/01/01

769

4.04

16

House of Leaves

15

Fantasy

Mark Z. Danielewski

2000/01/01

705

4.12

9

Swan Song

16

Apocalyptic

Robert R. McCammon

1987/01/01

956

4.28

3

Boy's Life

17

Fantasy

Robert R. McCammon

1991/01/01

580

4.34

3

Summer of Night

18

Thriller

Dan Simmons

1991/01/01

600

4.02

18

Full Dark, No Stars

19

Short Stories

Stephen King

2010/01/01

368

4.05

11

Doctor Sleep

20

Thriller

Stephen King

2013/01/01

531

4.1

11

The Green Mile

21

Fantasy

Stephen King

1996/01/01

465

4.43

0

Rot & Ruin

22

Young Adult

Jonathan Maberry

2010/01/01

458

4.1

12

Let the Right One In

23

Paranormal

John Ajvide Lindqvist

2008/01/01

513

4.07

6

The Silence of the Lambs

24

Thriller

Thomas Harris

1988/01/01

367

4.17

6

It

25

Fantasy

Stephen King

1986/01/01

1116

4.22

1

Different Seasons

26

Short Stories

Stephen King

1982/01/01

560

4.35

1

Misery

27

Thriller

Stephen King

1987/01/01

320

4.13

1

NOS4A2

28

Fantasy

Joe Hill

2013/01/01

692

4.06

13

This Book Is Full of Spiders

29

Humor

David Wong

2012/01/01

406

4.26

End of Watch

30

Thriller

Stephen King

2016/01/01

432

4.09

A função **COUNT** retorna o número de linhas em uma tabela:

```
SELECT
    COUNT(*) AS cnt
FROM
    books;
```

```
30
```

Você também pode encontrar o número de linhas em uma fatia de dados. Para fazer isso, use COUNT junto com WHERE. Vamos descobrir quantos livros foram escritos por Dean Koontz:

```
SELECT
    COUNT(*) AS cnt
FROM
    books
WHERE
    author = 'Dean Koontz';
```

```
4
```

O número de linhas pode ser calculado de várias maneiras, dependendo da tarefa:

-   **COUNT(\*)** retorna o número total de linhas da tabela
-   **COUNT(column)** retorna o número de valores em uma `coluna`
-   **COUNT(DISTINCT column)** retorna o número de valores _unívocos_ em uma `coluna`

Vamos obter o número total de linhas da tabela; o número de linhas com o identificador do editor; e o número de linhas com valores unívocos de `publisher_id`:

```
SELECT
    COUNT(*) AS cnt,
    COUNT(publisher_id) AS publisher_id_cnt,
    COUNT(DISTINCT publisher_id) AS publisher_id_uniq_cnt
FROM
    books;
```

```
30
28
19
```

Como você pode ver, temos valores diferentes. Isso ocorre porque a coluna `publisher_id` contém valores ausentes e identificadores de editor repetidos e não unívocos.

SUM (coluna) retorna a soma dos valores em uma `coluna`. Ela ignora os valores ausentes.

Vamos somar o número de páginas em todos os livros aplicando a função SUM na coluna `pages`:

```
SELECT
    SUM(pages) AS total_pages
FROM
    books;
```

```
16812
```

Observe que a função SUM só funciona com valores numéricos.

AVG (coluna) retorna o valor médio dos valores da `coluna`_._ Vamos determinar a classificação média dos livros da nossa tabela:

```
SELECT
    AVG(rating) AS average
FROM
    books;
```

```
--Algo deu errado!
```

Opa! Algo deu errado. Os dados na coluna `rating` estão em formato string, mas para calcular a média precisamos de valores numéricos.

Você aprenderá a converter tipos de dados na próxima aula.

Enquanto isso, vamos encontrar o número médio de páginas por livro:

```
SELECT
    AVG(pages) AS average_pages
FROM
    books;
```

```
--Result
560.4
```

O menor e o maior valor podem ser encontrados com as funções **MIN** e **MAX**. Elas trabalham da mesma maneira que suas funções de agregação irmãs. Você praticará a leitura de documentações e estudará as funções MIN e MAX por conta própria.

Veja como funcionam as funções MIN e MAX na documentação oficial:

-   MAX: [https:](https://oracleplsql.ru/max-postgresql.html)[//www.techonthenet.com/postgresql/functions/max.php](https://www.techonthenet.com/postgresql/functions/max.php) (a documentação está em inglês)
-   MIN: [https://](https://oracleplsql.ru/min-postgresql.html)[www.techonthenet.com/postgresql/functions/min.php](https://www.techonthenet.com/postgresql/functions/min.php)(a documentação está em inglês)

Quando terminar, volte para o TripleTen! Tarefas em funções de agregação te aguardam!

Funções de agregação

Tarefa7 / 7

1.

Escreva uma consulta para calcular o número total de linhas na tabela `products_data_all`. Nomeie a variável como `cnt`.

2.

Escreva uma consulta para calcular o número de linhas na tabela `products_data_all`, o número de linhas na coluna `name` e o número de produtos unívocos na coluna `name`. Nomeie as variáveis resultantes como `cnt`, `name_cnt` e `name_uniq_cnt`, respectivamente.

3.

Escreva uma consulta para calcular o preço médio dos produtos da tabela `products_data_all` . O preço é indicado na coluna `price`. Nomeie a variável resultante como `average`.

4.

Escreva uma consulta para calcular o preço médio do produto `'Borden Whole Milk, 1 gal'` na loja `'Wise Penny'`. Nomeie a variável resultante como `average`.

5.

Escreva uma consulta para calcular a soma dos preços de todos os produtos da loja `'T-E-B'`. Nomeie a variável como `total_cost`.

6.

Encontre o preço do produto mais caro na tabela `products_data_all`. Nomeie a variável como `max_price`.

7.

Escreva uma consulta para calcular a diferença entre o maior e o menor valor de preço para `'Meyenberg Goat Milk, 1/2 gal'` na loja `'Milk Market'`. Nomeie a variável `max_min_diff`.

9

1

2

3

4

5

6

7

8

SELECT

MAX(price)\- Min(price) as max\_min\_diff

FROM

products\_data\_all

WHERE

name \= 'Meyenberg Goat Milk, 1/2 gal'

AND name\_store \= 'Milk Market';

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-37-04-612Z.md
### Última modificação: 2025-05-28 19:37:04

# Convertendo Tipos de Dados - TripleTen

Teoria

# Convertendo Tipos de Dados

Nas aulas anteriores você aprendeu que algumas funções de agregação só podem ser usadas com valores numéricos. Esta consulta, por exemplo, não funcionou para o campo `rating`:

```
SELECT
    AVG(rating) AS average
FROM
    books;
```

Dê mais uma olhada na tabela:

name

book\_id

genre

author

date\_pub

pages

rating

publisher\_id

The City of Mirrors

1

Fantasy

Justin Cronin

2016/01/01

602

4.2

5

The October Country

2

Fiction

Ray Bradbury

1955/01/01

334

4.16

5

Saint Odd

3

Fiction

Dean Koontz

2015/01/01

352

4.14

8

From the Corner of His Eye

4

Thriller

Dean Koontz

2000/01/01

729

4.01

8

The Great and Secret Show

5

Fiction

Clive Barker

1990/01/01

658

4.05

15

World War Z: An Oral History of the Zombie War

6

Horror

Max Brooks

2006/01/01

342

4.01

19

Red Dragon

7

Thriller

Thomas Harris

1981/01/01

348

4.02

17

The Stand

8

Fantasy

Stephen King

1978/01/01

816

4.34

2

Lightning

9

Thriller

Dean Koontz

1988/01/01

384

4.06

14

The Exorcist

10

Classics

William Peter Blatty

1971/01/01

385

4.15

7

The Witching Hour

11

Fiction

Anne Rice

1990/01/01

1038

4.11

10

The Vampire Lestat

12

Paranormal

Anne Rice

1985/01/01

550

4.06

10

Intensity

13

Thriller

Dean Koontz

1995/01/01

436

4.04

10

The Terror

14

Historical Fiction

Dan Simmons

2007/01/01

769

4.04

16

House of Leaves

15

Fantasy

Mark Z. Danielewski

2000/01/01

705

4.12

9

Swan Song

16

Apocalyptic

Robert R. McCammon

1987/01/01

956

4.28

3

Boy's Life

17

Fantasy

Robert R. McCammon

1991/01/01

580

4.34

3

Summer of Night

18

Thriller

Dan Simmons

1991/01/01

600

4.02

18

Full Dark, No Stars

19

Short Stories

Stephen King

2010/01/01

368

4.05

11

Doctor Sleep

20

Thriller

Stephen King

2013/01/01

531

4.1

11

The Green Mile

21

Fantasy

Stephen King

1996/01/01

465

4.43

0

Rot & Ruin

22

Young Adult

Jonathan Maberry

2010/01/01

458

4.1

12

Let the Right One In

23

Paranormal

John Ajvide Lindqvist

2008/01/01

513

4.07

6

The Silence of the Lambs

24

Thriller

Thomas Harris

1988/01/01

367

4.17

6

It

25

Fantasy

Stephen King

1986/01/01

1116

4.22

1

Different Seasons

26

Short Stories

Stephen King

1982/01/01

560

4.35

1

Misery

27

Thriller

Stephen King

1987/01/01

320

4.13

1

NOS4A2

28

Fantasy

Joe Hill

2013/01/01

692

4.06

13

This Book Is Full of Spiders

29

Humor

David Wong

2012/01/01

406

4.26

End of Watch

30

Thriller

Stephen King

2016/01/01

432

4.09

Os dados no campo de classificação parecem números, mas no banco de dados estão armazenados como strings. Isso acontece com frequência, mais comumente devido a erros no design dos bancos de dados.

Vamos tentar converter o tipo de dados da coluna na própria consulta SQL. Isso pode ser feito com a ajuda da instrução **CAST**:

```
CAST (column_name AS data_type)
```

`column_name` é o campo cujo tipo de dados deve ser convertido. `data_type` é o tipo desejado. Também podemos escrever isso:

```
column_name :: data_type
```

Vamos dar uma olhada nos tipos de dados no PostgreSQL.

### Tipos de dados numéricos

**integer**: um tipo inteiro semelhante a _int_ em Python. No PostgreSQL, os números inteiros variam de -2147483648 a 2147483647.

**real**: um número de ponto flutuante, como _float_ em Python. A precisão do número para o tipo _real_ é de até 6 casas decimais.

### Tipos de dados de string

**'TripleTen'**: este é um exemplo de um valor do tipo string. Nas instruções SQL, ele vai entre as aspas simples.

**varchar(n)**: uma cadeia de caracteres de comprimento variável, em que **n** é o número máximo de caracteres.

**text**: uma string de qualquer tamanho. Este tipo é como o tipo string do Python.

### Data e hora

Datas e horas vão entre as aspas simples.

**timestamp**: uma data e hora. É análogo ao `datetime` em `pandas`. Este formato é frequentemente usado para armazenar eventos que ocorrem várias vezes ao dia, como logs de usuários de sites.

**date**: uma data

### Lógico

**boolean** — é um tipo de dados lógico. Ele pode ter três valores no PostgreSQL: **TRUE**, **FALSE** e **NULL** (desconhecido).

Esses são os tipos de dados mais comuns. Existem, claro, outros, que você pode estudar por conta própria. Esta documentação será útil (a documentação está em inglês):

[https://www.tutorialspoint.com/postgresql/postgresql\_data\_types.htm](https://www.tutorialspoint.com/postgresql/postgresql_data_types.htm)

[http://www.postgresqltutorial.com/postgresql-data-types/](http://www.postgresqltutorial.com/postgresql-data-types/)

Agora vamos voltar à tarefa da lição anterior e calcular a classificação média do livro de duas maneiras diferentes:

```
SELECT
    AVG(rating::real) AS average
FROM
    books;
```

ou:

```
SELECT
    AVG(CAST(rating AS real)) AS average
FROM
    books;
```

```
4.139333
```

A propósito, os dados dos produtos lácteos contêm um erro. Os dados `weight` estão escritos como strings, mas na verdade são números. Em um caso como esse você deveria enviar um relatório de bug, mas por enquanto apenas corrija esse erro localmente, dentro da consulta.

Convertendo Tipos de Dados

Tarefa4 / 4

1.

Escreva uma consulta para calcular o peso médio (`weight`) das mercadorias da tabela `products_data_all` cujo peso está em onças (filtre a tabela para o campo `units` igual a `'oz'`). Nomeie a variável como `average`. Converta o tipo de dados na coluna `weight` para que ela contenha números de ponto flutuante.

2.

Escreva uma consulta para achar o peso máximo do produto na categoria `'milk'`. Use o operador `CAST AS` e nomeie a variável `max_weight`.

3.

Converta o campo `date` da tabela `transactions` para o tipo date. Nomeie o campo resultante como `date_no_time`.

4.

Encontre as datas (`date`) das últimas e primeiras compras na tabela `transactions`. Nomeie as colunas `max_date` e `min_date`, respectivamente. Não se esqueça de converter as datas para o tipo `date`.

9

1

2

3

4

5

6

SELECT

MAX(CAST(date AS date)) AS max\_date,

MIN(CAST(date AS date)) AS min\_date

FROM

transactions;

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-37-06-605Z.md
### Última modificação: 2025-05-28 19:37:06

# Conclusão - TripleTen

Capítulo 3/8

SQL como uma Ferramenta para Trabalhar com Dados

# Conclusão

Neste capítulo você aprendeu:

-   Como trabalhar com bancos de dados
    
-   Como escrever consultas SQL usando instruções como BETWEEN , SELECT FROM WHERE, IN e recuperar dados
    
-   Como chamar as funções de agregação: COUNT, SUM, AVG, MIN e MAX
    
-   Como converter tipos de dados usando o operador CAST
    

E você aplicou todo esse conhecimento ao trabalhar com tabelas!

Ah, os lugares para onde você vai com SQL.

### Leve isso com você

Baixe a [folha de conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/moved_Folha_de_Concluses_SQL_como_uma_Ferramenta_para_Trabalhar_com_Dados.pdf) e o [sumário do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/moved_Resumo_do_Captulo_SQL_como_Ferramenta_para_Trabalhar_com_Dados.pdf) da Base de Conhecimento para que você possa consultá-los sempre que precisar.

### Leia mais sobre SGBD (os materiais estão em inglês):

[https://db-engines.com/en/ranking](https://db-engines.com/en/ranking)

[https://www.capterra.com/rdbms-software/](https://www.capterra.com/rdbms-software/)

### Leia mais sobre os tipos de bancos de dados:

[https://www.tutorialspoint.com/Types-of-databases](https://www.tutorialspoint.com/Types-of-databases) (os materiais estão em inglês)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-13-504Z.md
### Última modificação: 2025-05-28 19:38:13

# Introdução - TripleTen

Capítulo 4/8

Funcionalidades Avançadas de SQL para Análise

# Introdução

Antes de tirar as conclusões analíticas, você precisa colocar os dados em ordem. Neste capítulo, você aprenderá a classificar e agrupar dados e a trabalhar com datas e horas.

### O que você vai aprender

-   Como agrupar dados e trabalhar com os resultados
-   Como aplicar funções para processar os tipos de data e hora
-   Como ordenar dados em ordem crescente e decrescente
-   Como usar subconsultas

### Quanto tempo irá demorar:

5 aulas, com aproximadamente 15-20 minutos cada uma

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-14-837Z.md
### Última modificação: 2025-05-28 19:38:15

# Agrupando Dados - TripleTen

Teoria

# Agrupando Dados

No capítulo anterior, você aprendeu a chamar as funções de agregação. Vamos fazer uma revisão.

Segue a nossa tabela:

name

book\_id

genre

author

date\_pub

pages

rating

publisher\_id

The City of Mirrors

1

Fantasy

Justin Cronin

2016/01/01

602

4.2

5

The October Country

2

Fiction

Ray Bradbury

1955/01/01

334

4.16

5

Saint Odd

3

Fiction

Dean Koontz

2015/01/01

352

4.14

8

From the Corner of His Eye

4

Thriller

Dean Koontz

2000/01/01

729

4.01

8

The Great and Secret Show

5

Fiction

Clive Barker

1990/01/01

658

4.05

15

World War Z: An Oral History of the Zombie War

6

Horror

Max Brooks

2006/01/01

342

4.01

19

Red Dragon

7

Thriller

Thomas Harris

1981/01/01

348

4.02

17

The Stand

8

Fantasy

Stephen King

1978/01/01

816

4.34

2

Lightning

9

Thriller

Dean Koontz

1988/01/01

384

4.06

14

The Exorcist

10

Classics

William Peter Blatty

1971/01/01

385

4.15

7

The Witching Hour

11

Fiction

Anne Rice

1990/01/01

1038

4.11

10

The Vampire Lestat

12

Paranormal

Anne Rice

1985/01/01

550

4.06

10

Intensity

13

Thriller

Dean Koontz

1995/01/01

436

4.04

10

The Terror

14

Historical Fiction

Dan Simmons

2007/01/01

769

4.04

16

House of Leaves

15

Fantasy

Mark Z. Danielewski

2000/01/01

705

4.12

9

Swan Song

16

Apocalyptic

Robert R. McCammon

1987/01/01

956

4.28

3

Boy's Life

17

Fantasy

Robert R. McCammon

1991/01/01

580

4.34

3

Summer of Night

18

Thriller

Dan Simmons

1991/01/01

600

4.02

18

Full Dark, No Stars

19

Short Stories

Stephen King

2010/01/01

368

4.05

11

Doctor Sleep

20

Thriller

Stephen King

2013/01/01

531

4.1

11

The Green Mile

21

Fantasy

Stephen King

1996/01/01

465

4.43

0

Rot & Ruin

22

Young Adult

Jonathan Maberry

2010/01/01

458

4.1

12

Let the Right One In

23

Paranormal

John Ajvide Lindqvist

2008/01/01

513

4.07

6

The Silence of the Lambs

24

Thriller

Thomas Harris

1988/01/01

367

4.17

6

It

25

Fantasy

Stephen King

1986/01/01

1116

4.22

1

Different Seasons

26

Short Stories

Stephen King

1982/01/01

560

4.35

1

Misery

27

Thriller

Stephen King

1987/01/01

320

4.13

1

NOS4A2

28

Fantasy

Joe Hill

2013/01/01

692

4.06

13

This Book Is Full of Spiders

29

Humor

David Wong

2012/01/01

406

4.26

End of Watch

30

Thriller

Stephen King

2016/01/01

432

4.09

Foi assim que encontramos o número de livros escritos por Stephen King:

```
SELECT
    COUNT(name) AS cnt
FROM
    books
WHERE
    author = 'Stephen King';
```

```
8
```

Mas e se quisermos fazer isso para todos os autores? Escrever várias instruções semelhantes levaria muito tempo. Gostaríamos de pegar todos os autores e contar automaticamente todos os seus livros. É aí que entra o agrupamento.

O comando **GROUP BY** é utilizado quando os dados precisam ser divididos em grupos de acordo com os valores dos campos.

Aqui está um exemplo de uma instrução com agrupamento e uma função de agregação:

```
SELECT
    field_1,
    field_2,
    ...,
    field_n,
    AGGREGATE_FUNCTION (field) AS here_you_are
FROM
    TABLE
WHERE
    -- se necessário
    condition
GROUP BY
    field_1,
    field_2,
    ...,
    field_n;
```

Depois de saber quais campos você estará agrupando, certifique-se de que todos esses campos estejam listados no bloco SELECT e no bloco GROUP BY. A função de agregação não deve ser incluída no bloco GROUP BY; caso contrário, a consulta não funcionará. O GROUP BY em SQL opera do mesmo jeito que o método `groupby()` em pandas.

Voltemos ao nosso exemplo. Precisamos aplicar a função de agregação COUNT. O comando GROUP BY nos ajudará a encontrar o número de linhas por autor:

```
SELECT
    author,
    COUNT(name) AS cnt
FROM
    books
GROUP BY
    author;
```

author

cnt

Ray Bradbury

1

Joe Hill

1

Thomas Harris

2

Dean Koontz

4

Dan Simmons

2

Jonathan Maberry

1

Robert R. McCammon

2

William Peter Blatty

1

Mark Z. Danielewski

1

Clive Barker

1

John Ajvide Lindqvist

1

Max Brooks

1

David Wong

1

Justin Cronin

1

Stephen King

8

Anne Rice

2

Agora sabemos quantos livros escreveu cada um dos autores da nossa tabela.

Desta vez, vamos calcular os totais de livros agrupados por autor e gênero.

```
SELECT
    author,
    genre,
    COUNT(name) AS cnt
FROM
    books
GROUP BY
    author,
    genre;
```

author

genre

cnt

Dan Simmons

Historical Fiction

1

Joe Hill

Fantasy

1

Mark Z. Danielewski

Fantasy

1

Dan Simmons

Thriller

1

Anne Rice

Paranormal

1

Max Brooks

Horror

1

David Wong

Humor

1

Ray Bradbury

Fiction

1

Clive Barker

Fiction

1

Jonathan Maberry

Young Adult

1

Thomas Harris

Thriller

2

Dean Koontz

Thriller

3

Justin Cronin

Fantasy

1

Stephen King

Short Stories

2

Dean Koontz

Fiction

1

Robert R. McCammon

Fantasy

1

John Ajvide Lindqvist

Paranormal

1

William Peter Blatty

Classics

1

Stephen King

Fantasy

3

Anne Rice

Fiction

1

Robert R. McCammon

Apocalyptic

1

Stephen King

Thriller

3

Essa consulta retornou uma tabela com a quantidade de livros agrupados de acordo com dois critérios: gênero e autor. Por exemplo, Stephen King tem três romances de fantasia populares, três suspenses e dois contos, enquanto Thomas Harris escreve principalmente suspense.

GROUP BY pode ser usado com qualquer função de agregação: COUNT, AVG, SUM, MAX e MIN. Você pode chamar várias funções simultaneamente. Vamos calcular o número médio de páginas nos livros de cada autor e o número de páginas em seu livro mais longo.

```
SELECT
    author,
    AVG(pages) AS avg_pages,
    MAX(pages) AS max_pages
FROM
    books
GROUP BY
    author;
```

author

avg\_pages

max\_pages

Ray Bradbury

334

334

Joe Hill

692

692

Thomas Harris

357.5

367

Dean Koontz

475.25

729

Dan Simmons

684.5

769

Jonathan Maberry

458

458

Robert R. McCammon

768

956

William Peter Blatty

385

385

Mark Z. Danielewski

705

705

Clive Barker

658

658

John Ajvide Lindqvist

513

513

Max Brooks

342

342

David Wong

406

406

Justin Cronin

602

602

Stephen King

576

1116

Anne Rice

794

1038

Agora, vamos praticar os conceitos que você aprendeu!

Agrupando Dados

Tarefa4 / 4

1.

Antes de começarmos o exercício, pedimos que, primeiro, você extraia e confira as 5 primeiras linhas da tabela `products_data_all`. A ideia é se familiarizar com os dados com os quais você vai trabalhar. Em seguida, realize a tarefa abaixo:

Escreva uma consulta para calcular o número total de produtos (`name`) e o número de produtos unívocos para cada loja (`name_store`). Nomeie as variáveis `name_cnt` e `name_uniq_cnt`, respectivamente. Imprima os nomes das lojas, o número total de produtos e o número de produtos unívocos. As colunas devem aparecer nesta ordem: `name_store`, `name_cnt`, `name_uniq_cnt`.

2.

Escreva uma consulta para calcular o peso máximo do produto para cada categoria (`category`). Chame a variável de _`max_weight`_ e a converta para o tipo `real`. Imprima a categoria e o peso máximo.

3.

Escreva uma consulta para calcular o preço médio, mínimo e máximo do produto (`price`) para cada loja (`name_store`) da tabela `products_data_all`. Nomeie as variáveis como `average_price`, `max_price` e `min_price`, respectivamente.

Imprima o nome da loja e os preços médio, mínimo e máximo.

4.

Escreva uma consulta para calcular a diferença entre os preços máximo e mínimo para cada um dos produtos na categoria `'milk'` em 10 de junho de 2019. Nomeie a variável `max_min_diff`. Converta os valores de data de string para o formato `date`.

Em seguida, imprima o nome do produto e a diferença entre o preço máximo e mínimo.

Aqui estão as colunas que você usará:

Categoria — campo `category`

Data — campo `date_upd`

Preço — campo `price`

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

SELECT

max(price) \- min(price)as max\_min\_diff,

name

FROM

products\_data\_all

WHERE

category \='milk'and

date\_upd::date \= '2019-06-10'

GROUP BY

name;

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-16-187Z.md
### Última modificação: 2025-05-28 19:38:16

# Ordenando Dados - TripleTen

Teoria

# Ordenando Dados

Os resultados da análise geralmente são apresentados em uma determinada ordem. Para ordenar os dados por um campo, use o comando **ORDER BY**.

Veja como é uma instrução com agrupamento e ordenação:

```
SELECT 
    field_1, 
    field_2,
    ..., 
    field_n, 
    AGGREGATE_FUNCTION(field) AS here_you_are
FROM
    table_name
WHERE -- se necessário
    condition
GROUP BY  
    field_1, 
    field_2, 
    ..., 
    field_n,
ORDER BY -- Liste apenas os campos pelos quais os dados de tabela devem ser ordenados
    field_1, 
    field_2, 
    ..., 
    field_n, 
    here_you_are;
```

Ao usar ORDER BY, apenas os campos pelos quais queremos ordenar os dados devem ser listados no bloco de comando.

Poderiam ser usados dois modificadores com o comando ORDER BY para ordenar os dados em colunas:

-   **ASC** (o padrão) ordena os dados na ordem crescente.
-   **DESC** ordena os dados na ordem decrescente.

Os modificadores do ORDER BY são colocados logo após o campo pelo qual os dados são ordenados:

```
ORDER BY
    field_name DESC
    -- ordenando os dados na ordem decrescente
ORDER BY
    field_name ASC;

-- ordenando os dados na ordem crescente
```

name

book\_id

genre

author

date\_pub

pages

rating

publisher\_id

The City of Mirrors

1

Fantasy

Justin Cronin

2016/01/01

602

4.2

5

The October Country

2

Fiction

Ray Bradbury

1955/01/01

334

4.16

5

Saint Odd

3

Fiction

Dean Koontz

2015/01/01

352

4.14

8

From the Corner of His Eye

4

Thriller

Dean Koontz

2000/01/01

729

4.01

8

The Great and Secret Show

5

Fiction

Clive Barker

1990/01/01

658

4.05

15

World War Z: An Oral History of the Zombie War

6

Horror

Max Brooks

2006/01/01

342

4.01

19

Red Dragon

7

Thriller

Thomas Harris

1981/01/01

348

4.02

17

The Stand

8

Fantasy

Stephen King

1978/01/01

816

4.34

2

Lightning

9

Thriller

Dean Koontz

1988/01/01

384

4.06

14

The Exorcist

10

Classics

William Peter Blatty

1971/01/01

385

4.15

7

The Witching Hour

11

Fiction

Anne Rice

1990/01/01

1038

4.11

10

The Vampire Lestat

12

Paranormal

Anne Rice

1985/01/01

550

4.06

10

Intensity

13

Thriller

Dean Koontz

1995/01/01

436

4.04

10

The Terror

14

Historical Fiction

Dan Simmons

2007/01/01

769

4.04

16

House of Leaves

15

Fantasy

Mark Z. Danielewski

2000/01/01

705

4.12

9

Swan Song

16

Apocalyptic

Robert R. McCammon

1987/01/01

956

4.28

3

Boy's Life

17

Fantasy

Robert R. McCammon

1991/01/01

580

4.34

3

Summer of Night

18

Thriller

Dan Simmons

1991/01/01

600

4.02

18

Full Dark, No Stars

19

Short Stories

Stephen King

2010/01/01

368

4.05

11

Doctor Sleep

20

Thriller

Stephen King

2013/01/01

531

4.1

11

The Green Mile

21

Fantasy

Stephen King

1996/01/01

465

4.43

0

Rot & Ruin

22

Young Adult

Jonathan Maberry

2010/01/01

458

4.1

12

Let the Right One In

23

Paranormal

John Ajvide Lindqvist

2008/01/01

513

4.07

6

The Silence of the Lambs

24

Thriller

Thomas Harris

1988/01/01

367

4.17

6

It

25

Fantasy

Stephen King

1986/01/01

1116

4.22

1

Different Seasons

26

Short Stories

Stephen King

1982/01/01

560

4.35

1

Misery

27

Thriller

Stephen King

1987/01/01

320

4.13

1

NOS4A2

28

Fantasy

Joe Hill

2013/01/01

692

4.06

13

This Book Is Full of Spiders

29

Humor

David Wong

2012/01/01

406

4.26

End of Watch

30

Thriller

Stephen King

2016/01/01

432

4.09

Vamos achar o número de livros de cada autor e ordenar os dados resultantes:

```
SELECT
    author,
    COUNT(name) AS cnt
FROM
    books
GROUP BY
    author
ORDER BY
    cnt;
```

author

cnt

Ray Bradbury

1

Joe Hill

1

Mark Z. Danielewski

1

Clive Barker

1

John Ajvide Lindqvist

1

Max Brooks

1

David Wong

1

Justin Cronin

1

Jonathan Maberry

1

William Peter Blatty

1

Thomas Harris

2

Dan Simmons

2

Robert R. McCammon

2

Anne Rice

2

Dean Koontz

4

Stephen King

8

Como não incluímos um modificador para o ORDER BY, os dados foram ordenados em ordem crescente (padrão).

Agora vamos adicionar DESC:

```
SELECT
    author,
    COUNT(name) AS cnt
FROM
    books
GROUP BY
    author
ORDER BY
    cnt DESC;
```

author

cnt

Stephen King

8

Dean Koontz

4

Robert R. McCammon

2

Dan Simmons

2

Thomas Harris

2

Anne Rice

2

John Ajvide Lindqvist

1

Max Brooks

1

David Wong

1

Justin Cronin

1

Mark Z. Danielewski

1

Joe Hill

1

Jonathan Maberry

1

William Peter Blatty

1

Ray Bradbury

1

Clive Barker

1

Agora fica claro quem trabalhou mais! DESC ordenou os dados em ordem decrescente.

O comando **LIMIT** define um limite para o número de linhas no resultado. Sempre vem no final de uma instrução:

```
SELECT
    field_1,
    field_2,
    ...,
    field_n,
    AGGREGATE_FUNCTION (field) AS here_you_are
FROM
    table_name
WHERE -- se necessário
    condition
GROUP BY
    field_1,
    field_2,
    ...,
    field_n
ORDER BY -- se necessário. Liste apenas os campos
    --pelos quais os dados de tabela devem ser ordenados
    field_1,
    field_2,
...,
    field_n,
    here_you_are
LIMIT -- se necessário
n;

-- n - o número máximo de linhas a serem retornadas
```

Após LIMIT, indique o número necessário de linhas: _n_. Fica sempre mais fácil construir um ranking com um número limitado de elementos.

Vamos fazer uma lista dos três livros mais longos:

```
SELECT
    name,
    pages
FROM
    books
ORDER BY
    pages DESC
LIMIT 3;
```

name

pages

It

1116

The Witching Hour

1038

Swan Song

956

Não se preocupe! Tornar-se um analista é muito mais fácil do que derrotar um palhaço do mal.

Ordenando Dados

Tarefa3 / 3

1.

Antes de começarmos o exercício, pedimos que, primeiro, você extraia e confira as 5 primeiras linhas da tabela `products_data_all`. A ideia é se familiarizar com os dados com os quais você vai trabalhar. Em seguida, realize a tarefa abaixo:

Escreva uma consulta para encontrar o número de produtos em cada categoria (`category`) para a data `'2019-06-05'`. Nomeie a variável `name_cnt` e ordene os dados em ordem crescente. Imprima a data, a categoria do produto e o número de produtos. Nomeie a data escolhida como `update_date`.

Você não precisa usar CAST.

Observe que as datas são armazenadas como strings e precisam ser convertidas para o tipo `date`.

2.

Escreva uma consulta para calcular o número de produtos _unívocos_ para cada categoria na loja (`name_store`) `'T-E-B'` para a data `'2019-06-30'`. Nomeie a variável `uniq_name_cnt` e ordene os dados por este campo, em ordem decrescente. Converta a data para o tipo `date` e nomeie o campo resultante como `update_date`. Depois, imprima a data, o nome da loja, o nome da categoria e o número de produtos unívocos.

3.

Escreva uma consulta para imprimir os cinco produtos mais caros em ordem decrescente. Imprima o nome do produto e seu preço. Nomeie a variável `max_price`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

SELECT

name,

max(price) as max\_price

FROM

products\_data\_all

  

GROUP BY

name

ORDER BY

max\_price desc

LIMIT 5;

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-18-897Z.md
### Última modificação: 2025-05-28 19:38:19

# Processando os Dados dentro de um Agrupamento - TripleTen

Teoria

# Processando os Dados dentro de um Agrupamento

Você já sabe como agrupar os dados por vários campos usando a instrução GROUP BY.

E se quisermos exibir todos os autores que tenham mais de um livro na tabela? Então usamos **HAVING.** Funciona como **WHERE** para as funções de agregação.

Veja como fica uma instrução com uma cláusula HAVING:

```
 SELECT
    field_1,
    field_2,
    ...,
    field_n,
    AGGREGATE_FUNCTION (field) AS here_you_are
FROM
    TABLE
WHERE -- se necessário
    condition
GROUP BY
    field_1,
    field_2,
    ...,
    field_n
HAVING
    AGGREGATE_FUNCTION (field_for_grouping) > n
ORDER BY -- quando necessário. Liste apenas os campos
    --pelos quais devem ser ordenados os dados (use aliases do SELECT quando aplicável)
    field_1,
    field_2,
    ...,
    field_n,
    here_you_are
LIMIT -- se necessário
      n;
```

A seleção resultante incluirá apenas as linhas para as quais a função de agregação produz resultados que atendem à condição indicada no bloco HAVING.

HAVING e WHERE têm muito em comum. Então, por que não podemos passar todas as nossas condições para um deles? O fato é que o comando WHERE é compilado antes que o agrupamento e as agregações sejam realizadas. Por isso é impossível definir os parâmetros de filtragem que usam os resultados de uma função agregada com WHERE, daí a necessidade de HAVING.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_____where_heaving_1585476951.jpg)

Aqui está a nossa tabela de livros:

name

book\_id

genre

author

date\_pub

pages

rating

publisher\_id

The City of Mirrors

1

Fantasy

Justin Cronin

2016/01/01

602

4.2

5

The October Country

2

Fiction

Ray Bradbury

1955/01/01

334

4.16

5

Saint Odd

3

Fiction

Dean Koontz

2015/01/01

352

4.14

8

From the Corner of His Eye

4

Thriller

Dean Koontz

2000/01/01

729

4.01

8

The Great and Secret Show

5

Fiction

Clive Barker

1990/01/01

658

4.05

15

World War Z: An Oral History of the Zombie War

6

Horror

Max Brooks

2006/01/01

342

4.01

19

Red Dragon

7

Thriller

Thomas Harris

1981/01/01

348

4.02

17

The Stand

8

Fantasy

Stephen King

1978/01/01

816

4.34

2

Lightning

9

Thriller

Dean Koontz

1988/01/01

384

4.06

14

The Exorcist

10

Classics

William Peter Blatty

1971/01/01

385

4.15

7

The Witching Hour

11

Fiction

Anne Rice

1990/01/01

1038

4.11

10

The Vampire Lestat

12

Paranormal

Anne Rice

1985/01/01

550

4.06

10

Intensity

13

Thriller

Dean Koontz

1995/01/01

436

4.04

10

The Terror

14

Historical Fiction

Dan Simmons

2007/01/01

769

4.04

16

House of Leaves

15

Fantasy

Mark Z. Danielewski

2000/01/01

705

4.12

9

Swan Song

16

Apocalyptic

Robert R. McCammon

1987/01/01

956

4.28

3

Boy's Life

17

Fantasy

Robert R. McCammon

1991/01/01

580

4.34

3

Summer of Night

18

Thriller

Dan Simmons

1991/01/01

600

4.02

18

Full Dark, No Stars

19

Short Stories

Stephen King

2010/01/01

368

4.05

11

Doctor Sleep

20

Thriller

Stephen King

2013/01/01

531

4.1

11

The Green Mile

21

Fantasy

Stephen King

1996/01/01

465

4.43

0

Rot & Ruin

22

Young Adult

Jonathan Maberry

2010/01/01

458

4.1

12

Let the Right One In

23

Paranormal

John Ajvide Lindqvist

2008/01/01

513

4.07

6

The Silence of the Lambs

24

Thriller

Thomas Harris

1988/01/01

367

4.17

6

It

25

Fantasy

Stephen King

1986/01/01

1116

4.22

1

Different Seasons

26

Short Stories

Stephen King

1982/01/01

560

4.35

1

Misery

27

Thriller

Stephen King

1987/01/01

320

4.13

1

NOS4A2

28

Fantasy

Joe Hill

2013/01/01

692

4.06

13

This Book Is Full of Spiders

29

Humor

David Wong

2012/01/01

406

4.26

End of Watch

30

Thriller

Stephen King

2016/01/01

432

4.09

Agora vamos encontrar todos os autores que têm mais de um livro listado:

```
SELECT
    author,
    COUNT(name) AS name_cnt
FROM
    books
GROUP BY
    author
HAVING
    COUNT(name) > 1
ORDER BY
    name_cnt DESC;
```

author

name\_cnt

Stephen King

8

Dean Koontz

4

Thomas Harris

2

Dan Simmons

2

Robert R. McCammon

2

Anne Rice

2

Preste atenção especial à ordem em que os comandos são introduzidos:

1) GROUP BY

2) HAVING

3) ORDER BY

Esta ordem é **obrigatória**. Caso contrário, o código não funcionará.

Agora, vamos praticar.

Processando os Dados dentro de um Agrupamento

Tarefa3 / 3

1.

Você vai continuar trabalhando com a tabela `products_data_all`.

Escreva uma instrução para encontrar o preço mais alto (os preços estão armazenados na coluna `price`) para cada nome de produto (os nomes estão armazenados na tabela `name`). Salve-o como a variável `max_price`. Em seguida, imprima o nome e o preço máximo de cada produto cujo preço máximo seja maior que $10.

2.

Você vai continuar trabalhando com a tabela `products_data_all`.

Escreva uma consulta para encontrar o número de produtos com peso superior a 5 oz (`weight`) e (`units`) para cada loja (`name_store`) para a data `'2019-06-03'`. Salve o resultado como `name_cnt`, e a data (convertida para o tipo necessário) como `update_date`.

Imprima a data, o nome da loja e o número de produtos apenas para lojas com menos de 20 produtos. As colunas devem aparecer nesta ordem: `update_date`, `name_store`, `name_cnt`.

Não se esqueça de converter o peso do produto para o tipo _real_ e selecionar apenas os produtos cujos pesos sejam medidos em onças (`units = 'oz'`).

3.

Vamos continuar trabalhando com a mesma tabela `products_data_all`.

Escreva uma consulta para encontrar o número de produtos unívocos para cada loja (`name_store`) e nomeie a variável `name_uniq_cnt`. Encontre as três lojas com o menor número de produtos (classificar em ordem crescente) entre as lojas que possuem mais de 30 produtos unívocos.Imprima o nome de cada loja e o número de produtos unívocos que ela oferece.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

SELECT

name\_store,

count(distinct name) as name\_uniq\_cnt

FROM

products\_data\_all

GROUP BY

name\_store

having

count(distinct name) \>30

Order by

count(distinct name) asc

Limit 3

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-20-258Z.md
### Última modificação: 2025-05-28 19:38:20

# Operadores e Funções para Trabalhar com Datas - TripleTen

Teoria

# Operadores e Funções para Trabalhar com Datas

O tempo é um fator crucial nos processos de negócio.

Por exemplo, para avaliar o impacto do clima na atividade de compra de uma loja online, é importante saber quando cada compra foi feita e como estava o clima naquele momento. Você precisa coletar dados sobre o tempo de várias fontes e agrupá-los por mês, dia ou hora.

Ao trabalhar com valores de data e hora, teremos duas funções principais: **EXTRACT** e **DATE\_TRUNC** (truncar data). Ambas as funções são chamadas no bloco SELECT.

Veja como é uma função EXTRACT:

```
SELECT
    EXTRACT(date_fragment FROM column_name) AS new_column_with_date
FROM
    Table_with_all_dates;
```

EXTRACT, sem surpresa nenhuma, extrai alguma das seguintes partes de um timestamp (ou marca temporal, que indica a hora ou data que algo ocorreu). Você pode recuperar:

-   `century` (século)
-   `day` (dia)
-   `doy` — dia do ano, de 1 a 365/366
-   `isodow` — dia da semana no formato ISO 8601, o formato internacional de data e hora; Segunda-feira é 1, domingo é 7
-   `hour` (hora)
-   `milliseconds` (milissegundos)
-   `minute` (minuto)
-   `second` (segundo)
-   `month` (mês)
-   `quarter` (trimestre)
-   `week` — semana do ano
-   `year` (ano)

Vamos chamar a função EXTRACT e obter duas colunas do campo `log_on` da tabela `user_activity`. Essas colunas terão o mês e o dia dos logins dos usuários.

id\_user

log\_on

log\_off

6

2019-03-01 23:34:55

2019-04-01 01:20:45

156

2019-07-03 17:59:21

2019-07-03 19:31:34

65

2019-03-25 14:30:46

2019-03-25 17:47:53

```
SELECT
    id_user,
    EXTRACT(MONTH FROM log_on) AS month_activity,
    EXTRACT(DAY FROM log_on) AS day_activity
FROM
    user_activity;
```

id\_user

month\_activity

day\_activity

6

3

1

156

7

3

65

3

25

O usuário 6 fez login no primeiro dia do terceiro mês e o usuário 156 fez isso no terceiro dia do sétimo mês.

DATE\_TRUNC trunca a data quando você precisa apenas de um certo nível de precisão. (Por exemplo, se você precisa saber em que dia foi feito o pedido, mas a hora não importa, você pode usar DATE\_TRUNC com o argumento "day".) Diferentemente de EXTRACT, a data truncada resultante é fornecida como uma string. A coluna da qual deveria ser tirada a data completa vem depois de uma vírgula:

```
SELECT
    DATE_TRUNC('date_fragment_to_be_truncated_to', column_name) AS new_column_with_date
FROM
    Table_with_all_dates;
```

Você pode usar os seguintes argumentos com a função DATE\_TRUNC:

`'microseconds'` (microssegundos) `'milliseconds'` (milissegundos) `'second'` (segundo) `'minute'` (minuto) `'hour'` (hora) `'day'` (dia) `'week'` (semana) `'month'` (mês) `'quarter'` (trimestre) `'year'` (ano) `'decade'` (década) `'century'` (século)

Às vezes, é importante saber a hora exata na qual o usuário concluiu a ação. Mas quando você precisa agrupar logins por data e hora, os minutos e segundos atrapalham. Então vamos truncar os valores do campo `log_on` para a parte da hora.

```
SELECT
    DATE_TRUNC('hour', log_on) AS date_log_on
FROM
    user_activity;
```

```
date_log_on

2019-03-01 23:00:00

2019-07-03 17:00:00

2019-03-25 14:00:00
```

Com isso em mente, vamos praticar!

Operadores e Funções para Trabalhar com Datas

Tarefa4 / 4

1.

Recupere as horas do campo `date` na tabela `transactions`. Nomeie o campo resultante como `hours`.

2.

Recupere as horas do campo `date` da tabela `transactions`. Nomeie o campo resultante como `hours`. Em seguida, agrupe os dados por `horas` e encontre o número de mercadorias (`id_product`) compradas durante cada hora. Nomeie a variável resultante como `cnt`.

Ordene os resultados em ordem crescente pelo valor do campo `hours`. As colunas devem aparecer nesta ordem: `hours`, `cnt`

3.

Recupere os dias da coluna `date` da tabela `transactions`. Nomeie o campo resultante como `days`. Em seguida, agrupe os dados por dias e encontre o número de mercadorias (`id_product`) compradas para cada dia. Nomeie a variável resultante como `cnt`. Classifique os resultados em ordem crescente por `days`. As colunas devem aparecer nesta ordem: `days`, `cnt`

4.

Você determinou que a maioria das compras ocorreu no primeiro dia do mês. Trunque a data do campo `date` para o nível do dia e nomeie a variável `date_month`. Agrupe os dados por data truncada e encontre o número de mercadorias (`id_product`) compradas cada dia. Nomeie a variável resultante como `cnt`. Ordene os resultados em ordem crescente por `date_month`. As colunas devem aparecer nesta ordem: `date_month`, `cnt`

99

1

2

3

4

5

6

7

8

9

10

SELECT

Date\_trunc('day', date) as date\_month,

count(id\_product) as cnt

FROM

transactions

group by

date\_month

Order by

date\_month asc

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-21-574Z.md
### Última modificação: 2025-05-28 19:38:21

# Subconsultas - TripleTen

Teoria

# Subconsultas

Nesta lição, você vai aprimorar suas habilidades em SQL para não precisar escrever listas longas e complicadas com o operador IN. Olá, esta é a aula sobre subconsultas!

Uma **subconsulta** é uma consulta dentro de uma consulta. Por exemplo, um diretor está procurando um ator para interpretar Hamlet. Para ajudá-lo, o diretor assistente seleciona cinco candidatos fortes para o diretor escolher. A consulta, ou consulta externa, está "selecionando Hamlet", enquanto a subconsulta, ou consulta interna, está "selecionando cinco candidatos".

![](https://practicum-content.s3.amazonaws.com/resources/PT6.4.6_1705572345.png)

As subconsultas podem ser usadas em vários locais dentro de uma consulta.

Se uma subconsulta estiver dentro do bloco FROM, o comando SELECT selecionará os dados da tabela que é gerada pela subconsulta. O nome da tabela consultada no SELECT externo é indicado na consulta interna e a consulta externa se refere às colunas da tabela interna. As subconsultas são sempre colocadas entre parênteses:

```
SELECT 
SUBQUERY_1.column_name, 
SUBQUERY_1.column_name_2
FROM -- para tornar o código legível, coloque as subconsultas em linhas novas
    -- indente as subconsultas
    (SELECT 
        column_name,
        column_name_2
    FROM 
        table_name
    WHERE 
        column_name = value) AS SUBQUERY_1; 
-- não se esqueça de dar nome à sua subconsulta
```

name

book\_id

genre

author

date\_pub

pages

rating

publisher\_id

The City of Mirrors

1

Fantasy

Justin Cronin

2016/01/01

602

4.2

5

The October Country

2

Fiction

Ray Bradbury

1955/01/01

334

4.16

5

Saint Odd

3

Fiction

Dean Koontz

2015/01/01

352

4.14

8

From the Corner of His Eye

4

Thriller

Dean Koontz

2000/01/01

729

4.01

8

The Great and Secret Show

5

Fiction

Clive Barker

1990/01/01

658

4.05

15

World War Z: An Oral History of the Zombie War

6

Horror

Max Brooks

2006/01/01

342

4.01

19

Red Dragon

7

Thriller

Thomas Harris

1981/01/01

348

4.02

17

The Stand

8

Fantasy

Stephen King

1978/01/01

816

4.34

2

Lightning

9

Thriller

Dean Koontz

1988/01/01

384

4.06

14

The Exorcist

10

Classics

William Peter Blatty

1971/01/01

385

4.15

7

The Witching Hour

11

Fiction

Anne Rice

1990/01/01

1038

4.11

10

The Vampire Lestat

12

Paranormal

Anne Rice

1985/01/01

550

4.06

10

Intensity

13

Thriller

Dean Koontz

1995/01/01

436

4.04

10

The Terror

14

Historical Fiction

Dan Simmons

2007/01/01

769

4.04

16

House of Leaves

15

Fantasy

Mark Z. Danielewski

2000/01/01

705

4.12

9

Swan Song

16

Apocalyptic

Robert R. McCammon

1987/01/01

956

4.28

3

Boy's Life

17

Fantasy

Robert R. McCammon

1991/01/01

580

4.34

3

Summer of Night

18

Thriller

Dan Simmons

1991/01/01

600

4.02

18

Full Dark, No Stars

19

Short Stories

Stephen King

2010/01/01

368

4.05

11

Doctor Sleep

20

Thriller

Stephen King

2013/01/01

531

4.1

11

The Green Mile

21

Fantasy

Stephen King

1996/01/01

465

4.43

0

Rot & Ruin

22

Young Adult

Jonathan Maberry

2010/01/01

458

4.1

12

Let the Right One In

23

Paranormal

John Ajvide Lindqvist

2008/01/01

513

4.07

6

The Silence of the Lambs

24

Thriller

Thomas Harris

1988/01/01

367

4.17

6

It

25

Fantasy

Stephen King

1986/01/01

1116

4.22

1

Different Seasons

26

Short Stories

Stephen King

1982/01/01

560

4.35

1

Misery

27

Thriller

Stephen King

1987/01/01

320

4.13

1

NOS4A2

28

Fantasy

Joe Hill

2013/01/01

692

4.06

13

This Book Is Full of Spiders

29

Humor

David Wong

2012/01/01

406

4.26

End of Watch

30

Thriller

Stephen King

2016/01/01

432

4.09

Vamos escrever uma consulta para encontrar o número médio de avaliações dadas a livros de cada gênero. Para isso, teremos que chamar duas funções de agregação: COUNT encontrará o número de classificações e AVG calculará a média. No entanto, o comando `SELECT AVG(COUNT(rating))` resultará em uma mensagem de erro: `ERROR: aggregate function calls cannot be nested` (ERRO: as funções de agregação não podem ser aninhadas).

Assim como no exemplo do teatro, primeiro temos que convidar os atores para uma audição; só assim podemos escolher o melhor. Vamos encontrar o número de avaliações por gênero usando uma subconsulta e calcular a média na consulta externa:

```
SELECT 
    AVG(Sub.count_rating) AS avg_count_rating
FROM
    (SELECT 
        COUNT(rating) as count_rating
    FROM 
        books
    GROUP BY genre) AS Sub;
```

Aqui está o que obtemos quando a consulta interna é executada:

genre

count\_rating

Thriller

9

Classics

1

Paranormal

2

Short Stories

2

Young Adult

1

Apocalyptic

1

Historical Fiction

1

Humor

1

Fantasy

7

Horror

1

Fiction

4

A consulta interna calculou o número de avaliações de livros para cada gênero e salvou os valores no campo `count_rating`. Agora, a consulta externa pode pegar a tabela resultante e fazer seu trabalho.

Agora chamamos a coluna `count_rating` da tabela `Sub` e calculamos o valor médio usando AVG:

```
2.72
```

O que este número significa? Isso significa que cada gênero recebeu, em média, 2,72 avaliações.

Você pode precisar de subconsultas em vários locais da sua consulta. Vamos colocar uma no bloco WHERE.

A consulta principal comparará os resultados da subconsulta com os valores da tabela no bloco FROM externo. Quando houver uma correspondência, os dados serão selecionados:

```
SELECT 
    column_name, 
    column_name_1
FROM 
    table_name
WHERE 
    column_name = 
        (SELECT 
            column_1
        FROM 
            table_name_2 
        WHERE
            column_1  = value);
```

Agora vamos adicionar uma instrução IN à nossa consulta e coletar dados que correspondem a mais de um valor:

```
SELECT 
    column_name, 
    column_name_1
FROM 
    table_name
WHERE 
    column_name IN  
            (SELECT 
                column_1
            FROM 
                table_name_2  
            WHERE 
                column_1 = value_1 OR column_1 = value_2);
```

Exemplo: adicionaremos a tabela `publisher` com códigos de ID e nomes de editores para complementar nossa tabela `books`:

name

publisher\_id

Signet Books

0

Viking Press

1

Doubleday

2

Pocket Books

3

Ballantine Books

5

St. Martin's Press

6

Harper & Row

7

Bantam Publishing

8

Pantheon

9

Knopf

10

Scribner

11

Simon & Schuster

12

William Morrow and Company

13

G. P. Putnam's Sons

14

Collins

15

Little, Brown and Company

16

Dell Publishing

17

Putnam Pub Group

18

Crown

19

Vamos selecionar os livros publicados pela Knopf na tabela `books`:

```
SELECT 
    name,
    publisher_id
FROM 
    books
WHERE 
    publisher_id = 
            (SELECT 
                 publisher_id
            FROM 
                publisher
            WHERE 
                name ='Knopf');
```

id

name

10

Knopf

A subconsulta seleciona na tabela `publisher` apenas os valores `publisher_id` que correspondem ao nome do editor Knopf. O resultado é 10. Existe apenas um valor na tabela `publisher` relacionado ao Knopf.

Em seguida, a consulta externa seleciona os nomes e IDs de editora da tabela `books`, mas apenas aqueles que correspondem ao resultado de nossa consulta interna (ou seja, cujo ID de editor é 10).

Aqui está o resultado da consulta externa:

name

publisher\_id

The Witching Hour

10

The Vampire Lestat

10

Intensity

10

Vamos escrever uma consulta semelhante, adicionando Collins e Crown à lista de editoras. Aqui vamos precisar de IN:

```
SELECT 
    name,
    publisher_id
FROM 
    books
WHERE 
    publisher_id IN 
            (SELECT 
                 publisher_id
            FROM 
                publisher
            WHERE 
                name IN ('Knopf', 'Collins', 'Crown'));
```

A instrução IN da consulta interna seleciona os IDs do editor associados aos nomes Knopf, Collins e Crown da tabela `publisher`, fornecendo 10, 15 e 19. Esses valores então são passados para a consulta externa.

A consulta externa, por sua vez, seleciona da tabela `books` os nomes e editores associados com o `publisher_id` igual aos valores selecionados pela consulta interna: nossos velhos amigos 10, 15 e 19

name

publisher\_id

The Great and Secret Show

15

World War Z: An Oral History of the Zombie War

19

The Witching Hour

10

The Vampire Lestat

10

Intensity

10

Agora, com isso em mente, vamos praticar!

Subconsultas

Tarefa4 / 4

1.

O chefe do seu departamento gostaria de ter as listas de clientes que preferem produtos premium, bem como um relatório sobre o número médio diário de transações para cada semana. Segurem seus chapéus, é um estudo de público-alvo!

Escreva uma consulta para selecionar IDs de produtos (`id_product`) da tabela `products_data_all`:

-   de mercadorias na categoria `'milk'` que custam mais de $17
-   de mercadorias na categoria `'butter'` que custam mais de $9

Lembre-se, os nomes dos campos são os seguintes:

-   categoria — `category`
-   preço — `price`

2.

Escreva uma consulta para selecionar os identificadores **unívocos** (`user_id`) de clientes que compraram produtos (`id_product`) da categoria `'milk'`que custam mais de US$ 17 ou da categoria `'butter'` que custam mais de US$9. Use a tabela `transactions`.

3.

Escreva uma consulta para encontrar o número de transações **unívocas** (`id_transaction`) na tabela `transactions`. Agrupe os totais de transações por dia, truncando a coluna `data` para o nível do dia. Nomeie a coluna da transação resultante como `transactions_per_day` e aquela com a data truncada `trunc_date`. Você não precisará usar as subconsultas aqui.

O resultado terá entradas como esta:

transactions\_per\_day

trunc\_date

330

2019-06-01 00:00:00

4.

Transforme a consulta de nossa tarefa anterior em uma subconsulta de bloco `FROM`. Nomeie o resultado como `SUBQ`.

Enquanto isso, na consulta externa:

-   Chame a função `EXTRACT` para recuperar o número da semana da coluna `trunc_date` da tabela `SUBQ`. Nomeie o campo resultante como `week_number`.
-   Encontre a média da coluna `transactions_per_day` da tabela `SUBQ` e salve-a na variável `avg_week_transaction`.
-   Agrupe os dados por `week_number`.

As colunas devem aparecer nesta ordem: `week_number`, `avg_week_transaction`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

SELECT

extract(WEEK from SUBQ.trunc\_date)as week\_number,

avg(SUBQ.transactions\_per\_day) as avg\_week\_transaction

from

(SELECT

COUNT(DISTINCT id\_transaction) AS transactions\_per\_day,

DATE\_TRUNC('day', date) AS trunc\_date

FROM

transactions

GROUP BY

trunc\_date) as SUBQ

group by

week\_number

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-23-870Z.md
### Última modificação: 2025-05-28 19:38:24

# Funções de Janela - TripleTen

Teoria

# Funções de Janela

Às vezes, quando os analistas estão trabalhando com bancos de dados e executando funções de agregação, não é preciso obter dados agrupados, mas sim, outro parâmetro: valores de coluna somados, média e assim por diante.

Além das tarefas que envolvem funções de agregação, os analistas também encontram tarefas que exigem o processamento adicional de linhas (por exemplo, classificando ou descobrindo a diferença entre os valores de uma linha). É teoricamente possível resolver essas tarefas fazendo consultas complicadas com subconsultas e agrupamentos, mas isso é ineficiente e levaria muito tempo para executar.

Vamos analisar a tabela `books_price`, que contém um preço para cada livro:

book\_id

name

genre\_id

author\_id

publisher\_id

date\_pub

pages

rating

price

1

The City of Mirrors

0

3

5

2016/01/01

602

4.2

16.56

2

The October Country

5

5

5

1955/01/01

334

4.16

7.35

3

Saint Odd

5

7

8

2015/01/01

352

4.14

9.19

4

From the Corner of His Eye

4

7

8

2000/01/01

729

4.01

9.99

5

The Great and Secret Show

5

13

15

1990/01/01

658

4.05

18.99

6

World War Z: An Oral History of the Zombie War

10

15

19

2006/01/01

342

4.01

15.64

7

Red Dragon

4

4

17

1981/01/01

348

4.02

9.19

8

The Stand

0

0

2

1978/01/01

816

4.34

9.19

9

Lightning

4

7

14

1988/01/01

384

4.06

7.35

10

The Exorcist

6

6

7

1971/01/01

385

4.15

15.63

11

The Witching Hour

5

9

10

1990/01/01

1038

4.11

9.19

12

The Vampire Lestat

8

9

10

1985/01/01

550

4.06

8.27

13

Intensity

4

7

10

1995/01/01

436

4.04

9.19

14

The Terror

9

14

16

2007/01/01

769

4.04

17.47

15

House of Leaves

0

8

9

2000/01/01

705

4.12

20.24

16

Swan Song

2

1

3

1987/01/01

956

4.28

10.11

17

Boy's Life

0

1

3

1991/01/01

580

4.34

14.37

18

Summer of Night

4

14

18

1991/01/01

600

4.02

17.47

19

Full Dark, No Stars

1

0

11

2010/01/01

368

4.05

15.64

20

Doctor Sleep

4

0

11

2013/01/01

531

4.1

10.99

21

The Green Mile

0

0

0

1996/01/01

465

4.43

9.99

22

Rot & Ruin

7

10

12

2010/01/01

458

4.1

11.95

23

Let the Right One In

8

11

6

2008/01/01

513

4.07

18.39

24

The Silence of the Lambs

4

4

6

1988/01/01

367

4.17

16.55

25

It

0

0

1

1986/01/01

1116

4.22

15.2

26

Different Seasons

1

0

1

1982/01/01

560

4.35

16.56

27

Misery

4

0

1

1987/01/01

320

4.13

9.19

28

NOS4A2

0

12

13

2013/01/01

692

4.06

17.47

29

This Book Is Full of Spiders

3

2

2012/01/01

406

4.26

17.99

30

End of Watch

4

0

2016/01/01

432

4.09

9.99

Digamos que queremos encontrar a razão entre o preço de cada livro e o custo total. Com base no que aprendemos, podemos escrever o seguinte:

```
SELECT
    author_id,
    name,
    price,
    price / (
        SELECT
            SUM(price)
        FROM
            books_price)  AS ratio
FROM
    books_price;
```

O resultado fica assim:

author\_id

name

price

ratio

3

The City of Mirrors

16.56

0.04189223

5

The October Country

7.35

0.018593471

7

Saint Odd

9.19

0.023248164

7

From the Corner of His Eye

9.99

0.025271943

13

The Great and Secret Show

18.99

0.048039462

15

World War Z: An Oral History of the Zombie War

15.64

0.039564885

4

Red Dragon

9.19

0.023248164

0

The Stand

9.19

0.023248164

7

Lightning

7.35

0.018593471

6

The Exorcist

15.63

0.03953959

9

The Witching Hour

9.19

0.023248164

9

The Vampire Lestat

8.27

0.02092082

7

Intensity

9.19

0.023248164

14

The Terror

17.47

0.044194277

8

House of Leaves

20.24

0.051201615

1

Swan Song

10.11

0.02557551

1

Boy's Life

14.37

0.036352135

14

Summer of Night

17.47

0.044194277

0

Full Dark, No Stars

15.64

0.039564885

0

Doctor Sleep

10.99

0.027801668

0

The Green Mile

9.99

0.025271943

10

Rot & Ruin

11.95

0.030230204

11

Let the Right One In

18.39

0.046521626

4

The Silence of the Lambs

16.55

0.041866932

0

It

15.2

0.038451806

0

Different Seasons

16.56

0.04189223

0

Misery

9.19

0.023248164

12

NOS4A2

17.47

0.044194277

2

This Book Is Full of Spiders

17.99

0.045509737

0

End of Watch

9.99

0.025271943

Há um problema: o custo total é um valor fixo, mas é calculado repetidamente para cada linha. Isso pode ser aceitável para uma tabela curta, mas se estivermos lidando com milhões de linhas, isso seria realmente ineficiente.

Felizmente, o SQL possui uma ferramenta que nos permite evitar tais problemas: funções de janela, que realizam todas as suas operações dentro de **janelas**.

No SQL, uma janela é uma sequência de linhas nas quais se fazem cálculos. Poderia ser a tabela inteira ou, por exemplo, as seis linhas acima da atual. Trabalhar com essas janelas é diferente de trabalhar com consultas habituais.

Podemos resolver rapidamente a tarefa descrita acima com uma função de janela. A solução ficará assim:

```
SELECT
    author_id,
    name,
    price / SUM(price) OVER () AS ratio
FROM
    books_price;
```

A função que precede a palavra-chave OVER será executada nos dados dentro da janela. Se não indicarmos nenhum parâmetro (como aqui), será usado todo o resultado da consulta.

author\_id

name

ratio

3

The City of Mirrors

0.04189223

5

The October Country

0.018593471

7

Saint Odd

0.023248164

7

From the Corner of His Eye

0.025271943

13

The Great and Secret Show

0.048039462

15

World War Z: An Oral History of the Zombie War

0.039564885

4

Red Dragon

0.023248164

0

The Stand

0.023248164

7

Lightning

0.018593471

6

The Exorcist

0.03953959

9

The Witching Hour

0.023248164

9

The Vampire Lestat

0.02092082

7

Intensity

0.023248164

14

The Terror

0.044194277

8

House of Leaves

0.051201615

1

Swan Song

0.02557551

1

Boy's Life

0.036352135

14

Summer of Night

0.044194277

0

Full Dark, No Stars

0.039564885

0

Doctor Sleep

0.027801668

0

The Green Mile

0.025271943

10

Rot & Ruin

0.030230204

11

Let the Right One In

0.046521626

4

The Silence of the Lambs

0.041866932

0

It

0.038451806

0

Different Seasons

0.04189223

0

Misery

0.023248164

12

NOS4A2

0.044194277

2

This Book Is Full of Spiders

0.045509737

0

End of Watch

0.025271943

Agora vamos tornar nossa tarefa um pouco mais complexa: encontraremos a razão entre o preço de cada livro e o custo total dos livros daquele autor. Para fazer isso, precisaremos agrupar os dados com um método semelhante ao GROUP BY. Para as funções de janela, isso é PARTITION BY:

```
SELECT
    author_id,
    name,
    price / SUM(price) OVER (PARTITION BY author_id) AS ratio
FROM
    books_price;
```

author\_id

name

ratio

0

Full Dark, No Stars

0.16165376

0

End of Watch

0.10325582

0

Misery

0.09498709

0

Different Seasons

0.1711628

0

It

0.15710595

0

The Green Mile

0.10325582

0

Doctor Sleep

0.11359174

0

The Stand

0.09498709

1

Swan Song

0.41299018

1

Boy's Life

0.5870098

2

This Book Is Full of Spiders

1

3

The City of Mirrors

1

4

The Silence of the Lambs

0.6429682

4

Red Dragon

0.35703188

5

The October Country

1

6

The Exorcist

1

7

Saint Odd

0.2572788

7

Intensity

0.2572788

7

Lightning

0.20576707

7

From the Corner of His Eye

0.27967525

8

House of Leaves

1

9

The Witching Hour

0.5263459

9

The Vampire Lestat

0.47365412

10

Rot & Ruin

1

11

Let the Right One In

1

12

NOS4A2

1

13

The Great and Secret Show

1

14

The Terror

0.5

14

Summer of Night

0.5

15

World War Z: An Oral History of the Zombie War

1

Nesses exemplos, as nossas janelas acabaram sendo a tabela inteira. Na próxima aula, veremos exemplos nos quais esse não é o caso.

Por enquanto, vamos praticar!

Funções de Janela

Tarefa2 / 2

1.

Escreva uma consulta para calcular a proporção dos preços de produtos específicos (`name`) para o preço médio dos produtos vendidos de suas categorias (`category`) em suas lojas (`name_store`). Selecione também os preços dos produtos (`price`).

Salve o resultado na variável `product_mul`. Não é necessário usar `CAST`.

As primeiras linhas da tabela resultante devem ficar assim:

product\_name

store\_name

category

product\_price

product\_mul

a2 Milk Whole Milk, 59 oz

Uncle Joe's Store

milk

3.56

1.01888

a2 Milk Whole Milk, 59 oz

T-E-B

milk

3.45

1.02882

a2 Milk Whole Milk, 59 oz

Uncle Joe's Store

milk

3.43

0.981675

2.

Escreva uma consulta que encontre a participação de cada categoria (`category`) no total de vendas de cada loja (`name_store`) para cada dia (`sale_date`) no período de 1 a 6 de junho.

Classifique o resultado pela data de compra e nome da loja e salve-o na variável `percent`.

As primeiras linhas da tabela resultante devem ficar assim:

STORE\_NAME

SALE\_DATE

CATEGORY

PERCENT

Four

2019-06-01

butter

16.7999

Four

2019-06-01

milk

83.2001

Milk Market

2019-06-01

butter

14.063

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

SELECT DISTINCT

name\_store AS store\_name,

date\_upd::date AS sale\_date,

category AS category,

SUM(price) OVER (

PARTITION BY

name\_store, category, date\_upd

) \* 100 / SUM(price) OVER (

PARTITION BY

name\_store, date\_upd

) AS percent

FROM

products\_data\_all

WHERE

date\_upd::date BETWEEN '2019-06-01'

AND '2019-06-06'

ORDER BY

date\_upd::date,

name\_store;

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-25-212Z.md
### Última modificação: 2025-05-28 19:38:25

# Uma análise mais detalhada das funções de janela - TripleTen

Teoria

# Uma análise mais detalhada das funções de janela

Vamos ver de onde as funções de janela recebem o seu nome. Digamos que precisamos aplicar uma função de agregação a um fragmento de tabela, em vez de à tabela inteira, e encontrar um valor acumulado: por exemplo, o valor acumulado das mudanças no estoque da livraria. (No início, a loja oferece apenas um livro, por $10; o valor acumulado é de $10. Depois ela adquire um segundo livro de cinco dólares e o valor acumulado se torna $15.)

Para simplificar, diremos que a loja tem um livro de cada.

Então, como obteríamos o valor acumulado? Temos que fazer a tabela `sales_1` (aquela dentro do quadro roxo abaixo), `sales_2` (verde) e `sales_3` (vermelho), até `sales_i`, para que cada um deles tenha um, duas ... _i_ linhas com vendas. E os dados nessas linhas serão cumulativos: a tabela `sales_i` incluirá os dados de `sales_i-1`.

Em seguida, precisamos gerar a soma cumulativa juntamente com a linha da tabela principal. Para isso, teríamos que selecionar a tabela onde a linha em questão é a última linha e somar os valores. Mas construir essas tabelas demora muito. Aqui as tabelas virtuais são úteis: elas percorrem os dados enquanto a consulta está sendo executada. Essas tabelas virtuais são, na verdade, as janelas.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-08-16T215345.786_1597604036.png)

Para que uma janela funcione corretamente, é importante indicar os parâmetros de ordenação para que o SGBD compreenda em que ordem estão os dados.

Vamos obter nosso valor acumulado:

```
SELECT
    author_id,
    name,
    SUM(price) OVER (ORDER BY author_id ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)
FROM
    books_price;
```

Como você deve ter notado, temos duas novas palavras-chave aqui: ORDER BY e ROWS. O comando ORDER BY nos permite definir a ordem de classificação das linhas pelas quais a janela será executada. Em ROWS indicamos os quadros das janelas sobre as quais deve ser calculada uma função de agregação. Aqui está o resultado:

author\_id

name

sum

0

Full Dark, No Stars

15.64

0

End of Watch

25.630001

0

Misery

34.82

0

Different Seasons

51.379997

0

It

66.579994

0

The Green Mile

76.56999

0

Doctor Sleep

87.55999

0

The Stand

96.74999

1

Swan Song

106.85999

1

Boy's Life

121.229996

2

This Book Is Full of Spiders

139.22

3

The City of Mirrors

155.78

4

The Silence of the Lambs

172.33

4

Red Dragon

181.52

5

The October Country

188.87001

6

The Exorcist

204.50002

7

Saint Odd

213.69002

7

Intensity

222.88002

7

Lightning

230.23003

7

From the Corner of His Eye

240.22003

8

House of Leaves

260.46002

9

The Witching Hour

269.65002

9

The Vampire Lestat

277.92

10

Rot & Ruin

289.87003

11

Let the Right One In

308.26

12

NOS4A2

325.73

13

The Great and Secret Show

344.72

14

The Terror

362.19

14

Summer of Night

379.66

15

World War Z: An Oral History of the Zombie War

395.30002

Os quadros podem ser indicados de várias maneiras:

-   UNBOUNDED PRECEDING — todas as linhas que estão acima da atual
-   N PRECEDING — as _n_ linhas acima da atual
-   CURRENT ROW — a linha atual
-   N FOLLOWING — as _n_ linhas abaixo da atual
-   UNBOUNDED FOLLOWING — todas as linhas abaixo da atual

Os próprios parâmetros podem ser combinados.

Aqui está outro exemplo de uso de uma função de janela. Isso nos dá o número acumulado de páginas para cada autor:

```
SELECT
    author_id,
    name,
    pages,
    SUM(pages) OVER (PARTITION BY author_id ORDER BY author_id ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)
FROM
    books_price;
```

Resultado:

author\_id

name

pages

sum

0

Full Dark, No Stars

368

368

0

End of Watch

432

800

0

Misery

320

1120

0

Different Seasons

560

1680

0

It

1116

2796

0

The Green Mile

465

3261

0

Doctor Sleep

531

3792

0

The Stand

816

4608

1

Swan Song

956

956

1

Boy's Life

580

1536

2

This Book Is Full of Spiders

406

406

3

The City of Mirrors

602

602

4

The Silence of the Lambs

367

367

4

Red Dragon

348

715

5

The October Country

334

334

6

The Exorcist

385

385

7

Saint Odd

352

352

7

Intensity

436

788

7

Lightning

384

1172

7

From the Corner of His Eye

729

1901

8

House of Leaves

705

705

9

The Witching Hour

1038

1038

9

The Vampire Lestat

550

1588

10

Rot & Ruin

458

458

11

Let the Right One In

513

513

12

NOS4A2

692

692

13

The Great and Secret Show

658

658

14

The Terror

769

769

14

Summer of Night

600

1369

15

World War Z: An Oral History of the Zombie War

342

342

## Funções de agregação especiais

Além de SUM, AVG e COUNT, existem funções de agregação especiais que você pode usar ao trabalhar com as janelas. Vamos dar uma olhada nelas mais de perto.

### Funções de classificação: RANK

A função RANK retorna o número do índice da linha na janela atual. Se várias linhas tiverem a mesma combinação das colunas passadas para PARTITION BY e ORDER BY dentro da cláusula OVER, elas receberão o mesmo número. Por exemplo, se existem dois livros do mesmo autor com o mesmo número de páginas, um RANK() OVER (PARTITION BY author\_id ORDER BY pages) vai retornar o mesmo ranking para os dois livros. Esse recurso nos permite classificar os nossos dados.

Para classificar livros com base no número de páginas de cada autor, podemos executar a seguinte consulta:

```
SELECT
    author_id,
    name,
    pages,
    RANK() OVER (PARTITION BY author_id ORDER BY pages)
FROM
    books_price;
```

Resultado:

author\_id

name

pages

rank

0

Misery

320

1

0

Full Dark, No Stars

368

2

0

End of Watch

432

3

0

The Green Mile

465

4

0

Doctor Sleep

531

5

0

Different Seasons

560

6

0

The Stand

816

7

0

It

1116

8

1

Boy's Life

580

1

1

Swan Song

956

2

2

This Book Is Full of Spiders

406

1

3

The City of Mirrors

602

1

4

Red Dragon

348

1

4

The Silence of the Lambs

367

2

5

The October Country

334

1

6

The Exorcist

385

1

7

Saint Odd

352

1

7

Lightning

384

2

7

Intensity

436

3

7

From the Corner of His Eye

729

4

8

House of Leaves

705

1

9

The Vampire Lestat

550

1

9

The Witching Hour

1038

2

10

Rot & Ruin

458

1

11

Let the Right One In

513

1

12

NOS4A2

692

1

13

The Great and Secret Show

658

1

14

Summer of Night

600

1

14

The Terror

769

2

15

World War Z: An Oral History of the Zombie War

342

1

### Função de categorização: NTILE

Com o NTILE, podemos colocar uma determinada linha de saída em um grupo. Isso é semelhante ao particionamento de dados em quartis. O número de grupos em que os dados devem ser divididos é passado para a função.

Por exemplo, podemos precisar dividir os livros em cinco categorias com base no preço. Podemos fazer isso com a seguinte consulta:

```
SELECT
    author_id,
    name,
    price,
    ntile(5) OVER (ORDER BY price)
FROM
    books_price;
```

Resultado:

author\_id

name

price

ntile

5

The October Country

7.35

1

7

Lightning

7.35

1

9

The Vampire Lestat

8.27

1

7

Intensity

9.19

1

0

The Stand

9.19

1

4

Red Dragon

9.19

1

7

Saint Odd

9.19

2

0

Misery

9.19

2

9

The Witching Hour

9.19

2

0

The Green Mile

9.99

2

7

From the Corner of His Eye

9.99

2

0

End of Watch

9.99

2

1

Swan Song

10.11

3

0

Doctor Sleep

10.99

3

10

Rot & Ruin

11.95

3

1

Boy's Life

14.37

3

0

It

15.2

3

6

The Exorcist

15.63

3

15

World War Z: An Oral History of the Zombie War

15.64

4

0

Full Dark, No Stars

15.64

4

4

The Silence of the Lambs

16.55

4

3

The City of Mirrors

16.56

4

0

Different Seasons

16.56

4

14

The Terror

17.47

4

14

Summer of Night

17.47

5

12

NOS4A2

17.47

5

2

This Book Is Full of Spiders

17.99

5

11

Let the Right One In

18.39

5

13

The Great and Secret Show

18.99

5

8

House of Leaves

20.24

5

### Funções de deslocamento: **LAG e LEAD**

Muitas vezes, você precisará comparar o valor atual com os anteriores ou subsequentes. Para isso, existem as funções LAG e LEAD, respectivamente.

Você pode passar à função o nome do campo e o deslocamento (o número de linhas) sobre o qual o valor deveria ser obtido. Se você não indicar o deslocamento, ele reverterá para o padrão — 1.

Por exemplo, podemos descobrir se um livro é mais curto que o livro anterior do autor com esta consulta:

```
SELECT
    author_id,
    name,
    pages,
    LAG(pages) OVER (PARTITION BY author_id ORDER BY date_pub)
FROM
    books_price;
```

Resultado:

author\_id

name

pages

lag

0

The Stand

816

NULL

0

Different Seasons

560

816

0

It

1116

560

0

Misery

320

1116

0

The Green Mile

465

320

0

Full Dark, No Stars

368

465

0

Doctor Sleep

531

368

0

End of Watch

432

531

1

Swan Song

956

NULL

1

Boy's Life

580

956

2

This Book Is Full of Spiders

406

NULL

3

The City of Mirrors

602

NULL

4

Red Dragon

348

NULL

4

The Silence of the Lambs

367

348

5

The October Country

334

NULL

6

The Exorcist

385

NULL

7

Lightning

384

NULL

7

Intensity

436

384

7

From the Corner of His Eye

729

436

7

Saint Odd

352

729

8

House of Leaves

705

NULL

9

The Vampire Lestat

550

NULL

9

The Witching Hour

1038

550

10

Rot & Ruin

458

NULL

11

Let the Right One In

513

NULL

12

NOS4A2

692

NULL

13

The Great and Secret Show

658

NULL

14

Summer of Night

600

NULL

14

The Terror

769

600

15

World War Z: An Oral History of the Zombie War

342

NULL

Uma análise mais detalhada das funções de janela

Tarefa2 / 2

1.

Você vai trabalhar com a tabela `products_data_all`. Trabalhando com os dados da loja Four (`name_store`), escreva uma consulta para calcular a variação da receita total em cada categoria (`category`) e na loja em geral após a venda de cada produto (`name`) em 2 de junho de 2019. Armazene os resultados nas variáveis `category_accum` e `store_accum`, respectivamente. Ordene os valores por `id_product`. Imprima os preços dos produtos (`price`) antes dos resultados.

As primeiras linhas da tabela resultante devem ficar assim:

store\_name

category

product\_name

price

category\_accum

store\_accum

Four

milk

Borden Super Chox Chocolate Drink, 1 gal

2.38

2.38

2.38

Four

milk

Fairlife 2% Chocolate Reduced Fat Milk, 52 oz

3.16

5.54

5.54

Four

milk

Мoo-Moo Select Ingredients Fat Free Milk, 1 gal

2.28

7.82

7.82

2.

Escreva uma consulta que classificará o preço em cada loja (`name_store`) e a categoria (`category`) de 2 de junho. Imprima o nome do produto (`name`) e seu preço antes da coluna que contém a posição do produto dentro do grupo.

Classifique o resultado pelo nome da loja, categoria e posição no grupo.

As primeiras linhas da tabela resultante devem ficar assim:

STORE\_NAME

CATEGORY

SALE\_DATE

NAME

PRICE

RANK

Four

butter

2019-06-02

Land O Lakes Honey Butter Spread, 6.5 oz

1.87

1

Four

butter

2019-06-02

Pillsbury Butter Flake Crescent Dinner Rolls, 8 ct

1.97

2

Four

butter

2019-06-02

Nabisco Ritz Peanut Butter Cracker Sandwiches, 8 ct

2.86

3

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

SELECT DISTINCT

name\_store AS store\_name,

category AS category,

date\_upd::date AS sale\_date,

name,

price,

RANK() OVER (PARTITION BY name\_store, category ORDER BY price) AS rank

FROM

products\_data\_all

WHERE

date\_upd::date \= '2019-06-02'

ORDER BY

store\_name,

category,

rank;

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-26-540Z.md
### Última modificação: 2025-05-28 19:38:26

# Conclusão - TripleTen

Capítulo 4/8

Funcionalidades Avançadas de SQL para Análise

# Conclusão

Neste capítulo você aprendeu:

-   Quais comandos usar para agrupar e ordenar elementos em uma tabela (GROUP BY e ORDER BY))
    
-   Qual comando limita o número de linhas a serem impressas (LIMIT)
    
-   Que existe uma construção análoga a WHERE para as funções de agregação (HAVING)
    
-   Quais funções são projetadas para processar os valores de data e hora (EXTRACT e DATE\_TRUNC)
    
-   O que são as subconsultas e como usá-las
    

Ah, e as subconsultas ajudaram você a progredir na resolução da tarefa de produtos lácteos: você encontrou o número médio de transações por dia, agrupado por semana!

### Leve isso com você

Baixe a [folha de conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/moved_Folha_de_Concluses_Recursos_Avanados_de_SQL_para_Analistas.pdf) e o [sumário do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/moved_Resumo_do_Captulo_Recursos_Avanados_de_SQL_para_Analistas.pdf) da Base de Conhecimento para que você possa consultá-los sempre que precisar.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-28-680Z.md
### Última modificação: 2025-05-28 19:38:29

# Introdução - TripleTen

Capítulo 5/8

Relacionamentos Entre Tabelas

# Introdução

### O que você vai aprender:

-   Como identificar conexões entre tabelas de diagramas ER
-   Como juntar tabelas usando os métodos INNER JOIN e OUTER JOIN
-   Como procurar por valores vazios e substrings em tabelas.
-   Como combinar tabelas com os métodos UNION e UNION ALL

### Quanto tempo irá demorar:

12 lições, com aproximadamente 15-20 minutos cada

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-30-052Z.md
### Última modificação: 2025-05-28 19:38:30

# Tipos de Relacionamentos entre Tabelas - TripleTen

Capítulo 5/8

Relacionamentos Entre Tabelas

# Tipos de Relacionamentos entre Tabelas

Temos duas tabelas, `books` e `author`. Como podemos juntá-las?

Podemos adicionar o campo `author_id` à tabela `books`, para que ele esteja em ambas as tabelas. Em `books` ele vai se referir ao `author`, e na tabela `author` será possível usar o ID (chave primária) para determinar de que autor estamos falando.

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT6.5.2_1666176924.png)

Quando uma coluna contém os valores do campo de outra tabela, a isso se chama **chave estrangeira**. Ela é responsável pelo relacionamento dessas tabelas. Em nosso caso, o campo `author_id` é a chave estrangeira.

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT6.5.2.2_1666176955.png)

Há três tipos de relacionamento:

-   um-para-um
-   um-para-muitos
-   muitos-para-muitos

Em um relacionamento **um-para-um**, cada linha em uma tabela está conectada com apenas uma linha na outra tabela. É como se uma tabela tivesse sido dividida ao meio.

Vamos observar uma tabela contendo nomes de empregados e cargos, e outra com dados de pagamento. Cada funcionário está ligado a uma linha simples contendo dados de seu passaporte e da conta bancária em que seu salário é depositado.

![](https://practicum-content.s3.amazonaws.com/resources/PT6.5.2.3_1706606862.png)

Esse é um tipo raro de relacionamento.

Em um relacionamento **um-para-muitos**, cada linha de uma tabela corresponde a múltiplas linhas na outra tabela.

Esse é o tipo de relacionamento que observamos com os livros e autores. Um autor pode escrever diversos livros, mas cada livro possui um único autor.

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT6.5.2.4_1666176999.png)

Mas em alguns casos livros são escritos por mais de um autor (por exemplo, Neil Gaiman e Terry Pratchett). Nesses casos, o muitos-para-muitos será útil.

Em um relacionamento **muitos-para-muitos**, várias linhas de uma tabela correspondem a várias linhas da outra tabela. Esse tipo de relacionamento produz uma **tabela de associação,** que combina as chaves primárias de ambas tabelas.

Por exemplo,

-   Temos a tabela `author`onde a chave primária é `author_id`.
-   Temos a tabela `books` com dados sobre livros, cuja chave primária é `book_id`.
-   Agora precisamos criar uma tabela de associação `authors_books`, juntando `book_id` e `author_id`:

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT6.5.2.5_1666177027.png)

Pergunta

Qual desses é usado para conectar tabelas em bancos de dados?

Chaves primárias

Chaves estrangeiras

Isso mesmo!

Muitos-para-muitos

Você conseguiu!

Pergunta

Como você chama o tipo de relacionamento em que uma linha da primeira tabela corresponde a diversas linhas da outra, ou em que várias linhas da segunda tabela correspondem a uma linha da primeira?

um-para-muitos

Isso mesmo!

um-para-um

muitos-para-muitos

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-31-390Z.md
### Última modificação: 2025-05-28 19:38:31

# Diagramas ER - TripleTen

Capítulo 5/8

Relacionamentos Entre Tabelas

# Diagramas ER

A estrutura de bancos de dados pode ser visualizada com **diagramas** **ER** (**entidade-relacionamento**). Eles mostram as tabelas e seus relacionamentos.

### Tabelas

Este é um fragmento de um diagrama ER:

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT6.5.3_1666177110.png)

As tabelas são mostradas como retângulos (caixas) com duas partes. O nome da tabela vai na parte superior; nesse caso, temos tabelas chamadas `books` e `author`.

Na parte inferior vemos listas de campos de tabelas com uma indicação dos seus tipos de chaves: primária ou estrangeira. As chaves são geralmente marcadas com **PK** (primária) or **FK** (estrangeira), mas elas também podem ser marcadas com um ícone de chave, um #, ou outro símbolo.

Como lemos esse fragmento do diagrama ER?

-   A tabela `books` contém os seguintes campos: `book_id`, `name`, `genre`, `author_id`, `date_pub`, `pages`, `price`, `rating`, `pub_name`
-   Chave primária — o campo `book_id` (marcado com PK)
-   Chave estrangeira — o campo `author_id` (FK)

### Relacionamentos em diagramas ER

Diagramas ER também mostram relacionamentos. O fim da linha conectando duas tabelas indica se um ou diversos valores de uma tabela correspondem a valores da outra.

Abaixo vemos um relacionamento um-para-vários. Um autor pode ter diversos livros, mas cada livro possui apenas um autor.

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT6.5.3.2_1666177141.png)

Cada empregado recebe seu salário em apenas uma conta bancária. Aqui temos um relacionamento um-para-um:

![](https://practicum-content.s3.us-west-1.amazonaws.com/resources/PT6.5.3.3_1745428028.png)

Como mencionado anteriormente, às vezes um livro pode ter vários autores, cada um deles, por sua vez, podem ter vários livros. É um caso clássico de relacionamento muitos-para-muitos:

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/PT6.5.3.4_1666177172.png)

Pergunta

Os diagramas ER mostram:

dados das tabelas

tabela, nomes de campos e seus relacionamentos

Isso mesmo!

SGBD

tipos de dados para os valores de um campo da tabela

Trabalho maravilhoso!

Pergunta

Qual dessas imagens ilustra um relacionamento muitos-para-muitos?

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_odno_k_odnomu_1_1585482834.jpg)

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_odno_ko_mnogim_1585482856.jpg)

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_mnogie_ko_mnogim_1585482873.jpg)

Você conseguiu!

Pergunta

Escolha a descrição correta da tabela Designs de acordo com o diagrama ER.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1585482919.png)

Designs. A tabela contém os campos: DesignID, OrderID, Design Type, Design Location, Design Colors. Chave primária: OrderID. Chave estrangeira: DesignID.

Designs. A tabela contém os campos: Design Type, Design Location, Design Colors. Chave primária: OrderID. Chave estrangeira: DesignID.

Designs. A tabela contém os campos: DesignID, OrderID, Design Type, Design Location. Chave primária: DesignID. Chave estrangeira: OrderID.

Nome da tabela: Designs. A tabela contém os campos: DesignID, OrderID, Design Type, Design Location, Design Colors. Chave primária: DesignID. Chave estrangeira: OrderID.

Isso mesmo!

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-35-396Z.md
### Última modificação: 2025-05-28 19:38:35

# Prazer em Conhecê-las, Tabelas! - TripleTen

Capítulo 5/8

Relacionamentos Entre Tabelas

# Prazer em Conhecê-las, Tabelas!

Engenheiros de dados aprimoraram a tabela `products_data_all` dividindo-a em três tabelas: `products`, `stores`, e `products_stores`.

## products

A tabela `products` armazena dados sobre produtos. Ela contém os seguintes campos:

`id_product` — o identificador exclusivo do produto. Tipo de dados: `integer`. Chave primária.

`name` — o nome do produto. Tipo de dados: `varchar()`.

`category` — a categoria do produto. Tipo de dados: `varchar()`.

`units` — as unidades de medida do peso do produto. Tipo de dados: `varchar()`.

`weight` — o peso do produto. Tipo de dados: `varchar()`.

Infelizmente, os engenheiros não tiveram tempo suficiente para produzir relatórios de bugs. Não esqueça que os dados do campo `weight` estão escritos como strings, mas na verdade tratam-se de números.

## stores

A tabela `stores`contém informação sobre as lojas online cujos dados você está coletando.

`id_store` — o identificador exclusivo da loja. Tipo de dados: `integer`. Chave primária.

`name_store` — o nome da loja. Tipo de dados: `varchar()`.

## products\_stores

A tabela `products_stores` trata-se de uma tabela da associação de `stores` e `products`.

`id_product` — o ID de um produto que é vendido em `id_store`. Tipo de dados: `integer`. Chave estrangeira da tabela `products`, chave primária composta da tabela `products_stores`.

`id_store` — o ID de uma loja que vende `id_product`. Tipo de dados: `integer`. Chave estrangeira da tabela `stores`, chave primária composta da tabela `products_stores`.

`date_upd` — data e hora em que os dados foram coletados. Tipo de dados: `timestamp`. Chave primária composta da tabela `products_stores`.

`price` — o preço de `id_product` em `id_store` na data `date_upd`. Tipo de dados: `integer`.

O diagrama ER das tabelas `products`, `stores`, e `products_stores` ficará assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_image_2021-08-06_132945_1628245784.png)

E aqui está um diagrama ER que inclui as tabelas `transactions` e `weather`:

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/Frame_1_1671026811.png)

Esses diagramas foram criados com o [drawSQL](https://drawsql.app) (os materiais estão em inglês), um aplicativo útil para criar diagramas ER.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-36-763Z.md
### Última modificação: 2025-05-28 19:38:37

# Tipos de Usuários de Bancos de Dados - TripleTen

Capítulo 5/8

Relacionamentos Entre Tabelas

# Tipos de Usuários de Bancos de Dados

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_____roli_copy_1585483237.jpg)

Muitos funcionários usam o banco de dados principal de uma empresa ao mesmo tempo. Cada um deles precisa apenas de certos dados para fazer o seu serviço. Então tudo deve estar organizado de modo que os usuários não interfiram no trabalho dos outros. Por isso é necessário ter **administradores do banco de dados**. Eles gerenciam o acesso dos usuários, monitoram a carga de trabalho do sistema, cuidam da segurança e fazem backups.

Bancos de dados são como seres vivos, sempre crescendo. Os **Arquitetos de bancos de dados** e **desenvolvedores** garantem que eles cresçam de modo saudável. As decisões desses especialistas determinam a estrutura, integridade e completude do banco de dados, assim como suas possibilidades de escalabilidade (adição de novas tabelas, relações e funções). Os arquitetos e desenvolvedores são responsáveis pela performance do banco de dados.

Os **Engenheiros de dados** são responsáveis por adicionar dados no banco de dados. Eles também são chamados de **especialistas em ETL**, já que eles extraem, transformam e carregam dados em bancos de dados.

Os **Analistas** nesse modelo são usuários típicos. Eles escrevem consultas para bancos de dados e obtém os dados necessários, que eles então analisam e usam para testar hipóteses. Por trabalharem com tanta intimidade com os dados, eles são os primeiros a sentirem a necessidade de incluir determinadas tabelas e campos. Torne um hábito seu comunicar essas necessidades imediatamente, para que eles possam resolvê-las o mais rápido possível.

Pergunta

Você não tem acesso ao banco de dados, mas você precisa dele para trabalhar. Com quem você entra em contato?

analista

desenvolvedor(a)

administrador

Isso mesmo! É ele que pode nos dar acesso.

arquiteto

engenheira de dados

Seu entendimento sobre o material é impressionante!

Pergunta

Parece que existe um problema relacionado com a integridade dos dados. O que você faz?

um relatório para o desenvolvedor

Isso mesmo! O desenvolvedor lerá seu relatório e verificará se os dados foram inseridos corretamente.

um textão agressivo no Facebook

tirar satisfação com o CEO

nada

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-38-042Z.md
### Última modificação: 2025-05-28 19:38:38

# Procurando por Valores Vazios - TripleTen

Teoria

# Procurando por Valores Vazios

Você sabe, por experiência própria, o que são dados ausentes. Em SQL, dizemos que células vazias são **NULL**. O operador **IS NULL** efetua sua busca:

```
SELECT
    *
FROM
    table_name
WHERE
    column_name IS NULL;
```

Não esqueça que o IS é importante! Isso aqui não vai funcionar:

```
SELECT
    *
FROM
    table_name
WHERE
    column_name = NULL; -- este código não compila!
```

Vamos nos debruçar de novo sobre os nossos `livros`. A tabela mudou: agora temos os identificadores `author_id` e `genre_id` ao invés dos nomes dos autores e dos gêneros.

name

book\_id

genre\_id

author\_id

publisher\_id

date\_pub

pages

rating

The City of Mirrors

1

0

3

5

2016/01/01

602

4.2

The October Country

2

5

5

5

1955/01/01

334

4.16

Saint Odd

3

5

7

8

2015/01/01

352

4.14

From the Corner of His Eye

4

4

7

8

2000/01/01

729

4.01

The Great and Secret Show

5

5

13

15

1990/01/01

658

4.05

World War Z: An Oral History of the Zombie War

6

10

15

19

2006/01/01

342

4.01

Red Dragon

7

4

4

17

1981/01/01

348

4.02

The Stand

8

0

0

2

1978/01/01

816

4.34

Lightning

9

4

7

14

1988/01/01

384

4.06

The Exorcist

10

6

6

7

1971/01/01

385

4.15

The Witching Hour

11

5

9

10

1990/01/01

1038

4.11

The Vampire Lestat

12

8

9

10

1985/01/01

550

4.06

Intensity

13

4

7

10

1995/01/01

436

4.04

The Terror

14

9

14

16

2007/01/01

769

4.04

House of Leaves

15

0

8

9

2000/01/01

705

4.12

Swan Song

16

2

1

3

1987/01/01

956

4.28

Boy's Life

17

0

1

3

1991/01/01

580

4.34

Summer of Night

18

4

14

18

1991/01/01

600

4.02

Full Dark, No Stars

19

1

0

11

2010/01/01

368

4.05

Doctor Sleep

20

4

0

11

2013/01/01

531

4.1

The Green Mile

21

0

0

0

1996/01/01

465

4.43

Rot & Ruin

22

7

10

12

2010/01/01

458

4.1

Let the Right One In

23

8

11

6

2008/01/01

513

4.07

The Silence of the Lambs

24

4

4

6

1988/01/01

367

4.17

It

25

0

0

1

1986/01/01

1116

4.22

Different Seasons

26

1

0

1

1982/01/01

560

4.35

Misery

27

4

0

1

1987/01/01

320

4.13

NOS4A2

28

0

12

13

2013/01/01

692

4.06

This Book Is Full of Spiders

29

3

2

2012/01/01

406

4.26

End of Watch

30

4

0

2016/01/01

432

4.09

Vamos procurar valores nulos na coluna `publisher_id`:

```
SELECT
    name,
    publisher_id
FROM
    books
WHERE
    publisher_id IS NULL;
```

name

publisher\_id

This Book Is Full of Spiders

End of Watch

Às vezes, ao invés de imprimir linhas com valores nulos, nós queremos os excluir dos resultados.

Nesses casos, usamos o operador NOT:

```
SELECT
    *
FROM
    table_name
WHERE
    column_name IS NOT NULL;
```

Vamos selecionar todos os livros em que o campo `publisher_id` não está vazio:

```
SELECT
    name,
    publisher_id
FROM
    books
WHERE
    publisher_id IS NOT NULL;
```

name

publisher\_id

The City of Mirrors

5

The October Country

5

Saint Odd

8

From the Corner of His Eye

8

The Great and Secret Show

15

World War Z: An Oral History of the Zombie War

19

Red Dragon

17

The Stand

2

Lightning

14

The Exorcist

7

The Witching Hour

10

The Vampire Lestat

10

Intensity

10

The Terror

16

House of Leaves

9

Swan Song

3

Boy's Life

3

Summer of Night

18

Full Dark, No Stars

11

Doctor Sleep

11

The Green Mile

0

Rot & Ruin

12

Let the Right One In

6

The Silence of the Lambs

6

It

1

Different Seasons

1

Misery

1

NOS4A2

13

Apenas eliminar a linha NULL não é sempre a melhor opção. Às vezes nós vamos querer substituí-la por algum valor determinado (por exemplo, a média), e podemos fazer isso acontecer durante a própria consulta. É para isso que serve o operador **CASE** ("nos casos em que"). É como `if-elif-else` em Python:

```
CASE WHEN condition_1 THEN
    result_1
WHEN condition_2 THEN
    result_2
WHEN condition_3 THEN
    result_3
ELSE
    result_4
END;
```

Uma condição vem a seguir ao operador **WHEN**. Se a linha de uma tabela satisfaz essa condição, o código retorna o resultado indicado em **THEN**. Caso contrário, a mesma linha é testada com a próxima condição. Se a linha não corresponde a nenhuma das condições determinadas em WHEN, o código retorna o valor indicado após **ELSE**. A sentença CASE então é encerrada com o operador **END**.

Como isso funciona em SQL? Vamos observar um exemplo:

```
SELECT
    name,
    CASE WHEN publisher_id IS NULL THEN -1 -- se o editor não estiver especificado, nós atribuímos
-- é uma publisher_id de -1
    ELSE publisher_id END AS publisher_id_full
FROM
    books;
```

Traduzido do SQL:

Selecione os campos `name` e `publisher_id` da tabela `books`. Se o valor na coluna `publisher_id` estiver vazio, substitua-o por -1. Caso contrário, deixe o valor como está. Nomeie a coluna resultante como `publisher_id_full`.

name

publisher\_id\_full

The City of Mirrors

5

The October Country

5

Saint Odd

8

From the Corner of His Eye

8

The Great and Secret Show

15

World War Z: An Oral History of the Zombie War

19

Red Dragon

17

The Stand

2

Lightning

14

The Exorcist

7

The Witching Hour

10

The Vampire Lestat

10

Intensity

10

The Terror

16

House of Leaves

9

Swan Song

3

Boy's Life

3

Summer of Night

18

Full Dark, No Stars

11

Doctor Sleep

11

The Green Mile

0

Rot & Ruin

12

Let the Right One In

6

The Silence of the Lambs

6

It

1

Different Seasons

1

Misery

1

NOS4A2

13

This Book Is Full of Spiders

\-1

End of Watch

\-1

CASE pode ser usado para procurar e substituir outros valores também, não só valores NULL. Por exemplo, se um campo não pode conter números negativos, você pode fazer uma varredura com CASE e substituí-los por NULL ou número positivos.

Agora vamos praticar com nosso banco de dados inteiro. Aqui está o diagrama ER para referência.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_image_2021-08-06_133005_1628245804.png)

Procurando por Valores Vazios

Tarefa4 / 4

1.

Teste o campo `weight` da tabela `products` para ver se há NULL. Imprima os campos seguintes: `name,units,weight`.

2.

Encontre o número de valores NULL no campo `weight` da tabela `products`.

3.

Encontre a média do peso dos produtos, agrupados por unidades de medida (`units`). Salve-os no um novo campo com o nome `avg_weight`. As colunas devem aparecer nessa ordem: `avg_weight`, `units`.

4.

Use uma consulta contendo CASE pra substituir manualmente NULL pelas médias que calculamos na tarefa anterior para cada grupo `units`. Nomeie o campo resultante `weight_full`. Os valores da coluna devem ser strings e aparecer em ordem: `name`, `weight_full`.

Você vai precisar dos seus resultados da tarefa anterior:

```
23.0705263269575,oz
10.0,ct
12.0909090909091,pk
0.650793650793651,gal
1.0,%
1.0,pt
1.0,qt
```

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

SELECT

name,

CASE WHEN weight IS NULL

AND units \= '%' THEN

'1.0'

WHEN weight IS NULL

AND units \= 'pt' THEN

'1.0'

WHEN weight IS NULL

AND units \= 'ct' THEN

'10.0'

WHEN weight IS NULL

AND units \= 'qt' THEN

'1.0'

WHEN weight IS NULL

AND units \= 'gal' THEN

'0.650793650793651'

WHEN weight IS NULL

AND units \= 'pk' THEN

'12.0909090909091'

WHEN weight IS NULL

AND units \= 'oz' THEN

'23.0705263269575'

ELSE

weight

END AS weight\_full

FROM

products;

  

  

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-40-119Z.md
### Última modificação: 2025-05-28 19:38:40

# Procurando por Dados na Tabela - TripleTen

Teoria

# Procurando por Dados na Tabela

Recuperar os preços de determinadas marcas de produtos é uma tarefa bem típica de analistas. Mas e se o nome da marca é indicado na mesma coluna que o nome do produto e não há campo separador para isso? Você precisará saber como procurar por substrings.

O operador **LIKE** procura na tabela por valores que seguem um determinado padrão. Você pode procurar não apenas por uma palavra, mas também por um fragmento dela.

Aqui está a sintaxe da instrução LIKE:

```
column_name LIKE 'regular expression'
```

Indique a coluna necessária antes de LIKE e depois dela escreva uma expressão regular.

Aqui está a tabela `books` que você conhece e ama:

name

book\_id

genre\_id

author\_id

publisher\_id

date\_pub

pages

rating

The City of Mirrors

1

0

3

5

2016/01/01

602

4.2

The October Country

2

5

5

5

1955/01/01

334

4.16

Saint Odd

3

5

7

8

2015/01/01

352

4.14

From the Corner of His Eye

4

4

7

8

2000/01/01

729

4.01

The Great and Secret Show

5

5

13

15

1990/01/01

658

4.05

World War Z: An Oral History of the Zombie War

6

10

15

19

2006/01/01

342

4.01

Red Dragon

7

4

4

17

1981/01/01

348

4.02

The Stand

8

0

0

2

1978/01/01

816

4.34

Lightning

9

4

7

14

1988/01/01

384

4.06

The Exorcist

10

6

6

7

1971/01/01

385

4.15

The Witching Hour

11

5

9

10

1990/01/01

1038

4.11

The Vampire Lestat

12

8

9

10

1985/01/01

550

4.06

Intensity

13

4

7

10

1995/01/01

436

4.04

The Terror

14

9

14

16

2007/01/01

769

4.04

House of Leaves

15

0

8

9

2000/01/01

705

4.12

Swan Song

16

2

1

3

1987/01/01

956

4.28

Boy's Life

17

0

1

3

1991/01/01

580

4.34

Summer of Night

18

4

14

18

1991/01/01

600

4.02

Full Dark, No Stars

19

1

0

11

2010/01/01

368

4.05

Doctor Sleep

20

4

0

11

2013/01/01

531

4.1

The Green Mile

21

0

0

0

1996/01/01

465

4.43

Rot & Ruin

22

7

10

12

2010/01/01

458

4.1

Let the Right One In

23

8

11

6

2008/01/01

513

4.07

The Silence of the Lambs

24

4

4

6

1988/01/01

367

4.17

It

25

0

0

1

1986/01/01

1116

4.22

Different Seasons

26

1

0

1

1982/01/01

560

4.35

Misery

27

4

0

1

1987/01/01

320

4.13

NOS4A2

28

0

12

13

2013/01/01

692

4.06

This Book Is Full of Spiders

29

3

2

0

2012/01/01

406

4.26

End of Watch

30

4

0

0

2016/01/01

432

4.09

Nós estamos procurando por livros com a palavra "Vampire" no seu título.

Para procurar por elas, você precisa escrever uma expressão regular que irá incluir a base. Lembre-se de que uma expressão regular, ou **máscara**, é um modelo que permite que você encontre uma string inteira com base em uma substring. Expressões regulares são composta por símbolos que substituem os valores.

Expressões regulares em SQL são um pouco diferentes daquelas usadas em outros contextos. Por exemplo, em SQL, o símbolo `%` representa qualquer quantidade de caracteres.

A expressão regular que precisamos é basicamente esta: `'%Vampire%'`. (Nos títulos de livros a primeira letra será maiúscula, não se esqueça.)

Agora que nós formamos nossa expressão regular, nós podemos escrever a nossa consulta:

```
SELECT
    *
FROM
    books
WHERE
    name LIKE '%Vampire%';
```

name

book\_id

genre\_id

author\_id

publisher\_id

date\_pub

pages

rating

The Vampire Lestat

12

8

9

10

1985/01/01

550

4.06

O operador NOT nos dá uma consulta "invertida": ele seleciona todos os livros cujos nomes não incluem a palavra "Vampire." Por exemplo, podemos usar o operador NOT com LIKE, assim:

```
SELECT
    *
FROM
    books
WHERE
    name NOT LIKE '%Vampire%';
```

name

book\_id

genre\_id

author\_id

publisher\_id

date\_pub

pages

rating

The City of Mirrors

1

0

3

5

2016/01/01

602

4.2

The October Country

2

5

5

5

1955/01/01

334

4.16

Saint Odd

3

5

7

8

2015/01/01

352

4.14

From the Corner of His Eye

4

4

7

8

2000/01/01

729

4.01

The Great and Secret Show

5

5

13

15

1990/01/01

658

4.05

World War Z: An Oral History of the Zombie War

6

10

15

19

2006/01/01

342

4.01

Red Dragon

7

4

4

17

1981/01/01

348

4.02

The Stand

8

0

0

2

1978/01/01

816

4.34

Lightning

9

4

7

14

1988/01/01

384

4.06

The Exorcist

10

6

6

7

1971/01/01

385

4.15

The Witching Hour

11

5

9

10

1990/01/01

1038

4.11

Intensity

13

4

7

10

1995/01/01

436

4.04

The Terror

14

9

14

16

2007/01/01

769

4.04

House of Leaves

15

0

8

9

2000/01/01

705

4.12

Swan Song

16

2

1

3

1987/01/01

956

4.28

Boy's Life

17

0

1

3

1991/01/01

580

4.34

Summer of Night

18

4

14

18

1991/01/01

600

4.02

Full Dark, No Stars

19

1

0

11

2010/01/01

368

4.05

Doctor Sleep

20

4

0

11

2013/01/01

531

4.1

The Green Mile

21

0

0

0

1996/01/01

465

4.43

Rot & Ruin

22

7

10

12

2010/01/01

458

4.1

Let the Right One In

23

8

11

6

2008/01/01

513

4.07

The Silence of the Lambs

24

4

4

6

1988/01/01

367

4.17

It

25

0

0

1

1986/01/01

1116

4.22

Different Seasons

26

1

0

1

1982/01/01

560

4.35

Misery

27

4

0

1

1987/01/01

320

4.13

NOS4A2

28

0

12

13

2013/01/01

692

4.06

This Book Is Full of Spiders

29

3

2

0

2012/01/01

406

4.26

End of Watch

30

4

0

0

2016/01/01

432

4.09

Mas e se nós estivermos literalmente olhando para o símbolo `%` em um conjunto de dados?

Nesses casos, nós usamos o operador de **ESCAPE**. Um símbolo é passado (como um ponto de exclamação), que se torna o caractere de escape. Se um caractere de escape é atribuído na expressão regular, isso significa que o símbolo seguinte não é um caractere curinga, mas um caractere que a substring terá que ter.

```
column_name LIKE '%!%%' ESCAPE '!'
--encontre todas as substrings terminadas com %
```

Agora, um pouco de prática com o nosso conjunto de dados de laticínios. Aqui está o diagrama ER para referência.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_image_2021-08-06_133005_1628245804.png)

Procurando por Dados na Tabela

Tarefa2 / 2

1.

Nós aprendemos na última lição que alguns produtos tem unidades de medidas peculiares indicados como porcentagens. Um erro provavelmente arrastou-se para dentro dos nossos dados. Procure pela string que tem `%` no campo `units`. Imprima todos os seus campos.

2.

Encontre todos os produtos cujos nomes contêm a palavra "Moo." Imprima todos os dados deles da tabela `products`.

Os nomes dos produtos estão armazenados no campo `name` da tabela `products`.

99

1

2

3

4

5

6

7

8

9

10

SELECT

\*

FROM

products

where

name like '%Moo%'

  

  

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-41-480Z.md
### Última modificação: 2025-05-28 19:38:42

# JUNÇÃO. JUNÇÃO INTERNA. - TripleTen

Teoria

# JUNÇÃO. JUNÇÃO INTERNA.

É raro que todos os dados estejam armazenados em uma tabela. Analistas geralmente precisam juntar tabelas; para isso, se usa o operador **JOIN**.

Há duas maneiras de unir tabelas: as junções **INTERNA** e **EXTERNA**.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_inner_join_1585494277.jpg)

A junção INTERNA retorna apenas aquelas linhas que possuem valores correspondentes nas tabelas que estão sendo juntadas (a intersecção das tabelas). A junção EXTERNA recupera **todos** os dados de uma tabela e adiciona dados da outra quando há linhas correspondentes. Há dois tipos de junção EXTERNA, esquerda e direita:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_6.8.2.2_1652347873.png)

**JUNÇÃO INTERNA**

JUNÇÃO INTERNA seleciona apenas os dados que atendem à condição de junção. A ordem na qual as tabelas são juntadas não afeta o resultado final.

Aqui está um exemplo de consulta com JUNÇÃO INTERNA:

```
SELECT --listando apenas os campos que são necessários
    TABLE_1.field_1 AS field_1,
    TABLE_1.field_2 AS field_2,
    ...
        TABLE_2.field_n AS field_n
FROM
    TABLE_1
    INNER JOIN TABLE_2 ON TABLE_2.field_1 = TABLE_1.field_2;
```

Vamos observar a sintaxe mais de perto:

-   INNER JOIN é o nome do método de junção. Depois vem o nome da tabela a ser juntada à tabela do bloco FROM.
-   ON indica a condição de junção: `TABLE_2.field_1 = TABLE_1.field_2`. Isso quer dizer que apenas as linhas da tabela que satisfazem essa condição serão juntadas. Em nosso caso, a condição é que `field_1` da segunda tabela é igual a `field_2` da primeira.

Como campos em tabelas diferentes podem ter os mesmos nomes, nos referimos a eles com o nome da tabela e com o nome do campo. Primeiro o nome da tabela, então o do campo: `TABLE_1.field_1`.

Vamos ver com JUNÇÃO INTERNA funciona.

Aqui está nossa primeira tabela, `animals`, que contém dados de animais domésticos. Ela contém campos para `id` e `name`.

id

name

1

gato

2

cão

3

hamster

E aqui está a segunda tabela, `tails`, que têm dados dos comprimentos das caudas dos animais. O campo `id` nessa tabela é o `id` dos animais.

id

tail

1

10

2

5

4

1

Vamos aplicar JUNÇÃO INTERNA a `tails` e `animals` no campo `id`.

id

name

tail

1

gato

10

2

cão

5

O resultado é uma tabela com duas linhas. A coluna `id`inclui apenas os valores 1 e 2, pois apenas eles estão presentes nas duas tabelas.

O hamster não apareceu na tabela resultante porque seu `id` não tem correspondente na tabela `tails`.

Vamos considerar outro caso. A tabela `animals` continua igual.

id

name

1

cão

2

cão

3

hamster

Mas a tabela `tails` agora está assim:

id

tail

1

10

1

15

2

5

4

1

Vamos juntar os animais às suas novas caudas. Desta vez, temos três linhas, e não 2:

id

name

tail

1

gato

10

1

gato

15

2

cão

5

Por quê? O identificador (`id`) com o valor 1 corresponde a duas linhas na tabela `tails`. Ambas satisfazem a condição de junção, então as duas são incluídas na tabela resultante.

Tome cuidado ao usar essa junção de tabelas. Duplicação de linhas é um erro comum em consultas SQL de todos os níveis de complexidade.

Agora que sabemos sobre alguns detalhes da JUNÇÃO INTERNA, vamos voltar para nossa `books`. Essa tabela mudou; ao invés de nomes e sobrenomes de autores, agora ela tem `author_id`.

name

book\_id

genre\_id

author\_id

publisher\_id

date\_pub

pages

rating

The City of Mirrors

1

0

3

5

2016/01/01

602

4.2

The October Country

2

5

5

5

1955/01/01

334

4.16

Saint Odd

3

5

7

8

2015/01/01

352

4.14

From the Corner of His Eye

4

4

7

8

2000/01/01

729

4.01

The Great and Secret Show

5

5

13

15

1990/01/01

658

4.05

World War Z: An Oral History of the Zombie War

6

10

15

19

2006/01/01

342

4.01

Red Dragon

7

4

4

17

1981/01/01

348

4.02

The Stand

8

0

0

2

1978/01/01

816

4.34

Lightning

9

4

7

14

1988/01/01

384

4.06

The Exorcist

10

6

6

7

1971/01/01

385

4.15

The Witching Hour

11

5

9

10

1990/01/01

1038

4.11

The Vampire Lestat

12

8

9

10

1985/01/01

550

4.06

Intensity

13

4

7

10

1995/01/01

436

4.04

The Terror

14

9

14

16

2007/01/01

769

4.04

House of Leaves

15

0

8

9

2000/01/01

705

4.12

Swan Song

16

2

1

3

1987/01/01

956

4.28

Boy's Life

17

0

1

3

1991/01/01

580

4.34

Summer of Night

18

4

14

18

1991/01/01

600

4.02

Full Dark, No Stars

19

1

0

11

2010/01/01

368

4.05

Doctor Sleep

20

4

0

11

2013/01/01

531

4.1

The Green Mile

21

0

0

0

1996/01/01

465

4.43

Rot & Ruin

22

7

10

12

2010/01/01

458

4.1

Let the Right One In

23

8

11

6

2008/01/01

513

4.07

The Silence of the Lambs

24

4

4

6

1988/01/01

367

4.17

It

25

0

0

1

1986/01/01

1116

4.22

Different Seasons

26

1

0

1

1982/01/01

560

4.35

Misery

27

4

0

1

1987/01/01

320

4.13

NOS4A2

28

0

12

13

2013/01/01

692

4.06

This Book Is Full of Spiders

29

3

2

2012/01/01

406

4.26

End of Watch

30

4

0

2016/01/01

432

4.09

Enquanto a tabela `author` corresponde ao campo `author_id` (da tabela `books`) com `first_name` e `last_name` dos autores:

first\_name

author\_id

last\_name

Stephen

0

King

Robert R.

1

McCammon

David

2

Wong

Justin

3

Cronin

Thomas

4

Harris

Ray

5

Bradbury

William Peter

6

Blatty

Dean

7

Koontz

Mark Z.

8

Danielewski

Anne

9

Rice

Jonathan

10

Maberry

John Ajvide

11

Lindqvist

Joe

12

Hill

Clive

13

Barker

Dan

14

Simmons

Max

15

Brooks

Vamos escrever uma consulta que irá agrupar os seguintes campos em uma tabela:

-   título do livro (`name`) da tabela `books`
-   `author_id` da tabela `books`
-   `author_id` da tabela \`author\*
-   nome do autor (`first_name`) de `author`
-   sobrenome do autor (`last_name`) de `author`

Vamos dar uma olhada nas três primeiras linhas:

```
SELECT
    books.name AS name,
    books.author_id AS books_author_id,
    author.author_id AS author_id,
    author.first_name AS first_name,
    author.last_name AS last_name
FROM
    books
    INNER JOIN author ON author.author_id = books.author_id
LIMIT 3;
```

name

books\_author\_id

author\_id

first\_name

last\_name

The City of Mirrors

3

3

Justin

Cronin

The October Country

5

5

Ray

Bradbury

Saint Odd

7

7

Dean

Koontz

Na tabela resultante, os valores do campo `author_id` da tabela `books` e aqueles de `author_id` da tabela `author` são os mesmos. Para evitar duplicações desnecessárias, vamos imprimir apenas uma dessas colunas. Vamos pegar `author_id` da tabela `books`:

```
SELECT
    books.name AS name,
    books.author_id AS books_author_id,
    author.first_name AS first_name,
    author.last_name AS last_name
FROM
    books
    INNER JOIN author ON author.author_id = books.author_id
LIMIT 3;
```

name

books\_author\_id

first\_name

last\_name

The City of Mirrors

3

Justin

Cronin

The October Country

5

Ray

Bradbury

Saint Odd

7

Dean

Koontz

Ao juntar tabelas você pode especificar condições no bloco WHERE. Por exemplo, vamos recuperar apenas livros de Dean Koontz:

```
SELECT
    books.name AS name,
    author.first_name AS first_name,
    author.last_name AS last_name
FROM
    books
    INNER JOIN author ON author.author_id = books.author_id
WHERE
    author.first_name = 'Dean'
    AND author.last_name = 'Koontz';
```

name

first\_name

last\_name

Saint Odd

Dean

Koontz

From the Corner of His Eye

Dean

Koontz

Lightning

Dean

Koontz

Intensity

Dean

Koontz

Agora, um pouco de prática com o nosso conjunto de dados de laticínios. Aqui está o diagrama ER para referência.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_image_2021-08-06_133005_1628245804.png)

JUNÇÃO. JUNÇÃO INTERNA.

Tarefa5 / 5

1.

Escreva uma consulta para recuperar:

-   IDs de transações — `id_transaction` da tabela `transactions`
-   Nomes de categorias — `category` da tabela `products`
-   Product names — `name` da tabela `products`

A condição de junção é que os valores dos campos `products.id_product` e `transactions.id_product`sejam iguais. Os nomes dos campos na tabela resultante são `id_transaction`, `category`, `name`.

Imprima 10 linhas. Ordene os dados em ordem crescente pelo número de transação.

2.

Dados de vendas e clima estão armazenados em tabelas diferentes. Aqui o `INNER JOIN` irá nos ajudar.

Para cada transação, recupere os seguintes dados:

-   O dia e hora unívoco (`date`) das transações, da tabela `transactions`
-   Temperatura do ar (`temp`) da tabela `weather`
-   Informação sobre chuvas (`rain`) da tabela `weather`
-   O identificador da transação (`id_transaction`) da tabela `transactions`

Na nova tabela, dê aos campos os nomes `date`, `temp`, `rain` e `id_transaction`, e imprima-os.

Ordene os dados em ordem decrescente por data de compra (purchase date).

3.

Escreva uma consulta para imprimir **produtos distintos** com preço (`price`) mais alto do que $5.

Selecione nomes de **produtos distintos** (`name`) da tabela `products`. Nomeie o campo resultante como `name`e imprima-o.

Junte os dados da tabela `products_stores` em `products` usando o método `INNER JOIN` no campo `id_product`.

4.

Retorne transações (compras) para produtos da categoria `'butter'` para o dia: 20 de junho de 2019.

Selecione os seguintes dados das tabelas `transactions` e `products`:

-   dia e hora — `date` da tabela `transactions`
-   número da transação — `id_transaction` da tabela `transactions`
-   nome da categoria — `category` da tabela `products`
-   nome do produto — `name` da tabela `products`

Junte os dados da tabela `products` com `transactions` usando o método `INNER JOIN` no campo `id_product`.

Na nova tabela, nomeio os campos como `date`, `id_transaction`, `category`, `name` e imprima-os.

5.

Imprima os preços dos produtos com pesos medidos em onças (`'oz'`) em 13.06.2019.

Retorne:

nome do produto — `name` da tabela `products`

categoria — `category` da tabela `products`

unidades de medida — `units` da tabela `products`

peso — `weight` da tabela `products`

preço — `price` da tabela `products_stores`

Junte as tabelas `products_stores` e `products`, usando o método INNER JOIN no campo `id_product`.

Use os seguintes nomes para o que você vai recuperar: `name`, `category`, `units`, `weight`, `price`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

SELECT

products.name as name,

products.category as category,

products.units as units,

products.weight as weight,

products\_stores.price as price

FROM

products

INNER JOIN products\_stores on products\_stores.id\_product \= products.id\_product

WHERE

products.units \= 'oz' and

products\_stores.date\_upd \= '2019/06/13'

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-42-764Z.md
### Última modificação: 2025-05-28 19:38:43

# Junção externa. LEFT JOIN - TripleTen

Teoria

# Junção externa. LEFT JOIN

JUNÇÃO EXTERNA vem em dois sabores:

-   **LEFT OUTER JOIN (JUNÇÃO EXTERNA ESQUERDA)**
-   **RIGHT OUTER JOIN (JUNÇÃO EXTERNA DIREITA)**

Vamos dar nomes mais curtos para esses métodos: **LEFT JOIN** e **RIGHT JOIN**.

LEFT JOIN irá selecionar todos os dados da tabela à esquerda junto com as linhas da tabela direita que satisfazerem a condição de junção. RIGHT JOIN fará o mesmo, mas para a tabela à direita.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_6.5.9_1652347915.png)

Aqui está a sintaxe de uma sentença com LEFT JOIN:

```
SELECT
    TABLE_1.field_1 AS field_1,
    TABLE_1.field_2 AS field_2,
    ...
        TABLE_2.field_n AS field_n
FROM
    TABLE_1
    LEFT JOIN TABLE_2 ON TABLE_2.field = TABLE_1.field;
```

Assim como em consultas INNER JOIN (JUNÇÃO INTERNA), o nome da tabela é indicado para cada campo. Note que com JUNÇÃO EXTERNA a ordem na qual as tabelas são listadas é significante. Neste exemplo, TABLE\_1 é a tabela esquerda.

Vamos retornar a `animals` e `tails` e junte-os em `id` usando LEFT JOIN.

id

name

1

gato

2

cão

3

hamster

id

tail

1

10

2

5

4

1

Aqui está o resultado do nosso LEFT JOIN:

id

name

tail

1

cat

10

2

cão

5

3

hamster

NULL

Temos todas as linhas da primeira tabela (esquerda) junto com os dados da segunda tabela (direita) que satisfazem a condição. Temos um valor vazio para o comprimento de caudas de hamsters pois a linha dos hamsters não satisfaz a condição.

Agora vamos para os livros:

name

book\_id

genre\_id

author\_id

publisher\_id

date\_pub

pages

rating

The City of Mirrors

1

0

3

5

2016/01/01

602

4.2

The October Country

2

5

5

5

1955/01/01

334

4.16

Saint Odd

3

5

7

8

2015/01/01

352

4.14

From the Corner of His Eye

4

4

7

8

2000/01/01

729

4.01

The Great and Secret Show

5

5

13

15

1990/01/01

658

4.05

World War Z: An Oral History of the Zombie War

6

10

15

19

2006/01/01

342

4.01

Red Dragon

7

4

4

17

1981/01/01

348

4.02

The Stand

8

0

0

2

1978/01/01

816

4.34

Lightning

9

4

7

14

1988/01/01

384

4.06

The Exorcist

10

6

6

7

1971/01/01

385

4.15

The Witching Hour

11

5

9

10

1990/01/01

1038

4.11

The Vampire Lestat

12

8

9

10

1985/01/01

550

4.06

Intensity

13

4

7

10

1995/01/01

436

4.04

The Terror

14

9

14

16

2007/01/01

769

4.04

House of Leaves

15

0

8

9

2000/01/01

705

4.12

Swan Song

16

2

1

3

1987/01/01

956

4.28

Boy's Life

17

0

1

3

1991/01/01

580

4.34

Summer of Night

18

4

14

18

1991/01/01

600

4.02

Full Dark, No Stars

19

1

0

11

2010/01/01

368

4.05

Doctor Sleep

20

4

0

11

2013/01/01

531

4.1

The Green Mile

21

0

0

0

1996/01/01

465

4.43

Rot & Ruin

22

7

10

12

2010/01/01

458

4.1

Let the Right One In

23

8

11

6

2008/01/01

513

4.07

The Silence of the Lambs

24

4

4

6

1988/01/01

367

4.17

It

25

0

0

1

1986/01/01

1116

4.22

Different Seasons

26

1

0

1

1982/01/01

560

4.35

Misery

27

4

0

1

1987/01/01

320

4.13

NOS4A2

28

0

12

13

2013/01/01

692

4.06

This Book Is Full of Spiders

29

3

2

2012/01/01

406

4.26

End of Watch

30

4

0

2016/01/01

432

4.09

E autores:

first\_name

author\_id

last\_name

Stephen

0

King

Robert R.

1

McCammon

David

2

Wong

Justin

3

Cronin

Thomas

4

Harris

Ray

5

Bradbury

William Peter

6

Blatty

Dean

7

Koontz

Mark Z.

8

Danielewski

Anne

9

Rice

Jonathan

10

Maberry

John Ajvide

11

Lindqvist

Joe

12

Hill

Clive

13

Barker

Dan

14

Simmons

Max

15

Brooks

Mark

16

Twain

Ernest

17

Hemingway

Herman

18

Melville

Vamos escrever uma consulta para juntar os dados sobre livros da tabela `books`para a tabela `author`.

```
SELECT
    author.first_name AS first_name,
    author.last_name AS last_name,
    author.author_id AS author_id,
    books.name AS name,
    books.author_id AS books_author_id
FROM
    author
    LEFT JOIN books ON books.author_id = author.author_id;
```

first\_name

last\_name

author\_id

name

books\_author\_id

Ray

Bradbury

5

The October Country

5

Mark

Twain

16

NULL

NULL

Ernest

Hemingway

17

NULL

NULL

Herman

Melville

18

NULL

NULL

Justin

Cronin

3

The City of Mirrors

3

Dean

Koontz

7

Saint Odd

7

Dean

Koontz

7

From the Corner of His Eye

7

Clive

Barker

13

The Great and Secret Show

13

Max

Brooks

15

World War Z: An Oral History of the Zombie War

15

Thomas

Harris

4

Red Dragon

4

Stephen

King

0

The Stand

0

Dean

Koontz

7

Lightning

7

William Peter

Blatty

6

The Exorcist

6

Anne

Rice

9

The Witching Hour

9

Anne

Rice

9

The Vampire Lestat

9

Dean

Koontz

7

Intensity

7

Dan

Simmons

14

The Terror

14

Mark Z.

Danielewski

8

House of Leaves

8

Robert R.

McCammon

1

Swan Song

1

Robert R.

McCammon

1

Boy's Life

1

Dan

Simmons

14

Summer of Night

14

Stephen

King

0

Full Dark, No Stars

0

Stephen

King

0

Doctor Sleep

0

Stephen

King

0

The Green Mile

0

Jonathan

Maberry

10

Rot & Ruin

10

John Ajvide

Lindqvist

11

Let the Right One In

11

Thomas

Harris

4

The Silence of the Lambs

4

Stephen

King

0

It

0

Stephen

King

0

Different Seasons

0

Stephen

King

0

Misery

0

Joe

Hill

12

NOS4A2

12

David

Wong

2

This Book Is Full of Spiders

2

Stephen

King

0

End of Watch

0

Note que os dados resultantes não contém informação sobre livros para cada autor, apesar do fato de os nomes dos autores terem sido selecionados. Eles possuem valores NULL para seus livros. Além disso, a quantidade de linhas mudou, pois alguns autores possuem mais de um livro.

Isso é o que acontece quando fazemos o oposto, juntando `author` a `books`:

```
SELECT
    author.first_name AS first_name,
    author.last_name AS last_name,
    author.author_id AS author_id,
    books.name AS name,
    books.author_id AS books_author_id
FROM
    books
    LEFT JOIN author ON books.author_id = author.author_id;
```

Desta vez a tabela resultante tem a mesma quantidade de linhas do que na tabela `books`: 30.

first\_name

last\_name

author\_id

name

books\_author\_id

Ray

Bradbury

5

The October Country

5

Justin

Cronin

3

The City of Mirrors

3

Dean

Koontz

7

Saint Odd

7

Dean

Koontz

7

From the Corner of His Eye

7

Clive

Barker

13

The Great and Secret Show

13

Max

Brooks

15

World War Z: An Oral History of the Zombie War

15

Thomas

Harris

4

Red Dragon

4

Stephen

King

0

The Stand

0

Dean

Koontz

7

Lightning

7

William Peter

Blatty

6

The Exorcist

6

Anne

Rice

9

The Witching Hour

9

Anne

Rice

9

The Vampire Lestat

9

Dean

Koontz

7

Intensity

7

Dan

Simmons

14

The Terror

14

Mark Z.

Danielewski

8

House of Leaves

8

Robert R.

McCammon

1

Swan Song

1

Robert R.

McCammon

1

Boy's Life

1

Dan

Simmons

14

Summer of Night

14

Stephen

King

0

Full Dark, No Stars

0

Stephen

King

0

Doctor Sleep

0

Stephen

King

0

The Green Mile

0

Jonathan

Maberry

10

Rot & Ruin

10

John Ajvide

Lindqvist

11

Let the Right One In

11

Thomas

Harris

4

The Silence of the Lambs

4

Stephen

King

0

It

0

Stephen

King

0

Different Seasons

0

Stephen

King

0

Misery

0

Joe

Hill

12

NOS4A2

12

David

Wong

2

This Book Is Full of Spiders

2

Stephen

King

0

End of Watch

0

Autores que não têm livros (Mark Twain, Ernest Hemingway, Herman Melville) não estão incluídos na tabela.

Em resumo, preste atenção ao escolher em qual ordem você vai pôr as tabelas! Ela depende da tarefa.

Agora, um pouco de prática com o nosso conjunto de dados de laticínios. Aqui está o diagrama ER para referência.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_image_2021-08-06_133005_1628245804.png)

Junção externa. LEFT JOIN

Tarefa3 / 3

1.

Escreva uma consulta para selecionar produtos unívocos (distintos):

-   `id_product` da tabela `products`
-   `name` da tabela `products`
-   `id_store` da tabela `products_stores`

Anexe a tabela `products_stores` à tabela `products` usando o método `LEFT JOIN` no campo `id_product`. Selecione apenas as linhas em que o valor de `id_store` é NULL. Se não houver esse tipo de linha, então sabemos que todos os produtos estão à venda em pelo menos um local.

Nomeie os campos da tabela resultante como `id_product`, `name`, e `id_store`.

2.

Como fornecedor, você pode oferecer às lojas produtos que elas não estão vendendo no momento.

Imprima os nomes unívocos de produtos que nunca foram vendidos na loja (`id_store`) cujo identificador unívoco é 3.

1) Selecione nomes de produtos unívocos `name` da tabela `products`. Salve-os na variável `name`.

2) Usando JUNÇÃO ESQUERDA, junte a consulta externa com a subconsulta `subquery` no campo `id_product`. Dentro da subconsulta, selecione o `id_product` unívoco da tabela `transactions` em que o `id_store` é 3.

3.

Imprima os nomes dos produtos que não estavam à venda em qualquer loja em 11 de junho de 2019.

1) Selecione os nomes de produtos unívocos `name` da tabela `products`. Salve-os na variável `name`.

2) Usando LEFT JOIN, junte a consulta externa com a `subquery` no campo `id_product`. Na subconsulta selecione `id_product` e `id_store` para 11 de junho de 2019.

3) Como na tarefa anterior, filtre os `id_products` que são `NULL`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

SELECT distinct

products.name as name

FROM

products

LEFT JOIN (

SELECT \--selecione os id do produto e o id da loja para 2019-06-11

\*

FROM

transactions

WHERE

date::date \= '2019-06-11'

) AS subquery ON subquery.id\_product \= products.id\_product

WHERE

subquery.id\_product IS NULL;

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-44-916Z.md
### Última modificação: 2025-05-28 19:38:45

# Junção externa. RIGHT JOIN - TripleTen

Teoria

# Junção externa. RIGHT JOIN

O RIGHT JOIN é a irmão gêmeo do LEFT JOIN. Porém, ao contrário da irmã, ela pega todos os dados da tabela à _direita_, e as linhas correspondentes da tabela à esquerda.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_6.5.10_1652347951.png)

Essa é a aparência de uma consulta de RIGHT JOIN:

```
SELECT
    TABLE_1.field_1 AS field_1,
    TABLE_1.field_2 AS field_2,
    ...
        TABLE_2.field_n AS field_n
FROM
    TABLE_1
    RIGHT JOIN TABLE_2 ON TABLE_1.field = TABLE_2.field;
```

Desta vez vamos pegar as caudas e juntar os animais a elas (e não o contrário).

id

name

1

gato

2

cão

3

hamster

id

tail

1

10

2

5

4

1

Nós aplicamos o RIGHT JOIN e obtemos a seguinte tabela:

id

name

tail

1

gato

10

2

cão

5

4

NULL

1

Esta tabela contém todas as linhas da segunda tabela (direita) e dados da primeira tabela (esquerda) que satisfazem a condição. Aparentemente, há uma não-criatura mitológica (a célula com NULL) cuja cauda possui uma polegada de comprimento.

Agora vamos aplicar RIGHT JOIN aos livros.

Aqui está nossa tabela de gêneros de livros:

name

genre\_id

Fantasia

0

Histórias curtas

1

Apocalíptico

2

Humor

3

Filme de ação

4

Filme de ação

5

Clássicos

6

Jovem adulto

7

Paranormal

8

Ficção histórica

9

Horror

10

Romance

11

Biografia

12

Esta tabela está conectada com a tabela `books`pela condição `genre.genre_id = books.genre_id`:

name

book\_id

genre\_id

author\_id

publisher\_id

date\_pub

pages

rating

The City of Mirrors

1

0

3

5

2016/01/01

602

4.2

The October Country

2

5

5

5

1955/01/01

334

4.16

Saint Odd

3

5

7

8

2015/01/01

352

4.14

From the Corner of His Eye

4

4

7

8

2000/01/01

729

4.01

The Great and Secret Show

5

5

13

15

1990/01/01

658

4.05

World War Z: An Oral History of the Zombie War

6

10

15

19

2006/01/01

342

4.01

Red Dragon

7

4

4

17

1981/01/01

348

4.02

The Stand

8

0

0

2

1978/01/01

816

4.34

Lightning

9

4

7

14

1988/01/01

384

4.06

The Exorcist

10

6

6

7

1971/01/01

385

4.15

The Witching Hour

11

5

9

10

1990/01/01

1038

4.11

The Vampire Lestat

12

8

9

10

1985/01/01

550

4.06

Intensity

13

4

7

10

1995/01/01

436

4.04

The Terror

14

9

14

16

2007/01/01

769

4.04

House of Leaves

15

0

8

9

2000/01/01

705

4.12

Swan Song

16

2

1

3

1987/01/01

956

4.28

Boy's Life

17

0

1

3

1991/01/01

580

4.34

Summer of Night

18

4

14

18

1991/01/01

600

4.02

Full Dark, No Stars

19

1

0

11

2010/01/01

368

4.05

Doctor Sleep

20

4

0

11

2013/01/01

531

4.1

The Green Mile

21

0

0

0

1996/01/01

465

4.43

Rot & Ruin

22

7

10

12

2010/01/01

458

4.1

Let the Right One In

23

8

11

6

2008/01/01

513

4.07

The Silence of the Lambs

24

4

4

6

1988/01/01

367

4.17

It

25

0

0

1

1986/01/01

1116

4.22

Different Seasons

26

1

0

1

1982/01/01

560

4.35

Misery

27

4

0

1

1987/01/01

320

4.13

NOS4A2

28

0

12

13

2013/01/01

692

4.06

This Book Is Full of Spiders

29

3

2

0

2012/01/01

406

4.26

End of Watch

30

4

0

0

2016/01/01

432

4.09

Vamos escrever uma consulta para juntar dados de gêneros dos livros da tabela `genres`com a tabela `books`:

```
SELECT
    books.name AS name,
    genre.name AS genre_name
FROM
    books
    RIGHT JOIN genre ON genre.genre_id = books.genre_id;
```

name

genre\_name

NULL

Romance

NULL

Biografia

The City of Mirrors

Fantasia

The October Country

Ficção

Saint Odd

Ficção

From the Corner of His Eye

Filme de ação

The Great and Secret Show

Ficção

World War Z: An Oral History of the Zombie War

Horror

Red Dragon

Filme de açãor

The Stand

Fantasia

Lightning

Filme de açãor

The Exorcist

Clássicos

The Witching Hour

Ficção

The Vampire Lestat

Paranormal

Intensity

Filme de ação

The Terror

Ficção histórica

House of Leaves

Fantasia

Swan Song

Apocalíptico

Boy's Life

Fantasia

Summer of Night

Filme de ação

Full Dark, No Stars

Histórias curtas

Doctor Sleep

Filme de ação

The Green Mile

Fantasia

Rot & Ruin

Jovem adulto

Let the Right One In

Paranormal

The Silence of the Lambs

Filme de ação

It

Fantasia

Different Seasons

Histórias curtas

Misery

Filme de ação

NOS4A2

Fantasia

This Book Is Full of Spiders

Humor

End of Watch

Filme de ação

Assim como no LEFT JOIN, a tabela resultante inclui linhas com valores NULL. Isso é porque a tabela _books_ não possui livros de todos os gêneros da tabela `genres`. E perceba que diversos livros podem cair no mesmo gênero (thrillers, por exemplo).

Vamos mudar a ordem das tabelas:

```
SELECT
    books.name AS name,
    genre.name AS genre_name
FROM
    genre
    RIGHT JOIN books ON books.genre_id = genre.genre_id;
```

name

genre\_name

The City of Mirrors

Fantasia

The October Country

Ficção

Saint Odd

Ficção

From the Corner of His Eye

Filme de ação

The Great and Secret Show

Ficção

World War Z: An Oral History of the Zombie War

Horror

Red Dragon

Filme de ação

The Stand

Fantasia

Lightning

Filme de ação

The Exorcist

Filme de ação

The Witching Hour

Ficção

The Vampire Lestat

Paranormal

Intensity

Filme de ação

The Terror

Historical Fiction

House of Leaves

Fantasia

Swan Song

Apocalyptic

Boy's Life

Fantasy

Summer of Night

Filme de ação

Full Dark, No Stars

Histórias curtas

Doctor Sleep

Filme de ação

The Green Mile

Fantasia

Rot & Ruin

Jovem adulto

Let the Right One In

Paranormal

The Silence of the Lambs

Filme de ação

It

Fantasia

Different Seasons

Histórias curtas

Misery

Filme de ação

NOS4A2

Fantasia

This Book Is Full of Spiders

Humor

End of Watch

Filme de ação

Como cada livro tem seu gênero, a tabela resultante não possui células vazias, e não inclui gêneros que não podem ser encontrados na tabela `books`.

Como você deve ter imaginado, alterar a ordem das tabelas em RIGHT JOIN nos dá o mesmo resultado que fazer um LEFT JOIN. E vice-versa.

Agora, um pouco de prática com o nosso conjunto de dados de laticínios. Aqui está o diagrama ER para referência.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_image_2021-08-06_133005_1628245804.png)

Junção externa. RIGHT JOIN

Tarefa2 / 2

1.

Use o método `RIGHT JOIN` para imprimir na tela datas que não possuem entradas de transação mas que possuem dados sobre o clima:

1) Recupere a data (`date`) da tabela `weather`. Perceba que ela precisa ser convertida para o tipo de dados correto com `CAST`.

2) Junte a tabela `weather` à tabela `transactions`usando `RIGHT JOIN` no campo `date`.

3) Faça uma fatia de dados no bloco `WHERE`: selecione apenas datas vazias da tabela `transactions` usando `IS NULL`.

4) Imprima o campo `date` da tabela resultante.

2.

Qualquer consulta com JUNÇÃO ESQUERDA pode ser escrita como uma JUNÇÃO DIREITA, e vice versa. Complete a tarefa da lição anterior com o método JUNÇÃO DIREITA.

Imprima na tela os nomes de produtos que nunca foram vendidos na loja cujo identificador unívoco é 3.

1) Selecione nomes de produtos unívocos `name` da tabela `products`. Salve-os na variável `name`.

2) Usando JUNÇÃO DIREITA, junte a `subquery` com a consulta externa no campo `id_product`. Dentro da subconsulta, selecione produtos unívocos `id_product` da tabela `transactions` cujo valor `id_store` seja 3.

3) No bloco `WHERE` da consulta externa, procure por valores NULL de `id_product` na tabela da `subquery`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

SELECT

products.name as name

FROM ( SELECT DISTINCT \--selecione produtos unívocos da loja 3

id\_product

FROM

transactions

WHERE

id\_store \= 3

) AS subquery

RIGHT JOIN products ON products.id\_product \= subquery.id\_product

WHERE

subquery.id\_product is null

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-47-522Z.md
### Última modificação: 2025-05-28 19:38:47

# Combinação de agregações e junções - TripleTen

Teoria

# Combinação de agregações e junções

Na lição anterior você criou uma maravilhosa tabela com autores, livros e gêneros. Mas não dá par ficar sentado só admirando ela! Temos que fazer algo com ela, como chamar uma função de agregação, por exemplo.

Vamos descobrir quantos livros de cada gênero há na tabela juntada `books-genre`.

### books

name

book\_id

genre\_id

author\_id

publisher\_id

date\_pub

pages

rating

The City of Mirrors

1

0

3

5

2016/01/01

602

4.2

The October Country

2

5

5

5

1955/01/01

334

4.16

Saint Odd

3

5

7

8

2015/01/01

352

4.14

From the Corner of His Eye

4

4

7

8

2000/01/01

729

4.01

The Great and Secret Show

5

5

13

15

1990/01/01

658

4.05

World War Z: An Oral History of the Zombie War

6

10

15

19

2006/01/01

342

4.01

Red Dragon

7

4

4

17

1981/01/01

348

4.02

The Stand

8

0

0

2

1978/01/01

816

4.34

Lightning

9

4

7

14

1988/01/01

384

4.06

The Exorcist

10

6

6

7

1971/01/01

385

4.15

The Witching Hour

11

5

9

10

1990/01/01

1038

4.11

The Vampire Lestat

12

8

9

10

1985/01/01

550

4.06

Intensity

13

4

7

10

1995/01/01

436

4.04

The Terror

14

9

14

16

2007/01/01

769

4.04

House of Leaves

15

0

8

9

2000/01/01

705

4.12

Swan Song

16

2

1

3

1987/01/01

956

4.28

Boy's Life

17

0

1

3

1991/01/01

580

4.34

Summer of Night

18

4

14

18

1991/01/01

600

4.02

Full Dark, No Stars

19

1

0

11

2010/01/01

368

4.05

Doctor Sleep

20

4

0

11

2013/01/01

531

4.1

The Green Mile

21

0

0

0

1996/01/01

465

4.43

Rot & Ruin

22

7

10

12

2010/01/01

458

4.1

Let the Right One In

23

8

11

6

2008/01/01

513

4.07

The Silence of the Lambs

24

4

4

6

1988/01/01

367

4.17

It

25

0

0

1

1986/01/01

1116

4.22

Different Seasons

26

1

0

1

1982/01/01

560

4.35

Misery

27

4

0

1

1987/01/01

320

4.13

NOS4A2

28

0

12

13

2013/01/01

692

4.06

This Book Is Full of Spiders

29

3

2

2012/01/01

406

4.26

End of Watch

30

4

0

2016/01/01

432

4.09

### genre

name

genre\_id

Fantasia

0

Histórias curtas

1

Apocalíptico

2

Humor

3

Filme de ação

4

Ficção

5

Clássicos

6

Jovem adulto

7

Paranormal

8

Ficção histórica

9

Horror

10

Para fazer isso, chamaremos a função de agregação COUNT ao juntar as tabelas:

```
SELECT
    genre.name AS genre_name,
    COUNT(books.name) AS name_cnt
FROM
    books
    INNER JOIN genre ON genre.genre_id = books.genre_id
GROUP BY
    genre_name;
```

### Result

genre\_name

name\_cnt

Filme de ação

9

Clássicos

1

Paranormal

2

Histórias curtas

2

Jovem adulto

1

Apocalyptic

1

Ficção histórica

1

Humor

1

Fantasia

7

Horror

1

Ficção

4

Perceba que o agrupamento é feito de acordo com o campo `genre_name`, que é indicado no bloco SELECT (após AS). Mas nem todos SGBD permitem que você trabalhe com campos introduzidos em SELECT. Em alguns casos, você pode precisar agrupar os dados usando o nome completo do campo, `genre.name`.

### author

first\_name

author\_id

last\_name

Stephen

0

King

Robert R.

1

McCammon

David

2

Wong

Justin

3

Cronin

Thomas

4

Harris

Ray

5

Bradbury

William Peter

6

Blatty

Dean

7

Koontz

Mark Z.

8

Danielewski

Anne

9

Rice

Jonathan

10

Maberry

John Ajvide

11

Lindqvist

Joe

12

Hill

Clive

13

Barker

Dan

14

Simmons

Max

15

Brooks

Vamos descobrir a quantidade de livros para cada gênero, agrupados por autor:

```
SELECT
    genre.name AS genre_name,
    author.first_name AS author_first_name,
    author.last_name AS author_last_name,
    COUNT(books.name) AS name_cnt
FROM
    books
    INNER JOIN genre ON genre.genre_id = books.genre_id
    INNER JOIN author ON author.author_id = books.author_id
GROUP BY
    genre_name,
    author_first_name,
    author_last_name;
```

### Result

genre\_name

author\_first\_name

author\_last\_name

name\_cnt

Fantasia

Mark Z.

Danielewski

1

Ficção

Clive

Barker

1

Paranormal

Anne

Rice

1

Ficção

Ray

Bradbury

1

Paranormal

John Ajvide

Lindqvist

1

Horror

Max

Brooks

1

Fantasia

Justin

Cronin

1

Clássicos

William Peter

Blatty

1

Filme de ação

Dan

Simmons

1

Fantasia

Robert R.

McCammon

1

Ficção histórica

Dan

Simmons

1

Ficção

Anne

Rice

1

Jovem adulto

Jonathan

Maberry

1

Histórias curtas

Stephen

King

2

Fantasia

Stephen

King

3

Filme de ação

Dean

Koontz

3

Filme de ação

Thomas

Harris

2

Apocalíptico

Robert R.

McCammon

1

Ficção

Dean

Koontz

1

Fantasia

Joe

Hill

1

Filme de ação

Stephen

King

3

Humor

David

Wong

1

Às vezes você precisa juntar tabelas e um parâmetro não é o suficiente. Para aplicar regras mais estritas e juntar tabelas, apenas naquelas linhas que compartilham diversos valores simultaneamente, basta combinar todas as sentenças necessárias com `AND`:

```
SELECT
    # colunas, etc.
FROM
    table_1
    LEFT JOIN table_2 ON condition 1
        AND condition 2
        AND condition 3
```

Agora, um pouco de prática com o nosso conjunto de dados de laticínios. Aqui está o diagrama ER para referência.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_image_2021-08-06_133005_1628245804.png)

Combinação de agregações e junções

Tarefa5 / 5

1.

Descubra o número total de produtos e o número de produtos unívocos para cada transação. Imprima 10 linhas contendo os IDs das transações, a quantidade de produtos comprados, e a quantidade de produtos unívocos em cada compra. Nomeie os campos `id_transaction`, `name_cnt`, `name_uniq_cnt` respectivamente.

Selecione os valores `id_transaction` da tabela `transactions`. Aplique a função de agregação `COUNT` à coluna `name` da tabela `products`. Então aplique a função de agregação `COUNT(DISTINCT )` à coluna `name` da tabela `products`.

Use `INNER JOIN` para juntar as tabelas `transactions` e `products` no campo `id_product`.

Por fim, agrupe os valores por `id_transaction`.

2.

Descubra quais transações incluíram produtos não unívocos (produtos repetidos). Imprima o número da transação, a quantidade de produtos e a quantidade de produtos unívocos. Nomeie os campos `id_transaction`, `name_cnt`, `name_uniq_cnt`, respectivamente.

Selecione os valores `id_transaction` da tabela `transactions`. Aplique a função de agregação `COUNT` à coluna `name` da tabela `products`. Então aplique a função de agregação `COUNT(DISTINCT )` à coluna `name` da tabela `products`.

Use `INNER JOIN` para juntar as tabelas `transactions` e `products` no campo `id_product`.

Use a cláusula `HAVING` com a condição de que a quantidade de produtos (`products.name`) **não seja igual** à quantidade de produtos unívocos (`DISTINCT products.name`). Use a função `COUNT` para obter os totais.

3.

Descubra quantas transações foram feitas em dias ensolarados e em dias chuvosos. Crie uma tabela com os seguintes campos: precipitação (`rain`) e quantidade de transações (`uniq_transactions`).

Selecione os valores do campo `rain` da tabela `weather`. Calcule a quantidade de valores unívocos na coluna `id_transaction` da tabela `transactions` usando `COUNT(DISTINCT )`.

Agrupe os dados pelos valores de `rain`. Junte as tabelas `transactions` e `weather` no campo `date` usando `INNER JOIN`.

4.

Imprima a temperatura do ar e a quantidade de transações. Ordene os resultados em ordem crescente (a primeira de todas vem antes).

No bloco `SELECT`:

-   Converta os valores `date` na tabela `weather` para o tipo de data usando a função `CAST` e salve-os no campo `date`
-   Selecione os valores `temp` de `weather` e salve-os como `temp`
-   Descubra a quantidade de valores exclusivos na coluna `id_transaction` da tabela `transactions` e salve-os como `uniq_transactions`

Use `LEFT JOIN` para juntar as tabelas `transactions` e `weather` no campo `date`.

Agrupe os resultados por data e temperatura, e ordene-os por data.

5.

Para cada transação, descubra o preço final de compra, e também a quantidade de produtos comprados.

Selecione a coluna `id_transaction` da tabela `transactions`.

Calcule a soma dos valores na coluna `price` na tabela `products_stores`. Salve no campo `total`.

Descubra a quantidade de valores na coluna `id_product` de `products_stores`. Salve como `amount`.

Use `LEFT JOIN` para juntar `products_stores` à tabela `transactions` usando três condições simultaneamente:

1) Os valores nas colunas `products_stores.date_upd` e `transactions.date` são iguais. Converta os valores para os tipos de dados corretos com `CAST`.

2) Os valores nas colunas `products_stores.id_product` e `transactions.id_product` são iguais.

3) Os valores nas colunas `products_stores.id_store` e `transactions.id_store` são iguais.

Observe bem! Você pode escrever três condições após ON. Apenas conecte-as usando dois ANDs.

Agrupe os dados de acordo com os valores na coluna `id_transaction` de `transactions`.

Imprima apenas as transações que custam mais do que $35. Indique a seguinte condição em HAVING: `SUM(products_stores.price) > 35`.

**As colunas devem aparecer nesta ordem: `id_transaction`, `total`, `amount`.**.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

SELECT

transactions.id\_transaction AS id\_transaction,

SUM(products\_stores.price) AS total,

COUNT(products\_stores.id\_product) AS amount

FROM

transactions

LEFT JOIN products\_stores ON CAST(products\_stores.date\_upd AS date) \= CAST(transactions.date AS date)

AND products\_stores.id\_product \= transactions.id\_product

AND products\_stores.id\_store \= transactions.id\_store

GROUP BY

transactions.id\_transaction

HAVING

SUM(products\_stores.price) \> 35;

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-49-929Z.md
### Última modificação: 2025-05-28 19:38:50

# Juntando dados de instruções - TripleTen

Teoria

# Juntando dados de instruções

Às vezes é necessário juntar dados recuperados em consultas separadas.

Os operadores **UNION** e **UNION ALL** fazem a união dos dados das tabelas. Esta é a sintaxe:

```
SELECT
    column_name_1
FROM
    table_1
UNION --( ou UNION ALL)
SELECT
    column_name_1
FROM
    table_2;
```

Aqui duas sentenças SELECT - FROM estão separadas pelo comando UNION.

Estas são as condições que devem ser cumpridas para que um UNION funcione:

-   As duas tabelas devem possuir correspondências em relação ao número de colunas selecionadas e seus tipos de dados.
-   Os campos devem estar organizados na mesma ordem na primeira e na segunda tabela.

UNION evita linhas duplicadas ao gerar uma tabela.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_6.5.13_1652347991.png)

Vamos ver um exemplo para entender como UNION funciona. Queremos descobrir quanto vale o rebanho da fazenda do vovô e da vovó. Aqui estão duas tabelas que listam o rebanho de cada um:

**cows\_grandpa**

year\_tag

name

weight

breed

Column

234657

Betty Sue

350

Belgian Blue

456721

Penelope

500

Holstein

567243

Dorothy

400

Belgian Blue

**cows\_grandma**

tag

name

weight

breed

Column

456721

Penelope

500

Holstein

567243

Dorothy

400

Belgian Blue

778958

Olivia

300

Holstein

653216

Moon

450

Holstein

Precisamos de uma tabela que mostre todas as cabeças de gado da Vovó ou do Vovô, ou de ambos. Penelope and Dorothy são propriedades dos dois, mas a Betty Sue é do Vovô e a Olivia e Moon são da Vovó. Vamos usar UNION para excluir valores duplicados do resultado:

```
SELECT
    year_tag,
    name,
    weight
FROM
    cows_grandpa
UNION
SELECT
    tag,
    name,
    weight
FROM
    cows_grandma;
```

year\_tag

name

weight

234657

Betty Sue

350

456721

Penelope

500

567243

Dorothy

400

778958

Olivia

300

653216

Moon

450

Ao contrário de UNION, UNION ALL inclui duplicados na seleção resultante. Isso é útil quando temos certeza de que não há valores correspondentes nas tabelas que serão juntadas, ou quando precisamos realmente duplicar valores.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_6.5.13.2_1652348066.png)

Agora vamos adicionar a tabela `bulls` à nossa tabela `cows`.

**cow**

year\_tag

name

weight

234657

Betty Sue

350

456721

Penelope

500

567243

Dorothy

400

778958

Olivia

300

653216

Moon

450

**bulls**

year\_tag

name

weight

breed

567890

Cowboy

600

Holstein

987260

Duke

500

Holstein

675364

Homer

550

Belgian Blue

Para obter o valor da quantidade total do rebanho, devemos juntar essas tabelas. Sabemos que `cows` e `bulls` não possuem valores duplicados, porque eles possuem valores de `year_tag` diferentes, então podemos usar UNION ALL:

```
SELECT
    year_tag,
    name,
    weight
FROM
    cow
UNION ALL
SELECT
    year_tag,
    name,
    weight
FROM
    bulls;
```

year\_tag

name

weight

234657

Betty Sue

350

456721

Penelope

500

567243

Dorothy

400

778958

Olivia

300

653216

Moon

450

567890

Cowboy

600

987260

Duke

500

675364

Homer

550

Pronto, deixamos tudo arrumadinho em nossa zona rural. Vamos para a loja dizer olá para nossos amados laticínios.

Aqui está o diagrama ER para referência:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_image_2021-08-06_133005_1628245804.png)

Juntando dados de instruções

Tarefa5 / 5

1.

Escreva uma consulta para selecionar os nomes de todos os produtos que foram comprados ao menos uma vez em 1 de junho de 2019.

Na consulta externa selecione nomes de produtos (`name`) da tabela `products`. Use o comando DISTINCT para evitar duplicados. Salve os resultados no campo `name`.

Junte `products` com a tabela `subq`, que contém os IDs de produto transacionados em 1º de junho de 2019, usando `LEFT JOIN` no campo `id_product`.

Na subconsulta (`subq`) selecione `id_product` da tabela `transactions`.

Para obter `id_product` para 1 de junho, você deverá converter a coluna `transactions.date` para o tipo de data usando `CAST`.

2.

Escreva uma consulta para selecionar os nomes de todos os produtos que foram comprados ao menos uma vez em 8 de junho de 2019.

Na consulta externa selecione nomes de produtos (`name`) da tabela `products`. Use o comando DISTINCT para evitar duplicados. Salve os resultados no campo `name`.

Junte `products` com a tabela `subq` usando `LEFT JOIN` no campo `id_product`.

Na subconsulta (`subq`) selecione `id_product` da tabela `transactions`.

Para obter `id_product` para 8 de junho, você deverá converter a coluna `transactions.date` para o tipo de dados 'date' usando `CAST`.

3.

Junte os dados obtidos nas consultas anteriores em uma seleção sem duplicar os valores `name`.

4.

Transforme a consulta da tarefa anterior em uma subconsulta e chame-a `SUBQ`. Passe `name` de `SUBQ` para a consulta externa e descubra o comprimento da lista de produtos resultante (ou seja, a quantidade de produtos vendidos em 1 e 8 de junho).

5.

Junte as consultas dos blocos anteriores usando UNION ALL e transforme-as em uma subconsulta com o nome `SUBQ`.

Passe `name` de `SUBQ` para a consulta externa e descubra a quantidade total de produtos. Veja como o número muda devido à presença de duplicados.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

select

count(SUBQ.name) as name

from

( SELECT DISTINCT

products.name AS name

FROM

products

LEFT JOIN (

SELECT

id\_product

FROM

transactions

WHERE

CAST(transactions.date AS date) \= '2019-06-01') AS SUBQ1 ON products.id\_product \= SUBQ1.id\_product

WHERE

SUBQ1.id\_product IS NOT NULL

union all

SELECT DISTINCT

products.name AS name

FROM

products

LEFT JOIN (

SELECT

id\_product

FROM

transactions

WHERE

CAST(transactions.date AS date) \= '2019-06-08') AS SUBQ2 ON products.id\_product \= SUBQ2.id\_product

WHERE

SUBQ2.id\_product IS NOT NULL) as SUBQ

\--escreva seu código aqui;

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-38-51-272Z.md
### Última modificação: 2025-05-28 19:38:51

# Conclusão - TripleTen

Capítulo 5/8

Relacionamentos Entre Tabelas

# Conclusão

Agora você sabe como juntar tabelas usando métodos diferentes.

Neste capítulo você aprendeu:

-   Sobre relacionamentos entre tabelas
-   Como juntar e unir tabelas
-   Como procurar por substrings usando expressões regulares
-   Como procurar por valores vazios em uma tabela

### Leve isso com você

Baixe a [folha de conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/moved_Folha_De_Concluses_Relacionamentos_Entre_Tabelas.pdf) e o [sumário do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/moved_Resumo_do_Captulo_Relacionamentos_entre_Tabelas.pdf) da Base de Conhecimento para que você possa consultá-los sempre que precisar.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-39-44-786Z.md
### Última modificação: 2025-05-28 19:39:45

# Introdução - TripleTen

Capítulo 6/8

Soft Skills

# Introdução

Você realmente quer passar sua vida inteira como analista de dados júnior? Temos certeza que não. É hora de falar sobre como analistas juniores tornam-se analistas de nível intermediário.

Você obviamente nunca vai conseguir devorar os números como um computador. O que faz um bom analista é a sistemática e o pensamento crítico. O objetivo de um detetive é encontrar o criminoso, e não contar as linhas de uma impressão digital.

Mergulhar fundo nos dados e se comunicar com os outros: essas são habilidades que irão te ajudar a decolar na carreira.

Diz o senso comum que a eloquência é a responsável pelo sucesso na carreira.

Neste capítulo, vamos discutir alguns hábitos e abordagens de analistas bem sucedidos.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-39-46-125Z.md
### Última modificação: 2025-05-28 19:39:46

# Contexto da Tarefa - TripleTen

Capítulo 6/8

Soft Skills

# Contexto da Tarefa

Torne um hábito estudar profundamente o contexto de uma tarefa logo no início.

Times precisam de analistas mais pelos insights que eles fornecem com base na análise de dados, do que pelos dados propriamente ditos. Então bora, vamos fazer algo útil!

Pergunta

Você é um analista de uma loja online. O departamento de marketing pediu para você calcular taxas de conversão para diversos tipos de anúncios.

Como você acha que eles pensaram nessa tarefa?

É apenas uma coleção de métricas regulares

O pessoal do departamento de marketing não deve ter mais o que fazer.

O marketing está tentando introduzir mais uma melhoria

Problemas surgiram e estamos tentando explicá-los

Certo. Via de regra, problemas ou insatisfações são a origem da busca por informação e implantação de melhorias.

Fantástico!

Nesse caso, um analista questionador conseguiria a seguinte resposta: "Enviar nossas newsletters estava custando demais, mas não estava produzindo resultados. Os pagamentos têm caído bastante recentemente."

Isso dá uma ideia do local certo para procurar. Os e-mails são enviados na segunda-feira à tarde, quando muitas pessoas estão voltando do trabalho, verificando seus e-mails em seus celulares. Isso que dizer que precisamos recuperar dados sobre conversão em dispositivos móveis, e não todo tipo de conversão.

Após obter os dados para aparelho móveis, o analista então verifica os dados e descobre que os usuários colocaram produtos no carrinho de compras, e até clicaram no botão para fazer o checkout, mas então suas sessões foram interrompidas e esses usuários nunca mais voltaram.

Uma pista! Uma hipótese vai se formando aos poucos, de que há problemas com a versão móvel do site. Nosso analista informa isso ao departamento de marketing, que procura a equipe de suporte e descobre o seguinte: o layout do site móvel está distorcido, e o botão "pagar" não está aparecendo. Exatamente!

**O procedimento** para entender o contexto da tarefa é simples:

1.  Responda à pergunta: por que essa tarefa foi proposta?
2.  Discuta sobre onde você deveria procurar por respostas ou hipóteses com quem propôs a tarefa.

O próximo passo é esclarecer diversos detalhes da tarefa. Vamos falar sobre como fazer isso.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-39-47-438Z.md
### Última modificação: 2025-05-28 19:39:47

# Elaboração da Tarefa - TripleTen

Capítulo 6/8

Soft Skills

# Elaboração da Tarefa

As tarefas que você receberá não serão sempre claras e detalhadas, e você precisará coletar informações adicionais para completá-las. Acostume-se com isso ser da sua responsabilidade.

Seguindo esses seis passos você economizará tempo e energia:

1.  Clarifique quem é o "cliente final" do seu trabalho.

Pode ser qualquer um, inclusive o gestor da empresa. É pouco provável que seja o CEO, mas eles podem determinar tarefas comunicando-as aos _team leaders_. Saber quem é o cliente final pode te ajudar a decidir o que colocar exatamente nos gráficos e quais comentários escrever.

Pergunta

Escolha a pergunta que irá te ajudar a aprender quem é o cliente de uma maneira educada:

Por que estamos trabalhando nisso?

Qual é o seu cargo?

Quem precisa dos dados finais?

Com quem devo falar para que essa tarefa seja feita?

Eu não deveria receber mais por fazer esse tipo de trabalho?

Você conseguiu!

2.  Descubra porque os resultados são necessários.

Foi pedido a você que estudasse a efetividade de vários canais de newsletter. Existem dois objetivos: "encontrar o canal para futuros investimentos" e "decidir qual canal pode ser descontinuado." Eles determinam o quão profunda a análise deve ser e qual deve ser a recomendação final.

Pergunta

Qual questão você deve perguntar para esclarecer o objetivo final do seu trabalho?

Por que estou trabalhando aqui?

Qual é o objetivo dessa análise?

Qual é o objetivo dessa tarefa?

O que faremos com o resultado final?

Vou receber um bônus por isso?

Posso ser punido pelo meu trabalho?

Você conseguiu!

3.  Esclareça em quais dados exatamente o cliente está interessado. Tem alguma dificuldade escondida?

Por exemplo, talvez o líder do time peça para você retornar estatísticas de venda por mês. Mas ao discutir o projeto, você descobre que eles precisam que os dados sejam divididos por grupo de produtos e dias da semana.

Pergunta

Selecione todas as perguntas que irão te ajudar a esclarecer os detalhes desse projeto.

Escolha quantas quiser

Qual é a senha do wi-fi?

Quão detalhado deve ser o resultado?

Qual cor deve ser escolhida para os resultados da tabela?

Quais outros dados eu talvez precise para conseguir esse resultado?

Quando exatamente terei que apresentar os resultados?

Você precisa que o relatório seja assinado por um advogado?

Você conseguiu!

4.  Articule e faça um registro de como resultado final deve ser.
    
    Você pode fazer o registro em um ticket, e-mail, ou sistema de gerenciamento de tarefas, dependendo de como a empresa funciona. Mesmo que o cliente diga que um acordo verbal é suficiente, nós recomendamos que faça o registro por escrito para que tenha algo para comparar com os resultados.
    
5.  Faça um rascunho da sua solução e pergunte ao cliente se está bom.
    
    Talvez tenham vários ciclos "rascunhos —> feedback —> revisão" antes de vocês chegarem em um acordo sobre a solução adequada.
    
6.  Apresente o resultado final ao cliente.
    
    Se há alguma comunicação escrita, termine ela: feche os tickets ou a tarefa no sistema de gerenciamento de tarefas.
    
    Conforme você termina esses seis passos, você pode perceber que a tarefa, na verdade, não é sobre análise ou não é para você. Por exemplo, os dados já podem estar em um bom formato, a tudo o que você deve fazer é solicitá-los, ou a tarefa pode ser importante para outro departamento. Prestar atenção a tais coisas vai poupar seu tempo e dos colegas!
    

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-39-49-769Z.md
### Última modificação: 2025-05-28 19:39:50

# Revisão Adicional - TripleTen

Capítulo 6/8

Soft Skills

# Revisão Adicional

Você enviou seu trabalho e respirou fundo. Hora de pegar uma praia, oba! Então, de repente, o cliente diz que eles estavam esperando algo diferente, ou que agora eles querem um parâmetro diferente (digamos, uma análise detalhada por dias, e não meses). Eles possuem um argumento à prova de bala: é facinho fazer! Tirando o fato de que não é.

O que você faz nesses casos, como você reage? Há duas coisas para ficar atento aqui: suas emoções e a realidade da situação.

1.  Emoções:
    
    É claro que você não está feliz, especialmente se isso quiser dizer que você terá muito trabalho.
    
    -   Não faz sentido ficar nervoso—o mais importante é comunicar para seu colega que isso vai ser apenas perda de tempo para vocês dois.
    -   Explique de um jeito amigável que é melhor fazer correções durante o processo de trabalho, e não quando já está tudo pronto.
2.  A realidade da situação:
    
    -   Faça uma boa e honesta reflexão: a tarefa foi realmente bem feita? Sobre o que mais você poderia ter perguntado para obter uma descrição clara da tarefa?
    -   Ao criar códigos, faça-os de modo flexível, para que seja fácil fazer adições e correções.
    

Não há como evitar revisões, mas cabe a você fazer com que elas sejam construtivas.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-39-51-066Z.md
### Última modificação: 2025-05-28 19:39:51

# Conclusão - TripleTen

Capítulo 6/8

Soft Skills

# Conclusão

A arte da análise tem tudo a ver com a sua habilidade de entender o contexto das tarefas e de abordá-las da maneira adequada.

Salve as seis recomendações que foram dadas em "Elaboração da Tarefa" para toda vez que você pegar uma tarefa. Logo isso vai se tornar um hábito, contribuindo para seu crescimento pessoal.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-39-52-383Z.md
### Última modificação: 2025-05-28 19:39:52

# Resultados do Curso - TripleTen

Capítulo 7/8

Projeto do Curso

# Resultados do Curso

Você concluiu o curso sobre Coleta e Armazenamento de Dados. Muito bem!

Você aprendeu a recuperar dados de fontes online e a trabalhar com JSON. Mas, mais importante, você aprendeu a fazer o mesmo com bancos de dados usando SQL. Agora você sabe como escrever consultas e resolver tarefas analíticas simples. Essas são as habilidades necessárias para concluir os tipos de tarefas que mais costumam ser atribuídas a analistas de produtos em grandes corporações: trabalhar com conjuntos de dados grandes aos quais usuários externos não têm acesso.

Para determinar quais lições você pode precisar revisar antes de concluir o projeto final, responda às seguintes perguntas:

Pergunta

Sua tarefa é recuperar o código de uma página da web. Qual biblioteca você vai precisar?

json

beautifulsoup

requests

pandas

Excelente!

Pergunta

Qual dessas expressões regulares seleciona apenas letras?

\\d

\[0-9\]

\\w

\[A-z\]

Seu entendimento sobre o material é impressionante!

Pergunta

Qual comando você usa para selecionar valores unívocos de uma tabela?

ORDER BY

LIMIT

DESC

DISTINCT

Muito bem!

Pergunta

O campo **date\_upd** está no formato timestamp. Como ele pode ser convertido para o formato date?

case when date\_upd = _timestamp then date end_

cast (date\_upd as DATE)

date\_upd (cast as DATE)

date\_upd::::DATE

Fantástico!

Pergunta

Qual cláusula é usada para filtrar os dados após as linhas serem agrupadas e os valores, agregados?

WHERE

COUNT

HAVING

LIMIT

Você conseguiu!

Pergunta

Qual função é usada para calcular a soma total dos valores em um campo?

AVG()

COUNT()

COUNT(DISTINCT )

SUM()

Muito bem!

Pergunta

Existem valores vazios no campo _category_ de uma tabela. Como você achou eles?

WHERE category IS NULL

WHERE category LIKE 'NULL'

WHERE category = 'NULL'

category.isnull()

Muito bem!

Pergunta

Qual palavra-chave seleciona os registros que possuem valores correspondentes em ambas as tabelas?

INNER JOIN

OUTER JOIN

LEFT JOIN

RIGHT JOIN

Muito bem!

Pergunta

O que você recebe quando usa LEFT JOIN?

todos os registros da tabela direita e registros com valores correspondentes da esquerda

todos os registros de ambas as tabelas, exceto aqueles com valores correspondentes

apenas registros com valores correspondentes

todos os registros da tabela esquerda e registros com valores correspondentes da direita

Você conseguiu!

Pergunta

Escolha a descrição correta da tabela _airports_

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1585510424.png)

Nome da tabela : _airports._ Colunas: _airport\_code, airport\_name, city, timezone._ Chave primária: _airport\_code_

Nome da tabela : _airports._ Colunas: _airport\_code, airport\_name, city,model ._ Chave primária: _airport\_code_

Nome da tabela : _airports._ Colunas: _airport\_code, airport\_name, city, timezone._ Chave primária: _aeroportos_

Nome da tabela : _airports._ Colunas: _airport\_code, airport\_name, city, timezone._ Chave primária: _airports._ Chave estrangeira: _departure\_airport, arrival\_airport, aircraft\_code_

Excelente!

Pergunta

Qual é a relação entre _aircrafts_ e _flights_?

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1585510493.png)

muitos-para-muitos

um-para-um

um-para-muitos

Muito bem!

Pergunta

Alguma das tabelas tem relacionamentos muitos-para-muitos? Em caso afirmativo, escolha o nome da tabela de associação para esse relacionamento.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1585510604.png)

flights

ticket\_flights

tickets

não há relacionamentos muitos-para-muitos

Trabalho maravilhoso!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-39-54-468Z.md
### Última modificação: 2025-05-28 19:39:54

# Descrição do Projeto - TripleTen

Capítulo 7/8

Projeto do Curso

Parabéns! Você concluiu o curso de SQL. É hora de aplicar o conhecimento e as habilidades que você adquiriu em um projeto: um estudo de caso analítico da vida real que você concluirá por conta própria.

Quando você terminar o projeto, envie seu trabalho para o revisor do projeto para avaliação. Ele te dará feedback dentro de 48 horas. Use o feedback para fazer alterações e, em seguida, envie a nova versão de volta ao revisor do projeto.

Você pode obter mais feedback sobre a nova versão. Isso é completamente normal. Não é incomum passar por vários ciclos de feedback e revisão.

Seu projeto será considerado concluído assim que o revisor do projeto o aprovar.

### Descrição do Projeto

Você está trabalhando como analista da Zuber, uma nova empresa de compartilhamento de caronas que está sendo lançada em Chicago. Sua tarefa é encontrar padrões nas informações disponíveis. Você quer entender as preferências dos passageiros e o impacto de fatores externos nas corridas.

Trabalhando com um banco de dados, você analisará dados de concorrentes e testará uma hipótese sobre o impacto do clima na frequência das viagens.

### Descrição dos dados

Um banco de dados com informações sobre corridas de táxi em Chicago:

tabela `neighborhoods`: dados sobre os bairros da cidade

-   `name`: nome do bairro
-   `neighborhood_id`: código do bairro

tabela `cabs`: dados sobre os táxis

-   `cab_id`: código do veículo
-   `vehicle_id`: a identificação técnica do veículo
-   `company_name`: a empresa proprietária do veículo

tabela `trips`: dados sobre corridas

-   `trip_id`: código da corrida
-   `cab_id`: código do veículo que opera a corrida
-   `start_ts`: data e hora do início da corrida (tempo arredondado para a hora)
-   `end_ts`: data e hora do final da corrida (tempo arredondado para a hora)
-   `duration_seconds`: duração da corrida em segundos
-   `distance_miles`: distância percorrida em milhas
-   `pickup_location_id`: código do bairro de retirada
-   `dropoff_location_id`: código do bairro de entrega

tabela `weather_records`: dados sobre o clima

-   `record_id`: código de registro meteorológico
-   `ts`: grava data e hora (tempo arredondado para a hora)
-   `temperature`: temperatura quando o registro foi feito
-   `description`: breve descrição das condições meteorológicas, ex. "chuva leve" ou "nuvens esparsas"

### Esquema de tabela

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1_1585510727.png)

Nota: não há uma conexão direta entre as tabelas `trips` e `weather_records` no banco de dados. Mas você ainda pode usar JOIN e juntá-las usando a hora em que o passeio começou (`trips.start_ts`) e a hora em que o registro do tempo foi feito (`weather_records.ts`).

### Instruções para completar o projeto

**Passo 1. Escreva um código para analisar os dados sobre o clima em Chicago em novembro de 2017 no site:**

[https://practicum-content.s3.us-west-1.amazonaws.com/data-analyst-eng/moved\_chicago\_weather\_2017.html](https://practicum-content.s3.us-west-1.amazonaws.com/data-analyst-eng/moved_chicago_weather_2017.html)

**Passo 2. Análise Exploratória de Dados**

1.  Encontre o número de corridas de táxi para cada empresa de táxi de 15 a 16 de novembro de 2017. Nomeie o campo resultante como `trips_amount` e imprima-o junto com o campo `company_name`. Ordene os resultados pelo campo `trips_amount` em ordem decrescente.
    
2.  Encontre o número de corridas para cada empresa de táxi cujo nome contém as palavras "Yellow" ou "Blue" ("Amarelo" ou "Azul", respectivamente) de 1º a 7 de novembro de 2017. Nomeie a variável resultante como `trips_amount`. Agrupe os resultados pelo campo `company_name`.
    
3.  Em novembro de 2017, as empresas de táxi mais populares eram Flash Cab e Taxi Affiliation Services. Encontre o número de corridas para essas duas empresas e nomeie a variável resultante como `trips_amount`. Junte as corridas para todas as outras empresas no grupo "Other". Agrupe os dados por nomes de empresas de táxi. Nomeie o campo com os nomes das empresas de táxi como `company`. Classifique o resultado em ordem decrescente por `trips_amount`.
    

**Passo 3. Teste a hipótese de que a duração das corridas do Loop até ao Aeroporto Internacional O'Hare muda em sábados chuvosos.**

1.  Recupere os identificadores dos bairros O'Hare e Loop da tabela `neighborhoods`.
    
2.  Para cada hora, recupere os registros de condições meteorológicas da tabela `weather_records`. Usando o operador CASE, divida todas as horas em dois grupos: "Bad" se o campo `description` contiver as palavras "rain" (chuva) ou "storm" (tempestade) e "Good" para outros. Nomeie o campo resultante como `weather_conditions`. A tabela final deve incluir dois campos: data e hora (_ts_) e `weather_conditions`.
    
3.  Recupere da tabela `trips` todos as corridas que começaram no Loop (`neighborhood_id`: 50) e terminaram em O'Hare (`neighborhood_id`: 63) em um sábado. Obtenha as condições meteorológicas para cada corrida. Use o método que você aplicou na tarefa anterior. Também recupere a duração de cada corrida. Ignore corridas para as quais os dados sobre as condições meteorológicas não estão disponíveis.
    

**Passo 4. Análise exploratória de dados (Python)**

Além dos dados recuperados nas tarefas anteriores, você recebeu um segundo arquivo. Agora você tem estes dois CSVs:

`project_sql_result_01.csv`. Ele contém os seguintes dados:

-   `company_name`: nome da empresa de táxi
-   `trips_amount`: o número de corridas para cada empresa de táxi de 15 a 16 de novembro de 2017.

`project_sql_result_04.csv`. Ele contém os seguintes dados:

-   `dropoff_location_name`: bairros de Chicago onde as corridas terminaram
-   `average_trips`: o número médio de viagens que terminaram em cada bairro em novembro de 2017.

Para esses dois conjuntos de dados, agora você precisa:

-   importar os arquivos
-   estudar os dados que eles contêm
-   verifique se os tipos de dados estão corretos
-   identificar os 10 principais bairros em termos de destinos
-   fazer gráficos: empresas de táxi e número de corridas, 10 principais bairros por número de corridas em que esse - bairro é destino
-   tirar conclusões com base em cada gráfico e explicar os resultados

**Passo 5. Testando hipóteses (Python)**

`project_sql_result_07.csv` —o resultado da última consulta. Ele contém dados sobre viagens do Loop para o Aeroporto Internacional O'Hare. Lembre-se, estes são os valores dos campos da tabela:

-   `start_ts` — data e hora do começo da corrida
-   `weather_conditions` — condições meteorológicas no momento em que a corrida começou
-   `duration_seconds` — duração da viagem em segundos

Teste a hipótese: "A duração média dos passeios do Loop para o Aeroporto Internacional O'Hare muda nos sábados chuvosos."

Defina o valor do nível de significância (alfa) por conta própria.

Explique:

-   como você formou as hipóteses nula e alternativa
-   qual critério você usou para testar a hipótese e porque

## Como o meu projeto será avaliado?

Aqui estão os critérios de avaliação do projeto. Leia-os cuidadosamente antes de começar a trabalhar.

Aqui está o que o revisor do projeto procurará ao avaliar seu projeto:

-   como você recupera dados do site
-   como você faz fatias de dados
-   como você agrupa os dados
-   se você usa os métodos corretos para unir tabelas
-   como você formula hipóteses
-   quais critérios você usa para testar as hipóteses e por quê
-   a que conclusões você chega
-   se você deixa comentários a cada passo

As folhas de conclusões e resumos das lições anteriores têm tudo o que você precisa para completar o projeto.

Boa sorte!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-39-55-738Z.md
### Última modificação: 2025-05-28 19:39:56

# Analisando dados - TripleTen

Teoria

Você está trabalhando como analista da Zuber, uma nova empresa de compartilhamento de caronas que está sendo lançada em Chicago. Sua tarefa é encontrar padrões nas informações disponíveis. Você quer entender as preferências dos passageiros e o impacto de fatores externos nas corridas.

Você estudará um banco de dados, analisará dados de concorrentes e testará uma hipótese sobre o impacto do clima na frequência de corridas.

Nesta tarefa, você concluirá a primeira etapa do projeto.

Analisando dados

Tarefa

Escreva um código para analisar os dados sobre o clima em Chicago em novembro de 2017 no site:

[https://practicum-content.s3.us-west-1.amazonaws.com/data-analyst-eng/moved\_chicago\_weather\_2017.html](https://practicum-content.s3.us-west-1.amazonaws.com/data-analyst-eng/moved_chicago_weather_2017.html)

O nome do DataFrame deve ser weather\_records e deve ser especificado ao pesquisar: `attrs={"id": "weather_records"}` . Imprima o DataFrame em sua totalidade.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

import requests

from bs4 import BeautifulSoup

import pandas as pd

URL\='https://practicum-content.s3.us-west-1.amazonaws.com/data-analyst-eng/moved\_chicago\_weather\_2017.html'

req \= requests.get(URL)

soup \= BeautifulSoup(req.text, 'lxml')

table \= soup.find('table', attrs\={"id": "weather\_records"})

heading\_table\=\[\]

for row in table.find\_all('th'):

heading\_table.append(row.text)

content\=\[\]

for row in table.find\_all('tr'):

if not row.find\_all('th'):

content.append(\[element.text for element in row.find\_all('td')\])

weather\_records \= pd.DataFrame(content, columns \= heading\_table)

print(weather\_records)

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-39-57-064Z.md
### Última modificação: 2025-05-28 19:39:57

# Trabalhando com Bancos de Dados - TripleTen

Teoria

Você está trabalhando como analista da Zuber, uma nova empresa de compartilhamento de caronas que está sendo lançada em Chicago. Sua tarefa é encontrar padrões nas informações disponíveis. Você quer entender as preferências dos passageiros e o impacto de fatores externos nas corridas.

Você estudará um banco de dados, analisará dados de concorrentes e testará uma hipótese sobre o impacto do clima na frequência de corridas.

### Descrição dos dados

Um banco de dados com informações sobre corridas de táxi em Chicago:

tabela `neighborhoods`: dados sobre os bairros da cidade

-   _nome:_ nome do bairro
-   _neighborhood\_id_: código do bairro

tabela `cabs`: dados sobre os táxis

-   _cab\_id:_ código do veículo
-   _vehicle\_id:_ a identificação técnica do veículo
-   _company\_name_: a empresa proprietária do veículo

tabela `trips`: dados sobre corridas

-   _trip\_id:_ código da corrida
-   _cab\_id:_ código do veículo que opera a corrida
-   _start\_ts:_ data e hora do início da corrida (tempo arredondado para a hora)
-   _end\_ts:_ data e hora do fim da corrida (tempo arredondado para a hora)
-   _duration\_seconds:_ duração da corrida em segundos
-   _distance\_miles:_ distância do percurso em milhas
-   _pickup\_location\_id:_ código do bairro de retirada
-   _dropoff\_location\_id:_ código do bairro de entrega

tabela `weather_records`: dados sobre o clima

-   _record\_id:_ código de registro meteorológico
-   _ts:_ gravar data e hora (tempo arredondado para a hora)
-   _temperatura:_ temperatura quando o registro foi feito
-   _descrição:_ breve descrição das condições meteorológicas, ex. "chuva leve" ou "nuvens esparsas"

### **Esquema de tabela**

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1_1585510727.png)

Nota: não há uma conexão direta entre as tabelas _trips_ e _weather\_records_ no banco de dados. Mas você ainda pode usar JOIN e vinculá-los usando a hora em que a corrida começou (_trips.start\_ts_) e a hora em que o registro do tempo foi feito (_weather\_records.ts_).

Você já fez a primeira parte do projeto: escreveu um código para analisar os dados meteorológicos de um site. Agora você fará a segunda e a terceira parte:

Tarefas 1-3: **Análise exploratória de dados**

Tarefas 4-6: **Teste a hipótese de que a duração das corridas do Loop para o Aeroporto Internacional O'Hare muda em sábados chuvosos**

Trabalhando com Bancos de Dados

Tarefa6 / 6

1.

Imprima o campo _company\_name_. Encontre o número de corridas de táxi para cada empresa de táxi de 15 a 16 de novembro de 2017, nomeie o campo resultante como _trips\_amount_ e imprima-o também. Classifique os resultados pelo campo _trips\_amount_ em ordem decrescente.

2.

Encontre o número de corridas para cada empresa de táxi cujo nome contém as palavras "Yellow" ou "Blue" ("Amarelo" ou "Azul", respectivamente) de 1 a 7 de novembro de 2017. Nomeie a variável resultante como _trips\_amount._ Agrupe os resultados pelo campo _company\_name_.

3.

De 1 a 7 de novembro de 2017, as empresas de táxi mais populares foram Flash Cab e Taxi Affiliation Services. Encontre o número de corridas para essas duas empresas e nomeie a variável resultante como _trips\_amount._ Junte as corridas de todas as outras empresas no grupo "Other". Agrupe os dados por nomes de empresas de táxi. Nomeie o campo com os nomes das empresas de táxi _company_. Ordene o resultado em ordem decrescente por _trips\_amount_.

4.

Recupere os identificadores dos bairros O'Hare e Loop da tabela _neighborhoods_.

5.

Para cada hora, recupere os registros de condições climáticas da tabela _weather\_records_. Usando o operador CASE, divida todas as horas em dois grupos: `Bad` se o campo _descrição_ contiver as palavras `rain` ou `storm` e `Good` para outros. Nomeie o campo resultante como _weather\_conditions_. A tabela final deve incluir dois campos: data e hora (_ts_) e _weather\_conditions_.

6.

Recupere da tabela _trips_ todas as corridas que começaram no Loop (_pickup\_location\_id:_ 50) em um sábado e terminaram em O'Hare (_dropoff\_location\_id_: 63). Obtenha as condições meteorológicas para cada corrida. Use o método que você aplicou na tarefa anterior. Além disso, recupere a duração de cada corrida. Ignore corridas para as quais os dados sobre as condições meteorológicas não estão disponíveis.

As colunas da tabela devem estar na seguinte ordem:

-   _start\_ts_
-   _weather\_conditions_
-   _duration\_seconds_

Ordene por _trip\_id._

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

SELECT

start\_ts,

T.weather\_conditions,

duration\_seconds

FROM

trips

INNER JOIN (

SELECT

ts,

CASE

WHEN description LIKE '%rain%' OR description LIKE '%storm%' THEN 'Bad'

ELSE 'Good'

END AS weather\_conditions

FROM

weather\_records

) T on T.ts \= trips.start\_ts

WHERE

pickup\_location\_id \= 50 AND dropoff\_location\_id \= 63 AND EXTRACT (DOW from trips.start\_ts) \= 6

ORDER BY trip\_id

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-39-59-390Z.md
### Última modificação: 2025-05-28 19:39:59

# Sprint 7 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 7/8

Projeto do Curso

# Trabalhando com dados em Python

**Passo 4. Análise exploratória de dados (Python)**

Além dos dados recuperados nas tarefas anteriores, você recebeu um segundo arquivo. Agora você tem estes dois CSVs:

[/datasets/project\_sql\_result\_01.csv](https://practicum-content.s3.us-west-1.amazonaws.com/learning-materials/data-analyst-eng/moved_project_sql_result_01.csv). contém os seguintes dados:

contém os seguintes dados:

_trips\_amount_: o número de corridas para cada empresa de táxi de 15 a 16 de novembro de 2017.

[/datasets/project\_sql\_result\_04.csv](https://practicum-content.s3.us-west-1.amazonaws.com/learning-materials/data-analyst-eng/moved_project_sql_result_04.csv). contém os seguintes dados:

_dropoff\_location\_name_: bairros de Chicago onde as corridas terminaram

_average\_trips_: o número médio de viagens que terminaram em cada bairro em novembro de 2017.

Para esses dois conjuntos de dados, agora você precisa

-   importar os arquivos
-   estudar os dados que eles contêm
-   verificar se os tipos de dados estão corretos
-   identificar os 10 principais bairros em termos de destinos
-   fazer gráficos: empresas de táxi e número de corridas, top 10 bairros por número de corridas em que esse bairro é destino
-   tirar conclusões com base em cada gráfico e explicar os resultados

**Passo 5. Testando hipóteses (Python)**

[/datasets/project\_sql\_result\_07.csv](https://practicum-content.s3.us-west-1.amazonaws.com/learning-materials/data-analyst-eng/moved_project_sql_result_07.csv) — o resultado da última consulta. Ele contém dados sobre viagens do Loop para o Aeroporto Internacional O'Hare. Lembre-se, estes são os valores dos campos da tabela:

-   _start\_ts_
    -   data e hora de coleta
-   _weather\_conditions_
    -   condições meteorológicas no momento em que a corrida começou
-   _duration\_seconds_
    -   duração da corrida em segundos

Teste a hipótese:

"A duração média dos passeios do Loop para o Aeroporto Internacional O'Hare muda nos sábados chuvosos."

Decida onde definir o nível de significância (alfa) por conta própria.

Explique:

-   como você formou as hipóteses nula e alternativa
-   qual critério você usou para testar a hipótese e porque

## **Como o meu projeto será avaliado?**

Aqui estão os critérios de avaliação do projeto. Leia-os cuidadosamente antes de começar a trabalhar.

Aqui está o que o revisor do projeto procurará ao avaliar seu projeto:

-   como você recupera dados do site
-   como você faz fatias de dados
-   como você agrupa dados
-   se você usa os vários métodos para combinar dados corretamente
-   como você formula hipóteses
-   quais critérios você usa para testar as hipóteses e por quê
-   a que conclusões você chega
-   se você deixa comentários a cada passo

As folhas de conclusões e resumos das lições anteriores têm tudo o que você precisa para completar o projeto.

Boa sorte!

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-00103d2b-b3d9-44aa-a4ff-fed81dee911b.containerhub.tripleten-services.com/doc/tree/69989e16-903b-41d4-ac0c-ac8b47fe5a7f.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-40-03-948Z.md
### Última modificação: 2025-05-28 19:40:04

# Feedback do Sprint 7 - TripleTen

Capítulo 8/8

Conclusão

# Compartilhe sua opinião

**Olá!**

Você acabou de concluir um sprint e enviar seu projeto. Parabéns por alcançar este marco! 🚀

Agora é o momento perfeito para dar seu feedback sobre o sprint. Sua opinião vai nos ajudar a aprimorar ainda mais o processo de aprendizagem.

Este questionário curto vai levar apenas 2 minutos. Todas as respostas vão ser confidenciais: não vamos compartilhá-las de forma que identifique você.

Agradecemos por nos ajudar a evoluir com você!

Como você avalia a experiência com os materiais didáticos, as tarefas e os projetos neste sprint?

1

2

3

4

5

Respostas: 1 – Muito insatisfatória; 5 – Muito satisfatória.

Avançar

1 — 7

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-40-05-259Z.md
### Última modificação: 2025-05-28 19:40:05

# Você é um Campeão de SQL! - TripleTen

Capítulo 8/8

Conclusão

# Você é um Campeão de SQL!

Neste ponto do curso, você já pode fazer mais do que uma análise simples e programação em uma linguagem. Agora você também pode escrever código usando SQL, o que te dá um novo nível de liberdade e uma vantagem competitiva no mercado de trabalho!

E, claro, não se esqueça de mostrar suas novas habilidades a potenciais empregadores e demonstrar sua paixão por programação e aprendizagem!

### Compartilhe sua nova medalha com o mundo

Nunca se esqueça da importância do networking, então vá em frente e compartilhe a sua medalha "Campeão de SQL" com a comunidade do LinkedIn! As pessoas em sua rede que estão considerando entrar em tecnologia também poderão ver e pedir conselhos, e suas novas habilidades chamarão a atenção dos recrutadores imediatamente.

Fique à vontade para copiar a medalha e o texto do bloco abaixo para o seu post (claro, você pode personalizar o texto como quiser)! Antes de fazer isso, não se esqueça de tornar seus repositórios no GitHub públicos. Marque [@TripleTenBrasil](https://www.linkedin.com/school/tripleten-brasil/) para que possamos compartilhar seu post.

Jonathas Martins da Rocha

Agora posso trabalhar com bancos de dados complexos usando SQL. É fantastico! #TripleTen #TripleTenBrasil

![](https://practicum-content.s3.amazonaws.com/resources/Data_Collection_and_Storage_SQL_Champ_PT_1687156896.png)

Copiar textoSalvar imagem[Criar um post no LinkdIn](https://www.linkedin.com/feed/)

Pegue um bom drink, relaxe e veja o sucesso tomar forma.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-41-51-320Z.md
### Última modificação: 2025-05-28 19:41:51

# Introdução à Análise de Negócios - TripleTen

Capítulo 1/9

Introdução à Análise de Negócios

# Introdução à Análise de Negócios

Olá, esta é a análise de negócios!

Nesta seção do curso, você irá calcular métricas financeiras e comportamentais, fazer funis de vendas, desenvolver análise de coorte, estudar economia unitária e aprender a usar tudo isso para tomar as decisões corretas.

**Você irá aprender:**

-   Como determinar a rentabilidade do negócio ou sua ausência.
-   Como construir métricas de funis de vendas e tomar decisões com base neles.
-   Como usar a análise de coorte para estudar o comportamento do consumidor e calcular o valor de vida útil (Life Time Value ou LTV, na sigla em inglês) e o período de retorno de um negócio
-   Como calcular a economia unitária (custo por venda e custo por aquisição) e tomar decisões com base nisso.
-   Como avaliar métricas comportamentais.
-   Vários tipos de métricas comportamentais e os prós e contras de cada uma.

Desta vez, você vai desenvolver as seguintes habilidades:

![](https://practicum-content.s3.amazonaws.com/resources/Analise_de_Negocio_1713355183.png)

![](https://practicum-content.s3.amazonaws.com/resources/Narrativa_de_Dados_1713355199.png)

Você irá submeter outro projeto independente ao fim da seção. Desta vez, você irá analisar as métricas de negócio do Y.Afisha, um aplicativo que ajuda os usuários a descobrir eventos como sessões de cinema, exibições, shows etc., e a comprar ingressos. Seu objetivo será ajudar os especialistas de marketing do Y.Afisha a fazer investimentos efetivos em marketing. Clique [aqui](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_7/moved_DS_7_sprint_Projeto_do_Curso.pdf) para ver a descrição do projeto e o que ele irá envolver.

### Quanto tempo isso vai levar?

Nesta seção você irá aprender novos conceitos ligados aos negócios que vão fazer a ponte entre as habilidades que você está aprendendo e os problemas do mundo real. Completar este material pode levar de 30 a 50 horas. Se você sentir que está ficando para trás, sinta-se à vontade para entrar em contato com nossa Equipe de Orientação para mais informações. Temos o compromisso de ajudá-lo em cada passo do curso.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-41-52-664Z.md
### Última modificação: 2025-05-28 19:41:53

# Introdução - TripleTen

Capítulo 2/9

Métricas e Funis

# Introdução

Os analistas calculam métricas para ajudar as empresas a crescer e ganhar mais dinheiro. Neste capítulo, você aprenderá sobre as métricas básicas de negócio.

### O que você irá aprender:

-   Como calcular o lucro bruto, operacional e líquido
-   Como calcular o período de retorno e retorno dos investimentos em marketing
-   Como traçar funis de marketing e produtos em Python

### Quanto tempo vai demorar:

10 aulas, aproximadamente 10-20 minutos cada

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-41-53-988Z.md
### Última modificação: 2025-05-28 19:41:54

# Receita, Renda e Retorno do Investimento - TripleTen

Capítulo 2/9

Métricas e Funis

# Receita, Renda e Retorno do Investimento

O principal objetivo de qualquer negócio é fazer dinheiro para seus proprietários. Para garantir que haja dinheiro suficiente no banco, gerentes e analistas avaliam, estudam e melhoram suas **métricas de negócios**, como lucro e margem bruta, operacional e líquida.

No final das contas, a filosofia de todo negócio é "compre barato, venda caro". As lojas compram e vendem mercadorias. Agências de consultoria compram tempo e esforço de seus funcionários e vendem soluções para diversas empresas. A mídia compra a atenção do público e a vende para os anunciantes. Um negócio saudável sempre tem **despesas** e **receita**.

A importância relativa das métricas varia um pouco dependendo do modelo de negócios e do estágio de desenvolvimento da empresa. As seguintes métricas são fundamentais para o modelo mais genérico (um negócio lucrativo já estabelecido):

## Lucro bruto. Volume de negócios, custo primário e margem bruta

Digamos que a empresa Basil and Sons venda fones de ouvido sem fio. Em 2018, o **faturamento** da empresa foi de US$ 1.000.000. O **custo primário** de seus produtos era de US$ 300.000. Ela pagou US$ 150.000 a seus funcionários e gastou US$ 100.000 em aluguel e serviços para seu espaço de escritório. Seis por cento de seu faturamento foi para impostos.

Vejamos cada termo por vez.

"O **faturamento** da empresa foi de US$ 1.000.000"

**Faturamento**, ou receita bruta, é a quantia em dinheiro que os clientes pagaram à empresa. Por exemplo, o preço de um único produto é $ 10. A empresa vendeu 10 unidades. A receita bruta foi, portanto, $ 10 x 10 = $ 100.

"O **custo primário** de seus produtos foi de US$ 300.000"

O **custo primário** é o dinheiro que a empresa pagou para adquirir mercadorias. Os serviços também têm um custo primário; por exemplo, as despesas com combustível estão incluídas no custo primário de uma corrida de táxi.

Se você souber o custo primário e o faturamento, poderá calcular a primeira métrica de negócios, **lucro bruto**. **`Lucro bruto = faturamento - custo primário.`** A empresa comprou barato, mas vendeu caro - e voilà, temos lucro bruto. O lucro bruto é o primeiro indicador de um negócio saudável, e calculá-lo é rápido e fácil. Se os proprietários de uma empresa souberem o lucro bruto, poderão descobrir se venderam mercadorias suficientes para cobrir salários, aluguel e impostos.

O lucro bruto da Basil and Sons pode ser calculado da seguinte forma: `1 000 000 - 300 000 = 700 000`

Geralmente é um mau sinal se o lucro bruto for negativo. Isso significa que a empresa está vendendo mais barato do que compra. Precisa urgentemente reconsiderar seus preços ou sua gama de bens e serviços.

Pode haver exceções, é claro. Por exemplo, as butiques de moda costumam vender itens das coleções do ano passado com prejuízo. Dois fatores tornam isso possível. Primeiro, as empresas preparam-se e fazem orçamento para isso. Eles podem vender 1.000 itens com lucro considerável e os 10 restantes com uma pequena perda. A receita total será aceitável. Além disso, a venda da coleção anterior abre espaço para novos produtos, que podem render mais dinheiro.

Vamos considerar outro exemplo. Às vezes, as empresas vendem mercadorias a preços inferiores ao custo primário para envolver os clientes na compra de serviços e suporte. Por exemplo, o Xbox era muito mais barato que o Sony Playstation\*, o campeão de mercado na época, o que ajudou a Microsoft a conquistar uma fatia significativa do mercado. Mas o lucro da Microsoft veio com a venda de jogos, não do console em si.

A proporção entre lucro bruto e receita é chamada de **margem de lucro bruta** (ou **margem bruta**).

**`Margem bruta = lucro bruto/receita.`**

A margem de lucro bruto da Basil and Sons pode ser calculada da seguinte forma: `700 000 / 1 000 000 = 70%`. Assim, apenas 70% da receita total permanece dentro da empresa. A margem deve ser calculada para vários bens e categorias. Isso ajuda a determinar quais produtos a empresa deve se concentrar em vender.

Pergunta

Em 2015, a Robots R Us faturou US$ 345.023 vendendo mata-moscas automatizados. Seu custo primário foi de US$ 147.604, os salários totalizaram US$ 53.210 e o aluguel chegou a US$ 32.945. A Robots R Us paga 6% do seu faturamento em impostos. Calcule a margem bruta da empresa.

42%

26%

32%

74%

57%

Isso mesmo! Margem bruta = (receita - custo primário) / receita. (345,023 - 147,604) / 345,023 = 57.2%.

Nenhuma das alternativas anteriores

Seu entendimento sobre o material é impressionante!

Pergunta

Em 2018, a receita da Crazy Sheep com as vendas de meias de lã totalizou US$ 117.132. O custo primário foi de US$ 152.189, os salários totalizaram US$ 35.103 e o aluguel chegou a US$ 54.979. A empresa paga 6% do seu faturamento em impostos. Calcule o lucro bruto da empresa.

$35,057

$-35,057

Correto! 117,132 - 152,189 = -35,057. É o que acontece quando uma empresa vende um bem ou serviço por menos do que paga. Negócio arriscado!

$-125,139

$-132,167

Nenhuma das alternativas anteriores

Fantástico!

## Despesas operacionais e lucro operacional

Infelizmente, o lucro bruto não é o valor que os donos do negócio ganham. Como você já deve ter notado, os salários dos funcionários e o aluguel do escritório não estão incluídos no custo primário. Essas despesas compõem uma categoria específica denominada **custos operacionais**. Essas são todas as despesas que sustentam as atividades principais de uma empresa, incluindo salários, aluguel, eletricidade e conexões de internet.

Basil and Sons aluga seu escritório por $ 100.000 e paga $ 100.000 em salários. Assim, suas despesas operacionais totalizam $ 200.000 por mês. Se você subtrair os custos operacionais do lucro bruto, obterá **lucro operacional**. **`Lucro operacional = lucro bruto - custos operacionais`.** O lucro operacional, também conhecido como **receita operacional** ou **Lucro antes de juros e imposto de renda (EBIT, na sigla em inglês)**, é a quantidade de dinheiro que uma empresa tem após pagar os custos operacionais e custos primários, mas antes de pagar os impostos.

Para Basil and Sons: `700.000 - 200.000 = 500.000`.

O lucro operacional fornece uma imagem do desempenho geral do negócio, indicando quanto uma empresa ganha com suas atividades principais. Como regra, o lucro operacional se correlaciona com o lucro líquido: à medida que o primeiro aumenta, o segundo também aumenta. Calcular o lucro operacional é mais fácil e menos demorado.

Se o lucro operacional for negativo, há um prejuízo operacional. Isso sinaliza que os donos de uma empresa ainda não estão ganhando dinheiro com ela. Mas, assim como o lucro bruto negativo, um prejuízo operacional não significa que o jogo acabou. De tempos em tempos, as empresas **planejam a falta de lucratividade**, para que possam investir toda a sua receita em rápido desenvolvimento e crescimento. A Amazon passou muitos anos tendo prejuízos, mas seu fundador Jeff Bezos se tornou o homem mais rico do mundo.

Se você dividir o lucro operacional pela receita, obterá a **margem operacional**. Essa é a parcela da receita que permanece na empresa após o pagamento dos custos primários, salários, aluguel, marketing e outras despesas da atividade principal. Muitas vezes, os investidores estão particularmente interessados na margem operacional, pois ela permite a comparação de diferentes negócios.

**`Margem operacional = lucro operacional / receita`**

Por exemplo, a margem operacional da Basil and Sons é `500 000 / 1 000 000 = 50%`

Pergunta

Em 2016, a Robots R Us faturou US$ 115.417 vendendo mata-moscas automatizados. Seu custo principal foi de US$ 343.031, os salários totalizaram US$ 59.107 e o aluguel chegou a US$ 61.170. A empresa paga 6% do seu faturamento em impostos. Calcule seus custos operacionais.

$69,095

$120,277

Sim. Neste caso, os custos operacionais são provenientes de aluguéis e salários.

$464,308

$61,170

Nenhuma das alternativas anteriores

Muito bem!

Pergunta

Em 2015, a Keybucks faturou US$ 323.864 vendendo chaves. O custo primário das mercadorias foi de $ 156.731, os salários totalizaram $ 50.632 e o aluguel chegou a $ 54.595. A empresa paga 6% do seu faturamento em impostos. Calcule seu lucro operacional.

$106,009

$61,906

Isso mesmo!

$167,133

$42,474

Nenhuma das alternativas anteriores

Trabalho maravilhoso!

## Lucro líquido

Nem a receita nem o lucro operacional mostram a quantidade de dinheiro que a empresa realmente ganha. Todas as despesas da empresa (custos primários, despesas operacionais e responsabilidades com o estado e credores) são descritas por uma métrica diferente: **lucro líquido**. O lucro líquido é a quantidade de dinheiro que os empresários podem tomar para si ou reinvestir para desenvolver a empresa.

**`Lucro líquido = lucro operacional - impostos e empréstimos`**

Basil and Sons não tem empréstimos. Assim, os impostos são sua única responsabilidade. Ela paga 6% de seu faturamento (US$ 1.000.000) em impostos: **1.000.000** × **0,06 = US$ 60.000.**

O lucro líquido foi, portanto, de $ 500.000 − $ 60.000 = $ 440.000.

Infelizmente, o lucro líquido pode ser calculado apenas no final do ano, quando todas as obrigações fiscais e de crédito forem determinadas. É por isso que o lucro bruto e operacional são usados para a gestão cotidiana, enquanto o lucro líquido é deixado para a assembleia anual de acionistas.

Um lucro líquido negativo é chamado de **prejuízo líquido**. Assim como outros tipos de prejuízo, isso sinaliza que a empresa não conseguiu ganhar dinheiro.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.2.2PT_1655467519.png)

Pergunta

Em 2011, a Robots R Us faturou US$ 261.812 vendendo mata-moscas automatizados. O custo primário das mercadorias foi de $ 221.643, os salários totalizaram $ 69.021 e o aluguel chegou a $ 55.519. A empresa paga 6% do seu faturamento em impostos. Calcula seu lucro líquido.

$40,169

$-84,371

$-100,080

Certo, a empresa ainda é deficitária. Talvez o dono esteja lendo uma biografia de Jeff Bezos.

Nenhuma das alternativas anteriores

Fantástico!

Pergunta

Em 2011, a Concrete Stuff faturou US$ 604.000 vendendo decorações de Natal feitas de concreto. O custo primário das mercadorias foi de $ 34.500, os salários totalizaram $ 63.201 e o aluguel chegou a $ 32.945. A Concrete Stuff paga 6% do seu faturamento em impostos. Também recebeu um empréstimo naquele ano, resultando em despesas de US$ 230.550. Calcule o lucro líquido da empresa.

$569,500

$473,345

$206,564

Isso mesmo!

$96,146

Nenhuma das alternativas anteriores

Seu entendimento sobre o material é impressionante!

## Retorno do investimento

Até agora, falamos sobre lucro apenas em termos de um determinado período de tempo. Compramos um produto, vendemos e pagamos aluguel, salários, impostos e empréstimos. O que fica na balança é o lucro. Mas ainda não terminamos.

Para lançar um negócio, as pessoas investem dinheiro nele. Por exemplo, para lançar uma loja online, você precisa projetá-la, codificá-la, configurar o sistema de contabilidade de produtos, alugar um depósito e recrutar funcionários. Essas coisas custam dinheiro e as vendas não começam imediatamente.

Assim, em seus estágios iniciais, um negócio entra no vermelho, mas quando se torna lucrativo, gradualmente compensa suas perdas.

Os investidores querem saber quando seus investimentos serão recompensados. Para eles, a métrica mais importante é o **ROI** (retorno do investimento).

**`ROI = (lucro líquido - investimentos) / investimentos`**

Digamos que o investidor John Smith investiu US$ 1 milhão na construção de uma fábrica de patins. A construção durou dois anos e a inicialização e o comissionamento levaram mais um ano. Depois de um ano, a fábrica teve um lucro líquido de $ 200.000.

O ROI atual do investimento de John Smith é `(200.000 - 1 milhão) / 1 milhão =-80%`. Isso significa que ainda falta para ele recuperar 80% de seu investimento.

Pergunta

Steve é um empresário. Ele vendeu seu apartamento e investiu US$ 10 milhões no lançamento de um estúdio de web design. Em seu primeiro ano, o estúdio teve um lucro líquido de US$ 2,5 milhões. Qual é o ROI dele?

25%

\-25%

\-50%

\-75%

75%

Nenhuma das alternativas anteriores

Você conseguiu!

## Retorno do investimento em marketing

O retorno sobre o investimento pode ser calculado não apenas para um negócio como um todo, mas também para seus componentes. Veja o marketing, por exemplo.

Uma empresa investe dinheiro em anúncios no [Y.Direct](https://direct.yandex.com/). Os usuários clicam em um anúncio e são redirecionados para um site onde compram algo. O anúncio é eficaz se os usuários gerarem mais dinheiro do que a empresa gasta para atraí-los.

**`ROI da campanha publicitária = (receita − despesas) / despesas.`**

Para distinguir entre o retorno do negócio e o retorno de uma campanha publicitária, o **ROMI** (retorno do investimento em marketing) é calculado em vez do ROI.

Por exemplo, os desenvolvedores de jogos para dispositivos móveis gastaram US$ 100.000 em publicidade e atraíram 2.000 usuários. Os usuários, por sua vez, gastaram US$ 150.000 em compras no jogo.

**`ROMI = (150.000 − 100.000) / 100.000 = 50%`**. O negócio obteve um retorno de 50% sobre esses investimentos.

Na prática, o ROMI é calculado de forma mais simples:

**`ROMI da campanha publicitária = Lucro bruto/despesas da campanha.`**

Se você usar esta fórmula, o retorno (quando os investimentos forem 100% recuperados) virá quando **`ROMI = 100%`**, em vez de **`ROMI = 0%`**.

Se calcularmos o ROMI da nossa campanha publicitária desta forma, obteremos o seguinte resultado: **`150.000 / 100.000 = 150%.`** As despesas foram 100% recuperadas e geraram um lucro de 50%.

Ambas as fórmulas são relevantes e usadas com frequência na vida real. Ao discutir retornos de marketing, não se esqueça de especificar qual fórmula sua empresa usa para calcular essa métrica.

Pergunta

iTree, que vende decorações de Natal de luxo online, coloca anúncios no [Y.Direct](https://direct.yandex.com/) no pedido "decorações de Natal compre luxo". A empresa gastou US$ 2.500 na campanha. O anúncio recebeu 200 cliques e resultou em cinco compras de US$ 400 cada. A receita total foi de US$ 2.000. Excluindo o custo primário, o lucro bruto foi de US$ 1.000. Calcule o ROMI da campanha publicitária usando a fórmula simplificada.

8%

80%

4%

40%

16%

Nenhuma das alternativas anteriores

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-56-55-511Z.md
### Última modificação: 2025-05-28 19:56:56

# Conversão - TripleTen

Capítulo 2/9

Métricas e Funis

# Conversão

O processo de vendas é essencialmente o processo de transformar um visitante em um comprador.

Tudo começa quando alguém começa a interagir com uma empresa. Pode ser uma visita a um site ou loja, um telefonema ou a instalação de um aplicativo móvel. O importante é que a pessoa se interessou pelos bens ou serviços da empresa.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.2.3PT_1655467538.png)

Em seguida, o usuário adiciona um produto à sua cesta, completa o nível do tutorial do jogo ou conversa com um vendedor. Embora esses processos sejam diferentes, eles compartilham o mesmo objetivo principal: transformar um visitante em um comprador. Quanto mais usuários forem convertidos, melhores serão as coisas. É por isso que a conversão é uma das métricas mais importantes.

Você aprendeu sobre conversão quando estudou manipulação de dados. Lembre-se, a taxa de conversão é a parcela de pessoas que alteram seu status: por exemplo, a parcela de visitantes da loja que se tornam compradores reais.

Se 1.000 pessoas visitaram uma loja e apenas 10 delas compraram algo, a taxa de conversão é 10/1.000 = 0,01, ou 1%.

Pergunta

Um aplicativo móvel foi baixado por 22.755 usuários, 555 dos quais compraram a versão completa. Encontre a taxa de conversão do aplicativo.

1.342%

2.439%

3.112%

0.424%

2.001%

Nenhuma das alternativas anteriores

Excelente!

Pergunta

A taxa de conversão de um site é de 2,154% e o número de visitantes é de 21.824. Encontre o número de compradores.

611

235

470

0

329

Nenhuma das alternativas anteriores

Excelente!

Pergunta

A taxa de conversão de um jogo online é de 1,821% e o número de compradores é de 712. Encontre o número de visitantes.

50,842

0

39,099

19,554

27,376

Nenhuma das alternativas anteriores

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-56-56-842Z.md
### Última modificação: 2025-05-28 19:56:57

# Funis - TripleTen

Capítulo 2/9

Métricas e Funis

# Funis

Você está analisando o desempenho de uma loja online. Você observou que 100 de 10.000 visitantes compraram alguma coisa. A conversão é, portanto, de 1%. Mas, infelizmente, essa métrica não ajuda a responder à pergunta: "O que mais precisa ser feito para aumentar as vendas?" Para descobrir isso, especialistas e analistas de marketing traçam funis.

Um **funil** ilustra:

1) o caminho que um usuário percorre para comprar um produto

2) a parcela de usuários que passam para o próximo estágio

Por exemplo, um usuário realizou as seguintes etapas em uma loja online:

1.  Foi para a página principal
2.  Navegou para a página de um produto
3.  Adicionou o produto ao carrinho
4.  Navegou para a página de checkout
5.  Pagou pelo pedido

Para traçar um funil, primeiro você precisa encontrar o número de pessoas que chegam a cada estágio. 10.000 usuários foram para a página principal, 8.000 deles navegaram para a página do produto, 3.000 adicionaram o produto aos seus carrinhos de compras e apenas 1.000 chegaram à página de checkout. Finalmente, 100 pedidos foram feitos.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.2.4PT_1655467617.png)

O gráfico que mostra o número de usuários em cada estágio se assemelha a um funil:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.2.4.2PT_1655467628.png)

Saber o número de usuários em cada estágio permite determinar a parcela daqueles que atingem um determinado estágio. Você também pode determinar a parcela de usuários que concluiu cada etapa subsequente:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.2.4.3PT_1655467639.png)

A conversão total é a proporção de usuários em um estágio específico para o número de usuários no primeiro estágio. A conversão para o próximo estágio é a proporção de usuários em um estágio específico para o número de usuários no estágio anterior.

Os dados nesta pequena tabela nos permitem tirar muitas conclusões e hipóteses. Por exemplo, apenas 1 em cada 10 usuários que começam a fazer um pedido realmente terminam o processo. Talvez haja algo errado com a página de checkout?

Ou: 3.000 usuários adicionaram o produto aos seus carrinhos, mas nunca concluíram as compras. Devemos enviar e-mails lembrando que eles têm produtos em seus carrinhos?

A análise de funil ajuda você a fazer as perguntas certas, formular hipóteses, testá-las e acompanhar as alterações. Se você decidir enviar e-mails sobre carrinhos de compras abandonados e eles se mostrarem eficazes, a taxa de conversão nesse estágio aumentará.

Pergunta

Você é um analista de um jogo para celular. O jogo é gratuito, mas os usuários podem comprar armas e armaduras mais poderosas. Escolha a opção que melhor descreve as etapas do funil:

Tutorial completo — Criou um personagem — Instalou — Pagou

Instalou — Criou um personagem — Tutorial concluído — Pagou

Todas as opções acima

Nenhuma das alternativas anteriores

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-56-58-159Z.md
### Última modificação: 2025-05-28 19:56:58

# O Funil de Marketing: Impressões, Cliques, CTR e CR - TripleTen

Teoria

# O Funil de Marketing: Impressões, Cliques, CTR e CR

Especialistas em marketing pediram que você avalie o desempenho deles no trabalho.

Eles colocam banners de anúncios na Internet. Os clientes em potencial às vezes clicam neles e são redirecionados para uma página especial chamada **página de destino**, que tem um formulário de registro que os usuários preenchem se estiverem interessados no produto.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.2.5PT_1655467662.png)

O objetivo é obter o maior número possível de usuários registrados. Vamos traçar um funil mostrando a atividade do usuário para a página de destino. Precisaremos saber:

-   Quantas vezes o banner foi exibido (impressões)
-   Quantos usuários clicaram nele
-   Quantos deles navegaram para a página de destino
-   Quantos registros ocorreram

Dados sobre impressões e cliques podem ser recuperados de sistemas de publicidade. Você pode obter dados de navegação para formulários de páginas de destino e registros usando o Y.Metrica ou seus próprios sistemas de análise.

A **taxa de cliques (CTR)** é a proporção de cliques por impressões apresentadas. É dado em porcentagem:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.2.5.2PT_1655467674.png)

A **taxa de conversão (CR)** é a proporção de registros por cliques. Também é uma porcentagem:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.2.5.3PT_1655467692.png)

O Funil de Marketing: Impressões, Cliques, CTR e CR

Tarefa2 / 2

1.

Baixe os dados de despesas do arquivo`/datasets/ad_data.csv` e salve na variável `ad_data`. Obtenha os dados dos registros do arquivo `/datasets/site_data.csv` e salve-o na variável `site_data`.

Una os dados da tabela em um DataFrame pela coluna `date`. Salve o resultado como `funil` e imprima as primeiras cinco linhas do conjunto de dados resultante.

2.

Crie os funis para CTR e CR. Salve os resultados nas colunas `funnel['ctr, %']` e `funnel['cr, %']` respectivamente. Imprima as primeiras cinco linhas do DataFrame.

99

1

2

3

4

5

6

7

8

9

10

11

12

import pandas as pd

  

ad\_data \= pd.read\_csv('/datasets/ad\_data.csv' )

site\_data \= pd.read\_csv('/datasets/site\_data.csv')

funnel \= pd.merge(ad\_data, site\_data, on\='date')

  

funnel\['ctr, %'\] \= funnel\['clicks'\]/funnel\['impressions'\]\* 100

funnel\['cr, %'\] \= funnel\['registrations'\]/funnel\['clicks'\] \* 100

  

  

  

print(funnel.head(5))

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-00-698Z.md
### Última modificação: 2025-05-28 19:57:01

# O Funil de Marketing: Agrupamento por Semanas e Meses - TripleTen

Teoria

# O Funil de Marketing: Agrupamento por Semanas e Meses

Quando os dados são ruidosos, é difícil encontrar padrões. Então, às vezes, você terá que consolidá-los, por exemplo, agrupando-os por semana. Ao agrupar os dados em uma menor granularidade (por exemplo, de horas para semanas), as flutuações se cancelam e os dados se tornam menos ruidosos. Vamos descobrir como.

```
import pandas as pd

df = pd.read_csv('datasets/funnel_cr_example.csv')
print(df)
```

```
         date  clicks  regs  cr, %
0  2019-01-01    2000    36   1.80
1  2019-01-02   10000    26   0.26
2  2019-01-03    1000    15   1.50
3  2019-01-04    4000     4   0.10
4  2019-01-05    5000    24   0.48
5  2019-01-06    2000    66   3.30
6  2019-01-07    1000     2   0.20
7  2019-01-08    1500     2   0.13
```

Aqui está o primeiro algoritmo que vem à mente:

1.  Adicione uma coluna `week` ao DataFrame
2.  Agrupe o DataFrame por esta coluna
3.  Encontre a taxa de conversão semanal média

```
# isto está errado!

df['week'] = df['date'].dt.week
print(df.groupby('week')['cr, %'].mean())
```

```
week
36    1.091429
Name: cr, %, dtype: float64
```

Essa abordagem está errada. Bem, os dois primeiros passos estão corretos, mas o terceiro não está. Veja como deve ser feito:

1.  Adicione uma coluna `week` ao DataFrame
2.  Agrupe o DataFrame por esta coluna
3.  Some o número de impressões, cliques e registros para cada semana
4.  Recalcule a taxa de conversão semanal média

Se apenas calcularmos a média dos valores médios, perdemos uma informação muito importante: o número de cliques. Estaríamos então dando peso igual à taxa de conversão de 2 de janeiro, quando o site teve 10.000 visitantes, e à taxa de 7 de janeiro, quando havia apenas 1.000.

```
# correto

df['week'] = df['date'].dt.week
df_ = df.groupby('week')[['regs', 'clicks']].sum()
print(df_['regs'] / df_['clicks'] * 100)
```

```
week
36    0.692
dtype: float64
```

Essa abordagem é indicada para qualquer métrica relativa, quando um número é dividido por outro. Primeiro você encontra a soma, e só então você divide.

O Funil de Marketing: Agrupamento por Semanas e Meses

Tarefa2 / 2

1.

Encontre os valores semanais de CTR e CR.

Baixe os dados sobre as taxas de conversão diárias do arquivo `/datasets/funnel_daily.csv` e salve-os na variável `funnel_daily`. Converta os valores da coluna `funnel_daily['date']` para o formato `datetime`. Adicione a coluna `funnel_daily['week']` e procure o número da semana para cada data.

Calcule o CTR e o CR semanais. Salve o resultado como `funnel_weekly`.

2.

Esta tarefa é muito parecida com a anterior. Trace funis diários, semanais e mensais.

Leia os dados de despesas com publicidade (`/datasets/ad_data_2.csv`) e salve-os na variável `ad_data`. Salve os dados dos registros (`/datasets/site_data_2.csv`) como `site_data`.

Use o método `merge()` para unir `ad_data` e `site_data` na coluna `'date'`. Salve o DataFrame resultante com funis diários como `funnel_daily`. Calcule a conversão de `impressions` para `clicks`, salvando-a na coluna `ctr, %`, e de `clicks` para `registrations`, salvando-a na coluna `cr, %`. Em `funnel_daily` crie as colunas `'week'` e `'month'`. Recupere os números de ordem de semanas e meses com os métodos `dt.week` e `dt.month`, respectivamente. Armazene os resultados nas colunas mencionadas acima.

Salve o funil semanal em `funnel_weekly`. Agrupe os dados `funnel_daily` pela coluna `'week'` usando o método `groupby()`. Some os dados das colunas `['impressions', 'clicks', 'registrations']` dentro do agrupamento.

Faça um mensal do funil usando a mesma abordagem. Salve-o como `funnel_monthly`.

Calcule CTR e CR para cada um dos três funis. Imprima o funil mensal.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

import pandas as pd

  

ad\_data \= pd.read\_csv('/datasets/ad\_data\_2.csv')

site\_data \= pd.read\_csv('/datasets/site\_data\_2.csv')

  

funnel\_daily \= pd.merge(ad\_data,site\_data, on\='date')

funnel\_daily\['date'\] \= pd.to\_datetime(funnel\_daily\['date'\])

funnel\_daily\['ctr, %'\] \= funnel\_daily\['clicks'\] / funnel\_daily\['impressions'\]\* 100

funnel\_daily\['cr, %'\] \= funnel\_daily\['registrations'\] / funnel\_daily\['clicks'\]\* 100

  

funnel\_daily\['week'\] \= funnel\_daily\['date'\].dt.week

funnel\_daily\['month'\] \= funnel\_daily\['date'\].dt.month

  

funnel\_weekly \= funnel\_daily.groupby('week')\[\['impressions', 'clicks', 'registrations'\]\].sum()

funnel\_monthly \= funnel\_daily.groupby('month')\[\['impressions', 'clicks', 'registrations'\]\].sum()

  

funnel\_weekly\['ctr, %'\] \= funnel\_weekly\['clicks'\] / funnel\_weekly\['impressions'\]\* 100

funnel\_weekly\['cr, %'\] \= funnel\_weekly\['registrations'\] / funnel\_weekly\['clicks'\]\* 100

  

funnel\_monthly\['ctr, %'\] \= funnel\_monthly\['clicks'\] / funnel\_monthly\['impressions'\]\* 100

funnel\_monthly\['cr, %'\] \= funnel\_monthly\['registrations'\] / funnel\_monthly\['clicks'\]\* 100

  

print(funnel\_monthly)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-02-057Z.md
### Última modificação: 2025-05-28 19:57:02

# Funis de Produtos Simples - TripleTen

Teoria

# Funis de Produtos Simples

Os funis de produtos mostram como os usuários interagem com seus sites e aplicativos. Eles são semelhantes aos funis de marketing, pois os usuários passam de um estágio para outro e você analisa as conversões em cada estágio.

São as informações usadas para fazer os funis que os diferenciam. Como regra, os dados de marketing são agregados. Eles apresentam o número de impressões, cliques e registros por dia. Eles se parecem com o resultado de uma pesquisa.

Mas os dados de produto geralmente são "brutos". Cada linha da tabela é um único evento, por exemplo "usuário 42 acessou o site." Esses dados se parecem mais com um formulário de pesquisa individual.

Considere uma loja online em busca de novos pontos de crescimento nas vendas. O funil consiste em quatro etapas:

1.  Clientes em potencial visitam o site
2.  Eles adicionam produtos aos carrinhos
3.  Eles fazem pedidos
4.  Eles pagam

Você recebeu logs de eventos do servidor. Sua tarefa é fazer um funil de produto.

Vamos carregar os dados e examiná-los:

```
events = pd.read_csv('data/metrics/pfunnel_demo.csv')
print(events.head())
```

```
    event_name           event_time       uid
0   pageview  2019-02-27 10:16:38  99702298
1   pageview  2019-01-24 07:20:10  67118564
2   pageview  2019-05-16 06:23:10  87503951
3   pageview  2019-07-23 08:29:40  64675633
4   pageview  2019-03-11 21:47:56  73978848
```

Os logs podem diferir de sistema para sistema, mas todos compartilham três colunas obrigatórias: o nome de um evento (`event_name`), a data e hora em que ocorreu (`event_time`) e o identificador do usuário para quem ocorreu ( `uid`).

A maneira mais fácil de criar um funil é calcular quantas vezes cada um dos eventos ocorreu. Vamos agrupar os dados em um DataFrame no campo `event_name` e encontrar o número de linhas:

```
events_count = events.groupby('event_name').agg({'uid': 'count'})
print(events_count)
```

```
event_name       uid
pageview     1084       
add_to_cart  1233
checkout      714
payment       253
```

O evento `pageview` ocorreu 1.084 vezes e `add_to_cart` 1.233 vezes. Mas o usuário primeiro tinha que acessar o site, depois adicionar o produto ao carrinho. Isso indica que o número de eventos do carrinho não pode ser maior que o número de impressões.

Então o que aconteceu? A coisa é que um evento não é igual a um usuário. Uma pessoa pode adicionar vários itens ao carrinho.

Como no funil precisamos determinar uma proporção de usuários, não de eventos, precisamos refazer nosso relatório. Para encontrar o número de usuários unívocos em cada grupo, chamaremos **nunique** (número de elementos unívocos) ao agregar. Este método calcula o número de elementos _unívocos_ em um conjunto. Vamos classificar os resultados em ordem decrescente:

```
users_count = events.groupby('event_name').agg({'uid': 'nunique'})
print(users_count.sort_values(by = 'uid', ascending=False))
```

```
pageview     1084
add_to_cart   802
checkout      272
payment        97
```

Agora vemos claramente quantas pessoas chegaram a cada uma das etapas do funil: 80% dos usuários que acessaram o site adicionaram um produto ao carrinho. No entanto, dos 272 usuários que começaram a fazer pedidos, apenas 97 acabaram pagando por eles. Na verdade, isso não é tão raro, mas talvez possamos melhorar a conversão neste estágio.

Funis de Produtos Simples

Tarefa2 / 2

1.

Analise o funil de produtos de um serviço online que ajuda as empresas a enviar seus relatórios anuais. O funil tem três etapas: acessar o site (`pageview`), enviar uma mensagem para um funcionário do serviço (`chat_message`) e pagar (`success`).

Salve os logs do arquivo `/datasets/funnel_prod_events.csv` na variável `events`. Agrupe os dados em `events` pela coluna `event_name`, agregue (`agg({'uid': 'count'}`) e encontre o número de linhas em cada grupo.

Salve o resultado como `events_count` e imprima esta variável.

2.

Agrupe os dados em `events` pelo campo `event_name` e encontre o número de usuários únicos em cada grupo. Classifique os resultados em ordem decrescente.

Salve o resultado final como `users_count` e imprima esta variável.

99

1

2

3

4

5

6

7

8

9

10

import pandas as pd

  

events \= pd.read\_csv('/datasets/funnel\_prod\_events.csv')

users\_count \= (

events.groupby('event\_name')

.agg({'uid': \['nunique'\]})

.sort\_values(by\=('uid', 'nunique'), ascending\=False)

)

print(users\_count)

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-03-396Z.md
### Última modificação: 2025-05-28 19:57:03

# Funis de Produtos com Sequência de Eventos - TripleTen

Teoria

# Funis de Produtos com Sequência de Eventos

Se o seu objetivo é descobrir quantos usuários atingiram um determinado estágio do funil, o método da lição anterior será suficiente.

Mas às vezes os analistas precisam estudar detalhes da transição de um estágio para outro. Por exemplo, eles querem descobrir a proporção de usuários que acessam o site, usam a função de chat e só então se registram.

Para fazer isso, para cada usuário, eles primeiro precisam encontrar a hora exata em que uma determinada etapa foi realizada pela primeira vez. Então eles encontram aqueles que deram passos na ordem desejada.

Vamos considerar um exemplo. Para cada usuário, encontre a hora da primeira ocorrência de cada evento. Usaremos o método `pivot_table()`:

```
users = events.pivot_table(
    index='uid', 
    columns='event_name', 
    values='event_time',
    aggfunc='min')
```

```
uid        event_name    add_to_cart    checkout    pageview              payment
                
2554      2019-04-05  05:52:14      NaN         2019-04-05 05:52:32    NaN
14878      2019-05-05  16:09:51      NaN         2019-05-05 16:06:22    NaN
125581    2019-02-23  14:24:25      NaN         2019-02-23 14:25:09    NaN
129688    2019-03-10  03:51:19      NaN         2019-03-10 03:49:36    NaN
445788    2019-07-10  01:45:17      NaN         2019-07-10 01:42:28    NaN
```

Cada linha da tabela apresenta um usuário. As colunas contêm a hora em que cada um dos eventos ocorreu pela primeira vez. Se o evento não ocorreu, seu valor de célula é `NaN`.

Por exemplo, o usuário `129688` acessou o site pela primeira vez (o evento `pageview`) em 10 de março às 3h49. Dois minutos depois, às 3:51, eles adicionaram um produto ao carrinho pela primeira vez (`add_to_cart`).

Mas os dados do usuário `2554` parecem estranhos: eles acessaram o site pela primeira vez em 5 de abril às 5h52:32, mas adicionaram um produto ao carrinho às 5h52:14, 18 segundos antes do primeiro evento . Esses bugs podem ser encontrados em qualquer sistema analítico. Você deve sempre reportá-los aos seus colegas.

Vamos descobrir quantos usuários passaram de um estágio para outro. Encontraremos o número daqueles que acessaram o site. Calcule o número de linhas na tabela `users` para as quais a coluna `pageview` contém valores não vazios.

```
n_pageview = users[~users['pageview'].isna()].shape[0]
print('Visitantes:', n_pageview)
```

```
Visitantes: 1084
```

Agora vamos encontrar o número de usuários que acessaram o site e adicionaram algo aos seus carrinhos. Esses usuários têm 1) um horário para sua primeira visita e 2) um horário para sua primeira adição ao carrinho. Este último deve ser maior que o primeiro.

```
n_add_to_cart = users[~users['pageview'].isna() & (users['add_to_cart'] > users['pageview'])].shape[0]
print('Adicionaram um produto ao carrinho:', n_add_to_cart)
```

```
Adicionaram um produto ao carrinho: 637
```

Adicionaremos os estágios restantes usando a mesma abordagem para obter nosso funil. Para tornar o código legível, salvaremos as condições de cada estágio como variáveis. Observe que novas condições são adicionadas ao estágio anterior:

```
step_1 = ~users['pageview'].isna()
step_2 = step_1 & (users['add_to_cart'] > users['pageview'])
step_3 = step_2 & (users['checkout'] > users['add_to_cart'])
step_4 = step_3 & (users['payment'] > users['checkout'])

n_pageview = users[step_1].shape[0]
n_add_to_cart = users[step_2].shape[0]
n_checkout = users[step_3].shape[0]
n_payment = users[step_4].shape[0]

print('Visitantes:', n_pageview)
print('Adicionaram um produto ao carrinho:', n_add_to_cart)
print('Iniciaram o check-out:', n_checkout)
print('Pagaram:', n_payment)
```

```
Visitantes: 1084
Adicionaram um produto ao carrinho: 637
Iniciaram o check-out: 191
Pagaram: 63
```

Compare o funil resultante, em que a sequência de eventos é levada em consideração, com o funil simples. Lembre-se, ficou assim:

```
              uid
event_name       
pageview     1084
add_to_cart   802
checkout      272
payment        97
```

O número de usuários no primeiro estágio, é claro, coincide, mas os estágios seguintes diferem. Isso significa que nem todos os usuários seguem a rota esperada.

É realmente difícil detectar esses padrões com funis simples. É aqui que os funis que contabilizam a sequência de eventos são mais úteis: eles fornecem aos analistas dados valiosos para hipóteses futuras.

Funis de Produtos com Sequência de Eventos

Tarefa3 / 3

1.

Analise o funil de uma loja online.

Salve os logs do arquivo `/datasets/events.csv` como `events`. Imprima as primeiras cinco linhas desta variável para obter os nomes das colunas. Para cada usuário, encontre o primeiro horário de ocorrência de cada evento. Salve o resultado na variável `users` e imprima também as cinco primeiras linhas deste DataFrame.

2.

Encontre o número de visitas ao site. Salve o resultado como `pageview_count` e imprima esta variável.

3.

Construa o funil. Encontre:

-   O número de visitantes que adicionaram produtos aos carrinhos após a primeira visita ao site
-   O número de visitantes que iniciaram o processo de checkout após adicionar produtos aos seus carrinhos
-   O número de visitantes que pagaram por seus pedidos após o checkout

Salve os resultados nas variáveis `n_add_to_cart`, `n_checkout` e `n_payment`, respectivamente.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

import pandas as pd

  

events \= pd.read\_csv('/datasets/events.csv')

  

users \= events.pivot\_table(

index\='uid',

columns\='event\_name',

values\='event\_time',

aggfunc\='min')

  

step\_1 \= ~users\['pageview'\].isna()

step\_2 \= step\_1 & (users\['add\_to\_cart'\] \> users\['pageview'\])

step\_3 \= step\_2 & (users\['checkout'\] \> users\['add\_to\_cart'\])

step\_4 \= step\_3 & (users\['payment'\] \> users\['checkout'\])

  

n\_pageview \= users\[step\_1\].shape\[0\]

n\_add\_to\_cart \= users\[step\_2\].shape\[0\]

n\_checkout \= users\[step\_3\].shape\[0\]

n\_payment \= users\[step\_4\].shape\[0\]

  

print('Visitantes:', n\_pageview)

print('Adicionaram um produto ao carrinho:', n\_add\_to\_cart)

print('Iniciaram o check-out:', n\_checkout)

print('Pagaram:', n\_payment)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-04-942Z.md
### Última modificação: 2025-05-28 19:57:05

# Conclusão - TripleTen

Capítulo 2/9

Métricas e Funis

# Conclusão

Parabéns! Você concluiu métricas e funis com êxito.

### Você aprendeu:

-   Como calcular o lucro bruto, operacional e líquido
-   Como calcular o período de retorno e retorno do investimento em marketing
-   Construir gráficos de marketing e funis de produtos em Python

No próximo capítulo, vamos dar uma olhada na análise de coorte.

### Leve com você

Faça o download da [folha conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_7/PT/Additional_materials/moved_DA_7_2_Folha_de_Concluses_Mtricas_e_Funis.pdf) e do [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_7/PT/Additional_materials/moved_DA_7_Resumo_do_Captulo_Mtricas_e_Funis.pdf) para que você possa consultá-los quando necessário.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-06-249Z.md
### Última modificação: 2025-05-28 19:57:06

# Introdução - TripleTen

Capítulo 3/9

Análise de Coorte

# Introdução

Neste capítulo, vamos discutir a análise de coorte.

A análise de coorte é a ferramenta perfeita para obter percepções sobre grupos variados de consumidores. Ela te ajuda a analisar o comportamento de grupo e seu valor para o negócio.

### O que você irá aprender:

-   Como formar coortes baseadas no comportamento dos usuários
-   Como fazer a análise de coorte
-   Como aplicar a análise de coorte a tarefas práticas
-   Como interpretar o resultado da análise de coorte
-   Como definir importantes métricas analíticas de produtos: taxa de retenção e índice de cancelamento

### Quanto vai demorar:

9 _lições, de aproximadamente 20 a 25 minutos cada_

### Aplicação:

Você irá usar a análise de coorte para estudar as compras efetuadas por consumidores de uma loja online e o comportamento de usuários de um aplicativo para celular.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-07-564Z.md
### Última modificação: 2025-05-28 19:57:08

# Análise de Coorte - TripleTen

Capítulo 3/9

Análise de Coorte

# Análise de Coorte

Em que momento os usuários ficam entediados com seu aplicativo e saem? Quando eles voltam? Para responder essas questões, analistas realizam uma **análise de coorte**

Uma **coorte** é um grupo de sujeitos que possui algo em comum (geralmente, eles protagonizaram determinado evento—digamos, inscrever-se em um site—dentro do mesmo período de tempo).

Um exemplo perfeito de uma coorte seria pessoas que nasceram no mesmo ano. Aqueles nascidos em 1987 e 1986 seriam então representantes de coortes distintas, mas você poderia fazer uma coorte de pessoas que nasceram nos anos 80. Então tanto os "nascidos em 1987" e os "nascidos em 1986" encontrariam-se na mesma coorte.

Outros exemplos de coortes:

-   Empresas com as quais foram elaborados contratos no primeiro trimestre de 2018
-   Usuários que se registraram em um serviço em maio de 2019
-   Consumidores que realizaram seu primeiro pedido em uma loja online entre 5 e 11 de agosto de 2019

É fácil definir uma coorte. Você só precisa determinar:

1) o **evento** que é comum a eles

2) o **período de tempo** durante o qual o evento deve ocorrer

Um evento é um registro de um usuário realizando determinada ação. Exemplos de eventos: cadastro, primeiro pedido, pedido seguinte.

O período é o intervalo de tempo dentro do qual o evento ocorreu. Pode ser um ano, um semestre, um mês, uma semana, etc...

Uma vez que você determinou um evento e um período de tempo, você pode **formar uma coorte**. Em outras palavras, você agrupa os usuários que realizaram uma ação específica dentro de um mesmo período de tempo.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_____2_kogorti_1586541451.jpg)

Analistas dividem a base de consumidores em coortes para identificar tendências no comportamento dos usuários. Eles pegam uma coorte e estudam seu comportamento durante um certo período de tempo. Um exemplo de uma questão que eles podem querer perguntar é "Quantos usuários da coorte dos que se inscreveram em 19 de março ainda estavam ativos em junho de 2019?"

Conclusões derivadas da análise de coorte podem estabelecer as bases para modificações de produtos e processos e levar a novas hipóteses. Mas isso é nada mais do que análise retrospectiva; ela mostra como os usuários se comportaram _no passado_. Ela raramente é usadas para prever o comportamento dos usuários, que será influenciado por fatores externos no futuro (sazonalidade, mudanças na legislação etc.).

Pergunta

Qual grupo não é uma coorte?

Alunos de 2019

Usuários que efetuaram sua primeira compra usando um aplicativo entre 12 e 18 de agosto de 2019

Visitantes do site que vivem em Denver

Isso mesmo! Coortes são definidas por um evento específico que ocorreu durante um determinado período de tempo. Aqui não temos período de tempo.

Muito bem!

Pergunta

Temos dados de inscrições em um aplicativo ao longo de um ano. É possível que um mesmo usuário pertença a coortes distintas?

Sim, se conduzirmos diversas análises em que dividimos os usuários em grupos usando períodos de tempo diferentes.

Isso mesmo! Se conduzirmos uma análise de coorte na qual os usuários estejam divididos em grupos de acordo com a semana de realização da inscrição, e depois pelo mês da inscrição, os mesmos usuários serão encontrados em diversas coortes.

Não, um usuário não pode pertencer a mais de uma coorte, independente do período de tempo determinado

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-08-929Z.md
### Última modificação: 2025-05-28 19:57:09

# Análise de Coorte em Python - TripleTen

Teoria

# Análise de Coorte em Python

Você tem dados de todos os usuários de uma loja online: a data e o valor de seus pedidos. A tarefa é descobrir como a quantidade de usuários que efetua sua primeira compra varia de um mês para o outro. Quando você souber se a quantidade de usuários novos está subindo ou caindo, será possível determinar quem traz mais lucro, consumidores novos ou antigos.

Dê uma olhada nas primeiras 10 linhas:

```
import pandas as pd

orders = pd.read_csv('orders_data_for_cohort.csv')
print(orders.head(10))
```

```
   order_id           order_date  customer_id  revenue
0    536365  2010-12-01 08:26:00        17850   139.12
1    536366  2010-12-01 08:28:00        17850    22.20
2    536367  2010-12-01 08:34:00        13047   278.73
3    536368  2010-12-01 08:34:00        13047    70.05
4    536369  2010-12-01 08:35:00        13047    17.85
5    536370  2010-12-01 08:45:00        12583   855.86
6    536371  2010-12-01 09:00:00        13748   204.00
7    536372  2010-12-01 09:01:00        17850    22.20
8    536373  2010-12-01 09:02:00        17850   259.86
9    536374  2010-12-01 09:09:00        15100   350.40 
```

-   `order_id` — o identificador unívoco do pedido
-   `order_date` — a data do pedido
-   `customer_id` — o identificador unívoco do consumidor
-   `revenue` — o tamanho do pedido

Está tudo pronto para a análise de coorte: se sabemos as datas dos pedidos, podemos recuperar o primeiro de todos. Então, tendo agrupado os usuários pela data de sua primeira compra, podemos formar coortes.

Vamos agrupar os dados pela coluna `'customer_id'` usando o método `groupby()`. Para cada consumidor, obtenha a data de seu primeiro pedido, `['order_date'].min()`:

```
first_order_date_by_customers = orders.groupby('customer_id')[
    'order_date'
].min()
print(first_order_date_by_customers.head())
```

```
customer_id
12346    2011-01-18 10:01:00
12347    2010-12-07 14:57:00
12348    2010-12-16 19:09:00
12349    2011-11-21 09:51:00
12350    2011-02-02 16:01:00
Name: order_date, dtype: object
```

Temos um objeto Series com o nome `order_date`.

A tabela fonte `orders` já possui uma coluna `order_date`. Para evitar ter duas colunas com o mesmo nome no DataFrame resultante, vamos renomear o objeto Series `first_order_date` acessando seu atributo `name`. Note que temos que renomear o próprio objeto Series, e não a variável em que ele está armazenado:

```
first_order_date_by_customers.name = 'first_order_date'
# acessando o atributo name do objeto Series em first_order_date_by_customers
```

Usando o método `join()` vamos juntar o DataFrame `orders` com o objeto Series `first_order_date_by_customers` na coluna `'customer_id'`:

```
orders = orders.join(first_order_date_by_customers,on='customer_id')
print(orders.head(10))
```

```
     order_id           order_date  customer_id  revenue     first_order_date  
0    536365  2010-12-01 08:26:00        17850   139.12  2010-12-01 08:26:00
1    536366  2010-12-01 08:28:00        17850    22.20  2010-12-01 08:26:00
2    536367  2010-12-01 08:34:00        13047   278.73  2010-12-01 08:34:00
3    536368  2010-12-01 08:34:00        13047    70.05  2010-12-01 08:34:00
4    536369  2010-12-01 08:35:00        13047    17.85  2010-12-01 08:34:00
5    536370  2010-12-01 08:45:00        12583   855.86  2010-12-01 08:45:00
6    536371  2010-12-01 09:00:00        13748   204.00  2010-12-01 09:00:00
7    536372  2010-12-01 09:01:00        17850    22.20  2010-12-01 08:26:00
8    536373  2010-12-01 09:02:00        17850   259.86  2010-12-01 08:26:00
9    536374  2010-12-01 09:09:00        15100   350.40  2010-12-01 09:09:00
```

Agora nossa tabela possui uma coluna `first_order_date` com a data das primeiras compras dos clientes.

O próximo passo será isolar os meses das colunas `first_order_date` e `order_date`. Vamos chamar o método `astype()` e converter os valores da coluna para o tipo `'datetime64[M]'`. No tipo `datetime64`, `[M]` é o período da marca temporal—neste caso, um mês:

```
orders['first_order_month'] = orders['first_order_date'].astype(
    'datetime64[M]'
)
orders['order_month'] = orders['order_date'].astype('datetime64[M]')
print(orders.head(10))
```

```
   order_id           order_date  customer_id  revenue     first_order_date  \
0    536365  2010-12-01 08:26:00        17850   139.12  2010-12-01 08:26:00   
1    536366  2010-12-01 08:28:00        17850    22.20  2010-12-01 08:26:00   
2    536367  2010-12-01 08:34:00        13047   278.73  2010-12-01 08:34:00   
3    536368  2010-12-01 08:34:00        13047    70.05  2010-12-01 08:34:00   
4    536369  2010-12-01 08:35:00        13047    17.85  2010-12-01 08:34:00   
5    536370  2010-12-01 08:45:00        12583   855.86  2010-12-01 08:45:00   
6    536371  2010-12-01 09:00:00        13748   204.00  2010-12-01 09:00:00   
7    536372  2010-12-01 09:01:00        17850    22.20  2010-12-01 08:26:00   
8    536373  2010-12-01 09:02:00        17850   259.86  2010-12-01 08:26:00   
9    536374  2010-12-01 09:09:00        15100   350.40  2010-12-01 09:09:00   

  first_order_month order_month  
0        2010-12-01  2010-12-01  
1        2010-12-01  2010-12-01  
2        2010-12-01  2010-12-01  
3        2010-12-01  2010-12-01  
4        2010-12-01  2010-12-01  
5        2010-12-01  2010-12-01  
6        2010-12-01  2010-12-01  
7        2010-12-01  2010-12-01  
8        2010-12-01  2010-12-01  
9        2010-12-01  2010-12-01
```

A coluna `first_order_month`, portanto, será usada para formar coortes. Vamos agrupar os dados nessa coluna e avaliar as métricas para cada coorte.

Vamos aplicar o nosso querido método `agg`. É passado um dicionário cujas chaves são nomes de colunas e cujos valores são funções de agregação.

```
cohort_grouped = orders.groupby('first_order_month').agg(
    {'order_id': 'nunique', 'customer_id': 'nunique', 'revenue': 'sum'}
)
print(cohort_grouped)
```

```
first_order_month          order_id  customer_id      revenue                                  
2010-12-01             8318          885  4512148.220
2011-01-01             2153          417  1125882.281
2011-02-01             1561          380   593876.880
2011-03-01             1611          452   643758.940
2011-04-01              924          300   326621.031
2011-05-01              818          284   287075.780
2011-06-01              661          242   273455.230
2011-07-01              442          188   144468.341
2011-08-01              358          169   196049.050
2011-09-01              595          299   233296.821
2011-10-01              608          358   227369.540
2011-11-01              439          323   151876.730
2011-12-01               43           41    27059.460
```

Que conclusões podemos tirar a partir dessa tabela?

A receita de cada coorte é menor do que a da anterior. Isso faz sentido, já que os usuários de coortes mais antigas tiveram mais tempo para fazer pedidos.

Podemos chegar a outra conclusão: a quantidade de usuários em cada coorte está caindo drasticamente de um mês para o outro. Isso implica que a loja online está recebendo cada vez menos consumidores que fazem compras.

Análise de Coorte em Python

Tarefa4 / 4

1.

O conjunto de dados `'/datasets/game_purchases.csv'` contém dados de compras dentro do aplicativo para o jogo de estratégia _Battle Universe_. As colunas da tabela são:

-   `purchase_datetime` — a data e hora da compra
-   `player_id` — o identificador unívoco do jogador
-   `item` — o nome do item comprado: `life` (vida), `mana` (poder mágico usado para soltar feitiços), `armor` (armadura)
-   `price` — (na moeda corrente do jogo)
-   `purchase_id` — o identificador unívoco da compra

Leia os dados do arquivo `game_purchases.csv`. Salve o resultado como `purchases`.

Agrupe os dados da tabela `purchases`: para cada jogador, descubra a data da primeira compra. Nomeie o objeto Series resultante como `first_purchase_dates`.

Então renomeie o objeto Series `first_purchase_datetime` mudando o atributo `name`.

Imprima as 10 primeiras linhas de `first_purchase_dates`.

2.

Junte o DataFrame `purchases` com o objeto Series `first_purchase_dates` de modo que `first_purchase_dates` torne-se uma coluna separada do DataFrame `purchases`.

Salve o DataFrame juntado como `purchases`.

Imprima as 10 primeiras linhas do DataFrame juntado.

3.

No DataFrame `purchases`, crie uma coluna `purchase_month` para armazenar os meses recuperados da coluna `purchase_datetime`.

Também crie uma coluna `first_purchase_month` onde você irá armazenar os meses recuperados da coluna `first_purchase_datetime`. Essa coluna será usada para criar coortes.

Imprima as 10 primeiras linhas do DataFrame `purchases`.

4.

Faça uma tabela a partir do DataFrame `purchases`, onde você irá calcular o valor total gasto em compras dentro do jogo para cada coorte.

Salve a tabela agrupada como `cohort_stats`.

Imprima `cohort_stats`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

import pandas as pd

  

purchases \= pd.read\_csv('/datasets/game\_purchases.csv')

first\_purchase\_dates \= purchases.groupby('player\_id')\[

'purchase\_datetime'

\].min()

first\_purchase\_dates.name \= 'first\_purchase\_datetime'

purchases \= purchases.join(first\_purchase\_dates, on\='player\_id')

purchases\['purchase\_month'\] \= purchases\['purchase\_datetime'\].astype(

'datetime64\[M\]'

)

purchases\['first\_purchase\_month'\] \= purchases\[

'first\_purchase\_datetime'

\].astype('datetime64\[M\]')

cohort\_stats \= purchases.groupby('first\_purchase\_month')\['price'\].sum()

  

print(cohort\_stats.head(10))

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-10-239Z.md
### Última modificação: 2025-05-28 19:57:10

# Avaliando Mudanças em Valores Absolutos por Mês - TripleTen

Capítulo 3/9

Análise de Coorte

# Avaliando Mudanças em Valores Absolutos por Mês

Formamos coortes de usuários com base na primeira data de compra. Também aprendemos que a quantidade de novos consumidores está decaindo com o passar do tempo.

Agora tentaremos descobrir como o comportamento de cada coorte está mudando com o tempo. Por exemplo, vamos determinar:

-   Se o tamanho da compra média está diminuindo
-   Se consumidores atraídos em diferentes ocasiões são igualmente lucrativos

Primeiro, devemos formar coortes por mês de compra, como na lição anterior:

```
import pandas as pd

orders = pd.read_csv('orders_data_for_cohort.csv')
first_order_date_by_customers = orders.groupby('customer_id')[
    'order_date'
].min()
first_order_date_by_customers.name = 'first_order_date'
orders = orders.join(first_order_date_by_customers, on='customer_id')
orders['first_order_month'] = orders['first_order_date'].astype(
    'datetime64[M]'
)
orders['order_month'] = orders['order_date'].astype('datetime64[M]')
print(orders.head(10))
```

```
 order_id           order_date  customer_id  revenue     first_order_date  \
0    536365  2010-12-01 08:26:00        17850   139.12  2010-12-01 08:26:00   
1    536366  2010-12-01 08:28:00        17850    22.20  2010-12-01 08:26:00   
2    536367  2010-12-01 08:34:00        13047   278.73  2010-12-01 08:34:00   
3    536368  2010-12-01 08:34:00        13047    70.05  2010-12-01 08:34:00   
4    536369  2010-12-01 08:35:00        13047    17.85  2010-12-01 08:34:00   
5    536370  2010-12-01 08:45:00        12583   855.86  2010-12-01 08:45:00   
6    536371  2010-12-01 09:00:00        13748   204.00  2010-12-01 09:00:00   
7    536372  2010-12-01 09:01:00        17850    22.20  2010-12-01 08:26:00   
8    536373  2010-12-01 09:02:00        17850   259.86  2010-12-01 08:26:00   
9    536374  2010-12-01 09:09:00        15100   350.40  2010-12-01 09:09:00   

first_order_month order_month  
0        2010-12-01  2010-12-01  
1        2010-12-01  2010-12-01  
2        2010-12-01  2010-12-01  
3        2010-12-01  2010-12-01  
4        2010-12-01  2010-12-01  
5        2010-12-01  2010-12-01  
6        2010-12-01  2010-12-01  
7        2010-12-01  2010-12-01  
8        2010-12-01  2010-12-01  
9        2010-12-01  2010-12-01
```

## Avaliando mudanças em valores absolutos por mês

A maior parte das observações quantitativas são gravadas como valores absolutos.

Um **valor absoluto** é o volume, tamanho ou magnitude de um evento ou fenômeno observado. Pode ser, por exemplo, um valor total em dólares ou total de compras em unidades.

Se dividirmos o valor total de todas as compras pela quantidade de compras, iremos obter o preço médio para um produto. Esse será um **valor relativo**, que fornece a razão entre dois outros valores.

Ao calcular valores relativos, é fundamental que seus componentes estejam ligados. Por exemplo, o valor total de todas as compras pode ser dividido pela quantidade de compras: esses valores são relevantes um para o outro. Mas a razão entre o tamanho da compra e passos dados para ir até a loja não ajudam muito com a análise de produto.

A primeira tarefa que pode ser resolvida com uso de análise de coorte é rastrear as mudanças na quantidade de compradores ativos dentro de uma das coortes. Com isso, saberemos quantas pessoas continuam fazendo compras nos meses seguintes ao mês da primeira compra. Vamos criar uma tabela dinâmica usando o método `pivot_table()`:

-   As linhas (`index`) vão conter uma coorte de usuário: `first_order_month`
-   As colunas (`columns`) serão os meses de compra: `order_month`
-   Os valores (`values`) serão a quantidade de usuários unívocos (`customer_id`) que fizeram sua primeira compra na loja online
-   vamos indicar `'nunique'` no argumento `aggfunc`:

```
orders.pivot_table(
    index='first_order_month',
    columns='order_month',
    values='customer_id',
    aggfunc='nunique',
)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1586546518.png)

A quais conclusões podemos chegar a partir dessa tabela?

-   A quantidade de consumidores ativos dentro de uma coorte decai após o primeiro mês.
-   Em algumas coortes, a quantidade de consumidores ativos às vezes aumenta (Por exemplo, a coorte `2010-12-01`).
-   A quantidade de consumidores ativos decaiu em todas as coortes em dezembro de 2011. Isso provavelmente ocorreu devido à sazonalidade.
-   Os usuários da coorte de dezembro de 2010 ainda representam a maior parcela de consumidores ativos, mesmo após um ano. Em novembro de 2011, havia 445 deles.

Rastrear valores absolutos por mês pode te ajudar a identificar mudanças que ocorrem com o tempo. Além disso, são reveladas características comuns a todas as coortes, como quedas sazonais na quantidade de usuários. Também podemos chegar a uma conclusão em relação à participação de cada coorte na quantidade total de compras mensais.

Pergunta

Observe os resultados de uma análise de coorte de um serviço de limpeza. A tabela mostra a quantidade de usuários ativos em cada coorte (formadas com base na primeira data de compra):

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.3.4PT_1655467738.png)

Quais conclusões batem com os dados nessa tabela?

A coorte de fevereiro é a única que apresentou crescimento após a queda no segundo mês.

Realmente, a coorte de fevereiro parece gostar do serviço. Por exemplo, 841 de 852 desses consumidores encomendaram serviços de limpeza em junho. Será que era alguma promoção?

Os números para a coorte de janeiro parecem piores dos que os das outras, porque em junho ela teve a menor quantidade de clientes ativos.

Janeiro é o melhor mês para limpezas, já que a coorte desse mês possui a maior quantidade de clientes.

Trabalho maravilhoso!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-11-568Z.md
### Última modificação: 2025-05-28 19:57:11

# Avaliando Mudanças em Valores Relativos pelo Tempo de Vida - TripleTen

Teoria

# Avaliando Mudanças em Valores Relativos pelo Tempo de Vida

Outra tarefa da análise de coorte é descobrir como a **receita média por usuário** (total de vendas dividido pela quantidade de clientes) muda com o tempo. Isso nos ajudará a entender se a receita média de usuários que continua fazendo pedidos está aumentando ou não.

Vamos calcular métricas de coorte adicionais por mês:

-   Receita média por usuário por cada mês
-   A posição do mês do pedido em relação ao mês em que o primeiro pedido foi feito

Primeiro, vamos agrupar os dados pelas coortes `first_order_month` e pelo mês de compra (`order_month`):

```
import pandas as pd

orders = pd.read_csv('orders_data_for_cohort.csv')
first_order_date_by_customers = orders.groupby('customer_id')[
    'order_date'
].min()
first_order_date_by_customers.name = 'first_order_date'
orders = orders.join(first_order_date_by_customers, on='customer_id')
orders['first_order_month'] = orders['first_order_date'].astype(
    'datetime64[M]'
)
orders['order_month'] = orders['order_date'].astype('datetime64[M]')

orders_grouped_by_cohorts = orders.groupby(
    ['first_order_month', 'order_month']
).agg({'revenue': 'sum', 'customer_id': 'nunique'})
print(orders_grouped_by_cohorts.head())
```

```
first_order_month       order_month  revenue            customer_id          
2010-12-01              2010-12-01   572713.89          885
                        2011-01-01   276237.69          324
                        2011-02-01   233845.37          286
                        2011-03-01   303119.39          340
                        2011-04-01   204407.66          321
```

Vamos descobrir `revenue_per_user`. Vamos precisar dividir `revenue` pela quantidade de valores de `customer_id`:

```
orders_grouped_by_cohorts['revenue_per_user'] = (
    orders_grouped_by_cohorts['revenue']
    / orders_grouped_by_cohorts['customer_id']
)
```

Agora vamos construir uma tabela dinâmica mostrando as mudanças na receita por usuário, por coortes pelo mês de compra, e avaliar mudanças na receita por usuário ao longo do tempo:

```
orders_grouped_by_cohorts.pivot_table(
    index='first_order_month',
    columns='order_month',
    values='revenue_per_user',
    aggfunc='mean',
)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1_1586547345.png)

Mas e se tivermos que avaliar mudanças na receita por usuário em relação ao tempo que passou desde a primeira compra? No contexto das coortes, a esse valor chama-se **tempo de vida**. Ela mostra em que mês um evento ocorreu em termos da distância do mês da coorte. Vamos analisar o tempo de vida da coorte de consumidores que fizeram suas primeiras compras em dezembro de 2011.

-   Compras realizadas em dezembro de 2011 foram feitas no mês zero de tempo de vida.
-   Compras realizadas em janeiro de 2012 foram feitas no 1º mês de tempo de vida.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.3.5PT_1655897751.png)

Vamos descobrir o tempo de vida da coorte. Vamos subtrair o mês da coorte (`first_order_month`) do mês em que as compras foram realizadas (`order_month`):

```
orders_grouped_by_cohorts = orders_grouped_by_cohorts.reset_index()
orders_grouped_by_cohorts['cohort_lifetime'] = (
    orders_grouped_by_cohorts['order_month']
    - orders_grouped_by_cohorts['first_order_month']
)
orders_grouped_by_cohorts['cohort_lifetime'].head()
```

```
0     0 days
1    31 days
2    62 days
3    90 days
4   121 days
Name: cohort_lifetime, dtype: timedelta64[ns]
```

Obtemos uma coluna `cohort_lifetime` com o tipo `timedelta`. Ela armazena a diferença entre as duas datas. Você aprendeu sobre o método `pd.Timedelta()` ao estudar sobre manipulação de dados. Esse método é utilizado quando você precisa alterar o tempo (por exemplo, ao mudar de fuso horário).

Vamos converter esse tipo para a quantidade de meses entre as duas datas. Precisamos dividir `cohort_lifetime` pelo valor constante `np.timedelta64(1, 'M')` e descobrir quantos valores iguais a um mês `cohort_lifetime` contém. O valor constante `np.timedelta64(1, 'M')` só pode ser encontrado em NumPy, então precisamos importar essa biblioteca:

```
import pandas as pd
import numpy as np

orders_grouped_by_cohorts['cohort_lifetime'] = orders_grouped_by_cohorts[
    'cohort_lifetime'
] / np.timedelta64(1, 'M')
print(orders_grouped_by_cohorts['cohort_lifetime'].head())
```

```
0    0.000000
1    1.018501
2    2.037003
3    2.956940
4    3.975441
Name: cohort_lifetime, dtype: float64
```

Temos frações. Isso ocorreu porque o valor constante `np.timedelta64(1,'M')` é a duração média de meses em um ano. Os meses possuem entre 28 a 31 dias, por isso as frações. Vamos arredondar os valores da coluna `cohort_lifetime` usando o método `round()`, e então convertê-los para números inteiros usando o método `astype('int')`.

```
orders_grouped_by_cohorts['cohort_lifetime'] = (
    orders_grouped_by_cohorts['cohort_lifetime'].round().astype('int')
)
print(
    orders_grouped_by_cohorts[
        ['first_order_month', 'order_month', 'cohort_lifetime']
    ].head()
)
print(
    orders_grouped_by_cohorts[
        ['first_order_month', 'order_month', 'cohort_lifetime']
    ].tail()
)
```

```
  first_order_month order_month  cohort_lifetime
0        2010-12-01  2010-12-01                0
1        2010-12-01  2011-01-01                1
2        2010-12-01  2011-02-01                2
3        2010-12-01  2011-03-01                3
4        2010-12-01  2011-04-01                4
   first_order_month order_month  cohort_lifetime
86        2011-10-01  2011-11-01                1
87        2011-10-01  2011-12-01                2
88        2011-11-01  2011-11-01                0
89        2011-11-01  2011-12-01                1
90        2011-12-01  2011-12-01                0
```

Agora temos meses com números inteiros. Eles nos informam o número do mês de compra em relação ao mês da coorte.

Quando todos os cálculos estiverem prontos, nós vamos deixar apenas o ano e o mês no identificador da coorte. Vamos usar o método `dt.strftime()` (string format time), que transforma objetos Series de tipo `datetime` em linhas com formato `date`. Por exemplo:

```
orders_grouped_by_cohorts['first_order_month'].dt.strftime('%d.%m.%Y')
```

```
0     01.12.2010
1     01.12.2010
2     01.12.2010
3     01.12.2010
4     01.12.2010
         ...    
86    01.10.2011
87    01.10.2011
88    01.11.2011
89    01.11.2011
90    01.12.2011
Name: first_order_month, Length: 91, dtype: object
```

Vamos deixar apenas o ano e o mês nos dados do mês da primeira compra:

```
orders_grouped_by_cohorts['first_order_month'] = orders_grouped_by_cohorts[
    'first_order_month'
].dt.strftime('%Y-%m')
```

Agora vamos compilar uma tabela dinâmica de mudanças na receita média por usuário, cujas colunas conterão o tempo de vida, e suas linhas serão coortes.

```
revenue_per_user_pivot = orders_grouped_by_cohorts.pivot_table(
    index='first_order_month',
    columns='cohort_lifetime',
    values='revenue_per_user',
    aggfunc='mean',
)
print(revenue_per_user_pivot)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_2_1586548412.png)

Aqui vemos claramente para quais coortes a receita por usuário aumentou no mês seguinte ao mês da primeira compra. Para algumas coortes, a receita por usuário aumentou rapidamente com o passar do tempo (por exemplo, as coortes de dezembro de 2010 e agosto de 2011).

Avaliando Mudanças em Valores Relativos pelo Tempo de Vida

Tarefa4 / 4

1.

Compile uma tabela dinâmica para evidenciar as mudanças no total de compras realizadas por jogadores em coortes mensais. Salve-a como `purchase_pivot`.

Imprima a tabela.

2.

Descubra a quantidade de compras e a quantidade de jogadores que fizeram compras para cada coorte e cada mês. Salve como `purchases_grouped_by_cohorts`.

Acrescente a coluna `purchases_per_player` ao DataFrame `purchases_grouped_by_cohorts`. Nesta coluna, calcule a quantidade média de compras por jogador.

Compile uma tabela dinâmica e salve-a como `mean_purchases_pivot`. Ela irá conter as mudanças na quantidade média de compras por jogador em cada coorte em meses distintos.

Imprima a tabela dinâmica `mean_purchases_pivot`.

3.

Introduza a coluna `cohort_lifetime` na tabela `purchases_grouped_by_cohorts`. Dentro da coluna, converta o mês da compra dentro do jogo (`purchase_month`) para o mês de tempo de vida (o mês relativo ao mês da coorte).

A coluna `cohort_lifetime` deve conter números inteiros.

Essa tarefa apresenta um resultado temporário. Você não precisa imprimir nada.

4.

Compile uma tabela dinâmica para evidenciar as mudanças na quantidade média de compras por jogador em relação ao mês do tempo de vida da coorte. Salve a tabela como `lifetime_pivot` e imprima a variável.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

import pandas as pd

import numpy as np

  

purchases \= pd.read\_csv('/datasets/game\_purchases.csv')

first\_purchase\_dates \= purchases.groupby('player\_id')\[

'purchase\_datetime'

\].min()

first\_purchase\_dates.name \= 'first\_purchase\_datetime'

purchases \= purchases.join(first\_purchase\_dates, on\='player\_id')

purchases\['purchase\_month'\] \= purchases\['purchase\_datetime'\].astype(

'datetime64\[M\]'

)

purchases\['first\_purchase\_month'\] \= purchases\[

'first\_purchase\_datetime'

\].astype('datetime64\[M\]')

purchases\_grouped\_by\_cohorts \= purchases.groupby(

\['first\_purchase\_month', 'purchase\_month'\]

).agg({'purchase\_id': 'nunique', 'player\_id': 'nunique'})

purchases\_grouped\_by\_cohorts\['purchases\_per\_player'\] \= (

purchases\_grouped\_by\_cohorts\['purchase\_id'\]

/ purchases\_grouped\_by\_cohorts\['player\_id'\]

)

purchases\_grouped\_by\_cohorts \= purchases\_grouped\_by\_cohorts.reset\_index()

purchases\_grouped\_by\_cohorts\['cohort\_lifetime'\] \= (

purchases\_grouped\_by\_cohorts\['purchase\_month'\]

\- purchases\_grouped\_by\_cohorts\['first\_purchase\_month'\]

)

purchases\_grouped\_by\_cohorts\['cohort\_lifetime'\] \= purchases\_grouped\_by\_cohorts\[

'cohort\_lifetime'

\] / np.timedelta64(1, 'M')

purchases\_grouped\_by\_cohorts\['cohort\_lifetime'\] \= (

purchases\_grouped\_by\_cohorts\['cohort\_lifetime'\].round().astype('int')

)

lifetime\_pivot \= purchases\_grouped\_by\_cohorts.pivot\_table(

index\='first\_purchase\_month',

columns\='cohort\_lifetime',

values\='purchases\_per\_player',

aggfunc\='sum',

)

print(lifetime\_pivot)

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-12-908Z.md
### Última modificação: 2025-05-28 19:57:13

# Visualizando a Análise de Coorte - TripleTen

Teoria

# Visualizando a Análise de Coorte

Na lição anterior nós compilamos esta tabela:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_18_1586865043.png)

É difícil tirar conclusões a partir dela: os valores parecem ser monótonos e uniformes, nada chama nossa atenção. Mas se transformarmos a tabela em um **mapa de calor** adicionando cores, então os padrões e as tendências vão ficar mais evidentes.

Um **mapa de calor** é um modo de visualizar tabelas em que as células variam de cor, de acordo com sua proximidade de valores máximos ou mínimos. Aqui está uma tabela que apresenta com clareza a sazonalidade na demanda por bens e serviços:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.3.6PT_1655467761.png)

_Fonte: "Como a Demanda por Bens e Serviços Muda Através dos Anos"_

Nós podemos ver o que o cliente prefere comprar no inverno. Começando em dezembro, eles realmente entram de cabeça nos esportes de inverno e correm para comprar esquis e snowboards. Eles se interessam por DVDs. No verão, os usuários da Internet deixam de lado os sinos de trenó e esquecem as atividades de inverno.

A biblioteca seaborn é usada para construir mapas de calor. É projetada para visualização e é baseada no já muito conhecido matplotlib. Acredita-se que os desenvolvedores tenham nomeado a biblioteca em homenagem a Sam Seaborn, da série _The West Wing._

Vamos fazer um mapa de calor usando o método **heatmap()**. Nós vamos passar a tabela revenue\_per\_user\_pivot junto com os seguintes parâmetros:

-   **annot=True** (anotar), que significa que o valor será exibido em cada célula
-   **fmt='.1f'** (formato), que define o formato impresso (aqui, uma casa decimal)
-   **linewidths=1**, que define a largura da linha separando as célular do mapa de calor (1 pixel);
-   **linecolor='gray',**, que define a linha da cor para cinza.

Você já se familiarizou com os parâmetros `figsize` e `title`.

```
import seaborn as sns
from matplotlib import pyplot as plt

plt.figure(figsize=(13, 9))
plt.title('Volume Médio de Compras dos Clientes')
sns.heatmap(
    revenue_per_user_pivot,
    annot=True,
    fmt='.1f',
    linewidths=1,
    linecolor='gray',
)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.3.6.2PT_1655467770.png)

O volume médio de compras da coorte de dezembro de 2010 realmente se destaca. Essa mudança se torna especialmente nítida a partir do primeiro mês de tempo de vida. A coorte de agosto de 2011 também apresenta um comportamento interessante: os picos das compras médias no segundo e no terceiro mês. Isso provavelmente ocorreu em função de pedidos de grande volume de alguns usuários.

Visualizando a Análise de Coorte

Tarefa

No DataFrame `revenue_per_user_pivot`, você pode encontrar uma tabela dinâmica que reflete as mudanças no volume médio de compras dos clientes da loja Sports Are Good por coortes agrupadas por tempo de vida. Use o método `heatmap()` para fazer um mapa de calor das mudanças no volume médio de compras das coortes.

Defina o tamanho da figura: `plt.figure(figsize=(13, 9))`. Nomeie a visualização resultante como `'Volume médio de compra dos clientes'`.

Passe os parâmetros `annot`, `fmt`, `linewidths`, e `linecolor` para o método `heatmap()`, de modo que:

-   Os valores são expostos nas células do mapa de calor
-   Duas casas decimais sejam impressas
-   A linha separando as células tenha 1 pixel
-   A cor da linha seja preta (`'black'`).

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

import seaborn as sns

from matplotlib import pyplot as plt

import pandas as pd

  

  

revenue\_per\_user\_pivot \= pd.read\_csv('/datasets/revenue\_pivot.csv')

revenue\_per\_user\_pivot \= revenue\_per\_user\_pivot.set\_index('first\_order\_month')

  

  

plt.figure(figsize\=(13, 9))

plt.title('Volume médio de compra dos clientes')

  

sns.heatmap(

revenue\_per\_user\_pivot,

annot\=True,

fmt\='.2f',

linewidths\=1,

linecolor\='black',

)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-14-167Z.md
### Última modificação: 2025-05-28 19:57:14

# Taxa de Retenção e Índice de Cancelamento - TripleTen

Capítulo 3/9

Análise de Coorte

# Taxa de Retenção e Índice de Cancelamento

Clientes "vêm fácil e vão fácil". Sua tarefa é rastrear isso e calcular métricas.

A análise de coorte é utilizada para analisar o comportamento de usuários para produtos digitais. **Taxa de retenção** e **índice de cancelamento** são as métricas avaliadas com mais frequência.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_____2_kogorti_1_1586551041.jpg)

**Taxa de Retenção**

A taxa de retenção fornece a quantidade de usuários de uma coorte que permaneceram ativos, quando comparados com sua quantidade inicial.

Por que é importante rastrear essa métrica? Quanto mais um cliente utiliza certo produto, maior a receita potencial. Isso vale para produtos nos quais a receita provém diretamente dos usuários, como serviços por assinatura.

A taxa de retenção também é fundamental para produtos cuja principal fonte de receita é a publicidade. Quanto mais os clientes usam o serviço, maior a visibilidade dos anúncios.

Como calculamos essa métrica?

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.3.7PT_1655467846.png)

Vamos ver como isso funciona. Em fevereiro de 2019, 700 novos clientes utilizaram os serviços da empresa de limpeza. A coorte de fevereiro possui 700 usuários. Com zero meses de tempo de vida, a coorte tem 100% de retenção.

No mês seguinte (março de 2019, o 1º mês de tempo de vida) apenas 600 dos 700 clientes da coorte de fevereiro contrataram serviços de limpeza. **`Taxa de Retenção = 600 / 700 x 100% = 85.71%`**.

Se em abril (o segundo mês do tempo de vida) o serviço for contratado por 650 clientes da coorte de fevereiro, a taxa irá subir para 92.86%. Mas se a quantidade de clientes for menor— 550, por exemplo—então a taxa irá cair (para 78.57%).

**Índice de cancelamento**

Outra métrica que auxilia no monitoramento da saúde de um produto é o índice de cancelamento. Com ele você determina a parcela que desiste de usar o produto com o passar do tempo. Ao contrário da taxa de retenção, o índice de cancelamento é uma comparação entre a quantidade de clientes em determinado estágio com a quantidade do estágio anterior.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.3.7.2PT_1655467859.png)

Vamos descobrir o índice de cancelamento da empresa. Da coorte de fevereiro, 700 clientes contrataram serviços de limpeza em fevereiro, enquanto apenas contrataram 600 em março. Portanto, o índice de cancelamento para o primeiro mês de tempo de vida pode ser calculado: **`600 / 700 - 100% = -14.28%`**. Se 650 clientes compram serviços em abril, o índice de cancelamento será positivo: **`650 / 600 - 100% = 8.33%`**.

Agora você irá aprender a calcular essas métricas em Python.

Pergunta

É possível que a taxa de retenção apresente valores negativos?

Sim

Não

Isso mesmo!

Seu entendimento sobre o material é impressionante!

Pergunta

Qual a principal diferença entre os cálculos de taxa de retenção e índice de cancelamento?

Não há nenhuma grande diferença, essas métricas são baseadas nos mesmos valores.

O índice de cancelamento é baseado no período do período zero de tempo de vida, e a taxa de retenção se baseia no 1º

Cálculos de índice de cancelamento não se restringem a determinado período de tempo de vida, mas a taxa de retenção sempre depende da quantidade de usuários no período zero de tempo de vida.

A verdade definitiva.

Você conseguiu!

Pergunta

Digamos que uma coorte de clientes para outubro de 2018 possua 50 clientes. Em dezembro de 2018, apenas 25 usuários ainda estavam na coorte. Calcule a taxa de retenção e índice de cancelamento para dezembro.

Taxa de retenção = 25%, índice de cancelamento = -50%

Taxa de retenção = 50%, índice de cancelamento = -25%

Taxa de retenção = 50%, não foi possível determinar o índice de cancelamento

Isso mesmo! A taxa de retenção é 25/50, ou 50%. Porém, para descobrir o índice de cancelamento, temos que saber a quantidade de usuários no mês anterior, que seria novembro. Esses dados não foram fornecidos na tarefa.

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-15-485Z.md
### Última modificação: 2025-05-28 19:57:15

# Calculando Taxas de Retenção em Python - TripleTen

Teoria

# Calculando Taxas de Retenção em Python

A análise de aplicativos móveis geralmente envolve o cálculo de taxas de retenção.

O conjunto de dados `user_activity.csv` contém informações relativas à atividade de cada usuário de um aplicativo:

```
import pandas as pd

user_activity = pd.read_csv('user_activity.csv')
user_activity['activity_date'] = pd.to_datetime(user_activity['activity_date'])
print(user_activity.head())
```

```
                                user_id activity_date
0  cdf343d1-f733-4b2c-896c-d565255e1ecc    2019-05-01
1  bb59f0d1-b1bc-4e89-982e-f451ed83e3af    2019-05-01
2  92c0476c-2384-4955-803d-10a4bfb5df63    2019-05-01
3  4774aedc-6236-4f97-9279-04b096b62e4f    2019-05-01
4  68b203ea-7532-4bc0-aedc-2d1d7e71e492    2019-05-01
```

`user_id` é o ID de um usuário que executa alguma ação no aplicativo; `activity_date` é quando a ação foi realizada.

Vamos definir o evento e o período e formar as coortes adequadas. Vamos considerar a data em que o usuário esteve ativo pela primeira vez no aplicativo.

Vamos armazenar esses dados no objeto Series `first_activity_date` e juntá-los com o DataFrame inicial `user_activity`:

```
first_activity_date = user_activity.groupby(['user_id'])['activity_date'].min()
first_activity_date.name = 'first_activity_date'
user_activity = user_activity.join(first_activity_date,on='user_id')
```

Nas lições anteriores você formou coortes mensais. Porém, o intervalo de tempo usado para a formação das coortes pode mudar de acordo com a urgência da empresa para decidir se vai modificar um produto.

Para aplicativos móveis, os resultados das mudanças começam a influenciar os comportamentos dos usuários instantaneamente, então novas decisões devem ser tomadas rapidamente. Então é melhor formarmos coortes por semana, e não por mês.

Anteriormente, você usou objetos Timedelta para mudar fusos horários ao adicionar/subtrair os objetos das datas. Vamos ver o Timedeltas novamente. Em pandas, assim como em Python, esses objetos representam duração, a diferença entre duas datas ou horários fornecidos. Você pode adicionar ou subtrair não somente horas/minutos/segundos, mas também dias:

```
example_date = pd.to_datetime('2010-10-25')
print(example_date + pd.Timedelta(days=10))
print(example_date - pd.Timedelta(days=5))
```

```
2010-11-04 00:00:00
2010-10-20 00:00:00
```

Você pode fazer o mesmo com objetos Series do pandas. Vamos pegar uma coluna com datas de `2010-01-01` a `2010-01-10`:

```
# gerando Series com datas de 2010-01-01 a 2010-01-10
dates_series = pd.Series(pd.date_range('2010-01-01', periods=10, freq='D'))
print(dates_series)
```

```
0   2010-01-01
1   2010-01-02
2   2010-01-03
3   2010-01-04
4   2010-01-05
5   2010-01-06
6   2010-01-07
7   2010-01-08
8   2010-01-09
9   2010-01-10
dtype: datetime64[ns]
```

É assim que podemos gerar um novo objeto Series, subtraindo, por exemplo, `3` dias de cada valor:

```
offset_dates_series = dates_series - pd.Timedelta(days=3)
print(offset_dates_series) # novo objeto Series (3 dias são subtraídos de cada valor)
```

```
0   2009-12-29
1   2009-12-30
2   2009-12-31
3   2010-01-01
4   2010-01-02
5   2010-01-03
6   2010-01-04
7   2010-01-05
8   2010-01-06
9   2010-01-07
dtype: datetime64[ns]
```

Agora vamos voltar para nosso conjunto de dados de atividade dos usuários. Ao formar coortes por semana, nós identificamos cada semana pela data em que ela começou (neste contexto, **semanas começam na segunda-feira**). Se um evento ocorre na quarta-feira, 25 de agosto, ele ocorreu durante a semana de 23 de agosto (já que aquela semana começou em 23 de agosto, segunda-feira). Obtemos isso subtraindo o índice de quarta-feira (`2`) da data do evento. Essa ilustração pode ajudar:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.3.8PT_1655467912.png)

Você lembra da propriedade pandas `dayofweek`? Para cada objeto Series, ela retorna um **novo objeto Series** com os índices dos dias da semana, com um intervalo de `0` (segunda-feira) a `6` (Domingo).

Você pode verificar o resultados para as cinco primeiras linhas de nosso conjunto de dados:

```
import pandas as pd

user_activity = pd.read_csv('/datasets/work_user_activity.csv')
user_activity['activity_date'] = pd.to_datetime(user_activity['activity_date'])

print('\nOs índices de dias da semana para esses eventos:\n')
print(user_activity['activity_date'].dt.dayofweek[:5])
```

```
0   6
1   6
2   6
3   3
4   6
dtype: int64
```

Mas você precisa converter esse objeto Series `int`para um objeto Series `Timedelta`, para que ele possa ser subtraído do objeto Series de datas `activity_date`. É assim que podemos expressar essa subtração em código. (Note que o parâmetro `unit` do método `pd.to_timedelta` determina a unidade de medida. Em nosso caso, a unidade é "dia": `unit='d'`). As primeiras cinco linhas vão ficar assim:

```
print(pd.to_timedelta(user_activity['activity_date'].dt.dayofweek, unit='d')[:5])
```

```
0   6 days
1   6 days
2   6 days
3   3 days
4   6 days
Name: activity_date, dtype: timedelta64[ns]
```

Então você pode subtrair o objeto Series `Timedelta` com índices de dia da semana da coluna `activity_date` e salvar o resultado no objeto Series `activity_week`:

```
user_activity['activity_week'] = pd.to_datetime(user_activity['activity_date'], unit='d') - pd.to_timedelta(user_activity['activity_date'].dt.dayofweek, unit='d')
```

A coluna `activity_week` irá armazenar a data em que a semana em questão começou.

```
print(user_activity[:5])
```

```
     activity_date      user_id           activity_week
0    2019-06-23         ...               2019-06-17
1    2019-07-14         ...               2019-07-08
2    2019-06-23         ...               2019-06-17
3    2019-06-20         ...               2019-06-17
4    2019-06-09         ...               2019-06-03
```

O mesmo pode ser aplicado à coluna `first_activity_date`:

```
user_activity['first_activity_week'] = pd.to_datetime(user_activity['first_activity_date'], unit='d') - pd.to_timedelta(user_activity['first_activity_date'].dt.dayofweek, unit='d')
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.3.8.2PT_1655467944.png)

O código completo até agora:

```
import pandas as pd

user_activity = pd.read_csv('user_activity.csv')
user_activity['activity_date'] = pd.to_datetime(user_activity['activity_date'])
print(user_activity.head())

first_activity_date = user_activity.groupby(['user_id'])['activity_date'].min()
first_activity_date.name = 'first_activity_date'
user_activity = user_activity.join(first_activity_date, on='user_id')

user_activity['activity_week'] = pd.to_datetime(
    user_activity['activity_date'], unit='d'
) - pd.to_timedelta(user_activity['activity_date'].dt.dayofweek, unit='d')
user_activity['first_activity_week'] = pd.to_datetime(
    user_activity['first_activity_date'], unit='d'
) - pd.to_timedelta(
    user_activity['first_activity_date'].dt.dayofweek, unit='d'
)
```

(O parâmetro unidade é geralmente indicado para jogar pelo seguro. Ele não produz impacto nos dados contendo apenas a data, mas se houver horários, esse parâmetro irá ajudar no seu processamento).

Agora podemos calcular o tempo de vida do usuário dentro da coorte para cada linha do DataFrame. Nós fizemos isso na lição anterior, mas desta vez o divisor será o valor constante `np.timedelta64(1,'W')` (onde `'W'` quer dizer "week" - "semana"):

```
import numpy as np

user_activity['cohort_lifetime'] = (
    user_activity['activity_week'] - user_activity['first_activity_week']
)
user_activity['cohort_lifetime'] = user_activity[
    'cohort_lifetime'
] / np.timedelta64(1, 'W')
user_activity['cohort_lifetime'] = user_activity['cohort_lifetime'].astype(int)
```

Vamos agrupar os dados por coorte e tempo de vida e descobrir a quantidade de usuários ativos em determinada semana de tempo de vida para cada coorte:

```
cohorts = (
    user_activity.groupby(['first_activity_week', 'cohort_lifetime'])
    .agg({'user_id': 'nunique'})
    .reset_index()
)
```

Para calcular a taxa de retenção, primeiro precisamos recuperar a quantidade inicial de usuários em uma coorte e dividi-la pela quantidade de usuários para cada semana seguinte.

Vamos descobrir a quantidade inicial de usuários na coorte observando a 4ª semana:

```
initial_users_count = cohorts[cohorts['cohort_lifetime'] == 0][
    ['first_activity_week', 'user_id']
]
print(initial_users_count)
```

```
 first_activity_week  user_id
0           2019-04-29    22573
6           2019-05-06    40911
12          2019-05-13    39683
18          2019-05-20    38648
24          2019-05-27    31064
29          2019-06-03     7075
33          2019-06-10     2642
36          2019-06-17     1056
38          2019-06-24      281
```

Usando o método **rename()**, vamos renomear a coluna `user_id` como `cohort_users`. Então vamos passar um dicionário para o parâmetro `columns`, em que a chave é o nome antigo da coluna e o valor é seu novo nome.

```
initial_users_count = initial_users_count.rename(
    columns={'user_id': 'cohort_users'}
)
```

Vamos juntar os dados de coortes com a quantidade inicial de usuários na coorte. Vamos usar o método `merge()` desta vez, ao invés de `join()`. Os dois métodos são muito similares, mas `join()` junta colunas na tabela esquerda com as chaves-índice na tabela direita, enquanto `merge()` irá juntar colunas à esquerda com as coluna à direita:

```
cohorts = cohorts.merge(initial_users_count, on='first_activity_week')
```

Agora estamos prontos para calcular a taxa de retenção. Divida a quantidade de usuários ativos em cada semana pela quantidade inicial de usuários na coorte:

```
cohorts['retention'] = cohorts['user_id'] / cohorts['cohort_users']
```

Então compilamos uma tabela dinâmica e fazemos um mapa de calor:

```
retention_pivot = cohorts.pivot_table(
    index='first_activity_week',
    columns='cohort_lifetime',
    values='retention',
    aggfunc='sum',
)

import seaborn as sns
from matplotlib import pyplot as plt

sns.set(style='white')
plt.figure(figsize=(13, 9))
plt.title('Cohorts: User Retention')
sns.heatmap(
    retention_pivot, annot=True, fmt='.1%', linewidths=1, linecolor='gray'
)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.3.8.3PT_1655467964.png)

A taxa de retenção da primeira semana está decaindo para cada nova coorte. Enquanto a taxa de retenção da primeira semana, que abrangeu os dias entre 29 de abril e 5 de maio - inclusive, foi de 66,.9%, para a coorte de 10-16 de junho, ela foi de 24,3%. Então, em seis semanas a taxa de retenção caiu 45,6 pontos percentuais. Isso quer dizer que o comportamento dos usuários está mudando com cada nova coorte. Talvez fontes de tráfego tenham sido mal escolhidas, ou o trabalho do time de suporte esteja piorando.

Calculando Taxas de Retenção em Python

Tarefa3 / 3

1.

O DataFrame `work_user_activity` contém dados da atividade de usuários para um aplicativo móvel de encontros para gênios chamado Minder. Ele tem as seguintes colunas:

-   `first_activity_date` (a data em que os usuários escreveram pela primeira vez para seu primeiro match)
-   `activity_date` (todas as datas em que o usuário escreveu para matches pela primeira vez)
-   `user_id` (o ID dos usuários)

Crie duas colunas no DataFrame `user_activity`.

-   `activity_week` (a semana em que um chat começou)
-   `first_activity_week` (a semana em que o primeiro chat começou)

Para recuperar a semana, pegue o primeiro dia da semana em que o evento ocorreu.

Você não precisa imprimir nada.

2.

Crie a coluna `cohort_lifetime` no DataFrame `user_activity` para armazenar a quantidade de semanas (como um número inteiro) entre a semana em que um chat começou (`activity_week`) e a semana em que o primeiro chat começou (`first_activity_week`).

3.

Resolva essa tarefa de seis partes:

1.  Construa um DataFrame `cohorts` com as seguintes colunas:
    
    -   `first_activity_week` (com coortes baseadas na primeira atividade do usuário)
    -   `cohort_lifetime` (com semanas de tempo de vida)
    -   `user_id` (com a quantidade de usuários ativos)
    
2.  Construa o dataFrame `initial_users_count`, em que cada coorte terá a quantidade de usuários na semana zero de tempo de vida. Deixe apenas essas colunas no DataFrame:
    
    -   Semana da coorte — `first_activity_week`
    -   A quantidade de usuários ativos — `user_id`
    
3.  No DataFrame `initial_users_count` renomeie a coluna `user_id` como `cohort_users`. Ela vai armazenar a quantidade inicial de usuários em uma coorte.
    
4.  Junte os Dataframes `cohorts` e `initial_users_count` na coluna com coortes semanais e salve o resultado como `cohorts`.
    
5.  Calcule a taxa de retenção na coluna `retention`.
    
6.  Imprima a tabela dinâmica resultante `retention_pivot`, que irá refletir as alterações em `retention` por coorte (`first_activity_week`), dependendo do tempo de vida.
    

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

import pandas as pd

import numpy as np

  

user\_activity \= pd.read\_csv('/datasets/work\_user\_activity.csv')

user\_activity\['activity\_date'\] \= pd.to\_datetime(user\_activity\['activity\_date'\])

first\_activity\_date \= user\_activity.groupby(\['user\_id'\])\['activity\_date'\].min()

first\_activity\_date.name \= 'first\_activity\_date'

user\_activity \= user\_activity.join(first\_activity\_date, on\='user\_id')

user\_activity\['activity\_week'\] \= pd.to\_datetime(

user\_activity\['activity\_date'\], unit\='d'

) \- pd.to\_timedelta(user\_activity\['activity\_date'\].dt.dayofweek, unit\='d')

user\_activity\['first\_activity\_week'\] \= pd.to\_datetime(

user\_activity\['first\_activity\_date'\], unit\='d'

) \- pd.to\_timedelta(

user\_activity\['first\_activity\_date'\].dt.dayofweek, unit\='d'

)

user\_activity\['cohort\_lifetime'\] \= (

user\_activity\['activity\_week'\] \- user\_activity\['first\_activity\_week'\]

)

user\_activity\['cohort\_lifetime'\] \= user\_activity\[

'cohort\_lifetime'

\] / np.timedelta64(1, 'W')

user\_activity\['cohort\_lifetime'\] \= user\_activity\['cohort\_lifetime'\].astype(int)

  

cohorts \= (

user\_activity.groupby(\['first\_activity\_week', 'cohort\_lifetime'\])

.agg({'user\_id': 'nunique'})

.reset\_index()

)

initial\_users\_count \= cohorts\[cohorts\['cohort\_lifetime'\] \== 0\]\[

\['first\_activity\_week', 'user\_id'\]

\]

initial\_users\_count \= initial\_users\_count.rename(

columns\={'user\_id': 'cohort\_users'}

)

cohorts \= cohorts.merge(initial\_users\_count, on\='first\_activity\_week')

cohorts\['retention'\] \= cohorts\['user\_id'\] / cohorts\['cohort\_users'\]

retention\_pivot \= cohorts.pivot\_table(

index\='first\_activity\_week',

columns\='cohort\_lifetime',

values\='retention',

aggfunc\='sum',

)

print(retention\_pivot)

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-17-497Z.md
### Última modificação: 2025-05-28 19:57:17

# Calculando índices de cancelamento em Python - TripleTen

Teoria

# Calculando índices de cancelamento em Python

Você sabe como encontrar a taxa de retenção em Python. Agora vamos falar sobre o índice de cancelamento.

Vamos criar coortes baseados nas datas das primeiras atividades dos jogadores de um jogo online:

```
import pandas as pd
import numpy as np

user_activity = pd.read_csv('user_activity.csv')
user_activity['activity_date'] = pd.to_datetime(user_activity['activity_date'])

first_activity_date = user_activity.groupby(['user_id'])['activity_date'].min()
first_activity_date.name = 'first_activity_date'
user_activity = user_activity.join(first_activity_date, on='user_id')

user_activity['activity_week'] = pd.to_datetime(
    user_activity['activity_date'], unit='d'
) - pd.to_timedelta(user_activity['activity_date'].dt.dayofweek, unit='d')
user_activity['first_activity_week'] = pd.to_datetime(
    user_activity['first_activity_date'], unit='d'
) - pd.to_timedelta(
    user_activity['first_activity_date'].dt.dayofweek, unit='d'
)

user_activity['cohort_lifetime'] = (
    user_activity['activity_week'] - user_activity['first_activity_week']
)
user_activity['cohort_lifetime'] = user_activity[
    'cohort_lifetime'
] / np.timedelta64(1, 'W')
user_activity['cohort_lifetime'] = user_activity['cohort_lifetime'].astype(int)

cohorts = (
    user_activity.groupby(['first_activity_week', 'cohort_lifetime'])
    .agg({'user_id': 'nunique'})
    .reset_index()
)

inital_users_count = cohorts[cohorts['cohort_lifetime'] == 0][
    ['first_activity_week', 'user_id']
]
inital_users_count = inital_users_count.rename(
    columns={'user_id': 'cohort_users'}
)
cohorts = cohorts.merge(inital_users_count, on='first_activity_week')
```

Encontrar o índice de cancelamento é fácil: nós só precisamos comparar o número de usuários do período anterior. Nós podemos usar o método **pct\_change()** (porcentagem de mudança). Ele encontra a porcentagem de mudança em uma coluna comparando com o valor na linha anterior. Se você aplicar esse método junto com o agrupamento, ele irá rodar dentro do grupo.

Para coortes, _pct\_change()_ pode nos ajudar a comparar o número de usuários com o número no período de vida anterior:

```
cohorts['churn_rate'] = cohorts.groupby(['first_activity_week'])['user_id'].pct_change()
```

Vamos construir a tabela dinâmica e visualizar o resultados de um mapa de calor:

```
churn_pivot = cohorts.pivot_table(
    index='first_activity_week',
    columns='cohort_lifetime',
    values='churn_rate',
    aggfunc='sum',
)

sns.set(style='white')
plt.figure(figsize=(13, 9))
plt.title('Coortes: Índice de Cancelamento')
sns.heatmap(churn_pivot, annot=True, fmt='.1%', linewidths=1, linecolor='gray')
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.3.9PT_1655467989.png)

O índice de cancelamento da primeira semana está aumentando, o que significa que estamos perdendo mais e mais usuários a cada nova coorte. Isso é um sinal de que você precisa focar na retenção do usuário durante a primeira semana, já que a maioria dos usuários ativos está desistindo.

Na segunda semana o índice cai para todos as coortes, mas depois cresce rapidamente na quarta semana. Isso significa que durante essa semana usuários encontraram problemas que têm um impacto crítico no cancelamento. Isso pode ser porque a dificuldade do jogo aumenta, ou talvez usuários estão perdendo a motivação por alguma outra razão.

Calculando índices de cancelamento em Python

Tarefa4 / 4

1.

O arquivo localizado em `/datasets/churn_rate.csv` contém dados sobre grupos do jogo Cloud Wars:

-   `users_count` (o número de usuários em uma coorte)
-   `lifetime` (o tempo de vida de uma coorte)
-   `first_event_week` (a semana da coorte)

Importe os dados de `churn_rate.csv`para o DataFrame `cohorts`.

Imprima as primeiras 10 linhas do DataFrame `cohorts`.

2.

Crie a coluna `churn_rate` no DataFrame `cohorts` para calcular o índice de cancelamento para cada coorte.

Imprima as primeiras 10 linhas do DataFrame.

3.

Compile a tabela dinâmica `churn_pivot`, de modo que as linhas contenham as coortes, as colunas tenham as semanas de tempo de vida e os valores sejam os índices de cancelamento.

Imprima a tabela dinâmica.

4.

Faça um mapa de calor baseado em `churn_pivot` ilustrando mudanças no tamanho médio de compra na coorte.

Defina o tamanho da figura: `plt.figure(figsize=(13, 9))`. Nomeie a visualização como `'Índice de cancelamento'`.

Certifique-se de que:

-   Os valores são expostos nas células do mapa de calor
-   As porcentagens são impressas com uma casa decimal
-   A linha separando as células tenha 1 pixel
-   A cor da linha seja preta (`'black'`).

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

import pandas as pd

import seaborn as sns

from matplotlib import pyplot as plt

  

cohorts \= pd.read\_csv('/datasets/churn\_rate.csv')

  

cohorts\['churn\_rate'\] \= cohorts.groupby(\['first\_event\_week'\])\[

'users\_count'

\].pct\_change()

  

churn\_pivot \= cohorts.pivot\_table(

index\='first\_event\_week',

columns\='lifetime',

values\='churn\_rate',

aggfunc\='sum',

)

  

  

  

sns.set(style\='white')

plt.figure(figsize\=(13, 9))

plt.title('Índice de cancelamento')

sns.heatmap(churn\_pivot, annot\=True, fmt\='.1%', linewidths\=1, linecolor\='black')

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-18-806Z.md
### Última modificação: 2025-05-28 19:57:19

# Coortes Comportamentais - TripleTen

Teoria

# Coortes Comportamentais

**Análise de coortes comportamentais** é um tipo de análise que envolve o isolamento de uma coorte de usuários que realizaram uma ação ou uma sequência de ações durante um período de tempo determinado. O objetivo é analisar como uma métrica de objetivo (como taxa de retenção) mudou ao longo do tempo para a coorte comportamental.

Os usuários do Y.Music podem "Curtir" suas músicas favoritas. Mas isso significa que eles estão inclinados a usar o aplicativo novamente? Para responder a essa pergunta, você irá precisar isolar uma coorte comportamental de usuários que "curtiram" pelo menos uma música durante sua primeira semana de atividade. Então você precisa avaliar a taxa de retenção para esses usuários e compará-la com a mesma métrica para a coorte daqueles que não "curtiram" nenhuma faixa durante a primeira semana de atividade. É bem provável que os resultados sejam diferentes.

Digamos que você é analista para o app Neuropicasso, que permite a criação de fotos estranhas com o uso de uma rede neural. Você precisa descobrir se consultas frequentes com a equipe de Suporte durante a semana zero de tempo de vida têm algum impacto na retenção dos usuários.

Vamos começar analisando:

```
import pandas as pd

events = pd.read_csv('/datasets/behavioral_cohorts.csv')
print(events.head())
```

```
   user_id     event_type              event_datetime
0   175893   registration  2019-07-31 09:35:40.988574
1   175893  choice_option  2019-07-31 09:36:29.045261
2   175893           help  2019-07-31 09:36:31.992824
3   175893           help  2019-07-31 09:36:34.990551
4   175893   registration  2019-07-31 09:36:55.989291
```

-   `user_id` — o ID do usuário
-   `event_type` — o tipo de ação realizada
-   `event_datetime` — a data e horário do evento

Vamos formar uma coorte com base no tipo de evento _Suporte_.

Vamos converter `event_datetime` para o tipo `datetime`:

```
events['event_datetime'] = pd.to_datetime(events['event_datetime'])
```

Para cada usuário, vamos obter a data do primeiro evento e adicionar esses dados ao DataFrame `events`.

```
min_event_datetime = events.groupby(['user_id'])['event_datetime'].min()
min_event_datetime.name = 'min_event_datetime'
events = events.join(min_event_datetime,on='user_id')
```

A coorte será baseada em um evento realizado durante a semana zero de tempo de vida. Devemos calcular um parâmetro que tornará possível determinar o tempo decorrido entre cada evento e o primeiro de todos.

Vamos criar uma nova coluna, `time_to_event`, para armazenar o período de tempo entre o evento observado e o primeiro evento realizado por um usuário:

```
events['time_to_event'] = events['event_datetime'] - events['min_event_datetime']
```

Você precisa somente dos eventos `help` (suporte) realizados menos de sete dias após o primeiro evento. Veja com as fatias de dados `timedelta` são aplicadas: você pode contar apenas os dias. Isso também irá funcionar com outras unidades de tempo (minutos, por exemplo).

```
filtered_events = events[(events['event_type'] == 'help') & (events['time_to_event'] < '7 days')]
```

Vamos agrupar o DataFrame por usuário e descobrir a quantidade de ações realizadas:

```
count_events_by_users = filtered_events.groupby(['user_id']).agg({'event_datetime':'count'}).reset_index()
```

Agora vamos descobrir a o número mediano de visitas à seção de suporte durante a semana zero de tempo de vida:

```
count_events_by_users['event_datetime'].median()
```

```
5
```

Metade dos usuários que visitaram a seção de suporte pelo menos uma vez acabaram repetindo essa ação mais de cinco vezes.

Vamos dividir os usuários baseado no comportamento alvo, definindo o limite como cinco visitas.

```
count_events_by_users['is_target_behavior'] = count_events_by_users['event_datetime'] > 5
    
user_ids_with_target_behavior = count_events_by_users[count_events_by_users['is_target_behavior']]['user_id']
user_ids_without_target_behavior = count_events_by_users[~count_events_by_users['is_target_behavior']]['user_id']
```

Vamos acabar com três grupos de usuários em nosso DataFrame principal:

-   Usuários que solicitaram suporte mais do que cinco vezes—esses serão nossos usuários alvo na coorte comportamental
-   Usuários que solicitaram suporte menos do que cinco vezes
-   Usuários que nunca solicitaram suporte

```
events.loc[events['user_id'].isin(user_ids_with_target_behavior), 'is_in_behavioral_cohort'] = 'yes'
events.loc[events['user_id'].isin(user_ids_without_target_behavior), 'is_in_behavioral_cohort'] = 'no'
events['is_in_behavioral_cohort'] = events['is_in_behavioral_cohort'].fillna('no_behavior')
```

Vamos ver como esses grupos variam em tamanho:

```
print(events.groupby('is_in_behavioral_cohort')['user_id'].nunique())
```

```
is_in_behavioral_cohort
no             51
no_behavior    36
yes            50
Name: user_id, dtype: int64
```

Vamos analisar como a taxa de retenção varia para cada coorte comportamental ao longo do tempo. Vamos criar coortes semanais e calcular o tempo de vida de referência semanal para os usuários do aplicativo:

```
import numpy as np

events['event_week'] = pd.to_datetime(
    events['event_datetime'].dt.date
) - pd.to_timedelta(events['event_datetime'].dt.dayofweek, unit='d')
events['min_event_week'] = pd.to_datetime(
    events['min_event_datetime'].dt.date
) - pd.to_timedelta(events['min_event_datetime'].dt.dayofweek, unit='d')

events['cohort_lifetime'] = events['event_week'] - events['min_event_week']
events['cohort_lifetime'] = events['cohort_lifetime'] / np.timedelta64(1, 'W')
events['cohort_lifetime'] = events['cohort_lifetime'].astype(int)
```

Agora vamos criar uma função para calcular e imprimir a taxa de retenção de acordo com o tempo de vida:

```
def printRetentionRate(df):
    cohorts = (
        df.groupby(['min_event_week', 'cohort_lifetime'], as_index=False)
        .agg({'user_id': 'nunique'})
        .sort_values(['min_event_week', 'cohort_lifetime'])
    )

    inital_users_count = cohorts[cohorts['cohort_lifetime'] == 0][
        ['min_event_week', 'user_id']
    ]
    inital_users_count = inital_users_count.rename(
        columns={'user_id': 'cohort_users'}
    )

    cohorts = cohorts.merge(inital_users_count, on='min_event_week')

    cohorts['retention'] = cohorts['user_id'] / cohorts['cohort_users']

    print(cohorts.groupby(['cohort_lifetime'])['retention'].mean())
    cohorts.groupby(['cohort_lifetime'])['retention'].mean().plot.bar()
```

Vamos descobrir a taxa de retenção para aqueles que nunca solicitaram suporte:

```
printRetentionRate(events[events['is_in_behavioral_cohort'] == 'no_behavior'])
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_6_1586590804.png)

Agora calcule a taxa de retenção para usuários que solicitaram suporte mais de uma vez, mas menos do que cinco vezes.

```
printRetentionRate(events[events['is_in_behavioral_cohort'] == 'no'])
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_7_1586590835.png)

Vamos comparar a taxa de retenção para esses dois grupos. Começando a partir da segunda semana, a taxa de retenção para aqueles que solicitaram suporte é significativamente maior.

Agora dê uma olhada nos usuários da nossa coorte alvo que solicitaram suporte mais de cinco vezes:

```
printRetentionRate(events[events['is_in_behavioral_cohort'] == 'yes'])
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_8_1586590883.png)

Agora dê uma olhada nos usuários da nossa coorte alvo que solicitaram suporte mais de cinco vezes:

Usuários que solicitaram suporte durante a semana zero apresentaram uma tendência a usar o aplicativo novamente. Talvez seja porque eles tenham enfrentado menos dificuldades depois disso. Vale a pena considerar como encorajar os usuários a pedir ajuda durante a semana zero de tempo de vida.

Coortes Comportamentais

Tarefa6 / 6

1.

O arquivo localizado em `/datasets/coffee_home.csv` contém dados sobre um aplicativo de uma cafeteria chamado CoffeeHome:

-   `coffee_time` — o horário em que um café foi pedido
-   `user_id` — a identificação do cliente
-   `first_coffee_datetime` — a data e horário em que um café foi pedido pela primeira vez (utilizadas para formar coortes - `first_coffee_week`)
-   `order_id` — a identificação do pedido
-   `coffee_week` — a semana na qual o café foi pedido
-   `first_coffee_week` — a semana na qual o café foi pedido pela primeira vez (coorte semanal)
-   `cohort_lifetime` — o período de tempo de vida

Os clientes da cafeteria usam o aplicativo toda vez que eles pedem um café. A CoffeeHome tem uma oferta especial: se você pedir cinco cafés em um período de 30 dias a partir de sua primeira compra, você ganha uma café de graça. Teste a hipótese de que os clientes que ganham um café de graça retornam com mais frequência e apresentam taxas de retenção mais elevadas.

Importe os dados do arquivo `coffee_home.csv` para o DataFrame `events`.

Converta os dados nas colunas `coffee_time` e `first_coffee_datetime` para o tipo `datetime`.

Imprima as 10 primeiras linhas do DataFrame `events`.

2.

No DataFrame `events`, crie uma coluna `time_to_event`para armazenar a diferença entre a data e o horário nas quais um café foi pedido da data e do horário da primeira compra.

Faça fatias de dados, `filtered_events`, com todos intervalos de tempo menores do que 30 dias a partir da data do primeiro pedido.

Imprima as 10 primeiras linhas da fatia de dados `filtered_events`.

3.

Construa o DataFrame `count_events_by_users` e calcule a quantidade de registros de compra para cada cliente da fatia de dados `filtered_events`.

Crie a coluna `is_target_behavior` no DataFrame `count_events_by_users`. Marque clientes que pediram café mais do que quatro vezes com `True`.

Imprima as 10 primeiras linhas do DataFrame `count_events_by_users`.

4.

Crie o objeto Series `user_ids_with_target_behavior` para armazenar clientes com o número alvo de pedidos (mais que quatro).

Crie o objeto Series `user_ids_without_target_behavior` para armazenar os clientes restantes.

No DataFrame `events`, crie a coluna `is_in_behavioral_cohort` para armazenar um dos dois valores, de acordo com o identificador do consumidor (`user_id`):

-   `yes` se o `user_id` estiver em `user_ids_with_target_behavior`
-   `no` se o `user_id` estiver em `user_ids_without_target_behavior`

Imprima as 10 primeiras linhas do DataFrame `events`.

5.

Chame a função `printRetentionRate()` para imprimir dados da taxa de retenção média por cada período de tempo de vida para clientes que **não entraram** na coorte comportamental daqueles que pediram um café mais do que quatro vezes no decorrer dos primeiros 30 dias.

6.

Chame a função `printRetentionRate()` para imprimir os dados de taxa de retenção média por cada período de tempo de vida para clientes que **conseguiram entrar** na coorte comportamental de quem pediu um café mais de quatro vezes nos primeiros 30 dias.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

import pandas as pd

  

events \= pd.read\_csv('/datasets/coffee\_home.csv')

events\['coffee\_time'\] \= pd.to\_datetime(events\['coffee\_time'\])

events\['first\_coffee\_datetime'\] \= pd.to\_datetime(

events\['first\_coffee\_datetime'\]

)

  

events\['time\_to\_event'\] \= (

events\['coffee\_time'\] \- events\['first\_coffee\_datetime'\]

)

filtered\_events \= events\[events\['time\_to\_event'\] < '30 days'\]

  

count\_events\_by\_users \= (

filtered\_events.groupby(\['user\_id'\])

.agg({'coffee\_time': 'count'})

.reset\_index()

)

count\_events\_by\_users\['is\_target\_behavior'\] \= (

count\_events\_by\_users\['coffee\_time'\] \> 4

)

  

user\_ids\_with\_target\_behavior \= count\_events\_by\_users.query(

'is\_target\_behavior == True'

)\['user\_id'\].unique()

user\_ids\_without\_target\_behavior \= count\_events\_by\_users.query(

'is\_target\_behavior != True'

)\['user\_id'\].unique()

  

events.loc\[

events\['user\_id'\].isin(user\_ids\_with\_target\_behavior),

'is\_in\_behavioral\_cohort',

\] \= 'yes'

events.loc\[

events\['user\_id'\].isin(user\_ids\_without\_target\_behavior),

'is\_in\_behavioral\_cohort',

\] \= 'no'

  

  

def printRetentionRate(df):

cohorts \= (

df.groupby(\['first\_coffee\_week', 'cohort\_lifetime'\], as\_index\=False)

.agg({'user\_id': 'nunique'})

.sort\_values(\['first\_coffee\_week', 'cohort\_lifetime'\])

)

  

inital\_users\_count \= cohorts\[cohorts\['cohort\_lifetime'\] \== 0\]\[

\['first\_coffee\_week', 'user\_id'\]

\]

inital\_users\_count \= inital\_users\_count.rename(

columns\={'user\_id': 'cohort\_users'}

)

  

cohorts \= cohorts.merge(inital\_users\_count, on\='first\_coffee\_week')

  

cohorts\['retention'\] \= cohorts\['user\_id'\] / cohorts\['cohort\_users'\]

  

print(cohorts.groupby(\['cohort\_lifetime'\])\['retention'\].mean())

cohorts.groupby(\['cohort\_lifetime'\])\['retention'\].mean().plot.bar()

  

printRetentionRate(events\[events\['is\_in\_behavioral\_cohort'\] \== 'yes'\])

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-57-20-132Z.md
### Última modificação: 2025-05-28 19:57:20

# Conclusão - TripleTen

Capítulo 3/9

Análise de Coorte

# Conclusão

Esse capítulo foi bem difícil, mas você se saiu bem! Você evoluiu bastante:

-   Você aprendeu o que são coortes
-   Você aprendeu como criar coortes em Python
-   Você aprendeu diferentes modos de analisar coortes
-   Você estudou retenção e índice de cancelamento e como calculá-los em Python
-   Você aprendeu sobre coortes comportamentais

### Leve com você

Faça o download da [folha conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_7/PT/Additional_materials/moved_DA_7_3_Folha_de_Concluses_Anlise_de_Coorte.pdf) e do [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_7/PT/Additional_materials/moved_DA7_3_Resumo_do_Captulo_Anlise_de_Coorte.pdf) para que você possa consultá-los quando necessário.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-58-51-615Z.md
### Última modificação: 2025-05-28 19:58:52

# Introdução - TripleTen

Capítulo 4/9

Unit Economics (Economia Unitária)

# Introdução

Neste capítulo, vamos discutir quanto um negócio ganha e perde em cada compra.

### O que você irá aprender:

-   Como diagnosticar a saúde de um negócio usando economia unitária
-   Como determinar qual o volume de vendas que um negócio precisa para se tornar lucrativo
-   Como calcular o valor de tempo de vida do cliente e usar essa métrica na tomada de decisão

### Quanto vai demorar:

4 aulas, aproximadamente 10-20 minutos cada

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-58-52-961Z.md
### Última modificação: 2025-05-28 19:58:53

# Economia Unitária por Venda - TripleTen

Capítulo 4/9

Unit Economics (Economia Unitária)

# Economia Unitária por Venda

Pediram para você analisar as vendas da loja de bicicletas online Ryde My Byke. A loja está sofrendo perdas todos os meses e já está quase sem dinheiro. O que o proprietário deve fazer com o negócio?

**Receita:** A loja vende em média 50 bicicletas por mês e obtém uma receita de $10,000.

**Custos:** O procurement custa $5,000 por mês, a expedição dos produtos $500, o marketing custa um total de $3,000, o aluguel é $1,000, e o pagamento dos salários custa $2,000.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.4.2PT_1655468016.png)

A loja está realmente sofrendo perdas. Você precisa achar uma maneira de alavancar as vendas e cortar gastos (e decidir quais gastos cortar). Seu primeiro impulso é cortar o orçamento com marketing? Pense bem — isso pode acarretar em uma queda nas vendas.

Vamos analisar com mais atenção as finanças da loja. Quais custos são variáveis e quais são fixos?

**Custos variáveis** estão diretamente ligadas ao volume de vendas. Para vender 20 bicicletas, você primeiro tem que comprar 20.

**Custos fixos** não mudam de acordo com a receita. Não importa quanto dinheiro você ganhe, a quantia que você paga de aluguel não será alterada.

Quais custos aqui são variáveis?

-   Procurement (quanto mais quisermos vender, mais vamos ter que comprar)
-   Expedição e manuseio dos produtos (as bicicletas precisam ser entregues aos consumidores)
-   Marketing (quanto mais anunciamos, mais vendemos).

As despesas fixas são:

-   Salários (Vamos assumir que a loja não irá precisar contratar novos funcionários no futuro próximo, mesmo que as vendas aumentem)
-   Aluguel (a despesa é igual todos os meses)

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.4.2.2PT_1655468027.png)

Pergunta

Quais despesas são fixas?

John vendeu cinco bicicletas hoje. Ele precisa imprimir listas de embalagens para os clientes, mas a empresa fornecedora de energia elétrica cortou a **eletricidade** devido a atrasos. Agora o gerente do escritório precisa conversar com a companhia elétrica enquanto o John corre pra o escritório vizinho para imprimir as listas.

A energia elétrica é necessária para o funcionamento normal da loja. Todo mês (independente do volume de vendas) a empresa paga o aluguel e serviços, incluindo energia elétrica.

John é o Funcionário do Mês, tendo vendido 30 bicicletas. A sorte sorriu para ele... assim como a Anna da contabilidade. Ela deu um **bônus** para ele.

Mike, o motorista, não conseguiu entregar uma bicicleta. Como Funcionário do Mês, John decidiu entregá-la pessoalmente. Depois ele procurou Anna, a contadora, e pediu para ela um reembolso pelo **combustível**. Desta vez não houve espaço para sorrisos; Anna estava ocupada pensando sobre como eliminar essa despesa.

Fantástico!

Mas e se a loja online vender apenas uma bicicleta, e não 50? Vamos dividir a receita e os custos variáveis por 50. Os custos fixos irão permanecer iguais.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.4.2.4PT_1655468054.png)

O prejuízo total do negócio aumentou de $1,500 para $2,970. A receita por unidade (sem contar os custos fixos) é $30.

Para cobrir os custos fixos de $3,000, a loja precisa vender 100 bicicletas por mês: $30 x 100 = $3,000. (Se vender mais, o negócio se tornará rentável).

Se forem vendidas 200 bicicletas por mês, esses serão os números:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.4.2.3PT_1655468044.png)

Para ganhar mais dinheiro, a empresa precisa aumentar o número de unidades vendidas. O importante é calcular o quanto será necessário aumentar.

Esse modo de abordar a análise chama-se **economia unitária** . O objetivo é encontrar a quantidade de vendas unitárias necessárias para tornar o negócio lucrativo.

Você irá aprender a executar cálculos por venda na próxima lição.

Pergunta

Sam abriu uma cafeteria hipster com um fogão a lenha. Ele oferece a sua clientela uma deliciosa sopa de cebola francesa com pão de alho. Na primeira semana, Sam vendeu 500 porções e teve uma receita de $1,500.

Despesas do Sam:

-   $600 em ingredientes para a sopa
-   $400 em ingredientes para o pão
-   $300 em aluguel (e lenha — uma quantidade fixa)
-   $500 em salários para os caixas e cozinheiros

O prejuízo foi de $300. Determine quantas porções precisam ser vendidas para atingir o ponto de equilíbrio (não ter nem lucro e nem prejuízo) na semana seguinte.

530

750

800

Muito bem!

950

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-58-54-273Z.md
### Última modificação: 2025-05-28 19:58:54

# Economia Unitária por Venda: Construindo um Modelo - TripleTen

Teoria

# Economia Unitária por Venda: Construindo um Modelo

Antes de começarmos a fazer cálculos por venda, vamos conversar sobre a avaliação da saúde de um negócio:

1.  Descubra a margem de lucro por venda do negócio. Considere apenas os custos variáveis.
2.  Se a margem de lucro é negativa, as finanças da companhia estão fora de ordem. Ela precisa cortar custos ou aumentar a receita. Senão, o fim está próximo.
3.  Se a margem de lucro é positiva em relação aos custos variáveis, adicione os custos fixos. Calcule o volume de vendas necessário para que o negócio se torne lucrativo.

Vamos calcular o volume de vendas da Ride My Bike.

Vamos declarar as seguintes variáveis.

-   `items_sold` para a quantidade de itens vendidos
-   `revenue` para a receita
-   `var_costs` para custos variáveis
-   `fixed_costs` para custos fixos

Vamos registrar os dados das despesas em um objeto `Series`.

```
items_sold = 50
revenue = 10000

var_costs = pd.Series({
    'procurement': 5000,
    'delivery':  500,
    'marketing': 3000
})

fixed_costs = pd.Series({
    'rent':      1000,
    'salaries':  2000
})
```

Vamos calcular os custos totais e a receita da empresa:

```
total_costs = var_costs.sum() + fixed_costs.sum()

print('Receita:', revenue)
print('Custos:', total_costs)
print('Total:', revenue - total_costs)

Revenue: 10000
Costs: 11500
Total: -1500
```

Um prejuízo de $1,500: é o que estamos esperando.

Agora vamos analisar a economia unitária. Sem considerar os custos fixos, vamos dividir a receita e os custos variáveis pela quantidade de bicicletas vendidas.

```
one_unit_revenue = revenue / items_sold
one_unit_var_costs = var_costs / items_sold

print('Custos variáveis por venda:')
print(one_unit_var_costs)

Custos variáveis por venda:
cogs          100.0
delivery      10.0
marketing     60.0
```

Vamos calcular a receita, custos e lucro por venda:

```
print('Receita:', one_unit_revenue)
print('Custos:', one_unit_var_costs.sum())
print('Total:', one_unit_revenue - one_unit_var_costs.sum())

Revenue: 200.0
Costs: 170.0
Profit: 30.0
```

O lucro de cada venda é $30. Os negócios podem e devem aumentar!

Agora vamos construir um modelo para calcular o volume de vendas que permitirá qua a loja cubra todas seus custos fixos.

Vamos descobrir de qual fator todos os outros dependem. O custo primário e as despesas com expedição dependem do número de itens vendidos, já as vendas dependem dos custos com marketing. Esses fatores não causam nenhum impacto nos custos fixos. Como resultado, chegamos à seguinte dependência: quanto mais gastamos com marketing, mais nós vendemos, gerando aumento da receita e dos custos.

Vamos construir um modelo:

1.  Marketing = $3,000 (das condições iniciais)
2.  Custos com marketing / custos para atrair um pedido = quantidade de itens vendidos
3.  Quantidade de itens vendidos ∗ preço por item = receita
4.  Quantidade de itens vendidos ∗ custos associados = custos variáveis
5.  Custos fixos = $3,000 (das condições iniciais)
6.  Receita - custos variáveis - custos fixos = lucro.

```
marketing = 3000

items_sold = marketing / one_unit_var_costs['marketing']
revenue = one_unit_revenue  * items_sold
var_costs = one_unit_var_costs * items_sold

print(revenue - sum(var_costs) - sum(fixed_costs))
```

Como em nosso modelo o orçamento do marketing é $3,000, temos um prejuízo de $1,500 (como na tarefa da lição anterior).

O que acontece se cortarmos ou aumentarmos o orçamento do marketing? Vamos testar diversos cenários.

Vamos substituir o orçamento do marketing com valores entre 0 e $10,000. Vamos escrever um script na função:

```
def unit_economics(marketing):    
    items_sold = marketing / one_unit_var_costs['marketing']
    revenue = one_unit_revenue * items_sold
    var_costs = one_unit_var_costs * items_sold
    return revenue - sum(var_costs) - sum(fixed_costs)

for m in range(0, 10000, 1000):
    print('Profit/loss: {} com um orçamento de {}'.format(unit_economics(m), m))

Profit/loss: -3000.0 with a budget of 0
Profit/loss: -2500.0 with a budget of 1000
Profit/loss: -2000.0 with a budget of 2000
Profit/loss: -1500.0 with a budget of 3000
Profit/loss: -1000.0 with a budget of 4000
Profit/loss: -500.0 with a budget of 5000
Profit/loss: 0.0 with a budget of 6000
Profit/loss: 500.0 with a budget of 7000
Profit/loss: 1000.0 with a budget of 8000
Profit/loss: 1500.0 with a budget of 9000
```

O negócio irá atingir o ponto de equilíbrio mesmo com um orçamento de marketing de $6,000. Com $9,000, haverá um lucro de $1,500.

Economia Unitária por Venda: Construindo um Modelo

Tarefa3 / 3

1.

O estúdio Peacegaming lançou um jogo chamado _World of Tractors_. Jogadores do mundo todo competem em uma excitante colheita de grãos e legumes de uma plantação infinita. O que deve ser feito para tornar o jogo lucrativo?

Fatos:

-   Eles possuem uma audiência de 50.000 usuários.
-   Cada usuário paga uma média de $10 por mês.
-   O orçamento com marketing é de $200.000 por mês.
-   O suporte ao servidor custa $200.000 por mês (quanto maior a audiência, mais servidores são necessários).
-   O salário total dos programadores é de $300.000 por mês.
-   O aluguel do escritório é $150.000 por mês.

Armazene os dados em variáveis e calcule a receita, os custos totais e o lucro.

2.

Calcule a economia unitária para _World of Tractors_ sem considerar os custos fixos. Salve o resultado como `one_unit_var_costs`. Imprima a receita por venda, os custos variáveis por venda e o total.

3.

A economia está sadia. Construa um modelo para calcular o volume de vendas necessário para fazer com que o negócio se torne lucrativo. A quantidade de usuários irá aumentar se você aumentar os custos com marketing.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

import pandas as pd

  

n\_users \= 50000

revenue\_per\_user \= 10

  

var\_costs \= pd.Series({'marketing': 200000, 'servers': 200000})

  

fixed\_costs \= pd.Series({'salaries': 300000, 'rent': 150000})

  

revenue \= n\_users \* revenue\_per\_user

total\_costs \= var\_costs.sum() + fixed\_costs.sum()

  

one\_unit\_var\_costs \= var\_costs / n\_users

  

  

def unit\_economics(marketing):

items\_sold \= marketing / one\_unit\_var\_costs\['marketing'\]

revenue \= revenue\_per\_user \* items\_sold

var\_costs \= one\_unit\_var\_costs \* items\_sold

return revenue \- sum(var\_costs) \- sum(fixed\_costs)

\# escreva aqui seu código

  

for m in range(300000, 1500000, 100000):

print('Profit/loss: {} com um orçamento de {}'.format(unit\_economics(m), m))

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-58-57-098Z.md
### Última modificação: 2025-05-28 19:58:58

# Economia Unitária por Cliente para uma Loja Online - TripleTen

Teoria

# Economia Unitária por Cliente para uma Loja Online

Imagine que você está trabalhando em uma loja online que está crescendo rapidamente. As vendas (`gross_profit`) estão aumentando a cada mês, mas as despesas (`costs`) estão subindo ainda mais depressa, especialmente nos últimos tempos:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.4.5PT_1655468097.png)

O gráfico de perdas está assustador:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.4.5.2PT_1655468108.png)

Os especialistas em marketing têm certeza de que está tudo bem: os clientes irão voltar para fazer mais compras, e a loja se tornará lucrativa. Mas a gerência não está tão otimista. Eles querem cortar despesas e também demitir a equipe de marketing. Ajude-os a escolher qual a decisão certa em termos de análise.

Você tem dados sobre os pedidos e despesas da loja. Você também sabe que ela faz uma margem média de 50% nas vendas. Vamos analisar os dados. O arquivo "orders" (pedidos) contém registros de cada compra por um determinado período de tempo:

```
import pandas as pd

orders = pd.read_csv('/datasets/ltv_orders_1.csv')
orders.head()
```

```
             oid    order_date    revenue           uid
0    o_4034055635555    2018-10-01    2165    7767516515987
1    o_4534485003752    2018-11-08     790    7767516515987
2    o_0533312141074    2018-08-31    1655    7011763199088
3    o_4930211082105    2018-10-02    1779    7011763199088
4    o_7850020997261    2018-10-26    1934    7011763199088
```

Para cada pedido, temos seu identificador (`oid`), a data (`order_date`), receita (`revenue`), e o identificador do cliente (`uid`).

Vamos examinar a distribuição de datas dos pedidos:

```
print(orders['order_date'].describe())
```

```
count                   10501
unique                    296
top       2018-12-21 00:00:00
freq                       67
first     2018-03-09 00:00:00
last      2018-12-30 00:00:00
Name: order_date, dtype: object
```

Verificamos que a primeira compra foi realizada em 9 de março de 2018, e a última em 30 de dezembro do mesmo ano.

Agora vamos ver o arquivo que contém as despesas com marketing divididas por dia:

```
costs = pd.read_csv('/datasets/ltv_costs_1.csv')
costs.head()
```

```
        date    costs
0    2018-03-09    2706
1    2018-03-10    7008
2    2018-03-12    5102
3    2018-03-13    3101
4    2018-03-14    2925
```

Temos suas datas (`date`) e a quantia total (`costs`). Vamos dar uma olhada na distribuição:

```
print(costs['date'].describe())
```

```
count                     295
unique                    295
top       2018-04-03 00:00:00
freq                        1
first     2018-03-09 00:00:00
last      2018-12-30 00:00:00
```

O intevalo das datas com despesas coincide com o das datas com pedidos.

Ajuda bastante descobrir o LTV para diferentes coortes. Vamos rever o que precisamos fazer:

1.  Descubra a data da primeira compra de cada cliente
2.  Calcule a quantidade de novos clientes por cada data (ou intervalo de datas — neste caso, para cada mês)
3.  Inclua o mês do primeiro pedido na tabela de pedidos
4.  Agrupe a tabela e calcule a receita

Temos dados de quase um ano. Será melhor fazer coortes mensais. Vamos capturar os meses das datas nas tabelas de pedidos e despesas:

```
orders['order_month'] = orders['order_date'].astype('datetime64[M]')
costs['month'] = costs['date'].astype('datetime64[M]')
```

Agora vamos recuperar o mês da primeira compra de cada cliente:

```
first_orders = orders.groupby('uid').agg({'order_month': 'min'}).reset_index()
first_orders.columns = ['uid', 'first_order_month']
first_orders.head()
```

```
  uid     first_order_month
0    12505001352    2018-12-01
1    12551349712    2018-11-01
2    17395115257    2018-05-01
3    19935112191    2018-09-01
4    21100696320    2018-04-01
```

Vamos calcular a quantidade de novos clientes (`n_buyers`) para cada mês:

```
cohort_sizes = first_orders.groupby('first_order_month').agg({'uid': 'nunique'}).reset_index()
cohort_sizes.columns = ['first_order_month', 'n_buyers']
cohort_sizes.head()
```

```
 first_order_month    n_buyers
0    2018-03-01          61
1    2018-04-01          226
2    2018-05-01          276
3    2018-06-01          301
4    2018-07-01          307
```

Vamos construir coortes. Vamos acrescentar os meses das primeiras compras dos clientes à tabela de pedidos:

```
orders_ = pd.merge(orders,first_orders, on='uid')
orders_.head()
```

```
                 oid    order_date    revenue uid    order_month    first_order_month
0    o_4034055635555    2018-10-01    2165    7767516515987    2018-10-01    2018-10-01
1    o_4534485003752    2018-11-08     790    7767516515987    2018-11-01    2018-10-01
2    o_0533312141074    2018-08-31    1655    7011763199088    2018-08-01    2018-08-01
3    o_4930211082105    2018-10-02    1779    7011763199088    2018-10-01    2018-08-01
4    o_7850020997261    2018-10-26    1934    7011763199088    2018-10-01    2018-08-01
```

Agora vamos agrupar a tabela de pedidos por mês da primeira compra e mês de compra e somar a receita. Vamos ajustar o índice para o default usando o método `reset_index()`.

```
cohorts = orders_.groupby(['first_order_month','order_month']).agg({'revenue': 'sum'}).reset_index()
cohorts.head()
```

```
  first_order_month    order_month     revenue
0          2018-03-01     2018-03-01     118529
1         2018-03-01     2018-04-01      95905
2         2018-03-01     2018-05-01     124972
3         2018-03-01     2018-06-01     107273
4         2018-03-01     2018-07-01      70575
```

A linha da tabela `cohort` especifica a receita que cada coorte agregou. Por exemplo, na linha 0 podemos ver que os clientes que fizeram seu primeiro pedido em março de 2018 trouxeram uma receita total de $118,529 naquele mês.

A fim de efetuar uma análise de coorte, o LTV é uma receita acumulada de uma coorte, representativa da quantidade de pessoas na coorte. Vamos acrescentar dados de quantos usuários efetuaram suas primeiras compras em cada mês à tabela `coortes`:

```
report = pd.merge(cohort_sizes, cohorts, on='first_order_month')
report.head()
```

```
first_order_month    n_buyers    order_month    revenue
0       2018-03-01          61    2018-03-01    118529
1       2018-03-01          61    2018-04-01    95905
2       2018-03-01           61    2018-05-01    124972
3       2018-03-01          61    2018-06-01    107273
4       2018-03-01          61    2018-07-01    70575
```

Uma coluna com novos clientes já apareceu antes na tabela: `n_buyers`. Os primeiros cinco valores nessa coluna são os mesmos, pois todos eles se relacionam com a mesma coorte.

Só mais duas etapas e teremos o LTV. Para começar, como o LTV é calculado com base no lucro bruto, e não na receita, devemos descobrir o lucro bruto multiplicando a receita pela lucratividade. Em segundo lugar, o LTV é um parâmetro relativo, e ele é mais fácil de ser estudado para coortes "maduros", então vamos fazer com que as colunas exibam as idades das coortes ao invés do mês do pedido.

```
margin_rate = 0.5

report['gp'] = report['revenue'] * margin_rate
report['age'] = (
    report['order_month'] - report['first_order_month']
) / np.timedelta64(1, 'M')
report['age'] = report['age'].round().astype('int')

report.head()
```

```
first_order_month    n_buyers     order_month    revenue       gp    age
0         2018-03-01        61        2018-03-01    118529    59264.5     0
1         2018-03-01        61        2018-04-01     95905    47952.5     1
2         2018-03-01        61        2018-05-01    124972    62486.0     2
3         2018-03-01        61        2018-06-01    107273    53636.5     3
4         2018-03-01        61        2018-07-01     70575    35287.5     4
```

Finalmente podemos calcular o LTV. Vamos dividir o lucro bruto da coorte de cada mês pela quantidade total de usuários em cada coorte. Vamos visualizar as coortes como uma tabela dinâmica para deixar tudo mais claro:

```
report['ltv'] = report['gp'] / report['n_buyers']

output = report.pivot_table(
    index='first_order_month', columns='age', values='ltv', aggfunc='mean'
).round()

output.fillna('')
```

```
age            0    1      2        3        4        5     6
first_order_month                            
2018-03-01     972    786    1024    879    578    241    58
2018-04-01    1127    860     994    701    349    101    9
2018-05-01    1142    881     908    460    147    29    4
2018-06-01    1136    929     684    280     33        
2018-07-01    1043    900     694    263     36        
2018-08-01    1092    828     618    198     22        
2018-09-01    1023    710     228    33            
2018-10-01    1031    445      74                
2018-11-01     987    453                    
2018-12-01     930
```

Vamos descobrir o LTV da primeira coorte somando o valor de cada mês:

```
ltv_201803 = output.loc['2018-03-01'].sum()
ltv_201803
```

```
4538.0
```

Em média, cada cliente da primeira coorte trouxe $4,538 de receita ao longo de seu tempo de vida de sete meses.

Vamos calcular o CAC para esta coorte. Vamos dividir as despesas da coorte pela quantidade de pessoas:

```
# conseguindo a coorte necessária
cohort_201803 = report[report['first_order_month'] == '2018-03-01']

# calculando despesas para o mês da coorte
costs_201803 = costs[costs['month'] == '2018-03-01']['costs'].sum()

n_buyers_201803 = cohort_201803['n_buyers'][0]
cac_201803 = costs_201803 / n_buyers_201803
ltv_201803   = output.loc['2018-03-01'].sum()

print('CAC =', cac_201803)
print('LTV =', ltv_201803)
```

```
CAC = 3145.655737704918
LTV = 4538.0
```

A aquisição de cada cliente custou para a empresa uma média de $3,145.70, sendo que cada cliente trouxe $4,538. Então o investimento na aquisição de clientes se pagou e ainda gerou lucro.

Vamos fazer cálculos para as outras coortes e determinar a quantidade de meses que cada coorte precisa para cobrir as despesas com marketing. Vamos calcular o CAC para cada coorte. Primeiro, vamos descobrir as despesas totais para cada mês, então vamos dividí-las pela quantidade de clientes em cada coorte:

```
# calculando as despesas mensais
monthly_costs = costs.groupby('month').sum()
monthly_costs.head()
```

```
             costs
month    
2018-03-01    191885
2018-04-01    696282
2018-05-01    845773
2018-06-01    912186
2018-07-01    940385
```

Vamos adicionar os dados das despesas ao relatório e calcular o CAC:

```
report_ = pd.merge(report, monthly_costs, left_on='first_order_month', right_on='month')
report_['cac'] = report_['costs'] / report_['n_buyers']
report_.head()
```

```
  first_order_month    n_buyers    order_month    revenue    gp    age    ltv    costs    cac
0    2018-03-01    61    2018-03-01    118529    59264.5    0    971.549180    191885    3145.655738
1    2018-03-01    61    2018-04-01     95905    47952.5    1    786.106557    191885    3145.655738
2    2018-03-01    61    2018-05-01    124972    62486.0    2    1024.360656    191885    3145.655738
3    2018-03-01    61    2018-06-01    107273    53636.5    3    879.286885    191885    3145.655738
4    2018-03-01    61    2018-07-01     70575    35287.5    4    578.483607    191885    3145.655738
```

E agora vamos descobrir o ROMI (return on marketing investiment - retorno sobre o investimento com marketing) dividindo o LTV pelo CAC.

Para calcular o ROMI acumulado, vamos precisar chamar o método `cumsum()` (cumulative sum - soma acumulada). Isso adiciona cada elemento novo em um set. Por exemplo, `pd.Series([1, 2, 3]).cumsum()` irá retornar uma série de três elementos: 1, 3, and 6. Cada valor é a soma dos elementos anteriores: `1, 3 = 1+2, 6 = 1+2+3`.

```
report_['romi'] = report_['ltv'] / report_['cac']
output = report_.pivot_table(
    index='first_order_month', columns='age', values='romi', aggfunc='mean'
)

output.cumsum(axis=1).round(2)
```

```
age            0        1        2        3        4       5      6
first_order_month                            
2018-03-01    0.31    0.56    0.88    1.16    1.35    1.42    1.44
2018-04-01    0.37    0.64    0.97    1.19    1.31    1.34    1.34
2018-05-01    0.37    0.66    0.96    1.11    1.15    1.16    1.17
2018-06-01    0.37    0.68    0.91    1.00    1.01        
2018-07-01    0.34    0.63    0.86    0.95    0.96        
2018-08-01    0.37    0.65    0.86    0.93    0.93        
2018-09-01    0.35    0.60    0.67    0.69            
2018-10-01    0.35    0.50    0.53                
2018-11-01    0.32    0.47                    
2018-12-01    0.31
```

Vamos ler o relatório:

1.  A coorte de março cobriu suas despesas no 4º mês (ROMI = 1.16). (Não se esqueça de que começamos a contar do 0).
2.  A coorte de abril também cobriu suas despesas no quarto mês (ROMI = 1.19).
3.  Em geral, por volta do 4º ou 5º mês, cada coorte cobriu suas despesas ou chegou perto.

Analistas geralmente estudam a **coorte média** tirando a média do ROMI ou LTV de todas coortes para cada mês de tempo de vida. Por exemplo, vamos tentar descobrir quanto demora para as coortes cobrirem suas despesas em média. Vamos tirar a média dos valores em cada coluna e converter a tabela para um único Series com o método `mean(axis=0)`:

```
output.cumsum(axis=1).mean(axis=0)
```

```
age
0    0.346404
1    0.599110
2    0.829296
3    1.003225
4    1.118678
5    1.309746
6    1.317369
dtype: float64
```

Em média, coortes cobrem suas despesas ao fim do 4º mês.

Mas por que o negócio está gerando prejuízo, se que as coortes estão com bons resultados? O problema é que a empresa começou a crescer mais rapidamente em outubro. As despesas aumentaram, assim como a quantidade de novos clientes. Mas é preciso esperar quatro meses para cobrir os custos, e então o lucro ainda não conseguiu superar as despesas. O negócio parece saudável, e não precisamos demitir os especialistas em marketing. Ou analistas. Ufa!

Economia Unitária por Cliente para uma Loja Online

Tarefa5 / 5

1.

Você está trabalhando em uma loja online. Sua tarefa é analisar a economia unitária e ajudar os especialistas em marketing, para eles entenderem se devem cortar ou aumentar as despesas com marketing. Você possui dados completos de vendas e despesas. Você também está ciente de que a margem de lucro da loja é de 40%.

1.  Salve os dados de pedidos de `'/datasets/ltv_orders_2.csv'` como `orders`, e os dados de despesas de `'/datasets/ltv_costs_2.csv'` como `costs`.
    
2.  Converta as datas para os tipos de dados corretos e acrescente as colunas do mês da encomenda e da despesa mensal.
    

Você não precisa imprimir nada.

2.

Descubra a quantidade de usuários unívocos em cada coorte.

1.  Descubra o mês em que o primeiro pedido de cada cliente foi feito e salve o resultado como `first_orders`.
2.  Renomeie as colunas de `first_orders` como `'uid'` e `'first_order_month'`.
3.  Para cada mês, calcule a quantidade de pessoas que fez seu primeiro pedido. Salve a tabela resultante como `cohort_sizes`.
4.  Renomeie as colunas de `cohort_sizes` como `'first_order_month'` e `'n_buyers'`.
5.  Imprima as primeiras cinco linhas do DataFrame `cohort_sizes`.

3.

Crie um relatório de coortes, adicionando colunas com a idade das coortes e o lucro bruto total para cada coorte em cada mês. Calcule o LTV também.

Imprima uma tabela dinâmica em que as linhas contenham o mês da coorte, as colunas contenham a idade da coorte e os valores sejam o LTV médio.

Não se esqueça de que a loja tem uma margem de lucro de 40%, e certifique-se de arredondar os valores no resultado final como fizemos nesta lição.

4.

Calcule o CAC e o ROMI para cada coorte.

1.  Descubra a soma total das despesas com marketing para cada mês e salve-a como `monthly_costs`.
    
2.  Adicione os dados relativos às despesas ao relatório de coortes e calcule o CAC para cada coorte.
    
3.  Armazene o resultado na tabela `report['cac']`.
    
4.  Finalmente, calcule o ROMI da coorte dividindo o LTV pelo CAC.
    
5.  Imprima a tabela dinâmica com o relatório. As linhas terão o mês da coorte, as colunas terão a idade da coorte e os valores serão a média acumulada do ROMI.
    

5.

Calcule o LTV para a coorte média. Descubra quanto a equipe de marketing pode gastar para adquirir um cliente, sendo que eles querem que as despesas dos anúncios sejam cobertas em seis meses.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

import pandas as pd

import numpy as np

  

orders \= pd.read\_csv('/datasets/ltv\_orders\_2.csv')

costs \= pd.read\_csv('/datasets/ltv\_costs\_2.csv')

  

orders\['order\_date'\] \= pd.to\_datetime(orders\['order\_date'\])

costs\['date'\] \= pd.to\_datetime(costs\['date'\])

  

orders\['order\_month'\] \= orders\['order\_date'\].astype('datetime64\[M\]')

costs\['month'\] \= costs\['date'\].astype('datetime64\[M\]')

  

  

first\_orders \= orders.groupby('uid').agg({'order\_month': 'min'}).reset\_index()

first\_orders.columns \= \['uid', 'first\_order\_month'\]

  

cohort\_sizes \= (

first\_orders.groupby('first\_order\_month')

.agg({'uid': 'nunique'})

.reset\_index()

)

cohort\_sizes.columns \= \['first\_order\_month', 'n\_buyers'\]

cohort\_sizes.head()

  

margin\_rate \= 0.4

  

orders\_ \= pd.merge(orders, first\_orders, on\='uid')

cohorts \= (

orders\_.groupby(\['first\_order\_month', 'order\_month'\])

.agg({'revenue': 'sum'})

.reset\_index()

)

report \= pd.merge(cohort\_sizes, cohorts, on\='first\_order\_month')

  

report\['gp'\] \= report\['revenue'\] \* margin\_rate

report\['age'\] \= (

report\['order\_month'\] \- report\['first\_order\_month'\]

) / np.timedelta64(1, 'M')

report\['age'\] \= report\['age'\].round().astype('int')

  

report\['ltv'\] \= report\['gp'\] / report\['n\_buyers'\]

  

result \= report.pivot\_table(

index\='first\_order\_month', columns\='age', values\='ltv', aggfunc\='mean'

).round()

  

result \= result.fillna('')

  

monthly\_costs \= costs.groupby('month').sum()

report\_ \= pd.merge(

report, monthly\_costs, left\_on\='first\_order\_month', right\_on\='month'

)

report\_\['cac'\] \= report\_\['costs'\] / report\_\['n\_buyers'\]

  

report\_\['romi'\] \= report\_\['ltv'\] / report\_\['cac'\]

result \= report\_.pivot\_table(

index\='first\_order\_month', columns\='age', values\='romi', aggfunc\='mean'

)

  

result \= report\_.pivot\_table(

index\='first\_order\_month', columns\='age', values\='ltv', aggfunc\='mean'

)

  

m6\_cum\_ltv = result.cumsum(axis=1).mean(axis=0)\[5\]

  

print('LVT médio aos 6 meses após a primeira encomenda:', m6\_cum\_ltv)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-58-58-410Z.md
### Última modificação: 2025-05-28 19:58:58

# Conclusão - TripleTen

Capítulo 4/9

Unit Economics (Economia Unitária)

# Conclusão

Neste capítulo, você aprendeu:

-   Como calcular a economia unitária por venda
-   Como construir modelos para calcular o volume de vendas necessário para que um negócio se torne lucrativo
-   Como calcular a economia unitária por cliente usando métricas como LTV e CAC

### Leve isso com você

Faça o download da [folha conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_7/PT/Additional_materials/moved_DA_7_4_Folha_de_Concluses_Economia_Unitria.pdf) e do [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_7/PT/Additional_materials/moved_DA_7_4_Resumo_do_Captulo_Economia_Unitria.pdf) para que você possa consultá-los quando necessário.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-58-59-723Z.md
### Última modificação: 2025-05-28 19:59:00

# Introdução - TripleTen

Capítulo 5/9

Métricas de Usuário

# Introdução

Neste capítulo, falaremos sobre a leitura do comportamento do usuário a partir de métricas. Você também estudará Y.Metrica mais em detalhe.

### O que você irá aprender:

-   Como calcular a audiência do produto e o tempo que os usuários passam com você
-   Como achar anomalias nos dados
-   Como trabalhar com registos API no Y.Metrica

### Quanto vai demorar:

7 aulas, com aproximadamente 20-30 minutos cada uma

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-59-02-853Z.md
### Última modificação: 2025-05-28 19:59:03

# Avaliando a Atividade do Usuário - TripleTen

Teoria

# Avaliando a Atividade do Usuário

Sua equipe está lançando um novo serviço de entrega — futurista, com drones. Os usuários podem fazer pedidos usando um aplicativo.

As métricas da empresa mostram o número de usuários que pagaram pela entrega. E a atividade geral do usuário? Quantas pessoas abrem o aplicativo todos os dias e por que nem todas usam seus drones?

Você vai responder a segunda pergunta um pouco mais tarde. Por enquanto, pode analisar a atividade do usuário. Você precisa descobrir quantas visitas únicas você recebe. Isso informa até que ponto o serviço é interessante para os usuários que ainda não estão familiarizados com ele.

Existem três métricas principais que descrevem o número de usuários ativos:

-   **DAU** (Daily Active Users, ou Usuários ativos diários) — é o número de usuários ativos diários (unívocos)
-   **WAU** (Weekly Active Users, ou Usuários ativos semanais) — é o número de usuários ativos semanais
-   **MAU** (Monthly Active Users, ou Usuários ativos mensais) — é o número de usuários ativos mensais

Vejamos os dados sobre a atividade do usuário para o aplicativo:

```
import pandas as pd

users_data = pd.read_csv('/datasets/users_data.csv')
users_data['session_start_ts'] = pd.to_datetime(users_data['session_start_ts'])
print(users_data.head())
```

```
session_end_ts     session_start_ts                    id
0  2018-05-12 13:01:00  2018-05-12 12:59:00   6166747268563051447
1  2018-05-12 13:01:00  2018-05-12 13:01:00  14914378861449143644
2  2018-05-12 13:02:00  2018-05-12 13:00:00   3454575477456344412
3  2018-05-12 13:02:00  2018-05-12 13:02:00    630504356053318052
4  2018-05-12 13:03:00  2018-05-12 13:02:00    596727621648472758
```

Existem IDs de usuário, as fontes de origem dos usuários e carimbos de data/hora de início e término de sessão arredondados para o minuto por um período de dois meses.

Para calcular a atividade semanal e mensal, primeiro criaremos as colunas separadas para os valores de ano, mês e semana.

```
users_data['session_year']  = users_data['session_start_ts'].dt.year
users_data['session_month'] = users_data['session_start_ts'].dt.month
users_data['session_week']  = users_data['session_start_ts'].dt.week
users_data['session_date'] = users_data['session_start_ts'].dt.date
print(users_data.head())
```

```
session_end_ts    session_start_ts                    id  session_year  \
0 2018-05-12 13:01:00 2018-05-12 12:59:00   6166747268563051447          2018   
1 2018-05-12 13:01:00 2018-05-12 13:01:00  14914378861449143644          2018   
2 2018-05-12 13:02:00 2018-05-12 13:00:00   3454575477456344412          2018   
3 2018-05-12 13:02:00 2018-05-12 13:02:00    630504356053318052          2018   
4 2018-05-12 13:03:00 2018-05-12 13:02:00    596727621648472758          2018   

   session_month  session_week session_date  
0              5            19   2018-05-12  
1              5            19   2018-05-12  
2              5            19   2018-05-12  
3              5            19   2018-05-12  
4              5            19   2018-05-12
```

Agora vamos calcular as métricas. Vamos agrupar os dados por data/semana da sessão e encontrar as médias:

```
dau_total = users_data.groupby('session_date').agg({'id': 'nunique'}).mean()
wau_total = users_data.groupby(['session_year', 'session_week']).agg({'id': 'nunique'}).mean()

print(int(dau_total))
print(int(wau_total))
```

```
1536
8297
```

Observe que essas são métricas distintas. Multiplicar DAU por sete não lhe dará WAU.

Por que é importante acompanhar essas métricas? Elas não ajudarão a empresa a tomar uma decisão sobre a introdução de novas funções do aplicativo, pois não são muito sensíveis a alterações no produto. Também não vão dar uma noção de como os usuários reagirão às mudanças. Geralmente, essas métricas aumentam de forma constante, por isso elas desempenham um papel importante na motivação da equipe. Por esse motivo, às vezes são chamadas de **métricas de vaidade**.

Existe também a **taxa de retenção**. Essa métrica informa a fidelidade do público – com que frequência eles retornam ao aplicativo. A fórmula é simples: `taxa de retenção = DAU/WAU` ou `taxa de retenção = DAU/MAU`.

O aplicativo de entrega por drone parece bastante popular, então sua equipe decide encontrar investidores. Eles querem saber como o público está crescendo. Calcule as métricas necessárias.

Avaliando a Atividade do Usuário

Tarefa2 / 2

1.

O arquivo `/datasets/users_data.csv` armazena os dados sobre a atividade do usuário no aplicativo. Leia-o e armazene-o na variável `users_data`. Calcule a métrica MAU para todo o período e salve o resultado como `mau_total`. Imprima-o como um único número inteiro.

2.

Dê outra olhada em `mau_total` no pré-código, e encontre `dau_total` e `wau_total` usando a mesma abordagem.

Encontre a taxa de retenção expressa como a razão entre a audiência mensal e semanal. Salve o resultado da audiência semanal como `sticky_wau` e o resultado da audiência mensal como `sticky_mau`. Multiplique o resultado da sua taxa de retenção por 100 para obter uma porcentagem. Imprima os resultados (primeiro semanalmente, depois mensalmente) sem transformá-los em números inteiros. Ao adicionar colunas aos seus dados originais, mova de maior para menor: ano, mês, semana, data.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

import pandas as pd

users\_data \= pd.read\_csv('/datasets/users\_data.csv')

users\_data\['session\_start\_ts'\] \= pd.to\_datetime(

users\_data\['session\_start\_ts'\], format\="%Y-%m-%d %H:%M"

)

users\_data\['session\_end\_ts'\] \= pd.to\_datetime(

users\_data\['session\_end\_ts'\], format\="%Y-%m-%d %H:%M"

)

users\_data\['session\_year'\] \= users\_data\['session\_start\_ts'\].dt.year

users\_data\['session\_month'\] \= users\_data\['session\_start\_ts'\].dt.month

users\_data\['session\_week'\] \= users\_data\['session\_start\_ts'\].dt.week

users\_data\['session\_date'\] \= users\_data\['session\_start\_ts'\].dt.date

dau\_total \= (

users\_data.groupby(\['session\_year', 'session\_date'\])

.agg({'id': 'nunique'})

.mean()

)

wau\_total \= (

users\_data.groupby(\['session\_year', 'session\_week'\])

.agg({'id': 'nunique'})

.mean()

)

mau\_total \= (

users\_data.groupby(\['session\_year', 'session\_month'\])

.agg({'id': 'nunique'})

.mean()

)

sticky\_wau \= dau\_total / wau\_total \* 100

print(sticky\_wau)

sticky\_mau \= dau\_total / mau\_total \* 100

print(sticky\_mau)

  

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-59-04-195Z.md
### Última modificação: 2025-05-28 19:59:04

# Sessões do Usuário - TripleTen

Teoria

# Sessões do Usuário

Você sabe quantos usuários seu aplicativo tem, mas seria ótimo ter dados sobre quanto tempo eles passam nele. Aqui, o conceito de **sessões de usuário** vai ajudar. A sessão do usuário é o conjunto de ações do usuário que começa com a abertura do aplicativo e termina quando ele é fechado.

Por exemplo, pode ser útil determinar o número de sessões por usuário por mês; esse é um bom indicador da frequência de uso de um aplicativo.

Calcular essa métrica é fácil. Basta dividir o número de sessões pelo número de usuários em um determinado período:

```
import pandas as pd

sessions_df = pd.read_csv('/datasets/users_sessions_data.csv')
sessions_df['session_start_ts'] = pd.to_datetime(
    sessions_df['session_start_ts']
)
sessions_df['session_end_ts'] = pd.to_datetime(sessions_df['session_end_ts'])
sessions_df['session_year'] = sessions_df['session_start_ts'].dt.year
sessions_df['session_month'] = sessions_df['session_start_ts'].dt.month
sessions_per_user = sessions_df.groupby(['session_year', 'session_month']).agg(
    {'client_id': ['count', 'nunique']}
)
sessions_per_user.columns = ['n_sessions', 'n_users']
sessions_per_user['sessions_per_user'] = (
    sessions_per_user['n_sessions'] / sessions_per_user['n_users']
)
print(sessions_per_user)
```

```
n_sessions  n_users  sessions_per_user
session_year session_month                                        
2019         10                  22322    22184           1.006221
             11                   2867     2845           1.007733
```

Em média, cada usuário tem no máximo 1 sessão no aplicativo por mês. Não podemos dizer que ele é usado com muita frequência.

**Duração média da sessão** ou **ASL** é a quantidade média de tempo que os usuários passam com um produto na sessão.

A qualidade de uma determinada ASL depende da natureza do produto. Por exemplo, quanto mais tempo joga o jogo, melhor, pois significa que os usuários gostam de jogar. Mas para o aplicativo Alarm Clock, a mesma métrica não mostrará nada significativo. As pessoas usam o aplicativo em uma determinada hora do dia com um objetivo claramente definido. A taxa de retenção é uma métrica melhor para o Alarm Clock.

Vamos encontrar o ASL calculando a duração de todas as sessões e encontrando a média:

```
sessions_df['session_duration_sec'] = (sessions_df['session_end_ts'] - sessions_df['session_start_ts']).dt.seconds
print(sessions_df['session_duration_sec'].mean())
```

```
454.2025884314582
```

Parece que funcionou! Mas vamos dar uma olhada na distribuição:

```
sessions_df['session_duration_sec'].hist(bins=50)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_9_1586767277.png)

Quando a distribuição é normal ou próxima do normal, pode-se obter a média ou a mediana. Mas no nosso caso é impossível, então temos que calcular a moda:

```
print(sessions_df['session_duration_sec'].mode())
```

```
0    60
dtype: int64
```

Quando a duração média da sessão começa a diminuir, é importante desenvolver hipóteses sobre o porquê. Por exemplo, talvez a versão móvel do seu site seja difícil de usar, então as sessões de smartphone e tablet acabam sendo muito curtas.

Sessões do Usuário

Tarefa3 / 3

1.

Continue trabalhando com o aplicativo de entrega com drones. Encontre o número de sessões por usuário para cada mês, salvando o resultado como `sessions_per_user['sess_per_user']`. Imprima `sessions_per_user`.

2.

Encontre o comprimento da sessão e trace um histograma com 100 barras. Nomeie a coluna como `users_data['session_duration_sec']`.

3.

Encontre a métrica ASL (duração média da sessão). Salve o resultado como `asl` e imprima a variável.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

import pandas as pd

  

users\_data \= pd.read\_csv('/datasets/users\_data.csv')

users\_data\['session\_start\_ts'\] \= pd.to\_datetime(

users\_data\['session\_start\_ts'\], format\="%Y-%m-%d %H:%M"

)

users\_data\['session\_end\_ts'\] \= pd.to\_datetime(

users\_data\['session\_end\_ts'\], format\="%Y-%m-%d %H:%M"

)

users\_data\['session\_duration\_sec'\] \= (

users\_data\['session\_end\_ts'\] \- users\_data\['session\_start\_ts'\]

).dt.seconds

users\_data\['session\_duration\_sec'\].hist(bins\=100)

asl \= users\_data\['session\_duration\_sec'\].mode()

  

print(asl)

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-59-05-523Z.md
### Última modificação: 2025-05-28 19:59:06

# Frameworks de Métricas - TripleTen

Capítulo 5/9

Métricas de Usuário

# Frameworks de Métricas

A escolha de uma métrica depende do tipo de produto, da plataforma e de outros fatores. Você precisa medir um parâmetro, mas não tem como fazê-lo? Crie suas próprias métricas ou use um conjunto pronto de métricas, um **framework**. Mostraremos as mais comuns e úteis.

Vamos começar com um framework criado pelo Google. Ele ajuda a avaliar a experiência do usuário (UX) e é chamado de HEART, acronímico das cinco métricas envolvidas:

-   **H**appiness (Felicidade)

Os usuários gostam do produto/serviço?

-   **E**ngagement (Engajamento) Com que frequência eles usam o produto ou escrevem sobre ele nas mídias sociais?
-   **A**doption (Adoção) Quantos novos usuários temos?
-   **R**etention (Retenção) Quantos usuários permanecem fiéis ao serviço?
-   **T**ask success (Sucesso da tarefa) Até que ponto a ação objetivo (assinatura, compra, adição ao carrinho) foi realizada com sucesso?

Graças à sua simplicidade, o framework HEART é usado com frequência. Além disso, o framework pode ser aprimorado. Geralmente é usado em conjunto com a abordagem Objetivos-Sinais-Métricas.

Ele ajuda a identificar as principais métricas para cada área.

O processo de trabalho funciona assim:

1.  Identifique os objetivos do produto.
2.  Responda à pergunta "Como o fato de atingir ou não as metas afetará o comportamento do usuário"?
3.  Decida como medir os sinais.

Ficamos com a seguinte tabela:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_7/PT/moved_7.5.4PT.png)

As metas e métricas estão muito claras e prontas para serem usadas.

Outra estrutura útil é o **AARRR**, geralmente chamada de "métricas pirata" (diga o nome em voz alta). AARRR ajuda você a entender o tráfego de usuários e otimizar o funil.

AARRR envolve as seguintes métricas:

-   **A**quisição
    
    De onde vêm os clientes?
    
-   **A**tivação
    
    Os usuários concluíram uma ação objetivo quando começaram a interagir com o produto?
    
-   **R**etention (Retenção)
    
    Quantos clientes continuam usando seu produto e por que os outros desistem?
    
-   **R**eferências
    
    Os usuários estão prontos para recomendar seu produto?
    
-   **R**eceita
    
    Como aumentar a receita?
    

Se você juntar essas métricas, obterá um funil de marketing que deve parecer familiar:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_7.5.4.2PT_1655468155.png)

A conversão é calculada em cada etapa. Isso permite que você veja com que sucesso os clientes estão sendo atraídos e quais fatores influenciam as compras.

Geralmente são os gerentes de produto que impulsionam a adoção desses frameworks. Para que os analistas precisam deles?

-   Os frameworks ajudam você a entender o negócio que está analisando. Todas as métricas estão interconectadas e muitas vezes podem ser apresentadas como um sistema. Isso significa que você poderá interpretar os resultados de sua análise corretamente.
-   Eles ajudam você a realizar tarefas de negócios. Se você faz parte de uma equipe para a qual a analítica ainda está em desenvolvimento, mostre a seus colegas de equipe os frameworks mais comuns. (Você pode evitar reinventar a roda).
-   Eles ajudam você a criar seu próprio framework. Conjuntos de métricas podem ser aprimorados, possibilitando a criação de seu próprio sistema de avaliação.

Pergunta

Vamos estudar os dois primeiros estágios de trabalho com o framework AARRR. Tentaremos descobrir quais etapas terão impacto no crescimento das métricas do aplicativo de entrega com drones Dronezone.

As primeiras campanhas publicitárias tiveram um enorme sucesso e você conquistou clientes fiéis, mas agora é hora de desenvolver ainda mais. Quais canais de marketing você usará?

Segmentação de mídia social. Segundo analistas, este canal é o mais eficaz.

Nossos concorrentes enviam e-newsletters. Nós também precisamos!

Precisamos estudar o perfil do nosso público-alvo e selecionar canais de promoção relevantes. Não podemos esquecer aqueles que são eficazes, como as mídias sociais. Por que se livrar de algo que funciona bem?

Isso mesmo! Não faz sentido enviar as coisas pelo correio se seus clientes têm menos de 30 anos.

Vamos fazer camisetas fashion para a nossa equipe!

Excelente!

Pergunta

Seus usuários vêm de canais diferentes. Em que situações os clientes começam a usar o Dronezone?

Quando pesquisam por "entrega".

Quando eles abrem o aplicativo e passam algum tempo com ele.

Quando selecionam as opções de entrega.

Quando pedem uma entrega por drone.

Vitória! De agora em diante, você é um provedor de serviços e seu cliente está usando o produto como foi pretendido.

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-59-07-781Z.md
### Última modificação: 2025-05-28 19:59:08

# Detecção de Anomalias - TripleTen

Capítulo 5/9

Métricas de Usuário

# Detecção de Anomalias

A análise de coortes de várias lojas conectadas ao serviço de entrega revelou quedas repentinas para algumas coortes. O que causou isso? Aconteceu que as lojas em questão fazem parte da mesma rede. Foram feitas alterações nos seus sites, onde foi colocado um widget para o seu serviço.

Os analistas se deparam com esses problemas com frequência. Aqui apenas estudar dados não é suficiente. Você tem que saber o que está acontecendo com as lojas e usar sua cabeça.

Vamos considerar dois casos:

-   Na semana anterior houve um aumento na atividade do usuário, que atingiu valores de pico. (Cadê o champanhe que queríamos beber faz um ano?)
-   Os resultados do mês demonstram que a duração média das sessões diminuiu (estamos cancelando o projeto. Tchau!)

Qual é a diferença entre eles? No primeiro caso temos um cenário positivo, enquanto o segundo descreve um cenário negativo, é verdade, mas é uma análise superficial.

Vamos entrar em detalhes:

-   _A atividade do usuário aumentou_

Abrimos o calendário e descobrimos que é quase Natal. Talvez haja apenas mais entregas do que os entregadores conseguem entregar.

-   _A duração média da sessão diminuiu_

Sua equipe soltou uma atualização. Agora, quando você inicia o aplicativo, não vê a página de onboarding, precisa se registrar imediatamente. Lembre-se de que os usuários realmente não entendem do que se trata esse novo serviço (entrega por drone). Na falta de encontrar uma explicação de como funciona, eles saem, obviamente.

Vamos começar a investigar anomalias. Este processo tem vários ambientes de preparação:

1.  Coleta de dados e cálculo das principais métricas.
2.  Visualização de dados.
3.  Separação de dados com comportamento anômalo (fazer uma fatia de dados e "isolar" eles, como diriam os analistas).
4.  Estudar os dados e procurar o motivo:
    
    -   comparar a anomalia com outras anomalias
    -   levar em conta eventos externos
    -   analisar os problemas dos concorrentes
    -   coletar dados (como foi feito, o que pode ter dado errado)
    -   permitir sazonalidade, lançamentos futuros e grandes melhorias
5.  Tirar conclusões.
    

As métricas podem ser afetadas por sazonalidade, feriados e problemas técnicos. A investigação deveria ser realizada quando a anomalia já apareceu nos dados.

Vamos considerar um exemplo. Sua empresa, Pé d'água, entrega água. O chefe quer aumentar o lucro. Mas como? Aumentando o número de usuários! Pronto. Mas poucos usuários estão comprando nosso produto. Deveríamos aumentar a conversão então! Mas a taxa de conversão já é bastante decente... Mas agora olhe para a situação desde várias perspectivas. As taxas de conversão são as mesmas para o dia e a noite? Em diferentes plataformas? Em cidades diferentes? Você certamente encontrará as diferenças significativas. Esta é a chave para aumentar a conversão.

Este processo ajuda a revelar os pontos de crescimento. É mais relevante nos casos em que o produto é sólido e você precisa fazer apenas pequenos ajustes para melhorá-lo.

Pergunta

Seu chefe tem uma grande tarefa para você: ele quer que sua empresa cresça em três vezes até o final do ano.

O que você faz?

Descubra quais projetos são os mais rentáveis. E se pudéssemos aumentar o número de usuários?

Melhorar as coisas que você já tem é a melhor ideia. Estude cuidadosamente as principais métricas.

Sugira com cautela que novos funcionários sejam contratados. Quanto mais pessoas você tiver, mais dinheiro você terá.

Tenha como meta aumentar o número de usuários.

Recomendar o lançamento de um novo produto.

Excelente!

Pergunta

Você começou a melhorar um dos projetos mais lucrativos, um aplicativo de análise chamado Medida à Três. Os clientes são grandes corporações. Qual métrica você vai impulsionar?

O número de usuários únicos por mês.

Retenção/compras repetidas.

Bingo! É muito importante descobrir se os clientes voltam.

O custo de aquisição de um novo cliente.

Seu entendimento sobre o material é impressionante!

Pergunta

Seu aplicativo tem 95% de retenção! Você está feliz, mas também chateado; não há espaço para crescer nesse sentido. O que fazer?

Veja mais produtos da empresa, existem muitas opções interessantes.

Lançar campanhas publicitárias em outras áreas e atrair novas corporações.

Muito bem! Se seu aplicativo é tão popular em nossa área, é hora de ampliar a nossa base de clientes. Pegue o primeiro voo disponível!

Desespere e esqueça.

Diga ao seu chefe que você atingiu o teto e não há outras possibilidades de engajamento.

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-59-09-068Z.md
### Última modificação: 2025-05-28 19:59:09

# Y.Metrica - TripleTen

Capítulo 5/9

Métricas de Usuário

# Y.Metrica

O Y.Metrica é uma ferramenta de análise da web que ajuda a rastrear as sessões do site, avaliar o comportamento do usuário, avaliar o desempenho de canais de promoção e realizar análises de conteúdo.

Como você já deve ter percebido ao trabalhar com dados, quanto mais você souber sobre seus usuários, mais alta será a qualidade de suas conclusões.

O que o Y.Metrica pode lhe dizer sobre os usuários?

-   Sua idade e sexo
-   Seus interesses
-   Os países e cidades de onde vêm e as línguas que falam
-   Os dispositivos ou navegadores que eles usam
-   Quão leais eles são e quanto dinheiro eles trazem

O Y.Metrica oferece análises resumidas que informam em detalhes como está o desempenho do seu site.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_11_1586778589.png)

Você pode encontrar uma conta demo em [https://metrica.yandex.com/dashboard?id=44147844](https://metrica.yandex.com/dashboard?id=44147844) (os materiais estão em inglês). A primeira página contém a informação mais importante sobre o site que está sendo analisado.

À esquerda, você verá o painel de ferramentas.

-   Relatórios
    
    Você pode usar relatórios padrão ou criar os seus próprios. Os relatórios contêm informação resumida sobre dados demográficos, interesses e comportamento do usuário. Por exemplo, você pode ter uma ideia de como os usuários seguem links externos e se eles têm bloqueadores de anúncios instalados.
    
-   Mapas
    
    Mapas de calor, analítica de formulários, mapas de rolagem — tudo isso ajuda a fornecer uma compreensão visual dos dados.
    
-   Session Replay (Repetição da Sessão)
    
    Session Replay registra uma sessão como ela é vista pelo usuário. A visualização desses registros ajuda a entender o comportamento do usuário e detectar dificuldades que os usuários podem encontrar.
    
-   Usuários
    
    Você saberá quem acessa seu site, quanto tempo eles passam nele e se fazem compras.
    

O Y.Metrica ajuda você a resolver várias tarefas:

-   Encontrar problemas no site
-   Recriar as jornadas do usuário no site
-   Marketing Analítico
-   Analítica para lojas online

Vamos considerar um caso típico. Quais perguntas você precisa responder ao procurar problemas no site?

-   As páginas de destino funcionam bem?
-   O que exatamente os usuários fazem no site?
-   Como eles passam pelo funil de conversão?
-   Quais elementos da página são visíveis e quais não são?
-   O site é compatível com vários dispositivos? Existem problemas?

O Session Replay mostra o que impede os usuários de fazer pedidos (ou "converter", como diriam os analistas). Pode ser que a versão mobile seja ruim. Você também pode filtrar os usuários de acordo com alguns critérios: se eles visualizaram uma determinada página, por status (se fizeram um pedido ou não), etc.

Os mapas de rolagem mostram onde os usuários foram mais ativos e quais elementos da interface são negligenciados.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_funnel1_1589791644.png)

Talvez os usuários decidam sair quando veem um formulário que precisam preencher. Esse tipo de problema pode ser identificado e eliminado usando a analítica de formulários.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_image_2020-12-28_153219_1609158740.png)

Aqui estão links úteis dos nossos colegas da Y.Metrica:

[https://www.youtube.com/playlist?list=PLes02YtPKABmUNneqfc9DZuSEatPEbdVz](https://www.youtube.com/playlist?list=PLes02YtPKABmUNneqfc9DZuSEatPEbdVz)

[https://yandex.com/support/metrica/index.html](https://yandex.com/support/metrica/index.html)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-59-10-417Z.md
### Última modificação: 2025-05-28 19:59:10

# Y.Metrica API - TripleTen

Capítulo 5/9

Métricas de Usuário

# Y.Metrica API

A interface do Metrica responde por si só a muitas perguntas que você possa ter, mas não pode cobrir tudo. Quando você precisar se aprofundar, poderá exportar dados brutos e criar os relatórios necessários por conta própria. Aqui a API da Y.Metrica é útil.

## 1\. Introdução e obtenção de um token de acesso

Você pode usar a API de Logs para obter dados não agregados coletados pela Y.Metrica.

Os dados agregados ou resumidos que você vê na interface Metrica ou exporta dos relatórios da API são calculados para um grupo específico de sessões. Por exemplo, a métrica "tempo no site" pode ser calculada para todas as origens de tráfego, todas as sessões de usuários do sexo masculino ou todas as sessões de tablets.

A base para esses cálculos são dados brutos: os registros em sessões individuais ou visualizações de página. A tabela que contém esses registros pode ser obtida por meio da API de Logs, e cada registro contém informação útil da Y.Metrica. Esses são os dados detalhados sobre [Y.Direct](http://yandex.Direct) e comércio eletrônico e sobre os países e cidades dos usuários. Os dados brutos também incluem informação técnica sobre as sessões (por exemplo, navegadores ou modelos de telefone dos usuários).

Para acessar a Y.Metrica por meio da API de Logs, você precisa obter autorização usando um **token de acesso**. O token de acesso deve ser especificado em cada solicitação da API.

Saiba como registrar aplicativos para acessar dados e obter um token de acesso na página de suporte: [https://tech.yandex.com/oauth/doc/dg/tasks/register-client-docpage/](https://tech.yandex.com/oauth/doc/dg/tasks/register-client-docpage/)(os materiais estão em inglês)

A maneira mais fácil é obter autorização com um token de debug. Leia mais em [https://tech.yandex.com/oauth/doc/dg/tasks/get-oauth-token-docpage/](https://tech.yandex.com/oauth/doc/dg/tasks/get-oauth-token-docpage/) (os materiais estão em inglês)

Vamos declarar uma variável contendo o token:

```
TOKEN = 'AgAAABAHrQEBAAKn-Axl_7qD0Ee1icfnAGUnlIc'
```

O token contém os dados de autorização. Pertence apenas a você e não pode ser transferido a mais ninguém, pois é o seu acesso às estatísticas da Y.Metrica. Recomendamos que você armazene seu token em um arquivo especial. Isso permitirá que você compartilhe seu Jupyter Notebook com o algoritmo de forma segura.

## 2\. Avaliando a possibilidade de criar uma requisição

Nossa primeira requisição usará o método **evaluate**. Este método avalia a possibilidade de criar uma solicitação de logs com base em seu tamanho aproximado. O problema é que a Y.Metrica pode armazenar muitos dados para determinados parâmetros, de modo que os servidores da Y.Metrica levariam muito tempo para processar esses dados. É por isso que existem restrições: o volume total de dados que podem ser transmitidos em requisições para um contador Y.Metrica é de 10 GB.

Você pode encontrar a documentação do método `evaluate` aqui: [https://tech.yandex.com/metrika/doc/api2/logs/queries/evaluate-docpage/](https://tech.yandex.com/metrika/doc/api2/logs/queries/evaluate-docpage/) (os materiais estão em inglês)

Veja como as requisições `evaluate` se parecem:

```
<https://api-metrika.yandex.net/management/v1/counter/{counterId}/logrequests/evaluate>
 ? [date1=<string>]
 & [date2=<string>]
 & [fields=<string>]
 & [source=<log_request_source>]
```

`counterId` na URL é o identificador do contador.

Também são passados os seguintes parâmetros de requisição GET:

-   `date1` — a data de início da coleta de estatísticas
-   `date2` — a data final (não pode ser a data atual)
-   `fields` — uma lista de campos separados por vírgulas. Este parâmetro especifica os campos da base da Y.Metrica que devem ser recebidos
-   `source` — origem do log. Pode ter um dos dois valores a seguir: `visits` ou `hits`. Dependendo do valor inserido, os dados serão recuperados por sessões/acessos ou por **hits** (visualizações de páginas específicas, cumprimento de metas específicas...)

Obteremos os dados do contador de testes `44147844` para o período de 1 de abril a 31 de maio.

Agora vamos ter que decidir de qual fonte vamos obter os dados e quais campos vamos escolher. Essas escolhas sempre dependem da tarefa. Por exemplo, vamos solicitar dados de acessos com os seguintes campos:

-   `ym:s:visitID` — identificador de acesso
-   `ym:s:dateTime` — hora do acesso
-   `ym:s:isNewUser` — se é o primeiro acesso ou não
-   `ym:s:visitDuration` — duração do acesso em segundos
-   `ym:s:startURL` — URL da página inicial
-   `ym:s:clientID` — identificador de usuário
-   `ym:s:lastTrafficSource` — origem do tráfego

Armazenaremos todos os parâmetros necessários em variáveis:

```
COUNTER_ID = 44147844
DATE1 = '2019-04-01'
DATE2 = '2019-05-31'
SOURCE = 'visits'
FIELDS = ['ym:s:visitID', 'ym:s:dateTime', 'ym:s:isNewUser','ym:s:visitDuration',
         'ym:s:startURL', 'ym:s:clientID', 'ym:s:lastTrafficSource']
```

Vamos fazer um dicionário com os parâmetros:

```
PARAM = {
    'date1': DATE1,
    'date2':DATE2,
    'source':SOURCE,
    'fields':','.join(FIELDS)
}
```

Observe que `fields` são passados em uma linha. Para reunir as strings da lista `fields`, chame o método `join()`.

Declaramos a URL para a solicitação e colocamos o identificador do contador `counter_id` na URL como parte do método `evaluate`:

```
URL = 'https://api-metrika.yandex.ru/management/v1/counter/{counter_id}/logrequests/evaluate?'\
    .format(counter_id=COUNTER_ID)
```

Agora está tudo pronto para a requisição. Basta importar a biblioteca `requests`, fazer uma requisição GET e armazenar os resultados em `r`:

```
import requests

r = requests.get(URL,headers={'Authorization': 'OAuth {0}'.format(TOKEN)},params=PARAM)
```

Observe que além de `params`, o método `get()` também recebe o parâmetro `headers`. Ele contém os **cabeçalhos HTTP** — dados de serviço que enviam informação adicional ao servidor (por exemplo, o formato no qual a resposta deve ser retornada). Especificamos o token de acesso no cabeçalho HTTP `'Authorization'`.

Vamos ver qual resposta recebemos da API:

```
print(r.text)
```

```
{"log_request_evaluation":{"possible":true,"max_possible_day_quantity":122049}}
```

Está no formato JSON. Você já sabe como processá-la:

```
import json

data = json.loads(r.text)
```

Podemos fazer uma requisição para recuperar dados da API de Logs da Y.Metrica?

```
data['log_request_evaluation']['possible']
```

```
True
```

Podemos sim!

## 3\. Como criar uma requisição de logs

É impossível obter tantos dados imediatamente! Primeiro, se cria uma requisição de logs. Ele informa à Y.Metrica API que tipo de requisições queremos. Depois, os servidores da Y.Metrica começam a preparar esta requisição.

A requisição de relatório é criada com o método **logrequests**. Veja como fica:

```
<https://api-metrika.yandex.net/management/v1/counter/{counterId}/logrequests>
 ? [date1=<string>]
 & [date2=<string>]
 & [fields=<string>]
 & [source=<log_request_source>]
```

Os parâmetros de requisição GET para o método `logrequests` são exatamente como em `evaluate`:

-   `date1` — a data de início da coleta de estatísticas
-   `date2` — a data final (não pode ser a data atual)
-   `fields` — uma lista de campos separados por vírgulas
-   `source` — origem do log

Vamos declarar o URL da requisição:

```
URL = 'https://api-metrika.yandex.ru/management/v1/counter/{counter_id}/logrequests?'\
    .format(counter_id=COUNTER_ID)
```

E enviar a requisição POST:

```
r = requests.post(URL,headers={'Authorization': 'OAuth {0}'.format(TOKEN)},params=PARAM)
```

Como na requisição anterior, o cabeçalho HTTP é passado no parâmetro `headers`.

Armazenaremos a resposta da nossa requisição em `data`:

```
data = json.loads(r.text)
print(data)
```

```
{'log_request': {'request_id': 4518541,
    'counter_id': 44147844,
    'source': 'visits',
    'date1': '2019-04-01',
    'date2': '2019-05-31',
    'fields': ['ym:s:visitID',
     'ym:s:dateTime',
     'ym:s:isNewUser',
     'ym:s:visitDuration',
     'ym:s:startURL',
     'ym:s:clientID',
     'ym:s:lastTrafficSource'],
    'status': 'created'}}
```

Temos os mesmos parâmetros que pedimos. Além disso, obtivemos o **request\_id**, o identificador da requisição de logs. Vamos armazená-lo na variável:

```
request_id = data['log_request']['request_id']
```

Precisaremos disso no futuro para verificar se os dados estão prontos.

## 4\. Verificando se os logs estão prontos

Agora que obtivemos o identificador de requisição de logs (`request_id`), podemos verificar se os dados deste log estão prontos para serem baixados.

Para obter a informação sobre requisições processadas pela Y.Metrica, precisaremos usar o método logrequest da **logrequests**. Veja como fica:

```
<https://api-metrika.yandex.net/management/v1/counter/{counterId}/logrequest/{request_id}>
```

Os seguintes parâmetros são passados na URL:

-   `counterId` — identificador do contador
-   `request_id` — o identificador da requisição de log

Depois de enviar a requisição para processar os dados, temos que esperar um pouco até que ela seja criada. Normalmente, isso leva alguns minutos. Em teoria, você pode enviar requisições manualmente para verificar o status uma vez por minuto, mas não é isso que um analista faria. Em vez disso, vamos escrever um algoritmo para verificar o status da requisição a cada minuto:

```
1. Status = created
2. Se Status = created:
3. Aguarde 60 segundos 
4. Requisitar Status da Metrica
5. Passe para a etapa 2
6. Se Status ≠ created:
7. Finalize o algoritmo
```

Precisamos que nosso ciclo fique inativo por 60 segundos e, em seguida, execute uma verificação de status. Se o status mudar de `'created'` para `'processed'`, o programa termina.

Para fazer o nosso ciclo pausar por um minuto, precisaremos chamar o método `sleep()` da biblioteca `time`. Passaremos para este método o número de segundos durante os quais o algoritmo "dormirá". Sem isso, as requisições de pausa serão enviadas com muita frequência, violando as políticas de serviço. Geralmente, há restrições quanto ao número de requisições que podem ser enviadas para APIs externas por segundo.

Também verificaremos o código de resposta para solicitar o status se e somente se o código for 200:

```
import time

status = 'created'
while status == 'created':
    time.sleep(60)
    print('trying')
    URL = 'https://api-metrika.yandex.ru/management/v1/counter/{counter_id}/logrequest/{request_id}'.format(
        counter_id=COUNTER_ID, request_id=request_id
    )
    r = requests.get(URL, headers={'Authorization': 'OAuth {0}'.format(TOKEN)})
    if r.status_code == 200:
        status = json.loads(r.text)['log_request']['status']
        print(json.dumps(json.loads(r.text)['log_request'], indent=4))
```

```
trying
{
    "request_id": 4518541,
    "counter_id": 44147844,
    "source": "visits",
    "date1": "2019-04-01",
    "date2": "2019-05-31",
    "fields": [
        "ym:s:visitID",
        "ym:s:dateTime",
        "ym:s:isNewUser",
        "ym:s:visitDuration",
        "ym:s:startURL",
        "ym:s:clientID",
        "ym:s:lastTrafficSource",
    ],
    "status": "processed",
    "size": 3674696,
    "parts": [{"part_number": 0, "size": 3674696}],
}
```

Se houverem muitos dados, a Y.Metrica API os retornará em partições. A resposta com o status `"processed"` armazena a lista de todas as partições dos dados em `parts`. O volume de dados em nossa amostra é de apenas 3,5 megabytes, portanto, é transmitido em uma partição só.

## 5\. Baixando logs

Para obter uma partição dos dados dos logs, você precisará enviar a seguinte requisição GET:

```
<https://api-metrika.yandex.net/management/v1/counter/{counterId}/logrequest/{request_id}/part/{part}/download>
```

Os seguintes parâmetros são passados na URL:

-   `counterId` — identificador do contador
-   `request_id` — o identificador da requisição de log
-   `part` — número de partições

Primeiro, obteremos os números de partições presentes na resposta:

```
parts = json.loads(r.text)['log_request']['parts']
print(parts)
```

```
[{'part_number': 0, 'size': 3674696}]
```

Esta é a lista de dicionários, onde cada um deles contém o número da partição e o seu tamanho. Para percorrer o número de cada partição com um ciclo, vamos colocá-los juntos em uma lista:

```
parts_list = []
for part in parts:
    parts_list.append(part['part_number'])
```

Dentro do ciclo, teremos que fazer uma requisição para obter parte dos dados.

```
from io import StringIO
import pandas as pd

tmp_dfs = []
for part_num in parts_list:
    URL = 'https://api-metrika.yandex.ru/management/v1/counter/{counter_id}/logrequest/{request_id}/part/{part}/download'.format(
        counter_id=COUNTER_ID, request_id=request_id, part=part_num
    )

    r = requests.get(URL, headers={'Authorization': 'OAuth {0}'.format(TOKEN)})
    if r.status_code == 200:
        tmp_df = pd.read_csv(StringIO(r.text), sep='\t')  # StringIO
        tmp_dfs.append(tmp_df)
logs_df = pd.concat(tmp_dfs)
```

Observe a construção `StringIO(r.text)`. A função `StringIO()` da biblioteca `io` permite que uma string seja lida como um arquivo. O problema é que o método `read_csv()` recebe apenas arquivos como argumento, enquanto a porção de dados na resposta da API é retornada como uma string. `StringIO(r.text)` converte a string em um **arquivo de memória**. Isso faz com que seja possível lê-lo com o método `read_csv()` no DataFrame `tmp_df`.

Vamos adicionar esses arquivos à lista `tmp_dfs` para que mais tarde possamos juntá-los em um único DataFrame `logs_df` com o método `pd.concat(`. Por fim, exportaremos os dados como um arquivo.csv chamando o método `to_csv()`:

```
logs_df.to_csv('sessions_logs.csv',index=False)
```

É assim que extraímos os dados da API de logs da Y.Metrica. É uma habilidade muito legal. Você poderá analisar os dados dos acessos ao site com mais detalhes e testar hipóteses mais complexas. É exatamente isso que faremos em nossa próxima aula.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-59-12-554Z.md
### Última modificação: 2025-05-28 19:59:12

# Trabalhando com Dados Brutos - TripleTen

Teoria

# Trabalhando com Dados Brutos

## Tarefas

O arquivo `/datasets/yandex_metrika_logs.csv` contém dados sobre as visitas à página de ajuda do Y.Metrica:

-   `ym:s:visitID` — identificador de acesso
-   `ym:s:dateTime` — hora do acesso
-   `ym:s:isNewUser` — se é o primeiro acesso ou não
-   `ym:s:visitDuration` — duração do acesso em segundos
-   `ym:s:startURL` — URL da página de login
-   `ym:s:clientID` — identificador de usuário
-   `ym:s:lastTrafficSource` — fonte de tráfego:
    -   `ad` — tráfego de anúncios publicitários
    -   `organic` — tráfego dos mecanismos de pesquisa
    -   `direct` — tráfego direto
    -   `referral` — tráfego de referências
    -   `internal` — tráfego interno
    -   `social` — tráfego de mídias sociais
    -   `email` — tráfego de e-mail
    -   `messenger` — tráfego de mensagens
    -   `saved` — tráfego de páginas salvas

1.  Construa dois histogramas em uma única visualização:
    
    -   A distribuição da duração do acesso para o tráfego de anúncios `(ym:s:lastTrafficSource = 'ad')`
    -   A distribuição da duração do acesso para o tráfego de mecanismos de pesquisa
        
        `(ym:s:lastTrafficSource = 'organic')`
        

**Ao construir histogramas, ignore os acessos com duração de 0 segundos.**

2.  Avalie se a diferença entre a duração média dos acessos provenientes dessas duas origens de tráfego é estatisticamente significativa. Para isso, teste a hipótese de que as duas médias são iguais. **Ao testar hipóteses, ignore os acessos com duração de 0 segundos.**

Trabalhando com Dados Brutos

Tarefa5 / 5

1.

Carregue dados do arquivo `/datasets/yandex_metrika_logs.csv` para o DataFrame `logs`. Imprima as primeiras 10 linhas de `logs`.

2.

Faça uma fatia de dados `logs_with_duration` para armazenar todos os acessos com mais de 0 segundos.

Imprima as primeiras 10 linhas da fatia de dados `logs_with_duration`.

3.

Pegue o DataFrame `logs_with_duration` e faça duas fatias de dados:

-   Uma fatia de dados `ad_visits` contendo acessos provenientes do tráfego de anúncios (`ym:s:lastTrafficSource == 'ad'`)
-   Uma fatia de dados `organic_visits` contendo acessos "orgânicos" provenientes do tráfego de mecanismos de pesquisa (`ym:s:lastTrafficSource == 'organic'`)

O nome da origem do tráfego é armazenado na coluna `"ym:s:lastTrafficSource"`.

Imprima 10 linhas de cada fatia de dados.

4.

Use uma visualização para exibir dois histogramas:

-   A distribuição das durações dos acessos para o tráfego de anúncios (`ym:s:lastTrafficSource = 'ad'`)
-   A distribuição das durações dos acessos para o tráfego de mecanismos de pesquisa (`ym:s:lastTrafficSource = 'organic'`)

Os histogramas deveriam incluir dados no intervalo de 0 a 2.000 segundos.

Para dar aos histogramas alguma transparência, especifique `alpha=0.5`.

5.

Teste a hipótese de que os dois valores médios são iguais. Em seguida, avalie se a diferença entre a duração média dos acessos das duas origens de tráfego é estatisticamente significativa.

Tira o alfa (o nível de significância) `0,05`.

Se a hipótese for rejeitada, imprima a mensagem: `"Rejeitamos a hipótese nula"`. Caso contrário, imprima `"Falhamos em rejeitar a hipótese nula"`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

import pandas as pd

from scipy import stats as st

  

logs \= pd.read\_csv('/datasets/yandex\_metrika\_logs.csv')

  

logs\_with\_duration \= logs\[logs\['ym:s:visitDuration'\] \> 0\]

  

ad\_visits \= logs\_with\_duration\[

logs\_with\_duration\['ym:s:lastTrafficSource'\] \== 'ad'

\]

organic\_visits \= logs\_with\_duration\[

logs\_with\_duration\['ym:s:lastTrafficSource'\] \== 'organic'

\]

  

ad\_visits\_durations \= ad\_visits\['ym:s:visitDuration'\]

organic\_visits\_durations \= organic\_visits\['ym:s:visitDuration'\]

  

print(ad\_visits\_durations.mean())

print(organic\_visits\_durations.mean())

  

  

alpha \= 0.05

  

results \= st.ttest\_ind(ad\_visits\_durations, organic\_visits\_durations)

  

if results.pvalue < alpha:

print("Rejeitamos a hipótese nula")

else:

print("Falhamos em rejeitar a hipótese nula")

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-59-13-880Z.md
### Última modificação: 2025-05-28 19:59:14

# Conclusão - TripleTen

Capítulo 5/9

Métricas de Usuário

# Conclusão

Neste capítulo, você começou a aprender como estudar o comportamento do usuário e se familiarizou com uma importante ferramenta para analistas, a Y.Metrica.

Vamos recapitular o que você aprendeu:

-   Como medir a atividade do usuário
-   O que é a métrica ASL e por que ela é importante
-   O que fazer se você detectar anomalias nos dados
-   O que é a Y.Metrica
-   Como obter dados através da API Y.Metrica e trabalhar com ela

### Leve isso com você

Faça o download da [folha conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_7/PT/Additional_materials/moved_DA_7_5_Folha_de_Concluses_Mtricas_do_Usurio.pdf) e do [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_7/PT/Additional_materials/moved_DA_7.5_Resumo_do_Captulo_Mtricas_de_Usurio.pdf) para que você possa consultá-los quando necessário.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-59-15-192Z.md
### Última modificação: 2025-05-28 19:59:15

# O Que Você Precisa Descobrir Sobre uma Empresa Durante sua Primeira Semana - TripleTen

Capítulo 6/9

Soft Skills

# O Que Você Precisa Descobrir Sobre uma Empresa Durante sua Primeira Semana

Você já sabe como o contexto é importante para os analistas de dados. Geralmente, o contexto dos dados são os objetivos do negócio e sua atividade. Você precisa entender qual é o papel dos analistas na empresa. Ao pensar sobre essas questões, você estará mais bem preparado para fazer uma entrevista e para passar a integrar o time.

Pergunta

John está sendo entrevistado para um cargo de analista júnior. O líder da equipe faz a seguinte pergunta a ele: "Qual métrica você iria analisar para avaliar o engajamento dos usuários com o aplicativo?"

John responde prontamente: "É melhor analisar a duração média da sessão: quanto mais tempo os usuários gastam com o aplicativo, melhor!"

Você concorda com o John?

Sim

Não

Não tenho certeza

Nós também não—precisamos de mais dados. Vamos descobrir!

Seu entendimento sobre o material é impressionante!

John estava sendo entrevistado para um cargo no departamento de análise de uma empresa renomada que desenvolve monitores de estilo de vida saudável para relógios e dispositivos móveis. O aplicativo trabalha a maior parte do tempo nos bastidores. Ele mede a pulsação e quantidade de passos do usuário, e as pessoas costumam abrir o aplicativo de manhã e de noite (para verificar seus resultados). Eles não passam muito tempo com ele aberto, pois o aplicativo envia notificações push.

Se você quiser analisar sessões de usuários, terá que coletar dados de duração média da sessão. Digamos que a duração é curtíssima. Então você deverá detectar o momento em que um usuário perde o interesse ou se distrai com algum objeto brilhante e fecha o aplicativo.

John se apressou para responder e não conseguiu analisar o caso com profundidade. O líder da equipe deve ter achado sua resposta muito superficial.

Você precisa conhecer a empresa que está entrevistando você. Abaixo segue uma lista de perguntas que te ajudarão a coletar **informações úteis sobre a empresa**.

1.  Ao se preparar para uma entrevista (ou para encontrar um cliente), verifique:
    -   A área de atividade da empresa/do cliente
    -   Quais de seus ativos são mais lucrativos
    -   Quem são seus clientes: indivíduos ou outras empresas
2.  Pesquise sobre a cultura corporativa da empresa: o que eles buscam em seu funcionários, qual a estrutura da equipe e sua escala de trabalho.
3.  Nas duas primeiras semanas, faça o possível para descobrir:
    -   Os objetivos chave da empresa e de seu departamento para o ano corrente, assim como para os períodos contábeis
    -   A posição do departamento na estrutura corporativa
    -   O seu KPI (indicador-chave de desempenho) e de seu departamento
    -   De quais processos corporativos você irá participar
    -   Qual a natureza das conexões de trabalho que você terá (colegas, clientes internos, empreiteiros)

Geralmente, os novatos descobrem essas informações conforme vão trabalhando, mas é melhor fazer isso de modo sistemático. Reflita de modo detalhado sobre sua compreensão da empresa em sua primeira semana de trabalho. Isso te dará uma ideia melhor de como abordar suas tarefas.

Se você quiser fazer uma carreira em uma empresa, aprenda qual a missão dela.

Pratique um pouco: escolha a empresa para a qual você sonha em trabalhar. Pesquise um pouco usando as perguntas listadas no Ponto 1. Compartilhe o que você descobrir no canal #community.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-59-17-397Z.md
### Última modificação: 2025-05-28 19:59:17

# Otimizando Dados em pandas - TripleTen

Capítulo 7/9

Licâo Bônus: Otimizando Dados em pandas

# Otimizando Dados em pandas

Você irá trabalhar com grandes volumes de dados no projeto final. Se você não tomar cuidado, os cálculos irão demorar demais ou até causar erros na plataforma online. Essa lição irá te ensinar o básico para otimizar dados em pandas para ajudar a evitar essas questões.

Esse conhecimento também será obviamente útil em seu trabalho no futuro.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/7.7.1PT_1661507637.png)

## Baixando uma pequena porção de dados

Você terá um arquivo .csv com logs do servidor como uma partição do projeto. Vamos passar o parâmetro `nrow=500` para o método `pd.read_csv()` e baixar as 500 primeiras linhas do arquivo:

```
import pandas as pd

df = pd.read_csv('data/prkt_metrics_data_afisha/visits.csv', nrows=500)
```

Analise os dados com o método `.info()`. Para determinar o tamanho exato dos dados, acrescente o parâmetro `memory_usage='deep'`.

```
df.info(memory_usage='deep')

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 500 entries, 0 to 499
Data columns (total 5 columns):
Device       500 non-null object
End Ts       500 non-null object
Source Id    500 non-null int64
Start Ts     500 non-null object
Uid          500 non-null uint64
dtypes: int64(1), object(3), uint64(1)
memory usage: 109.7 KB
```

Vamos ler o resultado. Primeiramente, podemos ver que os dados estão divididos em cinco colunas:

1.  Device — objeto (texto)
2.  End Ts — objeto
3.  Source Id — int64 (número inteiro)
4.  Start Ts — objeto
5.  Uid — uint64 (número inteiro sem assinatura)

Nós também podemos ver que o tamanho dos dados baixados é 109.7 kb. Nós precisamos dessa informação para a comparar com o tamanho dos dados otimizados.

Nesta lição, o ponto crucial da otimização é **nos livrarmos do tipo `object`** quando possível. Objetos em pandas são strings (texto) e usam um monte de memória.

## Categorias

Vamos analisar as colunas uma por uma. Pegue a coluna `Device`. Encontre o número de valores unívocos na coluna e como elas são distribuídas. Nós vamos usar o método `value_counts()`.

```
df['Device'].value_counts()

desktop    362
touch      138
Name: Device, dtype: int64
```

Bom, isso é interessante! Parece que essa coluna tem apenas dois valores: `desktop` e `touch`. Essas são presumivelmente categorias de dispositivos: computadores e dispositivos touch screen.

A pandas tem um tipo especial para esses dados: `category`. Ele se parece muito com `object` mas usa consideravelmente menos espaço. Para converter para o tipo `category`, use o método `astype('category')`:

```
df['Device'] = df['Device'].astype('category')
```

Vamos ver o que mudou:

```
df.head()

  Device                 End Ts    Source Id    Start Ts                     Uid
0    desktop    2017-06-01 00:02:00    5    2017-06-01 00:01:00    13890188992670018146
1    desktop    2017-06-01 00:02:00    3    2017-06-01 00:02:00    16152015161748786004
2    desktop    2017-06-01 00:03:00    3    2017-06-01 00:02:00    17364329443611800361
3    desktop    2017-06-01 00:04:00    3    2017-06-01 00:04:00     8842918131297115663
4    touch     2017-06-01 00:05:00    5    2017-06-01 00:03:00    15445098925224829050
```

Não há mudanças óbvias. Vamos ver se o tamanho mudou:

```
df.info(memory_usage='deep')

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 500 entries, 0 to 499
Data columns (total 5 columns):
Device       500 non-null category
End Ts       500 non-null object
Source Id    500 non-null int64
Start Ts     500 non-null object
Uid          500 non-null uint64
dtypes: category(1), int64(1), object(2), uint64(1)
memory usage: 79.4 KB
```

Agora, nosso conjunto de dados tem 79 kilobytes ao invés de 109. Nós conseguimos fazê-lo ficar menor mudando o tipo de dados em uma das colunas. Vamos ver o que mais nós podemos fazer.

## Datas

As colunas `Start Ts` e `End Ts` contêm as datas e horários. Você já sabe como converter textos em datas com o método `to_datetime()`. Vamos fazer isso:

```
df['Start Ts'] =  pd.to_datetime(df['Start Ts'], format="%Y-%m-%d %H:%M:%S")
df['End Ts'] =  pd.to_datetime(df['End Ts'], format="%Y-%m-%d %H:%M:%S")
```

Vamos ver como o tamanho muda:

```
df.info(memory_usage='deep')

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 500 entries, 0 to 499
Data columns (total 5 columns):
Device       500 non-null category
End Ts       500 non-null datetime64[ns]
Source Id    500 non-null int64
Start Ts     500 non-null datetime64[ns]
Uid          500 non-null uint64
dtypes: category(1), datetime64[ns](2), int64(1), uint64(1)
memory usage: 16.4 KB
```

Uau! 16.4 kilobytes! Praticamente um sétimo do tamanho inicial.

Agora vamos ver como conseguir os tipos corretos de dados quando nós baixarmos os dados, para começar.

## Baixando dados com os tipos corretos de dados

O método `pd.read_csv()` permite que você baixe dados no formato necessário já de cara. Use os parâmetros `dtype` e `parse_dates`.

Em `dtype` você terá que indicar o dicionário cujas chaves contém os cabeçalhos de colunas e cujos valores contém os tipos de dados aos quais essas colunas são convertidas. Por exemplo, para converter os dados na coluna Device para o tipo da categoria, nós vamos precisar escrever `dtype={'Device': 'category'}`.

Em `parse_dates` você irá precisar passar uma lista de cabeçalhos das colunas que contém datas. Por exemplo: `parse_dates=['Start Ts', 'End Ts']`.

Agora nós vamos colocar tudo junto:

```
dd = pd.read_csv(
    'data/prkt_metrics_data_afisha/visits.csv',
    nrows=500,
    dtype={'Device': 'category'},
    parse_dates=['Start Ts', 'End Ts'],
)
```

Vamos ver qual será o tamanho do nosso novo DataFrame.

```
dd.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 500 entries, 0 to 499
Data columns (total 5 columns):
Device       500 non-null category
End Ts       500 non-null datetime64[ns]
Source Id    500 non-null int64
Start Ts     500 non-null datetime64[ns]
Uid          500 non-null uint64
dtypes: category(1), datetime64[ns](2), int64(1), uint64(1)
memory usage: 16.2 KB
```

Nossa! Nós temos 16 kb novamente. Você pode pensar que para computadores modernos, a diferença entre 16 e 100 kilobytes é praticamente inexistente. Isso é verdade, é claro. Mas não se esqueça de que você baixou só 500 linhas. Se você pegar o volume total, a diferença será realmente significativa.

Por hora, é só isso. Boa sorte com seu projeto!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-59-18-732Z.md
### Última modificação: 2025-05-28 19:59:19

# Sprint 8 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 8/9

Projeto do Curso

# Sprint 8 - Projeto

Parabéns! Você concluiu a seção sobre a Análise de Negócios. É hora de aplicar o conhecimento e as habilidades que você adquiriu em um projeto: um estudo de caso analítico da vida real que você concluirá por conta própria.

Quando você terminar o projeto, envie seu trabalho para o revisor do projeto para avaliação. Ele te dará feedback dentro de 48 horas. Use o feedback para fazer alterações e, em seguida, envie a nova versão de volta ao revisor do projeto.

Você poderá receber mais feedback sobre a nova versão. Isso é completamente normal. Não é incomum passar por vários ciclos de feedback e revisão.

Seu projeto será considerado concluído assim que o revisor do projeto o aprovar.

## Descrição do projeto

Você se saiu muito bem no curso da TripleTen e recebeu uma oferta de estágio no departamento analítico da Y.Afisha. Sua primeira tarefa é ajudar a empresa a otimizar suas despesas com marketing.

Você tem:

-   Logs do servidor com dados sobre os acessos a Y.Afisha de janeiro de 2017 até dezembro de 2018
-   Arquivo de despejo (ou dump file, em inglês) com todos os pedidos feitos durante o período
-   Estatísticas de despesas com marketing

Você vai analisar:

-   Como as pessoas usam o produto
-   Quando elas começam a comprar
-   Quanto dinheiro cada cliente traz para a empresa
-   Quando as despesas serão cobertas

### **Instruções para completar o projeto**

**Passo 1. Carregue os dados e prepare-os para a análise**

Armazene os dados sobre os acessos, pedidos e despesas em variáveis. Otimize os dados para a análise. Certifique-se de que cada coluna contém o tipo correto de dados.

Caminhos de arquivo:

-   _/datasets/visits\_log\_us.csv_. [Baixe o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/visits_log_us.csv)
-   _/datasets/orders\_log\_us.csv_. [Baixe o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/orders_log_us.csv)
-   _/datasets/costs\_us.csv_. [Baixe o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/costs_us.csv)

**Passo 2. Faça relatórios e calcule as métricas:**

1.  Produto
    
    -   Quantas pessoas usam-no cada dia, semana e mês?
    -   Quantas sessões ocorrem por dia? (um usuário pode realizar várias sessões).
    -   Que comprimento tem cada sessão?
    -   Com que frequência os usuários voltam?
    
2.  Vendas
    
    -   Quando as pessoas começam a comprar? (Na análise de KPIs, nós geralmente estamos interessados em saber o período de tempo entre o registro e a conversão - quando o usuário se torna um cliente. Por exemplo, se o registro e a primeira compra de um usuário ocorrem no mesmo dia, ele pode encaixar na categoria de Conversão 0d. Se a compra é realizada no dia seguinte, isso será a Conversão 1d. Você pode usar qualquer abordagem que permita comparar as conversões de diferentes coortes, para que você possa determinar qual coorte ou canal de marketing tem a maior eficiência)
    -   Quantos pedidos os clientes fazem durante um determinado período de tempo?
    -   Qual é o volume médio de uma compra?
    -   Quanto dinheiro eles trazem para a empresa (LTV)?
    
3.  Marketing
    
    -   Quanto dinheiro foi gasto? No total/por origem/ao longo do tempo
    -   Quanto custou a aquisição de clientes para cada origem?
    -   Os investimentos valeram a pena? (ROI)
    

Construa gráficos para ver como essas métricas diferem para vários dispositivos e diferentes origens de anúncios e como elas mudam com o tempo.

**Passo 3. Escreva uma conclusão: recomende aos especialistas de marketing quanto dinheiro e onde seria melhor investir.**

Quais origens/plataformas você recomendaria? Fundamente sua escolha: em quais métricas você se concentrou? Por quê? Que conclusões você tirou ao encontrar os valores das métricas?

**Formato.** Complete a tarefa em um notebook Jupyter. Insira o código nas células _code_ e explicações de texto nas células _markdown_. Aplique formatação e cabeçalhos.

### Descrição dos dados

A tabela `visits` (os logs do servidor com dados sobre os acessos ao site):

-   _Uid_ — identificador unívoco do usuário
-   _Device_ — dispositivo do usuário
-   _Start Ts_ — data e hora do início da sessão
-   _End Ts_ — data e hora do final da sessão
-   _Source Id_ — identificador da origem do anúncio através do qual o usuário chegou

Todas as datas nesta tabela estão no formato YYYY-MM-DD.

A tabela `orders` (dados sobre os pedidos):

-   _Uid_ — identificador unívoco do usuário que faz um pedido
-   _Buy Ts_ — data e hora do pedido
-   _Revenue_ — a receita da Y.Afisha com o pedido

A tabela `costs` (dados sobre as despesas com marketing):

-   source\__id_ — identificador da origem de anúncio
-   _dt_ — data
-   _costs_ — despesas com esta origem de anúncio neste dia

## **Como meu projeto será avaliado?**

Seu projeto será avaliado com base nos seguintes critérios. Leia-os cuidadosamente antes de iniciar o projeto.

Isso é o que os revisores do projeto procuram ao avaliar seu projeto:

-   Como você prepara os dados para análise
-   Quais gráficos você fez para as métricas
-   Como você interpreta os gráficos resultantes
-   Como você calcula e interpreta cada parâmetro
-   Como você fundamenta suas recomendações aos especialistas de marketing e quais métricas usa
-   Se você segue a estrutura do projeto e mantém o código organizado
-   As conclusões que você tirou
-   Se você deixa comentários a cada passo

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-ac24fff3-d199-4ded-8a0f-e2125ccadaba.containerhub.tripleten-services.com/doc/tree/bf2511dd-3b02-44db-ab4a-27df4aa26269.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-59-23-169Z.md
### Última modificação: 2025-05-28 19:59:23

# Feedback do Sprint 8 - TripleTen

Capítulo 9/9

Conclusão

# Compartilhe sua opinião

**Olá!**

Você acabou de concluir um sprint e enviar seu projeto. Parabéns por alcançar este marco! 🚀

Agora é o momento perfeito para dar seu feedback sobre o sprint. Sua opinião vai nos ajudar a aprimorar ainda mais o processo de aprendizagem.

Este questionário curto vai levar apenas 2 minutos. Todas as respostas vão ser confidenciais: não vamos compartilhá-las de forma que identifique você.

Agradecemos por nos ajudar a evoluir com você!

![](https://practicum-content.s3.us-west-1.amazonaws.com/common/ok.svg)

Muito obrigado pelas suas respostas!

Já as encaminhamos para os departamentos correspondentes.

7 — 7

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T22-59-24-495Z.md
### Última modificação: 2025-05-28 19:59:25

# Conclusão - TripleTen

Capítulo 9/9

Conclusão

# Conclusão

Parabéns! Você completou o módulo da Análise de Negócios.

### Você aprendeu a:

-   Calcular prazos de retorno do negócio e o retorno sobre o investimento em marketing
-   Construir gráficos de marketing e funis de produtos em Python
-   Analisar coortes de várias maneiras
-   Encontrar a taxa de retenção e o índice de cancelamento
-   Analisar coortes comportamentais
-   Calcular a economia unitária por venda
-   Calcular a economia unitária por cliente
-   Medir a atividade do usuário
-   Obter dados através da API da Y.Metrica e trabalhar com ela

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-19-087Z.md
### Última modificação: 2025-05-28 20:00:19

# Introdução a "Tomando Decisões de Negócios Baseados em Dados". - TripleTen

Capítulo 1/9

Introdução a Tomando Decisões de Negócios Baseados em Dados

# Introdução a "Tomando Decisões de Negócios Baseados em Dados".

Nesta parte do curso, vamos falar sobre tomar decisões de negócios com base em dados.

Você irá aprender sobre hipóteses: como formulá-las, testá-las e analisar os resultados dos testes.

**O que você irá aprender:**

-   Como determinar quais métricas de negócios são as mais importantes
-   Como sugerir hipóteses
-   Métodos para testar hipóteses
-   Modos de priorizar hipóteses
-   Como analisar testes A/B, quais são as limitações do método e os erros mais comuns durante sua aplicação

Você também irá trabalhar novamente com análise estatística de dados e ver como ela se relaciona com a testagem A/B.

Você irá realizar tarefas relacionadas a lojas online nas duas lições e no projeto final. Clique [aqui](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/moved_DA_8%20Sprint_Descrio_do_Projeto_prt-br.pdf) para ver a descrição do projeto e o que ele irá envolver.

Neste sprint, você irá expandir seu conhecimento nestas áreas:

![](https://practicum-content.s3.amazonaws.com/resources/Analise_de_Negocio_1713355316.png)

![](https://practicum-content.s3.amazonaws.com/resources/Narrativa_de_Dados_1713355328.png)

![](https://practicum-content.s3.amazonaws.com/resources/Analise_Estatistica_1713355959.png)

### Quanto tempo isso vai levar?

Este capítulo irá te ensinar como mergulhar ainda mais fundo nos processos de negócios e a tomar decisões sobre os negócios usando a análise de dados. Você irá precisar de 30 a 50 horas para completar este material, dependendo do seu conhecimento prévio e de seus hábitos de estudo. Se você sentir que está ficando para trás não hesite em contactar a nossa Equipe de Orientação.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-20-433Z.md
### Última modificação: 2025-05-28 20:00:20

# Introdução - TripleTen

Capítulo 2/9

Introdução aos Testes de Hipóteses em Negócios

# Introdução

Hipóteses são formuladas e testadas para que as métricas de negócio possam ser aprimoradas. Neste capítulo, você vai estudar métodos básicos para identificar as métricas de negócio mais úteis, gerando hipóteses, construindo e mantendo uma base de experimento (base de conhecimento).

### O que você irá aprender:

-   Como definir pontos de crescimento usando a análise de métricas de negócio
-   Como decompor e construir métricas-guia
-   Como gerar ideias
-   Como formular hipóteses com base nessas ideias

### Quanto vai demorar:

7 lições, aproximadamente 10 a 20 minutos cada

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-21-737Z.md
### Última modificação: 2025-05-28 20:00:22

# O Que um Negócio Precisa? - TripleTen

Capítulo 2/9

Introdução aos Testes de Hipóteses em Negócios

# O Que um Negócio Precisa?

O objetivo de qualquer negócio é gerar receita e, eventualmente, lucro. Você já sabe como o lucro líquido difere da receita e o que é margem de lucro.

Cada métrica possui seu lugar, mas nós simplesmente não temos o tempo e os recursos para prestar atenção o suficiente em todas elas. Algumas vezes, até parece que as métricas se contradizem: por exemplo, quando uma loja vende produtos com desconto, isso aumenta a receita, mas diminui sua margem.

As métricas que deverão ser tratadas como prioritárias vão depender da estratégia e dos planos da empresa.

Por exemplo, hoje a Simple Things, que vende roupas que usam velcro ao invés de zíperes e botões, é a maior fonte de receita para seus fundadores. Eles recebem dividendos resultantes do lucro líquido da empresa. Isso faz com que o lucro líquido seja a métrica mais importante para seus proprietários.

Há 10 anos, quando Simple Things estava apenas começando, seus fundadores estavam sonhando com uma vida melhor, e tinham esperança de que a empresa ia gerar receita para eles no futuro. Foi por isso que eles investiram mais em marketing e no desenvolvimento de redes de contato de distribuição—em poucas palavras, eles sacrificaram o lucro corrente para investir no crescimento de sua parcela de mercado. Naquele momento, os objetivos mais importantes eram impulsionar o crescimento da receita corrente e assegurar a receita futura.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_____startup_1587473241.jpg)

A empresa Innovations for Everyone (Even You) - "Inovações para Todos (Até Você)" está desenvolvendo uma tecnologia inovadora de construção. O produto precisa de investimento e ainda não está à venda, então não está gerando receita. Mesmo assim, a empresa tem a esperança de ultrapassar seus concorrentes em termos de volume de vendas assim que seu produto for inserido no mercado. Investimentos futuros e o destino do CEO dependem do sucesso do produto.

Nesse caso, a receita futura e a velocidade com que a nova tecnologia é desenvolvida são as métricas chave.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_____stroitelstvo_1587473264.jpg)

Ações de empresas gigantes bem consolidadas no mercado são comercializadas publicamente. A empresa anunciou que possui a expectativa de uma receita de $2 bilhões este ano. Os próprios donos da empresa são a parte interessada, mas o valor das ações é determinado pelo mercado. Se a empresa não conseguir manter suas promessas relativas à receita, suas ações perderão valor e as partes interessadas, consequentemente, vão perder parte de seus investimentos.

Neste caso, a receita é a métrica mais importante para os proprietários.

A estratégia de uma empresa ou departamento determina quais métricas precisam ser aperfeiçoadas. Um analista precisa aprender a identificar a estratégia e usá-la para escolher métricas relevantes que possam ser aprimoradas.

Pergunta

A loja We Leave Much To Be Desired possui um sistema arrojado de gratificações, em que todos os funcionários recebem gratificações na mesma data anualmente. Cálculos preliminares da quantia total necessária para esses pagamentos evidenciam ser possível que a empresa acabe com problemas no caixa: uma situação em que o custo dos passivos (incluindo o pagamento dos salários) excede a reserva financeira. Qual métrica de negócio deve ser aprimorada para evitar isso?

Receita

Lucro líquido

Lucro bruto

Correto. Na hora de calcular o pagamento de gratificações, o lucro bruto é a métrica mais importante de todas. Se o lucro bruto não for suficiente para cobrir todos os passivos, a empresa deverá pesquisar outras formas de financiamento, com emprétimos. Por outro lado, se a empresa está preparada para contrair empréstimos em prol de um crescimento agressivo, não há nada errado com o capital da dívida.

Lucro operacional

Seu entendimento sobre o material é impressionante!

Pergunta

Um ano atrás, a empresa On My Own Dime pegou um empréstimo de 12 meses. Para evitar a falência e garantir que seria possível financiar suas atividades no futuro, a companhia precisa começar pagando os juros da sua dívida.

Em quais métricas de negócio a empresa deve se focar?

Receita

Lucro bruto

Lucro operacional

Certo! **`lucro líquido - impostos e empréstimos`**. Lucro operacional é a fonte para cobrir dívidas.

Lucro líquido

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-23-634Z.md
### Última modificação: 2025-05-28 20:00:23

# Principais Métricas. Decomposição - TripleTen

Capítulo 2/9

Introdução aos Testes de Hipóteses em Negócios

# Principais Métricas. Decomposição

Conhecer a métrica-chave de negócio é crucial para qualquer analista. Isso torna possível não apenas lidar com tarefas empolgantes e complexas, mas também entender a importância delas para a empresa.

Às vezes, porém, é muito difícil para analistas, gerentes de produto, ou qualquer outro especialista ter impacto nas métricas. Resultados financeiros dependem de vários fatores e do trabalho de dezenas, se não milhares, de empregados. Então é muito importante entender o que você pode melhorar e como as melhorias que você oferecer podem influenciar a métrica-chave de negócio.

Quando mudanças em um indicador têm um impacto direto nos objetivos principais da empresa ou do departamento, é chamado de **métrica principal**. Exemplos podem incluir CAC, LTV e taxas de conversão. O mais importante é se certificar de que a métrica que você escolheu é realmente principal. Métricas principais te ajudam a fazer predições; métricas defasadas ajudam a explicar o passado.

Por exemplo, o Facebook aprendeu que usuários continuam postando nas suas páginas se eles têm pelo menos 10 amigos. "Dez amigos adicionados" é uma boa métrica principal. O Facebook conseguiu engajar e reter novos usuários aumentando o número de links sociais.

Um serviço de streaming escolheu o número de filmes assistidos durante o primeiro mês como métrica principal para retenção de usuários. Isso efetivamente encorajou usuários a assinarem, oferecendo-lhes um mês de graça. Como resultado, usuários assistiram filmes por 30 dias, mas não assinaram. Essa métrica principal não se relaciona com a parcela de usuários que fazem uma assinatura após o primeiro mês.

O especialista em marketing, John, é responsável pela quantidade de visitantes do site e fontes de tráfego. Ele sabe muito bem que a receita é a métrica chave. John pode aumentá-la se descobrir uma maneira de conseguir mais visitantes. Isso pode ser feito tanto por meios orçamentários quanto com um ajuste na campanha publicitária.

Porém, isso irá funcionar somente se a taxa de conversão e o volume médio de compras permanecerem iguais. Na prática, você não pode afirmar com certeza que outras métricas permanecerão iguais, já que elas estão todas interconectadas. Os modos como elas estão conectadas precisam ser descritos. Em outras palavras, precisamos **decompor a métrica**.

Vamos ver como a decomposição da métrica funciona a partir do seguinte exemplo:

Pegue a receita.

Por um lado, **`receita = tamanho médio de compra x número de compras`**

onde **`número de compras = número de usuários adquiridos x rácio de conversão usuário-compra`**.

Por outro lado, **`receita = número de usuários adquiridos x receita por visitante (RPV)`**.

Ao mesmo tempo, **`receita por visitante = receita total / número de visitantes`**.

A receita média por usuário adquirido é chamada **receita por visitante** (**RPV**).

Uma mesma métrica (receita, por exemplo) pode ser decomposta de vários modos, e todos eles estarão corretos. Isso torna possível o estudo do funil por ângulos diferentes para encontrar modos para aprimorar a métrica do negócio.

John decide adicionar fontes adicionais de novos usuários. Ele descobriu que a RPV dos usários das novas fontes de tráfego é menor que a RPV para as fontes mais antigas. Como as ações de John não estão tendo impacto no comportamento dos usuários no site, ele procura o Tom, analista de produto, para compartilhar suas descobertas.

Tom analisa o comportamento de usuários que estão vindo de novos canais e percebe que seu LTV é menor que o CAC. Isso quer dizer que não é lucrativo atrair usuários dessas fontes.

Se John e Tom tivessem considerado a RPV, eles nunca iriam enxergar a raiz do problema. É por isso que eles examinaram mais métricas principais: LTV e CAC. Para entender a natureza de uma métrica, tente decompô-la de diversos modos.

Pergunta

Quantos são os modos com que podemos decompor uma determinada métrica?

0

1

2

Tantos quanto conseguirmos imaginar

Nenhuma das alternativas anteriores

Você conseguiu!

Pergunta

Você é um analista para uma loja online. Escolha os principais indicadores para a métrica "receita da loja".

A quantidade de usuários adquiridos, revisões do produto e empregados da central de atendimento

Taxa de conversão, volume médio de compra e a quantidade de cliques no banner na página principal

A quantidade de usuários adquiridos, duração média da sessão, e quantidade média de produtos visualizados

A quantidade de usuários adquiridos, taxa de conversão e volume médio de compras

Isso mesmo! Essas três métricas são quase sempre utilizadas para a analisar a eficiência da operação de uma loja online. Nosso foco é aprimorá-las, a fim de aumentar a receita e o lucro.

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-24-977Z.md
### Última modificação: 2025-05-28 20:00:25

# Identificando Métricas Importantes - TripleTen

Capítulo 2/9

Introdução aos Testes de Hipóteses em Negócios

# Identificando Métricas Importantes

Geralmente, a maior parte do trabalho relacionado à identificação de métricas principais e sua decomposição já terá sido realizado por outra pessoa. Sua empresa já possui modelos de produto, marketing e finanças, ou também pode se utilizar de boas práticas criadas por outras empresas que possam ser aplicadas em seu mercado.

Na seção sobre Análise de Negócios, você analisou a receita e as despesas, funis de marketing e de produto, coortes, economia unitária e métricas de usuário. Há outras abordagens que também podem te ajudar a identificar métricas chave para negócios:

-   **Análise da parte interessada**
-   **Análise de prejuízo e lucro (P&L)**
-   **Análise da jornada do consumidor**
-   **Análise da experiência do usuário (UX)**
-   **Entrevistas com usuários**

Essa lista contém os modos mais comuns para determinar métricas de negócio, mas há mais deles, é claro. Mesmo assim, aplicar os métodos dá uma ideia do que é importante para os usuários, empresários ou proprietários e revela pontos de alavancagem.

**Análise da parte interessada**

Uma **parte interessada** (ou stakeholder em inglês) é uma pessoa ou grupo com um interesse na atividade da empresa. Eles podem causar impacto no trabalho da empresa e vice-versa.

Partes interessadas, de modo mais abrangente, podem ser da chefia da empresa, seus empregados ou parceiros e autoridades fiscais. Às vezes, pessoas que não parecem ter relação direta com a empresa são partes interessadas: por exemplo, pessoas da vizinhança em que uma construção está sendo realizada. Ela produz poluição sonora e do ar, entre outras. Isso afeta os cidadãos locais, que então são partes interessadas da empresa de construção.

Em um sentido mais estrito, partes interessadas são pessoas com interesses financeiros na performance da empresa. Na prática, eles são gerentes de médio e alto nível, investidores e proprietários. Neste curso, vamos usar a definição mais restrita.

Ao conduzir a análise das partes interessadas, precisamos considerar seus objetivos e tarefas, assim como a maneira que a atividade da empresa os afeta. Se as motivações de curto e longo prazo forem formuladas corretamente, as necessidades das partes interessadas irão coincidir e representar de modo definitivo os interesses da empresa. Esses interesses são o que você irá rastrear.

**Exemplo de análise da parte interessada de um serviço para propulsionar a conversão do comércio eletrônico**

Nome

Interesses chave

Métricas-chave

Investidores, proprietários

Aumentar o valor do negócio, impulsionando dividendos

Receita, lucro

Diretor executivo

Atingir os KPIs de receita e lucro estabelecidos pelos proprietários

Taxas de crescimento de receita e lucro

Diretor de marketing

Aumentar a receita e o orçamento com marketing

Orçamento de marketing , ROMI , receita

Responsável Técnico Principal

Manter uma performance sustentável do produto, monitorar despesas com desenvolvimento e infraestrutura de TI, a taxa de crescimento do produto

Quantidade de tempo de que o site ficou em baixo, quantidade de bugs, elaboração do orçamento com desenvolvimento e infraestrutura, quantidade de tarefas cumprida

Usuários

Conveniência do produto, o custo do produto e sua introdução, receita extra, transparência de despesas e ROI

ROI do produto, tempo economizado, quantidade de solicitações de suporte, quantidade de horas gastas oferecendo suporte a cada usuário

**P&L**

Uma instrução **profit and loss**, (lucros e perdas - **P&L**) é um tipo de instrução de contabilidade. Ela é estudada no contexto, por exemplo, da contabilidade gerencial—um sistema para analisar a atividade de uma organização para auxiliar na tomada de decisões gerenciais.

![](https://practicum-content.s3.amazonaws.com/resources/FY23_Q1_Consolidated_Financial_Statements_page-0003_1708941128.jpg)

_Relatório de Lucros e Perdas (P&L) extraído do relatório anual da Apple_

A instrução P&L pode ser muito útil para a diretoria de uma empresa, porque:

-   Ela contém dados completos sobre receita e despesas da empresa. Ela inclui pagamentos contratuais que ainda serão realizados ou recebidos com diferimento.
-   A instrução P&L apresenta informações para um período, e não de uma data específica.
-   Ela possui uma estrutura padrão. É fácil achar o modelo necessário para o tipo de negócio em questão.
-   A instrução já é decomposta em despesas por área de atividade e departamento. Isso torna possível calcular a economia unitária com um mínimo de cálculos adicionais.

Ao determinar métricas-chave de negócio, você também pode analisar outras formas possíveis de modelagem e demonstração financeira.

**Análise da experiência do cliente**

Um **customer journey map** (**СJM)** - mapa de experiência do cliente) é um diagrama que reflete a interação de um usuário com o site (ou serviço ou produto) a partir da primeira visita.

Um **CJM** pode gerar percepções em relação ao modo como os usuários interagem com seu produto e ajudá-lo a entender seus objetivos, suas necessidades e emoções. Ele te mostra o que pode estar impedindo os usuários de atingirem seus objetivos e conseguirem aquilo que precisam.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_____analiz_1587475388.jpg)  

**Análise de UX**

A análise da user experience (experiência do usuário - UX) envolve uma gama de métodos. A ideia básica é estudar como os usuários interagem com a interface a partir da participação dos próprios usuários.

Estes são alguns métodos empregados na análise de UX:

-   **Teste de usabilidade**

Isso ajuda a determinar se a interface é conveniente. Os usuários recebem tarefas simples para realizar nela, e depois disso as dificuldades que eles enfrentaram são analisadas.

-   **Rastreamento ocular**

Isso revela como os usuários interagem com a tela. Equipamentos desenvolvidos especialmente para isso rastreiam os movimentos oculares dos usuários para determinar o que eles observam consciente e inconscientemente. Isso te ajuda a entender processos e percepções mentais dos usuários.

-   **Entrevistas**

Os usuários devem responder a perguntas padronizadas ou espontâneas sobre a interface, sua lógica e a experiência de interagir com ela.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.2.4PT.png)

Métricas relacionadas à satisfação do consumidor ajudam a determinar se os usuários gostam de seu produto e quanta receita ele poderá gerar.

Os analistas não costumam ser responsáveis pelas entrevistas, análises de UX, CJM ou instruções P&L. Mas elas podem servir de inspiração para novas ideias, especialmente quando funcionários de departamentos diferentes estão tentando identificar as métricas-chave do negócio.

Pergunta

Você quer descobrir o que precisa ser feito para melhorar a satisfação dos clientes. Quais métodos irão ajudar?

Análise de UX, entrevistas com usuários, mapas de experiência do usuário

Isso mesmo! Esses três métodos te ajudam a ver o produto do ponto de vista de seus usuários e a entender o que precisa ser melhorado para mantê-los satisfeitos.

Análise de UX, análise de P&L

Análise das partes interessadas, análise de P&L

Você conseguiu!

Pergunta

Qual análise irá te ajudar a entender como aumentar a margem de lucro de um produto?

Análise de UX

Entrevistas com usuários

Análise das partes interessadas

Análise de P&L

Isso mesmo! A instrução P&L traz informações sobre o produto ou empresa no que se refere às despesas e à receita.

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-26-310Z.md
### Última modificação: 2025-05-28 20:00:26

# Diários de Experimento - TripleTen

Capítulo 2/9

Introdução aos Testes de Hipóteses em Negócios

# Diários de Experimento

Ao trabalhar em um projeto, muitas vezes duas pessoas podem surgir com a mesma ideia (não necessariamente ao mesmo tempo). Uma delas pode dizer "Eu sei o que fazer!", apenas para ouvir que "Sim, nós já estamos fazendo isso", ou "Nós tentamos, mas não funcionou."

Para evitar testar a mesma hipótese diversas vezes, especialistas mantém **diário de experimento** e **diário de hipótese**, onde eles registram as informações ou hipóteses que são testadas, as condições externas e os resultados.

Diários de experimento podem ser muito úteis no sentido de compartilhar o conhecimento entre os departamentos. Eles também são essenciais ao discutir e planejar ações futuras com as partes interessadas, pois eles fornecem aos tomadores de decisão informações confiáveis e os ajudam a rejeitar ideias ruins.

Por exemplo, o chefe do departamento onde o analista John trabalha descobre que o site de um concorrente agora possui um design natalino. O chefe sugere que a empresa também dê um toque festivo para seu site.

John abre o diário de experimento e descobre que já houve uma tentativa de fazer isso, que não resultou em uma melhoria de métricas-chave. Ele passa essa informação para o chefe. Após uma discussão, eles concluem que não vale a pena investir em uma repetição do experimento. Eles não são pães-duros! Estão apenas sendo cautelosos com o capital da empresa.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.2.5PT.png)

### Como manter um diário de experimento?

Não existe uma ferramenta padrão ou universal para manter um diário, então, faça da forma que preferir. Você pode usar sistemas complexos de armazenamento de conhecimento, armazenamento em nuvem ou tabelas. Cada abordagem possui prós e contras. Escolha uma que irá se adequar a seu projeto, pelo menos a curto prazo.

Estes são alguns princípios gerais para a criação de diários de experimento:

-   A confiabilidade da informação que você insere deve ser confirmada não apenas por seu autor, mas também por outro usuário do diário (talvez outro autor). As regras para a testagem devem ser claras e estabelecidas explicitamente. Isso ajuda a evitar erros ao inserir os dados e evita que funcionários desonestos afetem o processo. É raro, mas acontece.
-   Transparência. As regras para adicionar, alterar e excluir dados do diário devem estar claras para todos os usuários.
-   Formatação padrão. Todos usuários devem entender o que devem fazer para encontrar as informações que precisam.
-   Acessibilidade a todas partes interessadas da empresa. Os níveis de acesso podem ser diferenciados, mas é útil para todos poder observar os resultados do trabalho dos outros. Isso pode pelo menos evitar que você repita aquilo que já foi feito. Às vezes, ele pode até te ajudar a ter suas próprias ideias, usando os resultados existentes.

Pergunta

Por que é necessário que outro usuário do diário verifique os dados que são inseridos?

Para garantir que os dados sejam válidos e não possuam erros.

Para assegurar que a informação esteja clara.

Para verificar se a formatação está de acordo com as regras e padrões.

Todas opções acima

Isso mesmo!

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-27-951Z.md
### Última modificação: 2025-05-28 20:00:28

# Fazer ou Não Fazer Experimentos? Eis a Questão. - TripleTen

Capítulo 2/9

Introdução aos Testes de Hipóteses em Negócios

# Fazer ou Não Fazer Experimentos? Eis a Questão.

As possibilidades de métricas de negócio impactantes são quase sempre limitadas. Por exemplo, um analista não pode influenciar relacionamentos com os empreiteiros.

Digamos que a Toy Universe assinou um acordo de fornecimento de acordo segundo o qual a nova edição dos kits de construção Armos serão entregues direto da fábrica. O fornecedor cumpre o pedido, contanto que o volume seja de pelo menos 1 milhão por mês.

A Armos irá trazer à loja online um público que irá pagar generosamente. É por isso que um analista da Toy Universe não pode recomendar que a empresa pare de vender Armos ou adquira um valor menor do que 1 milhão por mês.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_____lego_1587477427.jpg)

Além de limitações como essas, às vezes diferentes departamentos podem ter conflitos de interesse. Eles podem estar relacionados com contradições relativas ao sistema KPI ou à escolha entre metas e resultados de curto e longo prazo.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.2.6PT.png)

Algo do tipo ocorreu com o estúdio de webdesign Landingland. É uma empresa bem estabelecida, com um fluxo de trabalho simplificado e lucro estável. O único problema é que o volume médio de compras não é muito grande. Um analista fez algumas pesquisas e sugeriu ao diretor da empresa que, em conjunto com as páginas de destino, ele deveria começar a fazer sites mais complexos, que são mais caros.

Funcionários do departamento de vendas do Landingland recebem uma gratificação fixa no término de cada projeto. Os projetos não duram mais do que quatro semanas para sites simples... Mas o desenvolvimento de um produto mais complexo pode demorar vários meses, então os funcionários de vendas terão que esperar mais tempo por suas gratificações.

Por isso que é difícil iniciar um novo processo sem mudar o sistema motivacional. Mais do que isso, se o departamento de vendas dedicar mais tempo para cada site, a receita atual pode diminuir.

Um novo analista da panificadora Great Grain foi a uma conferência profissional. Nela, ele aprendeu que colocar avaliações de clientes satisfeitos no cabeçalho de um site aumenta a lealdade e a confiança dos clientes. Ele decidiu tentar esse método.

Mas ele não sabia que a empresa testou essa hipótese há um ano, e que isso provocou uma queda de 30% na conversão e quase não influenciou as vendas. Desta vez, novamente, não havia milagre, e as vendas caíram.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.2.6.2PT.png)

A fim de não repetir os erros do passado, tente sempre descobrir se a hipótese já foi testada antes. Se o experimento já foi conduzido, estude seus resultados.

Pergunta

Você decidiu testar a função callback. Se os usuários passam mais de 10 segundos no site, eles irão visualizar uma janela modal dizendo "Deixe seu número, nós iremos entrar em contato!". O que deve ser feito antes do início do experimento?

Descubra se existe qualquer tipo de restrição direta em relação à mudança do aspecto do site na empresa.

Descubra se existe alguma limitação técnica para a exibição desse tipo de janela.

Decida se uma inovação desse tipo irá ou não afetar o fluxo de trabalho de outros funcionários.

Especifique se isso já foi feito por outros.

Faça uma pesquisa para saber se as empresas que introduziram essa função ficaram satisfeitas com o aumento na conversão.

Todas opções estão corretas.

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-29-316Z.md
### Última modificação: 2025-05-28 20:00:29

# Como Gerar Ideias - TripleTen

Capítulo 2/9

Introdução aos Testes de Hipóteses em Negócios

# Como Gerar Ideias

Você nunca sabe de antemão se uma hipótese acabará sendo verdadeira e útil. É por isso que você deve sempre pensar em diversas possibilidades para serem priorizadas e testadas.

Ao desenvolver uma hipótese, os analistas devem raciocinar como detetives, estudando cuidadosamente todas as evidências e considerando cada possibilidade. Eles nunca devem descartá-las, mas sim guardá-las para posterior priorização e elaboração. Tudo isso irá ajudar a identificar a hipótese correta.

### Fazendo Brainstorms

Uma parte importante deste método é evitar críticas ou auto-limitação. Até as ideias mais esquisitas devem ser ouvidas e registradas para uma discussão posterior. Após a sessão de brainstorm, as ideias que se provarem inadequadas poderão ser rejeitadas.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_____ostonovis_copy_1587478278.jpg)

### Materiais didáticos

Livros, cursos, canais de vídeo de especialistas e podcasts podem ser ótimas fontes de ideias. A maioria das tarefas de análise cotidianas já foi resolvida. Casos similares ao seu podem já ter sido abordados, você deverá apenas refinar a abordagem. O mais importante é enxergar além das diferentes circunstâncias para encontrar características em comum.

### Sua experiência própria

Qualquer experiência de trabalho (mesmo em outras esferas) pode te ajudar a gerar ideias. Digamos que Tom trabalhava para uma fábrica onde ele analisava processos de negócios. Ele saiu e foi para uma empresa de TI. Nessa empresa, o fluxo de trabalho era organizado consoante o sistema de transporte, e Tom teve que se adaptar rapidamente. Ele aplicou os métodos de análise que ele usava em seu emprego anterior e conseguiu diminuir o tempo médio gasto nas tarefas em 15%.

### Analisando mercado e concorrentes

A análise de mercado envolve a pesquisa de clientes e concorrentes: quem são, o que eles querem, e do que eles são capazes?

Os perfis de consumidores trazem à tona características importantes do produto que poderiam agradar segmentos do público alvo.

A análise da concorrência nos permite identificar características valiosas que fazem com que os usuários comprem mais, com mais frequência, e pagando um preço mais alto.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_____urna_2_1587478383.jpg)

### Entrevistas com usuários

Entrevistar os usuários é outra maneira de descobrir as necessidades dos clientes, e o que está impedindo que eles comprem seu produto. Mais do que isso, as entrevistas ajudam a identificar as necessidades que podemos satisfazer e o que mais o produto poderia oferecer.

### Feedback do cliente

-   Feedback do departamento de vendas. Eles costumam estar cientes das demandas e problemas dos usuários, assim como daquilo que o clientes acham que está fazendo falta no produto existente.
-   Feedback do departamento de suporte técnico. Eles sabem o que mais incomoda os clientes e quais funções estão aquém das expectativas, ou não funcionam adequadamente.
-   Avaliações de lojas de aplicativos e agregadores. Essas avaliações não são estruturadas, e elas devem ser coletadas de diversas fontes, o que torna difícil trabalhar com elas. Porém, elas são muito úteis, já que dizem exatamente como os usuários se sentem quando interagem com seu produto.
-   Avaliações profissionais. Via de regra, o nível de detalhamento aqui é elevado, apesar de que isso possa não ser relevante em relação às impressões do usuário médio.

### Análise de UX

Traz informações sobre como os usuários enxergam seu produto e interagem com a interface. Ela pode servir como fonte para ideias de como tornar a experiência dos usuários melhor e mais conveniente.

### Percepções a partir do estudo dos dados

Você pode começar pesquisando possibilidades para o aprimoramento de algumas métricas de negócio, para depois gerar hipóteses para outras totalmente diferentes.

Por exemplo, a analista Jane estava trabalhando em um dashboard para avaliar a performance da equipe. Ela considerou o tempo necessário para completar uma tarefa como uma das métricas. Jane percebeu que dois colegas estavam trabalhando muito mais rápido do que os outros; isso porque eles desenvolveram uma ferramenta simples de automação para simplificar seu trabalho.

Após a introdução desse programa para toda a equipe, a velocidade média para a conclusão de tarefas aumentou em 20%. Isso permitiu que eles aceitassem mais projetos e aumentou a receita (sua métrica-chave) em 5%.

Ao trabalhar em qualquer tipo de tarefa, nunca se esqueça do contexto—os negócios de uma empresa como um todo.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_____urna_3_1587478417.jpg)

### TRIZ (a Teoria da Solução Criativa de Problemas)

TRIZ (a Teoria da Solução Criativa de Problemas—"TRIZ" é um acrônimo russo, pois a teoria foi criada por um intelectual soviético) é um conjunto de métodos para resolver tarefas criativas complexas. Ela te ajuda a mergulhar fundo no problema, a encontrar sua raiz, determinar contradições e resolvê-las. Você irá estudar essa metodologia por conta própria; procure pelo link na seção de leitura independente ao final deste capítulo.

Tudo isso, de modo algum, deve ser considerado como uma lista conclusiva de maneiras para gerar ideias. De acordo com seu crescimento profissional na área, você irá encontrar ferramentas adequadas para resolver problemas específicos e que são adequadas para você.

Pergunta

Quais são as regras essenciais para realizar brainstorms com sucesso?

Quando os participantes compartilham suas ideias, todas devem ser levadas em conta, até as mais loucas

Correto. Experimentos envolvendo MRIs do cérebro humano evidenciam que as pessoas tomam decisões e começam a agir antes de conseguirem entender qual seu objetivo. Por isso você nunca deve rejeitar uma ideia imediatamente, por pensar ser estúpida. Uma ideia louca pode acabar sendo a melhor opção, após estudada e adaptada. Ou pode acabar sendo boa para gerar outras ideias que poderão ser implantadas no futuro. Anote todas e discuta uma de cada vez.

O processo de gerenciar e escolher ideias para discussão deve ser supervisionado pelos membros mais competentes e inteligentes da equipe.

É melhor escolher a primeira ideia proposta e simular uma situação em que ela funciona. Se conseguirmos convencer a equipe toda, não iremos precisar de mais nenhuma ideia.

Alguém deveria trabalhar como uma espécie de advogado do diabo, tentando achar os pontos fracos de cada nova ideia. Alguém deveria defendê-las. E os outros deveriam gerar ideias.

Muito bem!

Pergunta

Como o feedback dos clientes é coletado?

Por meio de entrevistas diretas

Pelo departamento de vendas

Pelo departamento de suporte ao cliente

A partir da análise de avaliações

A partir da análise de avaliações profissionais

Todas opções acima

Isso mesmo! São infinitas as formas de se comunicar com os clientes.

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-30-682Z.md
### Última modificação: 2025-05-28 20:00:31

# Formulando uma Boa Hipótese - TripleTen

Capítulo 2/9

Introdução aos Testes de Hipóteses em Negócios

# Formulando uma Boa Hipótese

Você formulou uma lista de ideias; agora é necessário formular uma hipótese. Qual a diferença?

Uma hipótese é uma proposição que pode tanto ser provada como rejeitada em um experimento (mas que ainda não foi).

Por exemplo, a frase "Colocar um botão de Compras vermelho aumenta a conversão" é, basicamente, só uma ideia. Porém, se você disser "Colocar um botão de Compras vermelho (`#FF0000`) na página do produto poderia impulsionar a conversão para compras repetidas", isso irá se tornar uma hipótese.

Não existem critérios específicos para definir o que seria ou não uma hipótese, mas existem alguns princípios gerais. Uma hipótese deve:

-   ser lógica e consistente
-   ser testável
-   não contradizer fatos conhecidos
-   ser efetiva em termos teóricos ou práticos
-   ser detalhada
-   se referir a uma esfera específica

De modo geral, ela deveria ser formulada para que todos a compreendessem da mesma forma.

Por exemplo, a analista Maria teve a ideia de que introduzir anúncios pay-per-click iria aumentar a receita da loja Phone Home. Ela mostrou essa ideia para o Leonardo, especialista em marketing júnior. Ele acabou de receber o orçamento para uma nova fonte de aquisição e começou a campanha.

O experimento gerou um aumento da receita, mas o ROI dos clientes adquiridos acabou sendo negativo. Isso aconteceu porque o custo para atraí-los era muito elevado. Anúncios foram exibidos em solicitações de pesquisa muito genéricas (por exemplo, "iPhone", ao invés de algo mais específico "comprar capas de iPhone estilosas com a foto de um lobo"). O tráfego trouxe mais custos do que John imaginava, e a campanha acabou gerando prejuízo.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.2.8PT.png)

Uma avaliação revelou que a analista Maria achou que a requisição "não exiba anúncios para solicitações com alta frequência" era redundante e não mencionou isso ao explicar a tarefa para o Leonardo. Maria também não mencionou que o ROI dos novos clientes deve ser positivo.

Uma aplicação mal pensada da hipótese gerou perda de tempo, esforço e dinheiro.

Pergunta

Escolha a melhor formulação da hipótese:

Precisamos acrescentar avaliações de clientes à página de destino:

Podemos aumentar a conversão de abastecimento para compras adicionando um bloco com quatro avaliações de clientes à página de destino do produto. O bloco deve aparecer como um cursor que muda automaticamente a cada 5 segundos. Ele deve ser inserido entre o bloco que contém as informações relativas às necessidades dos clientes e o que contém os preços.Os estilos dos cabeçalhos e das fotos dos clientes devem ser uniformes.

Esta hipótese está completa e detalhada.Só tenha em mente que você não pode exceder os limites de sua autoridade ao especificar os detalhes.

Acrescentar quatro avaliações de clientes à página de destino irá aumentar a conversão.

Acrescentar quatro avaliações de clientes à pagina de destino entre as seções de informações sobre o produto e seu preço irá aumentar a conversão de abastecimento-por-compra.

Trabalho maravilhoso!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-33-104Z.md
### Última modificação: 2025-05-28 20:00:33

# Conclusão - TripleTen

Capítulo 2/9

Introdução aos Testes de Hipóteses em Negócios

# Conclusão

Parabéns! Você mergulhou fundo nos interesses de negócios e aprendeu a gerar hipóteses.

### O que você aprendeu:

-   Como usar a análise de métricas de negócios para encontrar pontos de crescimento
-   Como decompor e construir métricas-guia
-   Como gerar ideias
-   Como formular hipóteses baseadas nessas ideias

### Guarde isso com você:

Baixe o [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DS_sprint_8/moved_Resumo_do_Captulo_Introduo_aos_Testes_de_Hipteses_em_Negcios.pdf) para poder consultá-lo quando necessário.

### Leitura independente:

**Análise de Coorte**

[https://medium.com/analytics-for-humans/a-beginners-guide-to-cohort-analysis-the-most-actionable-and-underrated-report-on-google-c0797d826bf4](https://medium.com/analytics-for-humans/a-beginners-guide-to-cohort-analysis-the-most-actionable-and-underrated-report-on-google-c0797d826bf4) (o artigo está em inglês)

**Fazendo Brainstorms**

[https://www.wordstream.com/blog/ws/2019/01/03/brainstorming-techniques](https://www.wordstream.com/blog/ws/2019/01/03/brainstorming-techniques) (o artigo está em inglês)

**ТRIZ**

[https://www.mindtools.com/pages/article/newCT\_92.htm](https://www.mindtools.com/pages/article/newCT_92.htm) (o artigo está em inglês)

**Análise de Jornada do Usuário**

[https://www.mockplus.com/blog/post/user-journey-vs-user-flow](https://www.mockplus.com/blog/post/user-journey-vs-user-flow) (o artigo está em inglês)

**Pesquisa UX**

[https://www.youtube.com/watch?v=YTRIeWI0EGQ](https://www.youtube.com/watch?v=YTRIeWI0EGQ) (os materiais estão em inglês)

**Desenvolvimento do Cliente**

[https://producttribe.com/product-management/customer-development-guide](https://producttribe.com/product-management/customer-development-guide) (o artigo está em inglês)

No próximo capítulo, você vai aprender como escolher um método ao testar uma hipótese.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-34-401Z.md
### Última modificação: 2025-05-28 20:00:34

# Introdução - TripleTen

Capítulo 3/9

Escolhendo um Método Experimental

# Introdução

Neste capítulo, você estudará vários métodos para testar hipóteses e aprenderá sobre os prós e contras do teste A/B.

### O que você vai aprender:

-   Como selecionar o melhor método para testar uma hipótese
-   Os prós e contras do teste A/B

### Quanto vai demorar:

5 _lições, aproximadamente de 10 a 20 minutos cada uma_

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-35-727Z.md
### Última modificação: 2025-05-28 20:00:36

# Métodos Experimentais - TripleTen

Capítulo 3/9

Escolhendo um Método Experimental

# Métodos Experimentais

Assim que você tiver feito uma lista de hipóteses, decida quais métodos que você utilizará para testá-los. Os métodos podem ser qualitativos ou quantitativos.

**Métodos qualitativos** ajudam você a responder às perguntas "O quê?" "Como?" e "Por quê?"

Eles permitem que você entenda melhor os usuários e descubra o que eles fazem e como. Porém, o que é mais importante é que esses métodos revelam porque os usuários fazem o que fazem: o que os orienta e quais são suas motivações.

Uma desvantagem de métodos qualitativos é que eles estão sujeitos a tendências cognitivas de entrevistadores e entrevistados. Por exemplo, o pesquisador pode subconscientemente prestar mais atenção e dar mais importância aos fatos que apóiam a hipótese. Ao mesmo tempo, os entrevistados podem dar respostas socialmente aceitáveis para serem apreciadas.

Além disso, os resultados obtidos a partir de pesquisas qualitativas não te ajudarão a avaliar o impacto das alterações de quantidades. Também não te ajudarão a resumir os resultados, pois a amostra será muito limitada.

**Métodos quantitativos** respondem à pergunta "Quanto?"

Eles te ajudam a determinar em que medida as mudanças afetam o comportamento do usuário com bastante precisão. No entanto, eles possuem certas limitações em relação à interpretação dos resultados e à compreensão da motivação dos usuários.

Jessica, a especialista em Marketing, decidiu descobrir como os usuários leram as newsletters da empresa. Primeiro ela fez um estudo qualitativo: ela recebeu vários usuários, alugou equipamentos especiais e convidou um especialista em rastreamento ocular. Ela definiu uma tarefa para os usuários e estudou a maneira como leem o e-mail. Além do rastreamento ocular, Jessica também analisou como os usuários falaram sobre suas ações.

Jessica, a especialista em Marketing, decidiu descobrir como os usuários leram as newsletters da empresa. Primeiro ela fez um estudo qualitativo: ela recebeu vários usuários, alugou equipamentos especiais e convidou um especialista em rastreamento ocular.

Como resultado, Jessica descobriu quais partes de e-mails atraíram a maior atenção e o que permaneceu negligenciado. Ela finalmente entendeu por que os usuários leram a newsletter, como eles fizeram e que informação eles procuraram.

No entanto, com um experimento como este, é impossível descobrir quantos usuários pensam da mesma maneira que os respondentes. Além disso, os usuários estavam sendo observados, então eles podem estar agindo de forma um pouco diferente do habitual.

Uma semana depois, Jessica tentou a sorte em um estudo quantitativo: ela mudou o assunto do e-mail e adicionou uma introdução cativante. Como resultado, a conversão nos acessos do site cresceu em 20%.

Infelizmente, as razões para esse aumento permaneceram desconhecidas. Jessica nunca entendeu o que fez os usuários se interessarem em ir para o site ou como sua percepção do produto mudou.

Pergunta

Hipótese: Os usuários compram seu produto porque estão procurando um sentimento de segurança. Escolha o método certo de testar essa hipótese.

Pesquisa qualitativa

Correto. Esta é uma pergunta de "por que", então os métodos qualitativos são adequados.

Pesquisa quantitativa

"Sentir-se seguro" não pode ser medido, então qualquer pesquisa é inútil.

Trabalho maravilhoso!

Pergunta

Qual método de teste de hipótese deveríamos escolher se precisarmos encontrar a parcela dos usuários que precisa que o produto tenha mais funções?

Pesquisa qualitativa

Pesquisa quantitativa

Isso mesmo! Quando são necessários os números precisos, volte a métodos quantitativos.

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-38-104Z.md
### Última modificação: 2025-05-28 20:00:38

# Métodos Qualitativos de Testes de Hipóteses - TripleTen

Capítulo 3/9

Escolhendo um Método Experimental

# Métodos Qualitativos de Testes de Hipóteses

Os métodos qualitativos de testes de hipótese incluem as entrevistas de profundidade, de especialista e de problema, de análise UX e de grupos focais.

**Entrevistas de profundidade**

As entrevistas em profundidade são caracterizadas por um alto nível de confiança entre o entrevistado e o entrevistador. Isso permite que os motivos ou necessidades ocultas do usuário sejam revelados.

Este método requer um trabalho preparatório considerável e um entrevistador altamente qualificado. Também leva muito tempo: como regra, as entrevistas em profundidade duram mais de 30 minutos, mais do que as pesquisas padrão.

O ideal seria que os entrevistados nem percebessem que estão sendo entrevistados. Eles acham que estão participando de uma discussão amigável ou de um estudo sobre outro assunto.

O plano e as perguntas preparadas são apenas o esqueleto do estudo. Eles não definem os limites; apenas dão ao entrevistador um ponto de partida.

**Entrevistas de especialista**

Uma entrevista com um especialista é um tipo de entrevista minuciosa com um especialista no campo que está sendo estudado.

Entrevistas com especialistas identificam as opiniões profissionais, que são diferentes dos sentimentos de um usuário comum. Conversar com um especialista pode ajudá-lo a rejeitar hipóteses que já foram formuladas ou testadas.

**Entrevistas de problema**

As entrevistas de problemas são um tipo minucioso de pesquisa ou entrevista. Elas ajudam a revelar se os usuários possuem necessidades específicas que seu produto deveria satisfazer.

Você não deve fazer perguntas sobre o futuro, como "Você usaria isso?" ou "Quanto você pagaria por isso?" Concentre-se no presente ou no passado: "Como você resolve esse problema agora? Quanto você pagou por esta solução?"

Os usuários geralmente estão prontos para discutir qualquer problema abertamente sem distorcê-lo emocionalmente, até que comecem a pensar em quanto custa resolvê-lo.

**Análise de UX**

Esse grupo de métodos ajuda a determinar quais os problemas que os usuários enfrentam ao usar a interface. O teste de hipótese é uma parte essencial de qualquer análise de UX.

**Grupos focais**

Um grupo focal é uma discussão em grupo onde os participantes compartilham seus pensamentos sobre um assunto ou produto.

De modo geral, um grupo focal não deve incluir mais do que 10 pessoas.

Muitas vezes, vários grupos homogêneos são formados. Por exemplo, você pode montar grupos com base em idade, sexo, categorias sociais ou profissionais, para que os membros tenham alguma característica em comum. Isso te ajuda a entender as opiniões de um mesmo tipo de usuários. E também pode aumentar a produtividade da discussão, pois minimiza o atrito decorrente de contextos pessoais diversos.

Esses são os métodos de pesquisa qualitativa mais populares, mas existem mais. Se os estudos qualitativos são realizados em um grande número de pessoas, você também pode incorporar métricas quantitativas: por exemplo, a parcela de usuários que expressaram uma determinada opinião.

Pergunta

Qual método qualitativo você usará se precisar descobrir quem no setor já resolveu seu problema e como?

Grupo de foco

Entrevista de problema

Entrevista de especialista

Boa! Um especialista da sua área provavelmente ficará feliz em compartilhar sua experiência e dizer o que sabe sobre o problema.

Análise de UX

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-39-385Z.md
### Última modificação: 2025-05-28 20:00:39

# Métodos quantitativos de testes de hipóteses: teste A/B - TripleTen

Capítulo 3/9

Escolhendo um Método Experimental

# Métodos quantitativos de testes de hipóteses: teste A/B

**Pesquisas/sondagens**

As pesquisas podem ser realizadas pessoal e remotamente: por telefone, em sites de pesquisa, ou via messenger ou e-mail.

Os grupos geralmente são muito grandes, e todos respondem às mesmas perguntas. Geralmente, é solicitado que os entrevistados escolham uma ou várias respostas de uma lista.

A principal vantagem das pesquisas (e particularmente as pesquisas online ) é o alto nível de precisão. Os entrevistados anônimos são mais propensos a serem honestos. No entanto, se não formuladas com atenção, as perguntas podem gerar uma tendência à escolha de certas respostas, abrindo caminho para resultados distorcidos.

**Análise dos dados existentes**

Às vezes uma hipótese pode ser testada direta ou indiretamente, com a utilização de dados preexistentes.

Além disso, os dados preexistentes sobre o comportamento dos usuários podem ser úteis para priorizar as hipóteses.

**Testes A/B**

Um dos métodos mais populares e precisos de testes de hipóteses é o **Testes A/B**.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.3.6PT.png)

O tráfego é dividido em dois ou mais grupos. Uma versão específica da página é exibida a todos, e os dados de visualizações da página, cliques e compras são coletados. Os dados são então analisados, e as métricas são calculadas. O último passo é desenhar uma conclusão sobre a eficácia das alterações comparando o grupo de teste com o **grupo de controle**, ao qual se mostra a versão original da página da Web.

Digamos que o analista Tom lançou um teste A/B para descobrir quão grande pode ser o botão "Comprar" na página de destino de um curso on-line.

Ele dividiu o tráfego da página de destino: 28.000 participantes do teste foram divididos em dois grupos. Ao primeiro grupo (de controle) foi mostrada a versão da página com um botão pequeno, enquanto ao segundo mostrou-se a versão de botão grande. O experimento levou duas semanas.

Havia 140 inscrições do curso para o grupo de controle (taxa de conversão: 1%) e 175 para o grupo de teste (1,25%). O segundo número é 25% maior que o primeiro, então Tom decidiu parar o teste e mostrar o botão grande para todos os usuários. Isso levou a um aumento de 25% no número de inscrições.

**Resultados experimentais**

Grupo

Usuários

Registros

Conversão

Controle (botão pequeno)

14000

140

0.01

Teste (botão grande)

14000

175

0.0125

Pergunta

Você quer saber como aumentar a conversão: alterando o desenho do bloco que descreve as vantagens do seu produto ou movê-lo para o cabeçalho do site. Qual método de teste de hipóteses é melhor?

Pesquisas/sondagens

Analisando dados que você já tem

Testes A/B

Isso mesmo! O teste A/B irá mostrar-lhe como altera a conversão de impacto. Mostre a versão de uma página para o primeiro grupo, outra versão para o segundo, e assim por diante. Ao grupo de controle deve ser mostrado a versão original da página.

Você conseguiu!

Pergunta

Você gostaria de saber quão fortemente os usuários querem ver novas funções em seu produto. Qual método quantitativo você usará?

Pesquisas

Analisando dados que você já tem

Isso mesmo! A análise das solicitações de ajuda, discussões on-line, e comentários podem ajudá-lo a entender o quanto os usuários querem funções específicas em comparação com os outros.

Testes A/B

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-40-676Z.md
### Última modificação: 2025-05-28 20:00:41

# Prós e contras do teste A/B - TripleTen

Capítulo 3/9

Escolhendo um Método Experimental

# Prós e contras do teste A/B

O teste A/B ajuda a revelar as diferenças entre os grupos, se houver. Por exemplo, você pode comparar as métricas do site:

-   Conversão
-   Volume médio de compra
-   Profundidade de acesso
-   Número de pedidos
-   Número de cliques
-   Parcela de usuários que retornam
-   Parcela de usuários que desistiram do aplicativo em determinada fase

As mudanças a serem estudadas não se limitam ao layout do serviço. O teste A/B pode dizer:

-   Qual tratamento é mais eficaz para uma determinada doença (há uma história famosa de um teste A/B para descobrir a cura para o escorbuto em 1747)
-   Qual script de vendas funciona melhor
-   Qual base de assinantes compra mais
-   Qual tecnologia de fabricação torna um produto mais resistente ao alongamento

Uma vantagem do teste A/B é que ele pode ser aplicado a vários campos, pois é flexível. Além disso, é considerado o método mais preciso de pesquisa de marketing.

Ao mesmo tempo, esse método é o mais caro e consome muitos recursos, e os resultados são muito sensíveis à forma como os dados foram coletados. Se você ignorar alguns detalhes durante a configuração inicial do teste, pode acabar perdendo muito tempo! O experimento é realizado e os dados são coletados, mas um fragmento minúsculo, mas crucial, é deixado fora. Isso muitas vezes acontece na vida real.

Outra desvantagem do teste A/B é sua sensibilidade ao ambiente. Dividir os grupos A e B de forma que a única diferença seja o fator em estudo só é possível em condições de laboratório. Na realidade, os usuários são influenciados por muitos fatores que não podem ser completamente controlados. Tudo o que você pode fazer é esperar que os fatores afetem todos os grupos igualmente. E aqui está a nossa principal limitação: precisamos de muitos dados.

As amostras devem ser formadas corretamente (mais importante, os dados devem ser distribuídos aleatoriamente entre os grupos) e devem ser grandes. Se essas condições forem atendidas, a lei dos grandes números e o teorema central do limite entrarão em ação. Na prática, muitas vezes não há dados suficientes para realizar um bom teste A/B. Você terá que diminuir seus requisitos de precisão ou usar outros métodos de teste de hipóteses.

Tudo isso torna o teste A/B uma ferramenta bastante delicada. No entanto, se uma empresa fizer isso corretamente, tomar a decisão certa e aumentar seu lucro, o teste A/B valerá a pena.

Pergunta

Qual das seguintes opções **não** é uma vantagem do teste A/B?

Precisão

Universalidade

Baixo custo e simplicidade

O teste A/B definitivamente não é barato nem simples. É tão caro quanto poderia ficar a pesquisa de marketing.

Possibilidade de trabalhar com Macrodados

Fantástico!

Pergunta

Qual das seguintes opções **não** é uma desvantagem do teste A/B?

Altos requisitos de qualidade de dados

Altos requisitos para quantidade de dados

É bom apenas para testar hipóteses em gerenciamento de produtos e marketing

Certo, na verdade é empregado em muitas esferas – até médicos e engenheiros o usam.

É demorado

É intensivo em recursos

Os resultados são sensíveis ao ambiente

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-42-552Z.md
### Última modificação: 2025-05-28 20:00:42

# Um exemplo de resultados de teste A/B - TripleTen

Teoria

# Um exemplo de resultados de teste A/B

Você realizou um teste A/B. Vamos estudar os dados que você coletou:

```
import pandas as pd

data = pd.read_csv('lesson_3/lesson_data_3-3.csv', sep=',')

print(data.head(5))
```

```
     date     group  visitors  orders
0  1/1/2019     A      1013      10
1  1/2/2019     A       969      13
2  1/3/2019     A      1004      13
3  1/4/2019     A      1006      12
4  1/5/2019     A       968      14
```

A tabela contém as seguintes colunas:

-   `date`
-   `group` — A ou B
-   `visitors` — o número de visitantes do site em uma determinada data em um determinado grupo de teste
-   `orders` — o número de pedidos em uma determinada data em um determinado grupo de teste

Vamos calcular o número total de visitantes e pedidos para cada grupo de teste. Como não precisamos da coluna de data, vamos nos livrar dela com o método `drop()` e o parâmetro `axis=1`. Em seguida, agruparemos os dados pela coluna com grupos de teste A/B usando o método `groupby()` e o parâmetro `as_index=False`. Para encontrar o número total de visitantes e pedidos em cada grupo, usaremos funções agregadas:

```
data_new = (
    data.drop(['date'], axis=1)
    .groupby('group', as_index=False)
    .agg({'visitors': 'sum', 'orders': 'sum'})
)

print(data_new)
```

```
  group  visitors  orders
0     A     30996     385
1     B     31105     419 
```

Vamos encontrar a razão entre o número de pedidos e o número de usuários em cada grupo: `data_new['orders']/data_new['visitors']`. Vamos salvá-lo na nova coluna `['ordersToVisitorsRatio']` na tabela `data_new`.

Agora vamos definir o formato de saída dentro de uma função `lambda`. Você estudou esse tipo de função na parte introdutória do curso. Vamos recapitular sua sintaxe: `argumento:expressão`. `lambda x: x['number']*3` significa que a função precisa multiplicar o elemento da coluna `'number'` por três e retornar o resultado.

Vamos deixar quatro dígitos decimais: `"{0:.4f}".format(x)`. Vamos converter cada elemento da tabela para o formato necessário usando o método `map()`. É semelhante a `apply()` e nos permite empregar a função `lambda` para cada valor dentro de uma coluna:

```
data_new['ordersToVisitorsRatio'] = (data_new['orders']/data_new['visitors']).map(lambda x: "{0:.4f}".format(x))
print(data_new)
```

```
   group  visitors  orders ordersToVisitorsRatio
0     A     30996     385                0.0124
1     B     31105     419                0.0135
```

A proporção de pedidos para visitantes é maior no grupo B do que no grupo A.

O experimento foi um sucesso! Os visitantes farão mais pedidos se usarem a versão modificada da página.

Um exemplo de resultados de teste A/B

Tarefa2 / 2

1.

Encontre o número total de usuários e pedidos em cada grupo do teste. Armazene a tabela resultante em `table` e imprima esta variável. Não mude o pré-código.

2.

Encontre a razão entre o número de pedidos e o número de visitantes para cada grupo de teste. Adicione os resultados à `table`, armazenando-os na nova coluna `orders_to_visitors_ratio`.

Arredonde os valores para quatro casas decimais. Imprima `tabela`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

import pandas as pd

  

data \= pd.read\_csv('/datasets/data\_3-3.csv', sep\=',')

  

print(data.head(5))

  

table \= (

data.drop(\['date'\], axis\=1)

.groupby('group', as\_index\=False)

.agg({'visitors': 'sum', 'orders': 'sum'})

)

table\['orders\_to\_visitors\_ratio'\] \= (table\['orders'\]/table\['visitors'\]).map(lambda x: "{0:.4f}".format(x))

  

  

  

print(table)

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-00-43-851Z.md
### Última modificação: 2025-05-28 20:00:44

# Conclusão - TripleTen

Capítulo 3/9

Escolhendo um Método Experimental

# Conclusão

Que método você deveria usar para testar uma hipótese?

Não existe algoritmo universal; você terá que pensar em cada caso individual. Cada hipótese pode ser testada de várias maneiras, com custos e níveis de precisão variados.

Dicas úteis para escolher um método:

-   Qual pergunta precisa ser respondida? Se você estiver procurando por causas, motivação ou percepções sobre o pensamento dos usuários, precisará de métodos qualitativos. Se você está procurando números, use os quantitativos.
-   Quão importante é a precisão para você? Os métodos mais precisos são os mais caros e exigem um alto nível de preparação. Se a precisão não for extremamente importante, use análises de mercado e concorrentes ou dados que já estejam disponíveis. Mas se você precisar de medições precisas, o teste A/B é o caminho a percorrer.

### O que você aprendeu:

-   Como selecionar um método de teste de hipóteses
-   Como explicar as vantagens e desvantagens do teste A/B

### Takeaway

Baixe o [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/moved_Resumo_do_Captulo_Escolhendo_um_Mtodo_Experimental_pt.pdf) para poder consultá-lo quando necessário.

### Leitura independente:

[https://medium.com/@shengyuchen/problem-interviews-9982fb4cf3fc](https://medium.com/@shengyuchen/problem-interviews-9982fb4cf3fc) (os materiais estão em inglês).

No próximo capítulo, você aprenderá a priorizar hipóteses.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-13-066Z.md
### Última modificação: 2025-05-28 20:03:13

# Introdução - TripleTen

Capítulo 4/9

Priorizando Hipóteses

# Introdução

Priorizamos hipóteses para reduzir os gastos com experimentação e testar apenas as ideias mais promissoras.

Neste capítulo, você aprenderá como priorizar hipóteses e analisar em detalhes a aplicação do framework RICE.

### O que você vai aprender:

-   Como priorizar hipóteses usando os frameworks ICE, RICE e WSJF
-   Como avaliar os seguintes elementos das estruturas ICE/RICE: Alcance, Impacto, Confiança, Esforço

### Quanto tempo vai demorar:

5 _lições, aproximadamente de 10 a 20 minutos cada uma_

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-14-426Z.md
### Última modificação: 2025-05-28 20:03:14

# Como e por que priorizamos hipóteses - TripleTen

Teoria

# Como e por que priorizamos hipóteses

É uma situação comum: testar uma hipótese custará muito dinheiro, mas seus recursos são limitados. Então, você precisa escolher o que testar e o que deixar de lado. Para tentar garantir o crescimento máximo das principais métricas de negócios, as hipóteses que você formulou precisam ser **priorizadas**.

Estudaremos os métodos mais comuns de priorização de experimentos.

### A Matriz de Eisenhower

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.4.2PT.png)

A **Matriz de Eisenhower** é uma forma de priorizar onde o problema recebe duas qualidades: importância e urgência. Embora a matriz tenha sido originalmente projetada para priorizar tarefas, ela também funciona para hipóteses.

Cada hipótese entra em um dos quatro quadrantes: importante e urgente, não importante e urgente, importante e não urgente, não importante e não urgente. As hipóteses do quadrante A (importantes e urgentes) devem ser testadas primeiro. Depois vêm as hipóteses do quadrante B (importantes e não urgentes).

Para hipóteses, mas não para tarefas, aquelas que não são importantes (os quadrantes C e D) não chegam a ser testadas. Hipóteses urgentes, mas sem importância (o quadrante C) provavelmente têm resultados de curto prazo que não afetarão as metas de longo prazo do negócio .

### WSJF

O trabalho mais curto ponderado (Weighted Shortest Job First - WSJF) é um método de priorização que permite avaliar as tarefas com mais detalhes. Como a Matriz de Eisenhower, pode ser usada para testar hipóteses.

Fórmula:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_wsjf_1587652368.jpg)

O denominador contém `Job Duration`, ou quanto tempo se espera que o teste de hipóteses demore. `Cost of Delay` no numerador é a soma de vários parâmetros:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.4.2.3PT.png)

`User-Business Value` — em quanto aumentaremos uma métrica de usuário ou de negócios se a hipótese for verdadeira

`Time Criticality` — com que urgência a hipótese deveria ser testada

`Risk Reduction/Opportunity Enablement` — se testar a hipótese ajudará a evitar riscos sérios ou criará novas oportunidades de negócios

Cada parâmetro recebe uma classificação. Você pode escolher uma escala que funcione para você: por exemplo, classificações de 0 a 10 ou a sequência de Fibonacci (1, 1, 2, 3, 5, 8, 13, 21...).

### ICE e RICE

Impacto, confiança, esforço/facilidade (**ICE)** é uma das formas mais populares de priorizar problemas:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.4.2.4PT.png)    

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.4.2.5PT.png)

Existe também uma versão modificada, **RICE**:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.4.2.6PT.png)

RICE possui quatro componentes:

_Reach_ (Alcance) — quantos usuários serão afetados pela atualização que você deseja introduzir

_Impact_ (Impacto) — quão fortemente esta atualização afetará os usuários, sua experiência e sua satisfação com o produto

_Confidence_ (Confiança) — qual a certeza de que seu produto os afetará dessa maneira

_Effort_ (Esforço) — quanto custará testar a hipótese

Assim como no WSJF, você pode usar qualquer escala que seja conveniente: classificações de 0 a 10 ou a sequência de Fibonacci.

Como e por que priorizamos hipóteses

Tarefa1 / 3

1.

Observe as hipóteses e os parâmetros WSJF avaliados no arquivo.

Calcule o WSJF para as hipóteses. Adicione os valores WSJF à tabela existente. Nomeie a nova coluna `WSJF`. Não arredonde os valores.

Imprima as colunas `'hypothesis'` e `'WSJF'` ordenadas pela coluna `'WSJF'` em ordem decrescente.

2.

Estude as hipóteses e os parâmetros de ICE avaliados no arquivo.

Calcule o ICE para as hipóteses. Adicione os valores obtidos à tabela existente. Nomeie a nova coluna como `ICE`. Não arredonde os valores.

Imprima as colunas `'hypothesis'` e `'ICE'` ordenadas pela coluna `'ICE'` em ordem decrescente.

3.

Calcule RICE para as hipóteses. Adicione os valores obtidos à tabela existente. Nomeie a nova coluna como `RICE`. Não arredonde os valores.

Imprima as colunas `'hypothesis'` e `'RICE'` ordenadas pela coluna `'RICE'` em ordem decrescente.

9

1

2

3

4

5

6

7

8

import pandas as pd

  

data \= pd.read\_csv('/datasets/hypothesis\_4-1-2.csv', sep\=',')

  

print(data)

  

data\['RICE'\] \= (data\['reach'\]\*data\['impact'\]\*data\['confidence'\])/data\['efforts'\]

print(data\[\['hypothesis','RICE'\]\].sort\_values(by\='RICE', ascending\=False))

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-15-786Z.md
### Última modificação: 2025-05-28 20:03:16

# O parâmetro de Alcance - TripleTen

Capítulo 4/9

Priorizando Hipóteses

# O parâmetro de Alcance

Não é tão difícil descobrir quantos usuários você alcança. Você pode usar dados que já possui ou avaliar concorrentes ou volumes de mercado.

Por exemplo, Nick, o analista, trabalha para um fornecedor de bebidas e sugere que eles alterem a página de destino do suco de cereja. Este produto representa 10% da receita total.

A empresa usa uma escala linear para avaliar as hipóteses, onde 0 significa que a hipótese não terá nenhum impacto nos usuários e 10 significa que impactará pelo menos a metade. Como uma pontuação de 10 significa 50% dos usuários, 2 pontos significa 10% dos usuários que contribuem para a receita do produto.

Portanto, o alcance estimado da hipótese do Nick é 2/10.

John, o gerente de produto, pede a Cathy, a analista, que estime o alcance das novas funções do produto propostas. Cathy analisa relatórios e casos de concorrentes e descobre que metade de seus clientes usa os tipos de funções em questão.

A empresa usa uma escala baseada em Fibonacci:

`0, 0.5, 1, 2, 3, 5, 8, 13, 21, 34`.

0 significa que ninguém usa as funções, enquanto 34 significa que todos usam. Cathy usou o quinto número na sequência e atribuiu às funções um alcance de 3 pontos.

A partir deste exemplo, podemos ver claramente que escolher a escala certa é realmente importante. Para a empresa de Tom, as hipóteses raramente atingem metade dos usuários. A maioria das estimativas de alcance funcional varia de 0 a 3 pontos. Seria muito melhor adotar uma escala linear onde o valor mais alto (10) significasse 50% dos usuários, como fez o Nick.

Pergunta

Você está trabalhando no maior banco do país. Você está pensando em vender plano de saúde e acredita que poderia conquistar uma parte considerável do mercado vendendo apólices para clientes atuais do banco.

Qual método de teste de hipóteses pode ser usado para estimar o alcance do produto de plano proposto?

Pesquisas

Correto. Se você perguntar aos seus clientes se eles têm plano de saúde, você saberá a parte dos que podem se interessar por este produto. A margem de erro desse método é bastante alta, mas nesta etapa você não precisará dos mais precisos resultados. Além disso, este é um dos métodos mais baratos.

Testes A/B

Entrevistas de profundidade

Entrevista de expertise

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-17-911Z.md
### Última modificação: 2025-05-28 20:03:18

# O parâmetro de Impacto - TripleTen

Capítulo 4/9

Priorizando Hipóteses

# O parâmetro de Impacto

O parâmetro de impacto mostra o quanto uma mudança afeta os usuários alcançados.

Tenha cuidado para não confundir alcance e impacto: o primeiro é o número de usuários afetados, enquanto o último é o quanto a mudança em uma métrica os afeta.

Considere o seguinte exemplo. Um serviço de táxi altera o ícone da opção de preço mais popular em seu aplicativo. Esta atualização atinge a maioria dos usuários, muitos dos quais usam essa opção, então o alcance será alto. No entanto, este novo ícone não difere muito do anterior, pelo que esta alteração não terá um impacto significativo nos usuarios. Assim, o impacto será baixo.

Ou diga que o mesmo aplicativo de serviço de táxi adiciona uma opção para quem viaja com cadeiras de rodas, que inclui a ajuda do motorista durante a coleta/entrega.

O alcance desta atualização não será alto, pois afetará apenas 1-2% dos novos usuários. Mas terá o máximo impacto, pois possibilitará que eles usem o serviço de táxi.

Existem diferentes maneiras de medir o impacto:

-   Pela parte de toda a tela que os elementos modificados irão ocupar (%)
-   Pelo grau em que a experiência do usuário mudará
-   Por uma estimativa preliminar de importância para os usuários
-   Pelo número de novos usuários que a atualização atrairia

Você pode adotar seus próprios critérios personalizados para estimar o impacto com base em seu produto e estratégia de negócios.

Pergunta

Você está priorizando hipóteses sobre como mudar o design de um aplicativo. Atualmente, o DAU é de cerca de 100 usuários. Qual método de pesquisa você escolheria para estimar o impacto?

Pesquisas

Análise de UX

Isso mesmo! Um teste relativamente barato do impacto que os protótipos de interface têm no comportamento do usuário é a melhor maneira possível de estimar o impacto das mudanças.

Testes A/B

Análise de dados existentes

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-19-260Z.md
### Última modificação: 2025-05-28 20:03:19

# O parâmetro de Esforço - TripleTen

Capítulo 4/9

Priorizando Hipóteses

# O parâmetro de Esforço

O parâmetro de esforço informa o quão difícil é testar uma hipótese.

Para a maioria das empresas do mercado, o desenvolvimento é caro e é um recurso chave. Se você não precisar de nenhum desenvolvimento para testar sua hipótese, a classificação de esforço diminui significativamente.

Se você precisar de desenvolvimento, faria sentido pedir a um desenvolvedor de software para estimar quanto trabalho seria necessário. Qualquer prática em uso na sua empresa irá servir para isso.

O serviço de lavanderia Fresh Start gostaria de testar a hipótese de que seus clientes estariam interessados em uma opção mais barata que não inclui passar a roupa. Para testá-lo, Rudolf, o gerente de produto, criará uma página de destino com um desenvolvedor em duas horas. Em mais 20 minutos, ele escreverá um script de vendas. Explicar a outro funcionário como trabalhar com o script vai levar mais 10 minutos para John.

Portanto, testar a hipótese requer menos de três horas de Rudolf. Não implicará a mudança de nenhum processo operacional: a empresa pode vender e depois cancelar os pedidos feitos para o novo serviço, ou continuar passando, e isso será um bônus agradável para os clientes.

A pontuação de esforço para esta hipótese será baixa: talvez 1 ponto em uma escala de 0 a 10.

Outro exemplo: o serviço de newsletter por e-mail da lição anterior está considerando a substituição do RFM por um tipo diferente de segmentação.

Este produto exigirá:

-   Atualização e criação de novas páginas de interface
-   Construção e manutenção de um novo banco de dados para armazenar segmentos de usuários
-   Desenvolvimento de um módulo para calcular esses segmentos e transmiti-los ao banco de dados

Como resultado, precisaremos de pelo menos três desenvolvedores de software por um período prolongado. Eles consideram esta tarefa como um projeto próprio, com as primeiras tarefas exigindo vários dias de trabalho de cada um deles.

Assim, a classificação do esforço será muito alta: 9-10 pontos.

Pergunta

Por que priorizamos uma hipótese depois de escolher o método de teste?

Isso é apenas o que as pessoas fazem.

Porque a pergunta "O que fazemos?" vem antes de "Como vamos fazer isso?"

A priorização depende dos recursos necessários para testar as hipóteses. E os recursos, por sua vez, dependem do método escolhido.

Está correto! Uma hipótese pode ser testada com vários métodos, cada um dos quais terá sua própria estimativa de recursos. Portanto, pense bem antes de tomar a decisão final sobre o método de teste. Já considerou todas as opções? A sua escolha será o melhor em termos de relação precisão/custo?

Porque para priorizar uma hipótese, temos que perguntar aos desenvolvedores sobre o esforço necessário, mas eles não sabem realmente quais são as hipóteses e só podem avaliar as tarefas específicas.

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-20-588Z.md
### Última modificação: 2025-05-28 20:03:20

# O parâmetro de confiança - TripleTen

Capítulo 4/9

Priorizando Hipóteses

# O parâmetro de confiança

O parâmetro de confiança reflete a certeza de suas estimativas com relação a outros parâmetros.

Por exemplo, o nível de confiança será alto se

-   Você pode encontrar o número exato de usuários que serão afetados pelas mudanças
-   Você tem evidências que essa mudança terá exatamente o impacto que você previu (por exemplo, graças à sua experiência anterior ou casos de concorrentes)
-   Você calculou o esforço necessário com precisão e não há risco de aumentar ou o risco é muito baixo.

Digamos que o aplicativo de serviço de táxi lance uma opção de preço fixo. Ao pedir uma carona, os usuários sabem exatamente quanto vão pagar.

_Reach_ (Alcance) — claro, alcançará todos os usuários.

_Impact_ (Impacto) — identificado e claro: recentemente um concorrente lançou um recurso semelhante e muitos clientes começaram a usá-lo.

_Effort_ (Esforço)— o esforço necessário para introduzir esta função é bastante claro. No entanto, há incerteza sobre como as tarifas serão calculadas, e isso cria algum risco.

Dado que o alcance e o impacto são conhecidos com precisão e há risco relacionado ao esforço, a pontuação geral de confiança para essa hipótese será de 7 a 8 pontos em uma escala linear de 0 a 10.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.4.5PT.png)

Aqui está outro exemplo de medição de confiança.

Um serviço de newsletter por e-mail gostaria de lançar um novo tipo de segmentação de usuários. Atualmente, os clientes estão usando o modelo de recência, frequência e valor monetário (**RFM)**. Isso calcula a fidelidade do cliente com base na última vez que fez uma compra, com que frequência compra e quanto dinheiro gasta.

Vejamos detalhadamente como testar a hipótese sobre o novo tipo de segmentação.

_Reach_ — não está claro. Embora todos possam usar o novo tipo de segmentação, você não pode dizer com antecedência quantos usuários realmente o farão.

_Impact_ — não está claro. Há um artigo que argumenta que esse tipo de segmentação é mais lucrativo que o RFM. Mas o estudo de caso não inclui números exatos, nem descreve a amostra do experimento. E a fonte em si não parece muito confiável.

_Efforts_ — estimados com bastante precisão. Há muitos recursos a serem introduzidos, e provavelmente haverá erros. Os usuários podem solicitar determinadas atualizações após o lançamento do produto modificado, portanto, há riscos na avaliação atual.

Com base no exposto, a pontuação de confiança provavelmente será baixa, não superior a 1-2 pontos em uma escala de 0 a 10.

Pergunta

Você gostaria de otimizar a produção de um capacete que ajuda as pessoas a se livrarem dos maus pensamentos, o Brainema 2000. Parece que os robôs podem ser usados para algumas das tarefas de montagem na fábrica. Qual desses métodos pode ser aplicado rapidamente para aumentar a confiança e ajustar os outros parâmetros?

Análise de dados existentes

Testes A/B

Entrevista de especialista

Isso mesmo! Alguém provavelmente já começou a usar robôs desse tipo. Encontre essa pessoa e pergunte sobre os resultados do experimento. Você aumentará seu nível de confiança nos resultados da hipótese ou ajustará suas expectativas.

Entrevistas de profundidade

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-22-519Z.md
### Última modificação: 2025-05-28 20:03:22

# Conclusão - TripleTen

Capítulo 4/9

Priorizando Hipóteses

# Conclusão

Muito bem! Você concluiu o capítulo sobre como priorizar hipóteses.

### O que você aprendeu:

-   Priorizar hipóteses com frameworks como ICE, RICE e WSJF
-   Avaliar os parâmetros Reach (Alcance), Impact (Impacto), Confidence (Confiança) e Effort (Esforço) para as estruturas ICE e RICE

### Takeaway:

Baixe o [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/moved_Resumo_do_Captulo_Priorizando_Hipteses_Sprint%208_DA_PRT.pdf) para poder consultá-lo quando necessário.

### Leitura independente:

**Matriz de Eisenhower**

[https://jamesclear.com/eisenhower-box](https://jamesclear.com/eisenhower-box) (os materiais estão em inglês)

**WSJF**

[https://www.playbookhq.co/blog/wsjf-weighted-shortest-job-first](https://www.playbookhq.co/blog/wsjf-weighted-shortest-job-first) (Você também pode querer ler sobre o custo do atraso; veja os links no final deste artigo.) (os materias estão em inglês)

**ICE/RICE**

[https://www.mindtheproduct.com/three-simple-ways-to-make-product-prioritisation-easier/](https://www.mindtheproduct.com/three-simple-ways-to-make-product-prioritisation-easier/) (os materiais estão em inglês)

No capítulo seguinte você se preparará para realizar um teste A/B.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-23-863Z.md
### Última modificação: 2025-05-28 20:03:24

# Introdução - TripleTen

Capítulo 5/9

Como se preparar para um Teste A/B

# Introdução

O propósito do teste A/B é avaliar o impacto específico de mudanças propostas no comportamento dos usuários ou em Indicadores-chave de desempenho (KPIs, na sigla em inglês) ao comparar duas versões de um produto, serviço ou elemento de uma página da web (como em um botão de chamada para ação, layout de página ou mensagens promocionais) em condições controladas.

Antes de conduzir um teste A/B, primeiro é essencial conduzir um teste A/A. Essa etapa preliminar confere se as suposições para o teste A/B são válidas. Basicamente, um teste A/A compara duas versões idênticas de uma página ou produto para garantir que o ambiente de teste e as ferramentas estão funcionando corretamente, fornecendo uma linha de base confiável para a comparação A/B.

Para analisar um teste A/B, você precisa saber sobre os erros do tipo I e II e, além de como analisar testes com comparações múltiplas.

### O que você vai aprender:

-   Como calcular a probabilidade dos erros do tipo I e II
-   Como introduzir correções para comparações múltiplas
-   Como encontrar o mínimo tamanho de amostra necessário
-   Como encontrar a duração ideal de um teste A/B

### Quanto vai demorar:

5 _lições, aproximadamente de 10 a 20 minutos cada uma_

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-25-213Z.md
### Última modificação: 2025-05-28 20:03:25

# Testes A/A - TripleTen

Capítulo 5/9

Como se preparar para um Teste A/B

# Testes A/A

Antes de iniciarmos um teste A/B, executaremos um **teste A/A**, para ter certeza de que:

-   Os resultados não estão afetados por anomalias ou valores atípicos na população estatística
-   A ferramenta usada para dividir o tráfego funciona corretamente
-   Os dados são enviados para sistemas analíticos corretamente

Os testes A/A são como os A/B, com a diferença de que, nos testes A/A, todos veem a mesma versão da página, enquanto nos testes A/B, uma versão diferente é mostrada para cada grupo. Se o tráfego e a ferramenta utilizada para realizar o teste A/A estão funcionando como deveriam, não haverá nenhuma diferença (significativa) nos resultados. Testes A/A também ajudam a determinar quanto tempo deve durar o teste A/B e qual método deve ser usado para analisar os resultados.

Aqui estão os critérios para um teste A/A bem sucedido:

-   O número de usuários em diferentes grupos não deve variar mais do que 1%
-   Para todos os grupos, dados sobre o mesmo evento são gravados e enviados para sistemas analíticos
-   Nenhuma das métricas chave varia em uma quantidade estatisticamente significativa - normalmente não deve variar mais do que 1%
-   Usuários permanecem dentro de seus grupos até o final do teste. Se eles virem versões diferentes da página durante o estudo, não poderemos saber com certeza qual versão influenciou suas decisões, então a confiabilidade dos resultados será comprometida.

A medida onde as métricas chave diferem entre vários grupos depende do grau de sensibilidade que os experimentos devem ter. Em um teste no qual você espera um ganho de pelo menos 30%, a margem de erro de 10% pode ser aceitável. Mas isto acontece raramente, já que uma mudança de 30% em métricas é normalmente visível sem testes A/B. Eles são utilizados nos casos onde os resultados de grupos não variam mais de 10% ou quando eles estão sujeitos a flutuação. Aqui, a precisão de 1% para o teste A/A é um critério comum.

Pergunta

Qual das seguintes opções não é um motivo para conduzir um teste A/A?

Determinar o quão bem o tráfego foi dividido em grupos

Certificar-se de que não há valores atípicos ou anomalias na população estatística, ou encontrar maneiras de reduzir seu impacto

Verificar como os dados são enviados para sistemas analíticos

Avaliar o tamanho necessário de amostra e a duração do teste A/B futuro

Obter resultados preliminares do teste A/B

Não há nenhuma B em A/A.

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-27-309Z.md
### Última modificação: 2025-05-28 20:03:27

# Erros do Tipo I e II em Testes de Hipóteses. Poder e Significância - TripleTen

Capítulo 5/9

Como se preparar para um Teste A/B

# Erros do Tipo I e II em Testes de Hipóteses. Poder e Significância

Qualquer hipótese pode ser aceita ou rejeitada por erro. Quando calculamos a probabilidade de ambos os cenários, já que eles dependem do nível de significância.

Antes neste programa você estudou sistemas com duas hipóteses (nula e alternativa) e um nível de significância α (alfa). Lembre-se de que o nível de significância é a probabilidade de que um valor medido empiricamente será distante do valor predito pela hipótese nula.

Acontece que o nível de significância é também a _probabilidade de cometer um erro_, ou seja, rejeitar a hipótese nula porque a observação acaba por ser muito diferente do valor esperado. Isto é um **erro do tipo I**, ou um **resultado falso positivo de um teste estatístico**. Aqui não existe nenhuma diferença entre os grupos comparados, mas o teste retornou um valor-p menor do que o nível de significância. Como resultado, há motivos para rejeitar H₀. Assim, a probabilidade de cometer um erro do tipo I é igual ao nível de significância, α.

Um **erro do tipo II** é um **resultado falso negativo**. Isso significa que há diferenças entre os grupos, mas o teste retornou um valor-p maior que `α`, então não há razões para rejeitar H₀. Se chamamos a probabilidade de cometer um erro do tipo II de β, então `1 - β` será o **poder estatístico do teste de hipótese**. Enquanto β é a probabilidade de cometer um erro, `1 - β` é uma probabilidade de _não o cometer_, ou seja, rejeitar a hipótese nula corretamente quando ela é falsa.

Não podemos ter uma probabilidade arbitrariamente baixa de cometer um erro do tipo I _e_ uma probabilidade arbitrariamente baixa de cometer um erro do tipo I ao mesmo tempo; em qualquer teste de hipótese existe um compromisso entre esses dois. Na maioria das vezes, podemos conseguir um bom equilíbrio entre eles se escolhermos o nível de significância de 1% ou 5%.

Decisão de teste estatístico

H₀ verdadeiro

H₀ falso

Rejeitar H₀

Tipo de erro I (falso positivo)

Correto

Não rejeitar H₀

Correto

Tipo de erro II (falso negativo)

Pergunta

Se você elevar o nível de significância de 1% a 5%:

A probabilidade de um erro do tipo I vai aumentar e a chance de um erro do tipo II vai diminuir.

À medida que o nível de significância aumenta, o mesmo acontece à região crítica onde a hipótese nula é rejeitada. Já que a probabilidade de entrar nessa região também cresce, a probabilidade de cometer um erro do tipo I fica maior. (O alarme vai disparar mesmo se não houver incêndio, mas, por outro lado, se houver um, seremos avisados; então a probabilidade de um erro do tipo II diminui.)

A probabilidade de erros de ambos os tipos vão aumentar.

A probabilidade de um erro do tipo I será reduzido, enquanto a de um erro do tipo II vai aumentar.

A probabilidade de erros de ambos os tipos I e II vai aumentar.

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-28-636Z.md
### Última modificação: 2025-05-28 20:03:28

# Comparações Múltiplas: Testes A/A e A/B - TripleTen

Capítulo 5/9

Como se preparar para um Teste A/B

# Comparações Múltiplas: Testes A/A e A/B

Muitas vezes, uma hipótese é testada em diferentes variações. Por exemplo, você pode querer comparar três versões de um banner de página de destino ou cinco opções de preço.

Você pode comparar vários grupos com um grupo de controle, mas é preciso considerar o aumento da probabilidade de erros do tipo I e II que você estudou na lição anterior.

Fazer várias comparações usando os mesmos dados é chamado de **testes múltiplos**. Uma coisa importante para saber sobre isso é que a probabilidade de cometer um erro tipo I aumenta a cada novo teste de hipótese.

Se a probabilidade de cometer um erro é `ɑ` cada vez, então a probabilidade de não cometer erros é `1-ɑ`. Assim, a probabilidade de não cometer erros ao longo de `k` comparações vai ficar assim:

(1−a)k(1-a)^k(1−a)k

A probabilidade de cometer pelo menos um erro ao longo de `k` comparações vai ficar assim:

1−(1−a)k1-(1-a)^k1−(1−a)k

Digamos que você está testando uma hipótese que envolve três possíveis opções de cor para o botão Comprar de um site. Há a opção padrão de vermelho (grupo A), bem como amarelo, verde e branco (grupos B1, B2, B3). Você definiu o nível de significância como 0.05.

Se você fizer uma comparação par a par, a probabilidade de que o teste retornará um resultado falso positivo será igual ao nível de significância.

Porém, a probabilidade de pelo menos uma das três comparações par a par retornando um resultado falso positivo vai ficar assim:

1−(1−a)3\=0.14261-(1-a)^3 = 0.14261−(1−a)3\=0.1426

Com quatro grupos, a probabilidade de pelo menos um resultado falso positivo será 18.55%. Com dez, ele será 40%, embora raramente tenhamos tantos grupos em um teste A/B.

A seguinte tabela vai nos ajudar a entender o conceito de taxa de erro familiar (FWER, na sigla em inglês) e apresentar alguns procedimentos para ajustar o nível de significância global de um conjunto de testes ao realizar várias comparações.

\-

O número de hipóteses aceitas hypotheses

O número de hipóteses rejeitadas

Total

O número de hipóteses verdadeiras

U

V

m0

O número de hipóteses falsas

T

S

m1

Total

W

R

m

A taxa de erro familiar (FWER) é definida como a probabilidade de cometer pelo menos um Erro do tipo I (concluir incorretamente que há um efeito significativo quando não há) ao realizar vários testes estatísticos ao mesmo tempo.

**`FWER = P(V≥1)`**,

onde `V` é o número de hipóteses nulas rejeitadas entre as hipóteses nulas verdadeiras. Em outras palavras, `V` é o número de resultados falsos positivos, enquanto FWER é a probabilidade de receber pelo menos um resultado desse tipo. Se considerarmos o número total de hipóteses **`m` igual a 1, o `FWER` é igual a `a`** .

No contexto de múltiplos testes estatísticos, para minimizar o FWER, métodos de correção são aplicados ao nível de significância. O nível de significância é o limiar contra o qual o valor-p de cada teste estatístico é comparado a fim de determinar se um resultado é estatisticamente significativo. Os métodos de correção ajustam o nível de significância para baixo para cada teste, conforme o número total de testes realizados. Entre os mais comuns, temos os seguintes:

-   **A correção de Bonferroni**:

É a correção mais comum e mais aproximada do nível de significância necessário. O nível de significância em cada uma das comparações `m` é `m` vezes menor do que o nível de significância necessário para uma única comparação. De forma resumida, o nível de significância `ɑ` é dividido pelo número de testes a serem realizados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_3_1587657558.png)

Por exemplo, se você estiver testando 10 testes de hipóteses e queira um `ɑ` total de 0,05, o nível de significância de cada teste será `0,05 / 10 = 0,005`.

-   **O** **método de Holm**

O método de Holm também garante um FWER < `ɑ`, mas seus requisitos para o nível de significância são mais suaves. Isto é um procedimento descendente: para a primeira comparação o nível de significância é igual ao quociente de `ɑ` pelo número de comparações par a par, para a segunda é ao quociente de `ɑ` pelo `número de comparações -1`, etc. Para a última comparação, o nível de significância será igual a `ɑ`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_4_1587657616.png)

-   **O método de Šidák**

O método de Šidák também garante um FWER < `ɑ`. O valor corrigido do nível de significância exigido pode ser encontrado usando a seguinte fórmula:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_5_1587657650.png)

Por exemplo, se `ɑ = 0.05` e houver duas comparações, o nível de significância exigido pode ser encontrado da seguinte maneira: `1 - (1 - 0.05)^(1/2) = 0.0253`. Com quatro comparações vai ficar assim: `1 - (1 - 0.05)^(1/4) = 0.0127`.

A correção de Bonferoni é o método mais comum, devido à sua simplicidade. Não é difícil dividir o nível de significância desejado pelo número de comparações, que são feitas com o mesmo conjunto de dados, sem a necessidade de fazer novas observações para cada teste. No entanto, se estiver coletando novos dados para cada teste de hipóteses (ou seja, usando conjuntos de dados diferentes), você pode apenas conduzir o teste do jeito padrão, selecionando o valor-p necessário como faria em um teste padrão.

Ao estabelecer padrões muito rigorosos para a significância das diferenças entre grupos, ou seja, um alfa pequeno, a probabilidade de detectar diferenças verdadeiras é reduzida, o que diminui o poder estatístico do teste. Portanto, se você precisa aumentar a habilidade de detectar diferenças importantes entre grupos enquanto mantém o FWER controlado, recomendamos usar métodos como o de Holm ou Šidák. Esses métodos ajustam os níveis de significância individuais para aumentar o poder do teste sem comprometer o controle sobre o erro global.

Vamos ver um exemplo da vida real. Uma empresa testa três versões de um formulário de cliente na sua página de destino ao longo de um mês. Os conjuntos de dados `sample_A`, `sample_B` e `sample_C` contêm o número de visitantes por dia que preencheram a primeira, a segunda e a terceira versões respectivamente. Vamos testar a hipótese de que o número médio de usuários é igual para cada par de conjuntos de dados.

```
from scipy import stats as st
import numpy as np

sample_A = [3071, 3636, 3454, 3151, 2185, 3259, 1727, 2263, 2015, 
            2582, 4815, 633, 3186, 887, 2028, 3589, 2564, 1422, 1785, 
            3180, 1770, 2716, 2546, 1848, 4644, 3134, 475, 2686, 
            1838, 3352]
sample_B = [1211, 1228, 2157, 3699, 600, 1898, 1688, 1420, 5048, 3007, 
            509, 3777, 5583, 3949, 121, 1674, 4300, 1338, 3066, 
            3562, 1010, 2311, 462, 863, 2021, 528, 1849, 255, 
            1740, 2596]
sample_C = [1211, 1228, 2157, 3699, 600, 1898, 1688, 1420, 5048, 3007, 
            509, 3777, 5583, 3949, 121, 1674, 4300, 1338, 3066, 
            3562, 1010, 2311, 462, 863, 2021, 528, 1849, 255, 
            1740, 2596]

alpha = .05 # nível de significância

results_AB = st.ttest_ind(
    sample_A, 
    sample_B)

results_BC = st.ttest_ind(
    sample_B, 
    sample_C)

results_AC = st.ttest_ind(
    sample_A, 
    sample_C)

bonferroni_alpha = alpha / 3  # três comparações feitas

print('valor-p para comparar os grupos A e B: ', results_AB.pvalue)
print('valor-p para comparar os grupos B e C: ', results_BC.pvalue)
print('valor-p para comparar os grupos A e C: ', results_AC.pvalue)

if (results_AB.pvalue < bonferroni_alpha):
    print("Hipótese nula rejeitada para os grupos А e B")
else:
    print("Hipótese nula não rejeitada para os grupos А e B")

if (results_BC.pvalue < bonferroni_alpha):
    print("Hipótese nula rejeitada para os grupos B e C")
else:
    print("Hipótese nula não rejeitada para os grupos B e C")

if (results_AC.pvalue < bonferroni_alpha):
    print("Hipótese nula rejeitada para os grupos A e C")
else:
    print("Hipótese nula não rejeitada para os grupos А e C")
```

```
valor-p para comparar os grupos A e B:  0.1912450522572209
valor-p para comparar os grupos B e C:  1.0
valor-p para comparar os grupos A e C:  0.1912450522572209
Hipótese nula rejeitada para os grupos A e B
Hipótese nula rejeitada para os grupos B e C
Hipótese nula rejeitada para os grupos A e C
```

Os dados não confirmam que existe uma diferença do número de usuários em vários grupos.

Pergunta

Por que precisamos dos métodos de Bonferroni, Holm e Šidák?

Para aumentar a probabilidade de fazer um erro do tipo I.

Para aumentar a probabilidade de fazer um erro do tipo II.

Para aumentar a probabilidade de um resultado falso positivo.

Para diminuir a taxa de erro familiar e corrigir os níveis de significância exigidos.

Isso mesmo! Este é o propósito de todos os três métodos.

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-29-945Z.md
### Última modificação: 2025-05-28 20:03:30

# Calculando o Tamanho da Amostra e a Duração do Teste - TripleTen

Capítulo 5/9

Como se preparar para um Teste A/B

# Calculando o Tamanho da Amostra e a Duração do Teste

Analistas têm que ter em consideração as condições nas quais amostras de um teste A/B são criadas:

a) Quanto tempo o teste deve levar. Ao decidir sobre a duração de um teste, é preciso ter em conta variações cíclicas no tráfego (ao longo do dia, da semana, do mês) e o tempo necessário para um cliente decidir a fazer uma compra.

b) Se o **problema de espiar os resultados antes do tempo** é relevante. Esse problema é criado ao tirar conclusões antes que o experimento seja concluído e tomar decisões com base nelas. Essa prática aumenta o risco de enviesamento amostral, principalmente devido a interpretações incorretas de valores atípicos como tendências significativas. Analisar dados em fases iniciais pode fazer com que mesmo pequenas quantidades de novas informações pareçam ter um grande impacto, o que pode levar à conclusão errada de uma significância estatística foi alcançada. É crucial evitar tirar interpretações prematuras até que a coleta de dados planejada seja concluída.

Isso é uma demonstração da lei dos grandes números. Se houver poucas observações, a dispersão deles será maior. Se houver muitas, valores atípicos aleatórios se anulam. Isso quer dizer que, quando a amostra é pequena demais, será mais provável observar diferenças, mas elas não serão estatisticamente significativas. (Por exemplo, não é incomum obter 8 coroas em 10 jogadas de uma moeda não viciada, mas 800 coroas em 1000 lances seria um acontecimento histórico.) Para um teste estatístico, isso vai significar a diminuição do valor-p até que ele fique pequeno o suficiente para rejeitar a hipótese nula.

![](https://practicum-content.s3.amazonaws.com/resources/8.5.5PT_1712926334.png)

O gráfico mostra a diferença em conversão entre segmentos com base em um teste A/B simulado. Os dados foram coletados de uma só população estatística, então os valores médios da amostra não deveriam variar.

Apesar disso, devido a flutuações, a significância estatística foi atingida durante os primeiros dias do teste. Se isso fosse um teste real e uma decisão fosse feita depois de atingir significância, essa decisão seria errada.

Para compensar o problema de espiar os resultados antes do tempo, o tamanho da amostra é definido antes do início do teste.

Aqui está o procedimento correto para realizar um teste A/B:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.5.5.2PT.png)

E esta é a maneira errada de fazê-lo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.5.5.3PT.png)

### Calculadoras de duração de teste e de tamanho de amostra

Uma das maneiras mais simples de determinar o tamanho necessário de uma amostra é calculá-lo online.

Aqui estão algumas calculadoras:

-   [http://www.evanmiller.org/ab-testing/sample-size.html](http://www.evanmiller.org/ab-testing/sample-size.html) (os materiais estão em inglês)
-   [https://www.optimizely.com/sample-size-calculator/?conversion=20&effect=5&significance=95](https://www.optimizely.com/sample-size-calculator/?conversion=20&effect=5&significance=95) (os materiais estão em inglês)
-   [https://vwo.com/tools/ab-test-duration-calculator/](https://vwo.com/tools/ab-test-duration-calculator/) (os materiais estão em inglês)

Esses serviços são bons para avaliar o tamanho mínimo necessário para o qual uma mudança em métricas seria perceptível. Isso vai te ajudar a calcular a duração mínima do teste.

Digamos que você está testando um formulário de confirmação de pedido: uma versão dele tem quatro campos, enquanto a outra tem cinco. Você está disposto a sacrificar alguns dados de usuários que coletou em prol do aumento de conversão. A taxa de conversão atual é 1%. O efeito mínimo detectável que você quer é uma mudança relativa de 5% (1.05% contra 5%). Uma calculadora te diria que você precisa cerca de 900,000 observações para cada grupo. Sabendo que a página tem 100,000 visitas por dia, você pode facilmente encontrar o número preciso de dias: no mínimo 18 (`900,000 * 2 / 100,000`).

Pergunta

Abra a calculadora de tamanho de amostra em uma nova aba: [http://www.evanmiller.org/ab-testing/sample-size.html](http://www.evanmiller.org/ab-testing/sample-size.html) (os materiais estão em inglês). Encontre o tamanho mínimo necessário para detectar uma diferença relativa de 3% entre os grupos de teste, a partir de uma taxa de conversão base de 1%, potência de teste de 80% e nível de significância de 5%.

5,082,988 observações

1,734,434 observações

Isso mesmo!

236 observações

4,782 observações

Você conseguiu!

Pergunta

Abra a calculadora de duração de teste em uma nova aba: [https://vwo.com/tools/ab-test-duration-calculator/](https://vwo.com/tools/ab-test-duration-calculator/) (os materiais estão em inglês). Calcule a duração mínima de teste necessária para encontrar uma diferença relativa de 2% entre os grupos de teste se:

-   a taxa de conversão base é 10%
-   há 4 grupos (variações)
-   uma média de 50,000 usuários visitam a página por dia
-   metade dos usuários participam do teste

29 dias

13 dias

58 dias

Isso mesmo!

6 dias

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-31-879Z.md
### Última modificação: 2025-05-28 20:03:32

# Análise Gráfica de Métricas e Definindo a Indústria - TripleTen

Capítulo 5/9

Como se preparar para um Teste A/B

# Análise Gráfica de Métricas e Definição da Indústria.

As vantagens das calculadoras:

-   Cálculo simples da conversão
-   Solução do problema de espiar os resultados antes do tempo
-   A possibilidade de estimar a duração mínima de teste

As desvantagens:

-   Tamanho da amostra: necessário, mas muitas vezes insuficiente para que um teste seja válido
-   Calculadoras não podem levar em conta o fato de que na vida real a conversão e o efeito mínimo detectável relativo não permanecem no mesmo nível ao longo do teste
-   Calculadoras funcionam bem somente para calcular o tamanho da amostra para conversão. Existem também calculadoras para outros indicadores, mas elas são muito mais complexas.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.5.6PT.png)

No início do teste, os resultados mudavam significativamente dia após dia e se estabilizaram apenas após seis semanas. A taxa de conversão para o grupo de controle era 10%, e a mudança relativa era 2% (a taxa de conversão do grupo de teste era 10,2%). De acordo com a calculadora, o tamanho mínimo necessário de amostra era 460,000 observações por grupo. Este número foi atingido cinco dias após o início do teste.

Esse exemplo mostra claramente que não podemos contar apenas com calculadoras. Você sempre deve estudar como diferenças nas métricas consideradas estão mudando. Faça decisões apenas quando as diferenças estiverem estabilizadas.

### Determinando o período de realização e a duração mínima de um teste com base na indústria

Ao determinar a duração de um teste e seu período de realização, você precisa saber que tipos de altas na atividade caracterizam sua audiência.

Possíveis motivos para altas incluem:

-   Dias de trabalho ou fins de semana
-   Feriados (aumento da procura por presentes, por exemplo)
-   Vendas, ofertas, atividades de marketing (descontos aumentam a atividade da audiência, alterando seu comportamento de consumo)
-   Eventos especiais (ex. compra de material escolar no começo do ano)
-   Sazonalidade do produto (ex. aquecedores)
-   Atividade dos competidores (os competidores diminuíram o preço de um produto, então a atividade de seus consumidores cai);
-   Mudanças no cenário político e econômico (recessão, inflação, embargos, aumento dos preços em função da inclusão de impostos).

Além das altas na atividade, você também deve estar ciente do **ciclo de realização da métrica que está sendo avaliada**. Ele costuma estar ligado com o **processo de decisão de compra** ou o tempo entre o primeiro pensamento sobre comprar um produto e a realização da compra.

Por exemplo, o usuário médio da loja online "Only $3" precisa de três minutos para tomar uma decisão de compra, então todos os usuários da loja que participaram do teste A/B de sete dias conseguiram comprar alguma coisa antes do final do teste, e os resultados foram levados em consideração.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.5.6.2PT.png)

A situação é diferente para a loja online de móveis de luxo "Leonardo Divanci" que está conduzindo um teste A/B para seu formulário de descrição do produto. Ele dura sete dias. O processo de decisão de compra dos clientes da loja leva cerca de três semanas; quase todos eles visitam o showroom, falam com os consultores e examinam as sugestões de outros produtores. Neste caso, a maioria dos clientes que compraram seus móveis durante um teste de uma semana não serão afetados pela mudança, já que eles começaram o processo uma ou duas semanas antes do início do teste. E ao mesmo tempo, os clientes afetados pela mudança irão fazer compras duas ou três semanas depois, então eles não serão incluídos nos resultados do teste. Por essa razão, o teste da loja deve ser executado durante pelo menos quatro ou cinco semanas.

Além disso, no caso de móveis luxuosos, decisões de compra são influenciadas por muitos fatores externos. Executar um teste A/B usando uma pequena amostra e esperar que outros fatores sejam igualmente distribuídos entre os grupos não é muito profissional. Um teste A/B desse tipo requer um grande volume de tráfego e uma análise escrupulosa dos resultados.

Pergunta

Você é um especialista em marketing de uma loja online de brinquedos. Você recentemente acabou de investigar uma hipótese e agora quer testar mais uma. A duração estimada de seu teste é quatro semanas. Hoje é 15 de dezembro. O que você vai fazer?

Vou iniciar o teste hoje mesmo!

Eu vou adiar o teste até o fim de janeiro

Isto é uma boa decisão: o comportamento de compra é muito diferente durante as férias. Os resultados de um teste realizado agora não te ajudariam em outras épocas do ano.

Eu não preciso testar nada!

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-33-221Z.md
### Última modificação: 2025-05-28 20:03:33

# Conclusão - TripleTen

Capítulo 5/9

Como se preparar para um Teste A/B

# Conclusão

Muito bem! Você terminou o capítulo. Agora, você totalmente capaz de conduzir um teste A/B e analisar seus resultados.

### Você aprendeu:

-   Como avaliar a probabilidade dos erros do tipo I e II
-   Como introduzir correções para comparações múltiplas
-   Como calcular o mínimo tamanho de amostra necessário
-   Como determinar a duração ideal de um teste A/B

### Takeaway:

Baixe o [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/moved_Resumo_do_Captulo_Preparaes_para_um_Teste_AB.pdf) para poder consultá-lo quando necessário.

### Leitura independente:

[https://www.widerfunnel.com/3-mistakes-invalidate-ab-test-results/](https://www.widerfunnel.com/3-mistakes-invalidate-ab-test-results/) (os materiais estão em inglês)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-34-536Z.md
### Última modificação: 2025-05-28 20:03:34

# Introdução - TripleTen

Capítulo 6/9

Analisando os Resultados de um Teste A/B

# Introdução

Analisar testes A/B demanda um tipo especial de talento, muita prática, e um entendimento profundo de como usuários se comportam na sua área.

### O que você vai aprender:

-   Como aplicar métodos de análises estatísticas aos resultados A/B
-   Como analisar gráficos de métricas diárias e cumulativas para testes A/B
-   Como analisar valores atípicos
-   Como analisar gráficos e anomalias
-   Como aplicar critérios estatísticos

### Quanto vai demorar:

8 lições, aproximadamente 30 a 40 minuto cada

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-36-437Z.md
### Última modificação: 2025-05-28 20:03:36

# Testando a Hipótese de que as Proporções são Iguais - TripleTen

Teoria

# Testando a Hipótese de que as Proporções são Iguais

Quando discutimos estatística pela primeira vez, você testou hipóteses relativas a médias de populações. Você tirou conclusões com base em amostras, comparando suas médias com um determinado número ou verificando se duas médias eram iguais.

Outra tarefa típica é testar hipóteses sobre a igualdade de proporções de populações. Se uma parte de uma população estatística possui determinada característica enquanto outra parte não, podemos chegar a conclusões sobre o tamanho dessa proporção, com base em uma amostra retirada da população. Assim como com a média, proporções amostrais terão distribuição normal em torno do valor real.

O Python não possui um teste padronizado para isso, então nós mesmos vamos escrever um.

Lembre-se de que quando estimamos a média de uma população, as médias amostrais têm uma distribuição normal em torno da média da população total, independente de como a própria população estatística está distribuída. Esse é um modo de formular o teorema central do limite.

Vamos comparar as proporções de duas populações com base em amostras delas.

A diferença entre as proporções que observarmos em nossas amostras será nossa **estatística**. É isso que chamamos de uma variável cujos valores podem ser encontrados apenas a partir de dados amostrais. Você pode provar que sua distribuição é normal:

Z≈(P1−P2)−(π1−π2)P(1−P)(1/n1+1/n2)∼N(0,1)Z \\approx \\frac{(P\_{1} - P\_{2}) - (\\pi\_{1} - \\pi\_{2})}{\\sqrt{P(1-P)(1/n\_{1} + 1/n\_{2})}} \\sim N(0,1)Z≈P(1−P)(1/n1​+1/n2​)​(P1​−P2​)−(π1​−π2​)​∼N(0,1)

A estatística `Z`, que indica quantos desvios padrão um resultado está da média, corresponde a um valor obtido em testes de hipóteses e segue a distribuição normal padrão, em que a média é 0 e o desvio padrão é 1. Tudo isso é afirmado na parte direita da fórmula, após o sinal '～', que diz que a expressão é distribuída como **N**(0,1).

`n₁` e `n₂` representam os tamanhos das duas amostras que estão sendo comparadas (a quantidade de observações que elas contêm). `P₁` e `P₂` são as proporções observadas nas amostras, e `P` é a proporção das duas amostras combinadas. `π₁` e `π₂` são as proporções reais nas populações que estamos comparando.

Com testes A/B, é geralmente testada a hipótese de que π₁ = π₂. Então, se a hipótese nula for verdadeira, a expressão `(π₁ - π₂)` no numerador será igual a 0 e o critério poderá ser calculado usando apenas os dados amostrais.

A estatística obtida dessa forma terá distribuição normal, tornando possível conduzir testes de dois lados e de um lado (bilaterais e unilaterais). Usando a mesma hipótese nula de que as proporções de duas populações são iguais, podemos testar as hipóteses alternativas de que 1) as proporções simplesmente não são iguais, ou de que 2) uma proporção é maior ou menor do que a outra.

Um exemplo: vamos comparar as proporções de clientes que fizeram pedidos. Entre 830 usuários registrados provenientes da primeira fonte de publicidade, 78 fizeram pedidos. Dos 909 usuários provenientes da segunda fonte, 120 fizeram pedidos.

Podemos chegar a uma conclusão sobre a diferença entre essas taxas de conversão de registro-para-pedido dessas fontes de publicidade?

```
from scipy import stats as st
import numpy as np
import math as mth

alpha = .05 # nível de significância

successes = np.array([78, 120])
trials = np.array([830, 909])

# proporção de sucesso no primeiro grupo:
p1 = successes[0]/trials[0]

# proporção de sucesso no segundo grupo:
p2 = successes[1]/trials[1]

# proporção de sucesso no conjunto de dados combinado:
p_combined = (successes[0] + successes[1]) / (trials[0] + trials[1])

# a diferença entre as proporções dos conjuntos de dados
difference = p1 - p2
```

Vamos calcular a estatística em termos de desvios padrão da distribuição normal padrão:

```
# calculando a estatística em desvios padrão da distribuição normal padrão
z_value = difference / mth.sqrt(p_combined * (1 - p_combined) * (1/trials[0] + 1/trials[1]))

# configurando a distribuição normal padrão (média 0, desvio padrão 1)
distr = st.norm(0, 1) 
```

Se as proporções fossem iguais, a diferença entre elas seria 0. Vamos calcular qual a distância de nossa estatística em relação a 0. Qual é a probabilidade de obter tamanha diferença, ou uma maior ainda? Como a distribuição da estatística é normal, podemos chamar o método `cdf()`. Vamos obter o valor absoluto da estatística usando o método `abs()`. Tomar o valor absoluto da estatística Z permite que nosso foco fique na magnitude dessa diferença sem considerar se ela é positiva ou negativa. Então, ao calcular o valor-p, multiplicamos o resultado por 2 para levar em consideração ambas as caudas da distribuição normal. Fazemos isso porque queremos saber a probabilidade de observar a diferença tão extrema quanto a nossa ou mais, em qualquer direção relativa à média.

```
# calculando a estatística em desvios padrão da distribuição normal padrão
z_value = difference / mth.sqrt(p_combined * (1 - p_combined) * (1/trials[0] + 1/trials[1]))

# configurando a distribuição normal padrão (média 0, desvio padrão 1)
distr = st.norm(0, 1) 

p_value = (1 - distr.cdf(abs(z_value))) * 2

print('p-value: ', p_value)

if (p_value < alpha):
    print("Rejeitando a hipótese nula: há uma diferença significativa entre as proporções")
else:
    print("Falha ao rejeitar a hipótese nula: não há motivo para considerar as proporções diferentes")
```

```
p-value:  0.012621025223628068
Rejeitando a hipótese nula: há uma diferença significativa entre as proporções
```

Dado o tamanho da amostra, tal diferença entre as proporções é suficiente para concluir que há uma diferença estatisticamente significante (mesmo que a diferença entre 9% e 13% possa parecer pequena a princípio).

Testando a Hipótese de que as Proporções são Iguais

Tarefa

Uma empresa ofereceu a 400 de 900 **líderes** (usuários que demonstraram algum interesse por algum produto) uma oferta especial para um pacote com serviços adicionais. Aos 500 usuários restantes foi oferecido o pacote de serviços original.

Em cada grupo, 100 usuários compraram o pacote.

A oferta especial funcionou? Teste a hipótese de que as proporções de pedidos nesses dois segmentos líderes foi igual.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

from scipy import stats as st

import numpy as np

import math as mth

  

alpha \= .05 \# nível de significância

  

purchases \= np.array(\[100, 100\])

leads \= np.array(\[400, 500\])

  

p1 \= purchases\[0\]/leads\[0\]

p2 \= purchases\[1\]/leads\[1\]

\# proporção de sucesso no conjunto de dados combinado:

p\_combined \= (purchases\[0\] + purchases\[1\]) / (leads\[0\] + leads\[1\])

  

\# a diferença entre as proporções dos conjuntos de dados

difference \= p1 \- p2

z\_value \= difference / mth.sqrt(p\_combined \* (1 \- p\_combined) \* (1/leads\[0\] + 1/leads\[1\]))

distr \= st.norm(0, 1)

  

p\_value \= (1 \- distr.cdf(abs(z\_value))) \* 2

  

print('p-value: ', p\_value)

  

if (p\_value < alpha):

print("Rejeitando a hipótese nula: há uma diferença significativa entre as proporções")

else:

print("Falha ao rejeitar a hipótese nula: não há motivo para considerar as proporções diferentes")

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-37-744Z.md
### Última modificação: 2025-05-28 20:03:38

# Testes de Normalidade. O Teste Shapiro–Wilk - TripleTen

Teoria

# Testes de Normalidade. O Teste Shapiro–Wilk

Na virada do século XIX, cientistas descobriram que uma grande quantidade de variáveis possuem distribuição normal: alturas e medidas corporais de soldados, pesos de recém nascidos, quantidades de casamentos... Cada uma dessas variáveis possui uma média mais frequente, e quanto mais intenso o desvio de um valor em relação à média, mais raro ele é.

![](https://practicum-content.s3.amazonaws.com/resources/__1707463424.jpg)

O advento dos macrodados revelou que as pessoas não são tão padronizadas no fim das contas. Quando se trata do resultado de atividade humana, o conceito de médias não se aplica. Isso porque uma "pessoa padrão" realizando uma "atividade padrão" não existe. Há raras pessoas hiperativas ou uma "cauda longa" de pessoas com baixa atividade.

Em estatística, essa distribuição ganhou seu nome em homenagem a **Vilfredo Pareto**. Você deve ter ouvido falar sobre a **regra** **80/20**: "80% dos efeitos provém de 20% das causas, e os 80% restantes das causas geram apenas 20% dos efeitos." Isso também é conhecido como princípio de Pareto. Apesar de o próprio princípio não ter sido desenvolvido por Pareto, ele reflete o resultado de sua pesquisa sobre a distribuição desigual de receita.

Na vida real, muitas variáveis divergem da distribuição normal: elas contêm valores atípicos que não podem ser ignorados. Para testar se esses conjuntos de dados estão modelados corretamente pela distribuição normal, usamos **testes de normalidade**.

De acordo com o teorema central do limite, médias amostrais possuem uma distribuição normal em torno da média real de uma população (e proporções amostrais em torno da proporção real). Isso é válido até mesmo para distribuições contendo valores atípicos que se destacam. Essa abordagem funciona bem se você estiver tirando conclusões com base em dezenas de amostras. Cada uma das amostras pode conter valores atípicos que irão alterar os resultados e que afetam os resultados.

Primeiro, você precisa aprender a testar a hipótese de que uma amostra foi retirada de uma população distribuída normalmente.

Um critério simples é χ² (qui-quadrado). A soma dos quadrados das diferenças entre valores observados e esperados é dividida pelos valores esperados:

χ2\=∑i\=1n(Oi−Ei)2Eiχ^{2} = \\sum\\limits\_{i=1}^{n} \\frac{(O\_{i} - E\_{i})^{2}}{E\_{i}}χ2\=i\=1∑n​Ei​(Oi​−Ei​)2​

_O_ nesta fórmula são valores observados, enquanto _E_ são os valores esperados. É pressuposto que a diferença entre os valores esperados e observados é distribuída normalmente em torno de 0, de modo que a probabilidade de desvios diminui conforme você se afasta dos valores esperados para qualquer direção. Como resultado, esse critério será distribuído como a soma dos quadrados de _n_ distribuições normais padrão, onde _n_ é a quantidade de observações em uma amostra. Esta é a **distribuição qui-quadrado**.

O qui-quadrado é um critério comum, mas há um melhor para testes de normalidade: o **teste Shapiro–Wilk**. A vantagem é que o teste Shapiro-Wilk pode ter mais poder para detectar desvios da normalidade em amostras pequenas. Este critério costuma ser mais complexo, e é mais fácil de testar seu poder de fogo em vários conjuntos de dados do que comprová-lo teoricamente. O cálculo do critério Shapiro-Wilk é construído no módulo padrão `scipy.stats`. Vamos ver como isso funciona na prática.

`sample_1` contém dados de quantidades semanais de sessões de usuários em um site no decorrer de um ano. Vamos utilizar o método **st.shapiro(x)** para testar se podemos considerar que a variável possui uma distribuição normal:

```
from scipy import stats as st

sample_1 = [147370, 152806, 154268, 154570, 156869, 161300, 158908, 154499, 159760,
 154816, 150199, 150335, 152049, 150695, 156405, 158196, 163000, 155060,
 159906, 161635, 152265, 154191, 166409, 162048, 158006, 148260, 156046,
 149568, 153400, 157149, 151341, 155922, 148805, 157365, 159185, 159029,
 150370, 156801, 156520, 147587, 159004, 152702, 164429, 156915, 156685,
 158466, 149675, 161078, 142363, 153381, 154160, 157294, 149151, 152023,
 159271, 156266, 153598, 158988, 159654, 154394, 160494, 148227, 150683,
 160808, 162206, 154984, 152100, 153962, 158697, 167535, 149024, 153556,
 156801, 159323, 153131, 157763, 161555, 151529, 144765, 163783, 149504,
 151832, 160111, 153063, 162105, 151281, 152290, 156456, 155966, 148541,
 157985, 155979, 161416, 158958, 154449, 157113, 157141, 156666, 162254,
 158927]

alpha = .05 # nível de significância

results = st.shapiro(sample_1)
p_value = results[1] # o segundo valor no vetor de resultados (com índice 1) - o valor-p

print('p-value: ', p_value)

if (p_value < alpha):
    print("Hipótese nula rejeitada: a distribuição não é normal")
else:
    print("Falha ao rejeitar a hipótese nula: a distribuição parece ser normal")
```

```
p-value:  0.9630013108253479
Falha ao rejeitar a hipótese nula: a distribuição parece ser normal
```

Nada de gente esquisita, somente a distribuição normal.

Testes de Normalidade. O Teste Shapiro–Wilk

Tarefa

Uma amostra com recibos de uma loja virtual com valores arredondados para dólares está armazenada em `sales_data`. Verifique se os valores totais dessas receitas apresentam uma distribuição normal usando um teste Shapiro-Wilk.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

from scipy import stats as st

  

sales\_data \= \[324, 209, 217, 321, 210, 231, 235, 519, 210, 240, 213, 325,

252, 251, 246, 353, 260, 256, 203, 212, 211, 318, 529, 252,

227, 278, 221, 222, 257, 289, 208, 256, 308, 395, 485, 350,

214, 378, 218, 261, 216, 289, 533, 239, 326, 445, 210, 284,

317, 260, 420, 497, 321, 205, 237, 261, 205, 269, 246, 685,

246, 207, 317, 236, 519, 230, 208, 202, 216, 234, 242, 200,

226, 213, 440, 1026, 318, 286, 210, 216, 227, 256, 221, 216,

204, 498, 223, 287, 296, 292, 406, 213, 210, 291, 217, 200,

344, 296, 222, 258, 223, 422, 497, 325, 328, 201, 242, 255,

203, 252, 254, 221, 527, 231, 506, 203, 261, 678, 209, 261,

281, 210, 292, 354, 210, 235, 220, 204, 270, 218, 230, 295,

215, 372, 218, 230, 282, 284, 229, 210, 206, 267, 299, 263,

563, 215, 258, 214, 351, 201\]

  

alpha \= .05 \# nível de significância

  

results \= st.shapiro(sales\_data)

p\_value \= results\[1\] \# o segundo valor no vetor de resultados (com índice 1) - o valor-p

  

print('p-value: ', p\_value)

  

if (p\_value < alpha):

print("Hipótese nula rejeitada: a distribuição não é normal")

else:

print("Falha ao rejeitar a hipótese nula: a distribuição parece ser normal")

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-39-104Z.md
### Última modificação: 2025-05-28 20:03:39

# O Teste Não-Paramétrico Wilcoxon-Mann-Whitney - TripleTen

Teoria

# O Teste Não-Paramétrico Wilcoxon-Mann-Whitney

Quando seus dados contêm valores atípicos significativos (em comparação à distribuição normal), métricas algébricas não funcionam muito bem. É verdade, eles consideram todo tipo de valor, mas essa também é sua fraqueza: um valor atípico pode desbalancear tudo.

Na lição anterior você estudou critérios algébricos para testar hipóteses relativas à normalidade dos dados de origem. Eles são **paramétricos**, já que você utiliza uma amostra para avaliar os **parâmetros da expectativa de distribuição** (por exemplo, a média). Agora que você já sabe que esses tipos de hipóteses são irrelevantes para algumas variáveis, vamos observar um teste com base em uma **abordagem estrutural**, ou um teste **não-paramétrico**.

Esse teste foi concebido originalmente por Frank Wilcoxon para ser aplicado em amostras dependentes de pequeno porte, em 1945. Em 1947, Henry Mann e Donald Whitney adaptaram o teste para amostras independentes. O nome do método que iremos utilizar para os testes A/B é **st.mannwhitneyu()** (o teste U de Mann-Whitney).

A ideia chave por trás do teste é classificar duas amostras em ordem crescente e comparar as classificações de valores que verificamos nas duas amostras (ou seja, na mesma posição em que elas aparecem nas amostras).Se as diferenças entre seus ordenamentos forem as mesmas para todas as amostras, isso quer dizer que a mudança é **típica**. Isso quer dizer que alguns valores foram simplesmente acrescentados, provocando uma alteração em todos valores restantes.

Uma mudança **atípica** indica a ocorrência de uma mudança real. A soma desses tipos de mudança no ordenamento (de #1 até #4 seria 3 etc.) é o valor do critério. Quanto maior o critério, maior a probabilidade de haver diferença entre as distribuições das duas amostras.

As probabilidades de obter diversos valores a partir de um teste Mann-Whitney foram calculadas teoricamente, o que nos possibilita concluir que existe ou não uma diferença, independente do nível de significância estabelecido.

Métodos não-paramétricos são úteis porque não estabelecem pressupostos em relação ao modo como os dados foram distribuídos (de modo que você não precisará estimar os parâmetros de distribuição). Métodos como esses costumam ser empregados quando é difícil (ou até impossível) estimar parâmetros, devido à quantidade excessiva de valores atípicos.

Digamos que você possui a quantidade total de mensagens diretas trocadas por usuários de um determinado site em certa data, antes (`messages_old`) e depois (`messages_new`) da introdução de uma interface de troca de mensagens atualizada.

Tal como acontece com frequência com dados relacionados à atividade nas redes sociais, a distribuição desses valores não segue uma distribuição normal. Normalmente, há uma tendência para um número menor de mensagens enviadas pela maioria dos usuários e um pequeno grupo que envia muitas mensagens, conhecido como outliers, ou valores atípicos.

Vamos verificar se a introdução da nova interface teve algum impacto na quantidade de mensagens enviadas:

```
from scipy import stats as st

messages_old = [81,  125,  131,  113,   85,  133,   96,  160,   80,   91,  106,   83,
   86,   82,  104,  578,  100,  108,  252,  128,   93,  370,  336,  201,
   82,  113,  151,   94,  111,  102,  115,  585,   88,   90,  221,  203,
   88,  127,   95,  189,   98,   89,   84,  153,   89,  122,   85,  172,
  116,  182,  100,   82,  130,  108,   83,  172,  172,  136,  249,  176,
   87,  357,  130,  131,  174,  148,  124,   81,   98,  113, 1245,   85,
  165,   83,  277,  170,  176,   95,   88,  212,  140,  146,  106,  100,
   89,  151,  165,  125,   91,   84,  225,  120,  168,  688,  135,  112,
  113,  109,  285,   96,  120,   93,   86,  236,  129,  114,   95,   87,
  111,   98,  106,  205,  435,  115,   87,  213,  499,  124,  132,   93,
   86,   90,  118,   87,   91,  188,   96,   85,   85,  101,   94,  908,
  132,  121,   82,  152,   87,   83,   97,  149,   87,  281,  169,  131,
  140,  102,  154,  857,  190,   89]
messages_new = [99, 102, 100,  84, 194,  84,  85,  86, 282,  96, 140,  82, 104,  81,
 397, 100,  95, 111,  89, 199,  83, 219, 122, 227, 165, 189, 208, 212,
  82, 147,  83,  89, 203, 188, 330,  90, 106, 121, 110, 109,  87, 205,
  96,  95, 158, 308,  95, 249, 176, 158, 111, 115,  93, 163, 139, 108,
  99, 239, 108, 104, 239, 120, 331, 175,  87, 204, 143,  89, 261, 118,
 104, 157, 207, 140, 119,  96,  95, 107, 216, 253, 171,  81,  97,  83,
 158, 103, 144, 144, 207,  94, 104,  92, 127,  93, 135, 232,  91, 122,
  98, 137,  89, 104, 179,  82,  99, 125, 142,  98, 101,  92, 124, 265,
 451, 134,  98,  82, 110, 177,  96, 130, 109, 102, 127,  93, 125, 127,
  87, 201,  92, 344, 157, 124, 104, 145, 102,  82, 196, 836, 106, 150,
 186, 230, 115,  82, 112, 115,  87, 199,  94, 204]

alpha = .05 # nível de significância

results = st.mannwhitneyu(messages_old, messages_new)

print('p-value: ', results.pvalue)

if (results.pvalue < alpha):
    print("Hipótese nula rejeitada: existem diferenças significativas entre as distribuições dos dois grupos comparados.")
else:
    print("Falha a rejeitar a hipótese nula: não é possível chegar a conclusões sobre a diferença")
```

```
p-value:  0.2439696343542883
Falha ao rejeitar a hipótese nula: não é possível chegar a uma conclusão sobre a diferença
```

Como podemos ver, não é possível concluir que a nova interface teve qualquer tipo de impacto sobre a quantidade de mensagens enviadas.

O Teste Não-Paramétrico Wilcoxon-Mann-Whitney

Tarefa

Você possui dados relativos ao volume médio de compras antes e depois da introdução de um programa de fidelidade para clientes que realizam compras de grande volume: esses clientes receberam recompensas, em cashback, de um banco famoso. Cheque as amostras para verificar se seria possível dizer que o programa foi um sucesso.

Para completar essa tarefa você deverá testar uma hipótese de um lado (unilateral). Além dos conjuntos de dados, você irá passar mais dois parâmetros para o método.

O terceiro parâmetro é do tipo booleano e define se deveria haver um ajuste para manter a continuidade da distribuição usada para descrever os dados discretos no teste. Você precisa levar isso em consideração, mas o efeito sobre o resultado é praticamente irrelevante. Esse parâmetro é padronizado como `True`. Se você quiser inserir um quarto parâmetro, o parâmetro booleano deve ser `True`. O quarto parâmetro é escrito como uma string: `'less'`, `'two-sided'`, ou `'greater'`. É assim que comparamos o primeiro conjunto de dados que passamos para o método com o segundo. Nesta tarefa, especificamente, precisamos verificar se o segundo conjunto de dados é maior e o primeiro menor, para podermos estabelecer o parâmetro como `'less'`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

from scipy import stats as st

  

sales\_before \= \[25939, 14569, 15040, 28317, 21100, 13597, 62869, 46195, 13414,

13928, 17136, 14729, 25754, 17254, 16628, 16605, 40711, 74209,

14498, 32265, 13873, 16724, 22522, 14824, 21825, 32522, 14485,

16779, 17574, 16772, 18331, 19170, 13753, 15551, 17202, 13725,

15415, 16155, 49620, 33900, 23834, 25732, 16539, 24449, 14681,

15000, 14521, 13298, 14421, 17500, 15949, 16246, 19259, 15283,

14418, 18026, 25931, 14182, 13837, 23061, 14074, 25344, 19134,

14177, 19357, 96794, 26358, 16599, 15426, 23417, 68856, 44375,

14669, 39750, 34531, 14655, 28580, 25176, 55065, 64288, 16069,

16745, 13548, 19177, 19173, 16473, 15534, 20115, 16608, 15261,

13472, 47956, 21036, 19238, 25955, 14755, 16901, 13740, 13585,

23080, 17259, 51311, 47505, 19582, 13968, 46805, 14261, 18376,

13314, 37948, 18404, 16911, 18692, 19885, 16619, 15234, 21832,

228535, 28377, 16452, 13293, 17915, 15527, 17671, 24046, 15645,

14350, 16765, 17600, 14222, 25300, 16941, 14758, 17120, 14621,

25596, 20472, 24871, 14504, 17956, 20565, 18868, 16980, 40395,

13868, 14572, 13893, 17986, 14490, 16891\]

sales\_after \= \[17484, 18369, 19412, 35496, 30841, 18511, 16438, 16064, 27841,

18335, 20978, 18266, 24675, 16355, 15245, 14960, 15448, 14181,

20095, 15586, 18594, 14414, 50452, 18804, 16750, 17313, 20047,

25674, 30803, 14567, 16871, 17667, 48241, 15191, 135885, 104794,

18650, 16708, 26201, 15926, 40253, 17787, 28374, 22989, 21122,

14938, 115634, 18351, 15895, 14951, 15177, 25709, 76209, 99617,

16452, 16446, 19407, 21144, 14947, 26257, 23723, 18113, 27784,

38882, 15907, 15741, 21705, 32604, 16101, 17870, 15794, 18423,

18381, 194987, 15335, 14022, 21257, 29935, 14598, 26066, 47228,

37022, 15071, 21353, 38690, 40838, 26125, 24722, 30756, 17099,

21377, 14611, 44442, 15808, 17173, 93187, 30411, 15279, 25707,

35374, 70792, 14918, 21678, 16453, 40998, 27836, 18411, 46965,

15968, 22812, 15856, 17933, 23682, 33450, 21727, 17884, 21676,

124684, 20145, 16041, 14872, 17588, 17436, 81993, 20497, 17484,

58826, 26179, 17515, 27463, 14260, 27331, 17598, 41888, 14037,

15517, 19704, 16718, 32514, 38851, 18925, 23982, 14104, 21690,

60266, 21071, 42799, 16203, 16694, 22699\]

  

alpha \= 0.05 \# nível de significância

  

results \= st.mannwhitneyu(sales\_before, sales\_after, True, 'less')

  

  

  

print('p-value: ', results.pvalue)

  

if (results.pvalue < alpha):

print("Hipótese nula rejeitada: a diferença possui significância estatística")

else:

print("Falha a rejeitar a hipótese nula: não é possível chegar a conclusões sobre a diferença")

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-41-054Z.md
### Última modificação: 2025-05-28 20:03:41

# Estabilidade de Métricas Cumulativas - TripleTen

Teoria

# Estabilidade de Métricas Cumulativas

A fim de prevenir o problema dos picos, analistas examinam gráficos.

Eles analisam gráficos de métricas **cumulativas**. Suponhamos que um teste durou duas semanas. Se você construir um gráfico com dados **cumulativos**, no ponto referente ao primeiro dia você terá valores de métricas para aquele dia, no ponto do segundo dia você terá a soma das métricas para os dois primeiros dias, e assim por diante. Dessa maneira você pode rastrear mudanças nos resultados experimentais por cada dia de teste.

De acordo com o teorema central do limite, os valores de métricas cumulativas costumam convergir e se agrupar em torno de uma média específica. Então, um mapa cumulativo da métrica pode te ajudar a decidir se vale a pena prosseguir com o teste.

Por exemplo, um experimento durou aproximadamente três semanas. O gráfico da conversão diária ficou assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_6_1587730452.png)

É difícil dizer qual o melhor segmento, já que os resultados diários possuem uma intensa flutuação.

Vamos observar um gráfico de conversão cumulativo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_7_1587730667.png)

As métricas cumulativas do grupo vermelho são consistentemente menores do que as do grupo azul. No começo do teste, uma flutuação na conversão foi observada nos dois grupos. O gráfico estabilizou somente no final do teste.

Para tornar mais óbvia a diferença entre os grupos, analistas constroem **gráficos de diferença relativa**. Cada um de seus pontos é calculado da seguinte forma: `group B cumulative metric / group A cumulative metric - 1`.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_8_1587730710.png)

O grupo B demonstra consistentemente taxas de conversão piores do que o grupo A, e a diferença está aumentando com o passar do tempo. Se houver dados suficientes para atingir o nível de significância, será possível interromper o teste, pois o grupo B é definitivamente pior do que o A.

Antes de passarmos para a construção de gráficos cumulativos para métricas, vamos revisar os métodos de que vamos precisar. Vamos fazer um DataFrame com as colunas `'letter'` e `'number'`:

```
import pandas as pd

df = pd.DataFrame([['a',5], ['b',4], ['c',3], ['d',2]])
df.columns = ['letter', 'number']
print(df)
```

```
    letter  number
0      a       5
1      b       4
2      c       3
3      d       2
```

Agora vamos aplicar a função `lambda` para cada valor da coluna `number`:

```
df.apply(lambda x: x['number']*3, axis=1)
```

```
0    15
1    12
2     9
3     6
```

Agora você só precisa aplicar os métodos que você já conhece tão bem: `agg()`, `sort_values()`, e `nunique()`.

Vamos fazer um DataFrame usando as colunas `'number1'`, `'number2'`, e `'number3'` e nomeá-lo como `df`.

Vamos aplicar `agg()`:

-   Em `'number1'` vamos encontrar o valor máximo
-   Vamos somar os valores na coluna `'number2'`
-   Em `'number3'` vamos encontrar a quantidade de valores únicos

Agora vamos ordenar os resultados em ordem decrescente:

```
df = pd.DataFrame([[1,2,3], [4,5,6], [7,8,12], [10,11,12]])
df.columns = ['number1', 'number2', 'number3']
print(df)
df.agg({'number1' : 'max', 'number2' : 'sum', 'number3' : pd.Series.nunique}).sort_values()
```

```
number1  number2  number3
0        1        2        3
1        4        5        6
2        7        8       12
3       10       11       12
number3     3
number1    10
number2    26
dtype: int64
```

Outra função útil para calcular métricas cumulativas é **np.logical\_and()**. Ela nos permite aplicar operações booleanas a objetos Series. Isso é útil quando você precisa escolher um subconjunto de linhas de uma tabela que se conformam a diversas condições.

Agora vamos criar o DataFrame `df`. Vamos se verificar se cada linha está conforme com a condição "o valor `'number'` é maior que 2 e menor do que 10".

```
df = pd.DataFrame([2,4,6,8,10,12,14,16,18,20])
df.columns = ['number']
print(np.logical_and(df['number'] > 2, df['number'] < 10))
print(df[np.logical_and(df['number'] > 2, df['number'] < 10)])
```

```
0    False
1     True
2     True
3     True
4    False
5    False
6    False
7    False
8    False
9    False
Name: number, dtype: bool
   number
1       4
2       6
3       8
```

As funções **np.logical\_not()** e **np.logical\_or()** operam de modo análogo.

Vamos ler os dados de arquivos contendo os resultados de testes A/B:

```
import pandas as pd
import datetime as dt
import numpy as np

orders = pd.read_csv('lesson_6/orders_for_detecting_anomalies.csv', sep=',')
orders['date'] = orders['date'].map(
    lambda x: dt.datetime.strptime(x, '%Y-%m-%d')
)

visitors = pd.read_csv(
    'lesson_6/orders_for_detecting_anomalies_visitors.csv', sep=','
)
visitors['date'] = visitors['date'].map(
    lambda x: dt.datetime.strptime(x, '%d/%m/%Y')
)

print(orders.head())
print(visitors.head())
```

```
orderId group                userId  revenue       date
0        1     A   4765212904118882304     7503 2019-04-01
1        2     A  14477413223818084352    11424 2019-04-01
2        3     B  10223965268965748736     1299 2019-04-01
3        4     A   1399771946645095424      999 2019-04-01
4        5     A  13269081434837460992      499 2019-04-01
        date group  visitors
0 2019-04-01     A       455
1 2019-04-02     A       501
2 2019-04-03     A      1313
3 2019-04-04     A       555
4 2019-04-05     A       564
```

As colunas `orders`do DataFrame:

-   `orderId` — identificador do pedido
-   `userId` — o identificador do usuário que faz o pedido
-   `group` — o grupo de teste A/B (A ou B)
-   `revenue` — receita dos pedidos (volume médio de compra)
-   `date` — quando o pedido foi feito

As colunas `visitors`do DataFrame:

-   `date`
-   `group`
-   `visitors` — a quantidade de visitantes da loja online em um determinado grupo de teste A/B na data especificada

Para construir os gráficos, precisamos coletar dados cumulativos. Vamos declarar um DataFrame chamado `cumulativeData` com as seguintes colunas:

-   `date`
-   `group`
-   `orders` — a quantidade de pedidos na data especificada para o grupo especificado
-   `buyers` — a quantidade de usuários que fazem pelo menos um pedido na data especificada para o grupo especificado
-   `revenue` — a receita na data especificada para o grupo especificado (volume médio de compra)
-   `visitors` — a quantidade de visitantes da loja online na data especificada para o grupo especificado

Vamos construir um vetor com valores de pares unívocos de grupos de datas usando o método `drop_duplicates()`:

```
# construindo um vetor com valores de pares unívocos de grupos de datas 
datesGroups = orders[['date','group']].drop_duplicates()
```

Vamos coletar os dados diários cumulativos de pedidos em alguns passos.

Na primeira parte do código, vamos pegar as linhas da tabela `orders` em que a data é menor ou igual à data do elemento `datesGroups` e o grupo de teste é o mesmo que em `datesGroups:`

`orders[np.logical_and(orders['date'] <= x['date'], orders['group'] == x['group'])]`.

Na segunda parte, vamos agregar os valores. Vamos encontrar a data máxima. Vamos encontrar o valor máximo para o grupo, também, para que tenhamos a coluna `'group'` em nosso resultado final. Nós descobrimos a quantidade de pedidos unívocos e identificadores de usuários e calculamos a soma dos volumes médios de compra: `.agg({'date' : 'max', 'group' : 'max', 'orderId' : pd.Series.nunique, 'userId' : pd.Series.nunique, 'revenue' : 'sum'})`

Vamos aplicar os métodos para cada linha do DataFrame (`apply(axix=1)`) e ordenar os resultados pelas colunas `'date'` e `'group'`. Juntando tudo isso, vamos obter:

```
ordersAggregated = datesGroups.apply(lambda x: orders[np.logical_and(orders['date'] <= x['date'], orders['group'] == x['group'])].agg({'date' : 'max', 'group' : 'max', 'orderId' : pd.Series.nunique, 'userId' : pd.Series.nunique, 'revenue' : 'sum'}), axis=1).sort_values(by=['date','group'])

print(ordersAggregated.head())
```

```
         date group  orderId  userId  revenue
0  2019-04-01     A        6       6   128276
2  2019-04-01     B        3       3     4082
9  2019-04-02     A       12      12   157683
10 2019-04-02     B        7       6   136535
20 2019-04-03     A       23      20   199868
```

Com esse código, é como se nós estivéssemos calculando os resultados de nosso teste todos os dias até a data corrente e armazenando-os nas linhas da tabela `ordersAggregated`.

Vamos fazer algo similar para obter os dados diários agregados cumulativos sobre os visitantes:

```
visitorsAggregated = datesGroups.apply(lambda x: visitors[np.logical_and(visitors['date'] <= x['date'], visitors['group'] == x['group'])].agg({'date' : 'max', 'group' : 'max', 'visitors' : 'sum'}), axis=1).sort_values(by=['date','group'])
```

Vamos juntar as duas tabelas em uma e dar nomes descritivos às suas colunas:

```
# construindo um vetor com valores de pares unívocos de grupos de datas
datesGroups = orders[['date','group']].drop_duplicates()

# obtendo dados diários cumulativos agregados sobre pedidos 
ordersAggregated = datesGroups.apply(lambda x: orders[np.logical_and(orders['date'] <= x['date'], orders['group'] == x['group'])].agg({'date' : 'max', 'group' : 'max', 'orderId' : pd.Series.nunique, 'userId' : pd.Series.nunique, 'revenue' : 'sum'}), axis=1).sort_values(by=['date','group'])

# obtendo dados diários cumulativos agregados sobre visitantes  
visitorsAggregated = datesGroups.apply(lambda x: visitors[np.logical_and(visitors['date'] <= x['date'], visitors['group'] == x['group'])].agg({'date' : 'max', 'group' : 'max', 'visitors' : 'sum'}), axis=1).sort_values(by=['date','group'])

# juntando as duas tabelas em uma e dando nomes descritivos para suas colunas
cumulativeData = ordersAggregated.merge(visitorsAggregated, left_on=['date', 'group'], right_on=['date', 'group'])
cumulativeData.columns = ['date', 'group', 'orders', 'buyers', 'revenue', 'visitors']

print(cumulativeData.head(5))
```

```
        date group  orders  buyers  revenue  visitors
0 2019-04-01     A       6       6   128276       455
1 2019-04-01     B       3       3     4082       464
2 2019-04-02     A      12      12   157683       956
3 2019-04-02     B       7       6   136535       977
4 2019-04-03     A      23      20   199868      2269
```

Vamos construir gráficos da receita cumulativa diária e o grupo de teste A/B:

```
import matplotlib.pyplot as plt

# DataFrame com pedidos cumulativos e receita cumulativa por dia, grupo A
cumulativeRevenueA = cumulativeData[cumulativeData['group']=='A'][['date','revenue', 'orders']]

# DataFrame com pedidos cumulativos e receita cumulativa por dia, grupo B
cumulativeRevenueB = cumulativeData[cumulativeData['group']=='B'][['date','revenue', 'orders']]

# construindo o gráfico de receita do grupo A 
plt.plot(cumulativeRevenueA['date'], cumulativeRevenueA['revenue'], label='A')

# construindo o gráfico de receita do grupo B 
plt.plot(cumulativeRevenueB['date'], cumulativeRevenueB['revenue'], label='B')

plt.legend()
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_9_1587736218.png)

A receita apresenta um crescimento estável ao longo de todo o teste. Isso é um bom sinal!

Mesmo assim, podemos observar que os dois grupos apresentam picos de receita em determinados pontos. Isso pode sugerir tanto uma alta na quantidade de pedidos como também a presença de pedidos muito caros na amostra. Você irá aprender a analisar valores atípicos como esses em nossa próxima lição.

Vamos construir o volume médio de compra por grupo. Vamos dividir a receita cumulativa pela quantidade cumulativa de pedidos:

```
plt.plot(cumulativeRevenueA['date'], cumulativeRevenueA['revenue']/cumulativeRevenueA['orders'], label='A')
plt.plot(cumulativeRevenueB['date'], cumulativeRevenueB['revenue']/cumulativeRevenueB['orders'], label='B')
plt.legend()
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_10_1587736269.png)

O volume médio de compra também torna-se estável no fim do teste: ele se estabiliza para o grupo A e continua em queda para o grupo B. A alta no grupo B na primeira metade do teste pode ser decorrente da presença de pedidos volumosos e caros. Vamos precisar de mais dados sobre esse grupo para descobrir a o verdadeiro volume médio de compra e estabelecer seu nível.

Vamos construir um gráfico de diferença relativa para os volumes médios de compras. Vamos acrescentar um eixo horizontal com o **método axhline()** (ou seja, linha horizontal através do eixo):

```
# reunindo os dados em um DataFrame
mergedCumulativeRevenue = cumulativeRevenueA.merge(cumulativeRevenueB, left_on='date', right_on='date', how='left', suffixes=['A', 'B'])

# construindo um gráfico de diferença relativa para os volumes médios de compra
plt.plot(mergedCumulativeRevenue['date'], (mergedCumulativeRevenue['revenueB']/mergedCumulativeRevenue['ordersB'])/(mergedCumulativeRevenue['revenueA']/mergedCumulativeRevenue['ordersA'])-1)

# acrescentando o eixo X
plt.axhline(y=0, color='black', linestyle='--')
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_11_1587736369.png)

Em vários pontos, a diferença entre os segmentos apresenta picos. Isso indica a ocorrência de pedidos grandes e valores atípicos! Vamos encontrá-los depois.

Vamos analisar o gráfico de conversão cumulativo de modo análogo. Vamos escalonar os eixos com o método **plt.axis()**, passando os valores mínimos e máximos do eixo X e valores mínimos e máximos do eixo Y para o método: `["2019-04-01", '2019-04-23', 0, 0.015]`

```
# calculando a conversão cumulativa
cumulativeData['conversion'] = cumulativeData['orders']/cumulativeData['visitors']

# selecionando dados no grupo A
cumulativeDataA = cumulativeData[cumulativeData['group']=='A']

# Selecionando dados no grupo B
cumulativeDataB = cumulativeData[cumulativeData['group']=='B']

# construindo os gráficos
plt.plot(cumulativeDataA['date'], cumulativeDataA['conversion'], label='A')
plt.plot(cumulativeDataB['date'], cumulativeDataB['conversion'], label='B')
plt.legend()

# definindo a escala dos eixos
plt.axis(["2019-04-01", '2019-04-23', 0, 0.015])
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_12_1587736429.png)

Note que os gráficos são simétricos. Esse tipo de simetria é muito rara para dados experimentais, então neste caso— e sempre que você obtiver resultados anormais— não se esqueça de verificar mais uma vez se seu código não possui nenhum bug. Se não há nenhum, isso pode indicar que seus dados não são confiáveis. Nesse caso, porém, a simetria é válida e foi causada pelo modo como o experimento foi conduzido, então não se preocupe.

Quanto às outras observações, os grupos estavam flutuando em torno do mesmo valor, mas a taxa de conversão do grupo A subiu antes de se estabilizar, enquanto a taxa de conversão do grupo B caiu, e então se tornou estável.

Vamos construir um gráfico da diferença relativa para as taxas de conversão cumulativas:

```
mergedCumulativeConversions = cumulativeDataA[['date','conversion']].merge(cumulativeDataB[['date','conversion']], left_on='date', right_on='date', how='left', suffixes=['A', 'B'])

plt.plot(mergedCumulativeConversions['date'], mergedCumulativeConversions['conversionB']/mergedCumulativeConversions['conversionA']-1)
plt.legend()

plt.axhline(y=0, color='black', linestyle='--')
plt.axhline(y=-0.1, color='grey', linestyle='--')
plt.axis(["2019-04-01", '2019-04-23', -0.6, 0.6])
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1588763025.png)

No começo do teste, o grupo B estava claramente para trás, mas então, de repente, ele assumiu a liderança. E então sua conversão diminuiu novamente, e agora está crescendo gradualmente.

De modo geral, a taxa de conversão ainda não é estável, então ainda é cedo para tirar conclusões sobre o teste. De qualquer modo, primeiro devemos observar as anomalias; elas podem chegar a alterar o quadro geral de alguma forma. Vamos fazer isso na próxima lição.

Estabilidade de Métricas Cumulativas

Tarefa5 / 6

1.

Estude os dados nos arquivos com pedidos e visitantes.

A tabela com pedidos contém as seguintes colunas:

-   `orderId`
-   `userId`
-   `group`
-   `revenue`
-   `date`

A tabela com visitantes contém as seguintes colunas:

-   `date`
-   `group`
-   `visitors` — o número de visitantes na data especificada para grupos especificados

1) Crie um DataFrame chamado `datesGroups` com pares de `'date'` unívocos e os valores de `'group'` da tabela `orders`. Elimine valores duplicados usando o método `drop_duplicates()`.

2) Declare a variável `ordersAggregated` para armazenar:

-   a data
-   o grupo do teste A/B
-   a quantidade de pedidos distintos do grupo teste para a data especificada, inclusive
-   a quantidade de usuários distintos no grupo teste que fizeram pelo menos um pedido até a data especificada, inclusive
-   a receita total de pedidos no grupo teste até a data especificada, inclusive

3) Declare a variável `visitorsAggregated` para armazenar:

-   a data
-   o grupo do teste A/B
-   a quantidade de visitantes distintos no grupo teste até a data especificada, inclusive

4) Ordene `ordersAggregated` e `visitorsAggregated` de acordo com as colunas `'date'` e `'group'`, nessa ordem.

5) Defina a variável `cumulativeData` juntando `ordersAggregated` e `visitorsAggregated` com as colunas `'date'` e `'group'` com o método `merge()`.

6) Atribua os nomes `['date', 'group', 'orders', 'buyers', 'revenue', 'visitors']` para as colunas em `cumulativeData`.

7) Imprima as cinco primeiras linhas de `cumulativeData`.

2.

Declare as variáveis `cumulativeRevenueA` e `cumulativeRevenueB`, onde você vai armazenar os dados relativos a datas, receita e quantidade de pedidos para os grupos `A` e `B`

Construa gráficos de receita cumulativa diária para cada grupo.

3.

Para cada grupo, construa gráficos cumulativos (por dia) para o volume médio de compra.

4.

Junte as tabelas `cumulativeRevenueA` e `cumulativeRevenueB` com o método `merge()`, de modo que a tabela resultante contenha as colunas `['date', 'revenueA', 'revenueB', 'ordersA', 'ordersB']`. Armazene isso em `mergedCumulativeRevenue`.

Exiba em um gráfico a diferença entre o volume médio de compra cumulativo do grupo B comparado com o do grupo A.

Acrescente um eixo X com uma linha pontilhada (Y = 0) usando o método `plt.axhline()`.

5.

Adicione a coluna `'conversion'` em `cumulativeData`. Essa coluna contém a razão entre a quantidade de pedidos e a quantidade de usuários para um grupo específico em uma data específica.

Declare as variáveis `cumulativeDataA` e `cumulativeDataB`, onde você irá armazenar os dados relativos a pedidos nos segmentos `A` e `B`.

Construa o gráfico de conversão diária cumulativa para cada grupo.

Configure a escala dos eixos assim: `plt.axis([pd.to_datetime('2019-03-10'), pd.to_datetime('2019-04-23'), 0, 0.05])`.

6.

Junte as tabelas `cumulativeDataA` e `cumulativeDataB` usando o método `merge()`, de modo que a tabela resultante contenha as colunas `['date', 'conversionA', 'conversionB']`. Armazene-a em `mergedCumulativeConversions`.

Faça um gráfico com a diferença relativa entre a taxa de conversão cumulativa do grupo B comparada com o grupo A.

Usando o método `plt.axhline()`, acrescente um eixoX (Y = 0) com uma linha pontilhada, e defina sua cor como `'black'`. Acrescente um eixo X `'grey'` em `Y = 0.2`.

Defina a escala dos eixos da seguinte forma: `plt.axis([pd.to_datetime('2019-03-10'), pd.to_datetime('2019-04-23'), -0.5, 0.5])`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

67

68

69

import pandas as pd

import datetime as dt

import numpy as np

import matplotlib.pyplot as plt

  

orders \= pd.read\_csv('/datasets/data\_for\_tasks\_3.csv', sep\=',')

orders\['date'\] \= orders\['date'\].map(

lambda x: dt.datetime.strptime(x, '%d/%m/%Y')

)

  

visitors \= pd.read\_csv('/datasets/data\_for\_tasks\_3\_visitors.csv', sep\=',')

visitors\['date'\] \= visitors\['date'\].map(

lambda x: dt.datetime.strptime(x, '%d/%m/%Y')

)

  

datesGroups \= orders\[\['date', 'group'\]\].drop\_duplicates()

  

ordersAggregated \= datesGroups.apply(

lambda x: orders\[

np.logical\_and(

orders\['date'\] <= x\['date'\], orders\['group'\] \== x\['group'\]

)

\].agg(

{

'date': 'max',

'group': 'max',

'orderId': pd.Series.nunique,

'userId': pd.Series.nunique,

'revenue': 'sum',

}

),

axis\=1,

).sort\_values(by\=\['date', 'group'\])

  

visitorsAggregated \= datesGroups.apply(

lambda x: visitors\[

np.logical\_and(

visitors\['date'\] <= x\['date'\], visitors\['group'\] \== x\['group'\]

)

\].agg({'date': 'max', 'group': 'max', 'visitors': 'sum'}),

axis\=1,

).sort\_values(by\=\['date', 'group'\])

  

cumulativeData \= ordersAggregated.merge(

visitorsAggregated, left\_on\=\['date', 'group'\], right\_on\=\['date', 'group'\]

)

cumulativeData.columns \= \[

'date',

'group',

'orders',

'buyers',

'revenue',

'visitors',

\]

  

cumulativeData\['conversion'\] \= (

cumulativeData\['orders'\] / cumulativeData\['visitors'\]

)

  

cumulativeDataA \= cumulativeData\[cumulativeData\['group'\] \== 'A'\]

cumulativeDataB \= cumulativeData\[cumulativeData\['group'\] \== 'B'\]

  

mergedCumulativeConversions \= cumulativeDataA\[\['date','conversion'\]\].merge(cumulativeDataB\[\['date','conversion'\]\], left\_on\='date', right\_on\='date', how\='left', suffixes\=\['A', 'B'\])

  

plt.plot(mergedCumulativeConversions\['date'\], mergedCumulativeConversions\['conversionB'\]/mergedCumulativeConversions\['conversionA'\]\-1)

plt.legend()

  

plt.axhline(y\=0, color\='black', linestyle\='--')

plt.axhline(y\=0.2, color\='grey', linestyle\='--')

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-42-402Z.md
### Última modificação: 2025-05-28 20:03:43

# Analisando Valores Atípicos e Altas: Valores Extremos - TripleTen

Teoria

# Analisando Valores Atípicos e Altas: Valores Extremos

Você já sabe como encontrar a significância estatística de uma diferença de métricas entre amostras e como analisar gráficos. Esses são dois enormes passos em direção à tomada de decisões corretas em experimentos.

Porém, há mais obstáculos para superar: valores atípicos/anomalias, que podem distorcer os resultados de um teste A/B. Lembre-se de que uma anomalia é um valor que raramente aparece em uma população, mas que, quando aparece, pode gerar erros.

Vamos estudar os dados de pedidos de uma fornecedora de bens industriais. Queremos identificar valores atípicos em a) volume do pedido e b) quantidade de pedidos por usuário. Note `data` foi convertida para o tipo necessário com o método `dt.datetime.strptime(x, '%d/%m/%Y')`.

```
import pandas as pd
import datetime as dt

data = pd.read_csv('lesson_6/orders_for_detecting_anomalies.csv', sep=',')
data['date'] = data['date'].map(lambda x: dt.datetime.strptime(x, '%d/%m/%Y'))

print(data.head(10))
```

```
   orderId group                userId  revenue       date
0        1     A   4765212904118882304     7503 2019-04-01
1        2     A  14477413223818084352    11424 2019-04-01
2        3     A  10223965268965748736     1299 2019-04-01
3        4     A   1399771946645095424      999 2019-04-01
4        5     B  13269081434837460992      499 2019-04-01
5        6     B   7141419942738429952      482 2019-04-01
6        7     B  14179588155630520320     2999 2019-04-01
7        8     A   1225733206606480128   104852 2019-04-01
8        9     A   3362287473339592192     2301 2019-04-01
9       10     B  11670724616393357312      499 2019-04-01
```

Temos as seguintes colunas:

-   `orderId` — identificador do pedido
-   `userId` — identificador do usuário que realiza o pedido
-   `group` — grupo de teste A/B
-   `revenue` — receita do pedido
-   `date` — data do pedido

Os pedidos nas 10 primeiras linhas variam entre $482 e $11,424. Menos uma delas: o da oitava linha foi de $104,852!

Se um pedido tão grande acaba em um dos grupos de teste, ele irá distorcer imediatamente os resultados e tornar aquele grupo o líder. Mas pedidos como esse são raros; eles são exceções que aparecem aleatoriamente, não como um resultado de nosso teste de uma hipótese.

Tais pedidos anormalmente grandes devem ser removidos dos testes.

Vamos estudar o histograma de distribuição de preços de pedidos:

```
import matplotlib.pyplot as plt

plt.hist(data['revenue'])
```

Aha! Esses dados contém definitivamente pedidos com valores maiores do que $20,000: por exemplo, aproximadamente $80,000, $100,000, e $150,000. Mas a maioria dos pedidos foi menor.

Vamos utilizar mais um método para avaliar os valores dos pedidos.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_14_1587738340.png)

```
x_values = pd.Series(range(0, len(data['revenue'])))
plt.scatter(x_values, data['revenue'])
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_15_1587738385.png)

Assim como esperado, há vários pedidos grandes. Com base no gráfico, podemos definir o limite para pedidos anormais entre $20,000 e $40,000. Porém, em geral, de 1 a 5% dos valores mais extremos são retirados pela filtragem.

Relembre o conceito do quartil do curso sobre análise exploratória de dados. Quartis dividem conjuntos de dados ordenados em quatro partes: 25% dos elementos estão abaixo de Q1, e 75% estão acima. A mediana é representada por Q2, onde os elementos estão divididos exatamente em dois.

Se dividirmos um conjunto ordenado em 100 partes ao invés de quatro, iremos obter **percentis** (do Latim _per centum_, "por centena'_)._ Eles funcionam de acordo com o mesmo princípio dos quartis: o percentil _n-ésimo_ marca o valor maior do que _n_ por cento dos valores na amostra. Por exemplo, se a amostra contém todos números inteiros positivos de 1 a 500, o 95º percentil será 475:25 de 500 elementos (25/500 = 5%) são maiores que 475.

Resumindo: o percentil _n_º (ou centil) marca o valor abaixo do qual _n_ por cento das observações decaem.

A chance de que um valor aleatório seja menor que o percentil _n_º é _n_ por cento. Em uma distribuição normal padrão, em que a média é 0 e o desvio padrão é 1, o 95º percentil será 1.645.

Para calcular percentis, você irá precisar do método **percentile()** da biblioteca NumPy:

```
import numpy as np

print(np.percentile(range(1,1001), [95, 97.5, 99]))
```

```
[950.05 , 975.025, 990.01 ]
```

Importante:

-   O intervalo de valores é passado como o primeiro argumento. Você pode passar o vetor de percentil como o segundo; e então a função também irá retornar um vetor.
-   Esse método não gera números inteiros. Isso se deve à continuidade dos números reais, à finitude e possibilidade de contagem dos elementos da amostra e à maneira como esse método NumPy opera. Para os propósitos desta lição, esse tipo de precisão não será um problema.

Vamos determinar o 90º, 95º e 99º percentis para volume médio de pedidos em nossa amostra.

```
print(np.percentile(data['revenue'], [90, 95, 99]))
```

```
[  8113.8 ,  13817.  , 105038.04]
```

Menos de 5% dos pedidos possuem valores maiores do que $13,817, e menos de 1% custou mais do que $105,039.

Taxas de conversão, e não somente o volume médio dos pedidos, também são analisadas. Lembre-se, a conversão é a razão entre a quantidade de pedidos e a quantidade de visitantes que o site registrou.

Usuários que fazem muitos pedidos inflam o numerador quando calculamos a conversão. O comportamento deles provavelmente difere do padrão normal. Quando não estamos lidando com uma loja online com demanda constante, o usuário típico não costuma fazer mais do que dois pedidos em um período curto.

Vamos descobrir a quantidade de pedidos por usuário e imprimir o resultado:

```
import pandas as pd
import datetime as dt

data = pd.read_csv('lesson_6/orders_for_detecting_anomalies.csv', sep=',')
data['date'] = data['date'].map(lambda x: dt.datetime.strptime(x, '%d/%m/%Y'))

ordersByUsers = (
    data.drop(['group', 'revenue', 'date'], axis=1)
    .groupby('userId', as_index=False)
    .agg({'orderId': pd.Series.nunique})
)

ordersByUsers.columns = ['userId', 'orders']

print(ordersByUsers.sort_values(by='orders', ascending=False).head(10))
```

```
userId  orders
103  10915146588096065536      15
171  17524390106492114944      10
26    2181679483747416832       8
13    1072811847256720512       7
126  12759566205054128128       6
148  14477413223818084352       6
28    2332073096806479360       6
20    1577273240726417664       5
41    3362287473339592192       5
11     923223027330505600       5
```

Há usuários com 5, 6, 7, 8, 10, e 15 pedidos. É uma quantidade semanal de pedidos muito maior do que a de um usuário comum.

Vamos construir um histograma de distribuição com a quantidade de pedidos por usuário.

```
import matplotlib.pyplot as plt 

plt.hist(ordersByUsers['orders'])
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_16_1587738613.png)

A maioria dos clientes realizou apenas um pedido. Porém, uma parcela significante efetuou de dois a quatro pedidos cada.

Vamos construir um gráfico de dispersão com a quantidade de pedidos por usuário:

```
x_values = pd.Series(range(0,len(ordersByUsers)))

plt.scatter(x_values, ordersByUsers['orders'])
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_17_1587738662.png)

Nossas conclusões foram confirmadas: realmente há muitos usuários que realizaram de dois a quatro pedidos. Ainda não sabemos qual é a divisão exata, pois não está claro se deveríamos considerar ou não essas anomalias.

Vamos calcular percentis para a quantidade de pedidos por usuário:

```
import numpy as np

print(np.percentile(ordersByUsers['orders'], [90, 95, 99]))
```

```
[3.  , 4.85, 8.34]
```

Menos de 5% dos usuários fez mais do que 4.85 (ou seja, cinco) pedidos, e 10% deles fizeram mais do que três.

Portanto, seria razoável definir um limite de quatro ou cinco pedidos por usuário como o limite superior para o comportamento normal e usar isso para filtrar qualquer anomalia. Você irá aprender a remover linhas como essas na próxima lição.

Analisando Valores Atípicos e Altas: Valores Extremos

Tarefa7 / 7

1.

O arquivo `data_for_tasks_3.csv` contém dados de pedidos que usuários fizeram em uma loja online. As colunas são:

-   `orderId`
-   `userId`
-   `group`
-   `revenue`
-   `date`

Descubra a quantidade de pedidos por usuário. Para fazer isso, crie um DataFrame com duas colunas: `'userId'` and `'orders'`. Nomeie-o como `ordersByUsers`. Ordene os dados pela quantidade de pedidos em ordem decrescente e imprima as 10 primeiras linhas.

2.

Construa um histograma de distribuição para a quantidade de pedidos por usuário usando o método `hist()`.

3.

Construa um gráfico de dispersão usando o método `scatter()`. Não se esqueça de passar seus valores para os eixos X e Y.

Você pode encontrar os valores do eixo horizontal no pré-código, em `x_values`: a quantidade de observações que foi gerada. Use os valores do eixo vertical da coluna `'orders'` do DataFrame `ordersByUsers`.

4.

Calcule o 90º, 95º e 99º percentis para a quantidade de pedidos por usuário utilizando o método `np.percentile()`. Imprima os resultados.

5.

Construa um histograma de distribuição da receita dos pedidos (`'revenue'`) usando o método `hist()`.

6.

Construa um gráfico de dispersão utilizando o método `scatter()`. Não se esqueça de passar seus valores para os eixos X e Y.

Você pode encontrar os valores do eixo horizontal no pré-código, em `x_values`: a quantidade de observações que foi gerada. Use os valores do eixo vertical da coluna `'revenue'` do DataFrame `data`.

7.

Calcule o 90º, 95º e 99º percentis para a receita dos pedidos por usuário utilizando o método `np.percentile()`.

99

1

2

3

4

5

6

7

8

9

10

11

12

import pandas as pd

import numpy as np

import datetime as dt

  

data \= pd.read\_csv('/datasets/data\_for\_tasks\_3.csv', sep\=',')

data\['date'\] \= data\['date'\].map(lambda x: dt.datetime.strptime(x, '%d/%m/%Y'))

  

print(data.head(10))

print(np.percentile(data\['revenue'\], \[90, 95, 99\]))

  

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-43-707Z.md
### Última modificação: 2025-05-28 20:03:44

# Análise Passo a Passo de um Teste A/B - TripleTen

Teoria

# Análise Passo a Passo de um Teste A/B

Em lições anteriores, você:

-   Avaliou visualmente resultados de um teste A/B e determinou que os dados geralmente contém valores atípicos
-   Encontrou os valores atípicos e determinou onde estabelecer os limites para identificá-los

Vamos descobrir a significância estatística de diferenças na taxa de conversão e volume do pedido entre os grupos, usando os dados "brutos" (ou seja, os dados antes da remoção das anomalias).

Vamos começar preparando os dados.

Vamos carregar os dados dos pedidos.

**Código**

```
import pandas as pd
import scipy.stats as stats
import datetime as dt
import numpy as np

orders = pd.read_csv('/datasets/orders_for_detecting_anomalies.csv', sep=',')
orders['date'] = orders['date'].map(
    lambda x: dt.datetime.strptime(x, '%d/%m/%Y')
)

print(orders.head(5))
```

**Resultado**

```
    orderId group                userId  revenue       date
0        1     A   4765212904118882304     7503 2019-04-01
1        2     A  14477413223818084352    11424 2019-04-01
2        3     B  10223965268965748736     1299 2019-04-01
3        4     A   1399771946645095424      999 2019-04-01
4        5     A  13269081434837460992      499 2019-04-01
```

Agora vamos carregar os dados sobre usuários.

**Código**

```
visitors = pd.read_csv(
    'lesson_6/orders_for_anomalies_detection_visitors.csv', sep=','
)
visitors['date'] = visitors['date'].map(
    lambda x: dt.datetime.strptime(x, '%d/%m/%Y')
)

print(visitors.head(5))
```

**Resultado**

```
        date group  visitors
0 2019-04-01     A       455
1 2019-04-02     A       501
2 2019-04-03     A      1313
3 2019-04-04     A       555
4 2019-04-05     A       564
```

Vamos calcular a significância estatística da diferença de conversão entre os grupos.

Vamos criar as variáveis `ordersByUsersA` e `ordersByUsersB` para armazenar as colunas `['userId', 'orders']`. Para usuários com pelo menos um pedido, vamos indicar a quantidade de pedidos realizados.

```
ordersByUsersA = orders[orders['group']=='A'].groupby('userId', as_index=False).agg({'orderId' : pd.Series.nunique})
ordersByUsersA.columns = ['userId', 'orders']

ordersByUsersB = orders[orders['group']=='B'].groupby('userId', as_index=False).agg({'orderId' : pd.Series.nunique})
ordersByUsersB.columns = ['userId', 'orders']
```

Agora vamos declarar as variáveis `sampleA` e `sampleB`, com usuários de grupos diferentes e quantidades de pedidos correspondentes. Aqueles que não fizeram nenhum pedido terão 0. Isso é necessário para preparar as amostras para o teste Mann-Whitney.

`sampleA` deve conter duas partes:

1.  Uma lista da quantidade de pedidos por cada usuário: `ordersByUsersA['orders']`.
2.  Zeros para usuários que não realizaram nenhum pedido. Seu número é a diferença entre a quantidade total de visitantes e a quantidade de registros de pedidos: `visitors[visitors['group']=='A']['visitors'].sum() - len(ordersByUsersA['orders'])`
    
    Vamos criar um objeto `pd.Series` com o comprimento requisitado:
    
    ```
     pd.Series(0, index=np.arange(visitors[visitors['group']=='A']['visitors'].sum() - len(ordersByUsersA['orders'])), name='orders')
     
    ```
    
    Usamos a função **np.arange()** para criar uma lista de índices. Ela funciona como a função `range()`, mas cria um vetor de índices no formato `np.array` requerido por `pd.Series`.
    

Na biblioteca pandas, sequências são concatenadas com **pd.concat()**. Primeiro precisamos passar para ela o que precisa ser concatenado. Em nosso caso, é a lista da primeira e da segunda partes:

```
[ordersByUsersA['orders'],pd.Series(0, index=np.arange(visitors[visitors['group']=='A']['visitors'].sum() - len(ordersByUsersA['orders'])), name='orders')]
```

Então iremos passar o argumento que especifica que objetos Series devem ser concatenados por linha (em outras palavras, devem ser escritos em sequência): `pd.concat([...], axis=0)`. Aqui está o que obtemos após fazer o mesmo com `sampleB`:

```
sampleA = pd.concat([ordersByUsersA['orders'],pd.Series(0, index=np.arange(visitors[visitors['group']=='A']['visitors'].sum() - len(ordersByUsersA['orders'])), name='orders')],axis=0)

sampleB = pd.concat([ordersByUsersB['orders'],pd.Series(0, index=np.arange(visitors[visitors['group']=='B']['visitors'].sum() - len(ordersByUsersB['orders'])), name='orders')],axis=0)
```

Vamos aplicar o critério e formatar o valor-p arredondando-o pra três casas decimais.

Não se esqueça de que `sampleA` armazena uma amostra em que cada elemento é a quantidade de pedidos realizada por um usuário específico, incluindo os zeros. Então a quantidade de elementos `sampleA` é a quantidade de usuários, e a soma de todos elementos é a quantidade de pedidos. Para obter a taxa de conversão do grupo, vamos precisar dividir a soma dos pedidos pela quantidade de pedidos. Usando o método `mean()` vamos descobrir a média para `sampleA`. Vamos descobrir a taxa de conversão para o grupo B de modo similar: `sampleB.mean()`.

Vamos imprimir o ganho de conversão relativo para o grupo B: `group B conversion / group A conversion - 1`. Vamos arredondar o valor para três casas decimais.

```
ordersByUsersA = orders[orders['group']=='A'].groupby('userId', as_index=False).agg({'orderId' : pd.Series.nunique})
ordersByUsersA.columns = ['userId', 'orders']

ordersByUsersB = orders[orders['group']=='B'].groupby('userId', as_index=False).agg({'orderId' : pd.Series.nunique})
ordersByUsersB.columns = ['userId', 'orders']

sampleA = pd.concat([ordersByUsersA['orders'],pd.Series(0, index=np.arange(visitors[visitors['group']=='A']['visitors'].sum() - len(ordersByUsersA['orders'])), name='orders')],axis=0)

sampleB = pd.concat([ordersByUsersB['orders'],pd.Series(0, index=np.arange(visitors[visitors['group']=='B']['visitors'].sum() - len(ordersByUsersB['orders'])), name='orders')],axis=0)

print("{0:.3f}".format(stats.mannwhitneyu(sampleA, sampleB)[1]))

print("{0:.3f}".format(sampleB.mean()/sampleA.mean()-1))
```

```
0.441
-0.102
```

Conclusão: com base nos dados brutos, não existe diferença entre os grupos A e B.

A primeira linha do resultado nos dá o valor-p, 0.441, que é maior que 0.05. Então, não podemos rejeitar a hipótese nula de que não há diferença estatística significativa na conversão entre os grupos. Mas o prejuízo relativo do grupo B é 10.2% (a segunda linha do resultado).

Para calcular a significância estatística da diferença no volume médio de pedidos do segmento, vamos passar os dados sobre receita para o critério `mannwhitneyu()`.

Também vamos descobrir a diferença relativa em volume médio de pedidos entre os grupos:

```
print('{0:.3f}'.format(stats.mannwhitneyu(orders[orders['group']=='A']['revenue'], orders[orders['group']=='B']['revenue'])[1]))
print('{0:.3f}'.format(orders[orders['group']=='B']['revenue'].mean()/orders[orders['group']=='A']['revenue'].mean()-1))
```

```
0.260
-0.185
```

O valor-p é consideravelmente maior do que 0.05, então não há motivo para rejeitar a hipótese nula e concluir que o volume médio de pedidos seria diferente entre os grupos. Apesar disso, o volume médio de pedidos do grupo B é muito maior do que o do grupo A.

Lembre-se de que o 95º e 99º percentis para volume médio de pedidos, que você encontrou nas lições anteriores, eram $13,817 e $105,038. O 95º e 99º percentis para a quantidade de pedidos por usuário foram 4.85 e 8.34 pedidos.

Vamos definir como usuários anômalos aqueles que realizam mais do que quatro pedidos ou um pedido maior do que $20,000. Para isso vamos remover 5% dos usuários com as maiores quantidades de pedidos e de 1% a 5% dos usuários com os pedidos mais caros. Vamos fazer fatias de dados com os usuários que realizaram mais do que quatro pedidos (`usersWithManyOrders`) e com os usuários que realizaram pedidos maiores do que $20,000 (`usersWithExpensiveOrders`). Vamos juntá-los em uma tabela chamada `abnormalUsers`.

Vamos descobrir a quantidade total de usuários anômalos usando o método `shape()`.

```
usersWithManyOrders = pd.concat([ordersByUsersA[ordersByUsersA['orders'] > 4]['userId'], ordersByUsersB[ordersByUsersB['orders'] > 4]['userId']], axis = 0)
usersWithExpensiveOrders = orders[orders['revenue'] > 20000]['userId']
abnormalUsers = pd.concat([usersWithManyOrders, usersWithExpensiveOrders], axis = 0).drop_duplicates().sort_values()
print(abnormalUsers.head(5))
print(abnormalUsers.shape)
```

```
286     699218989329350656
8      1072811847256720512
7      1225733206606480128
248    1899214245961511936
15     2181679483747416832
Name: userId, dtype: uint64
(14,)
```

Temos 14 usuários anômalos no total.

Vamos tentar descobrir como suas ações afetaram os resultados dos testes. Vamos calcular a significância estatística das diferenças em conversão entre os grupos, usando dados filtrados. Primeiro vamos preparar amostras da quantidade total de pedidos para cada grupo de teste:

```
sampleAFiltered = pd.concat([ordersByUsersA[np.logical_not(ordersByUsersA['userId'].isin(abnormalUsers))]['orders'],pd.Series(0, index=np.arange(visitors[visitors['group']=='A']['visitors'].sum() - len(ordersByUsersA['orders'])),name='orders')],axis=0)

sampleBFiltered = pd.concat([ordersByUsersB[np.logical_not(ordersByUsersB['userId'].isin(abnormalUsers))]['orders'],pd.Series(0, index=np.arange(visitors[visitors['group']=='B']['visitors'].sum() - len(ordersByUsersB['orders'])),name='orders')],axis=0)
```

Vamos aplicar o critério estatístico Mann-Whitney às amostras resultantes:

```
print("{0:.3f}".format(stats.mannwhitneyu(sampleAFiltered, sampleBFiltered)[1]))
print("{0:.3f}".format(sampleBFiltered.mean()/sampleAFiltered.mean()-1))
```

```
0.446
-0.105
```

Os resultados relativos à conversão quase não mudaram. Vamos ver o que acontece com os valores para volume médio de pedidos:

```
print('{0:.3f}'.format(stats.mannwhitneyu(
    orders[np.logical_and(
        orders['group']=='A',
        np.logical_not(orders['userId'].isin(abnormalUsers)))]['revenue'],
    orders[np.logical_and(
        orders['group']=='B',
        np.logical_not(orders['userId'].isin(abnormalUsers)))]['revenue'])[1]))

print('{0:.3f}'.format(
    orders[np.logical_and(orders['group']=='B',np.logical_not(orders['userId'].isin(abnormalUsers)))]['revenue'].mean()/
    orders[np.logical_and(
        orders['group']=='A',
        np.logical_not(orders['userId'].isin(abnormalUsers)))]['revenue'].mean() - 1))
```

```
0.384
-0.062
```

O valor-p aumentou, mas agora a diferença entre os segmentos é 6%, ao invés de 18%.

Apesar de nossas conclusões sobre o teste não se alterarem, este exemplo demonstra claramente como anomalias podem afetar resultados de testes A/B.

Que conclusões podemos tirar a partir desse teste?

Aqui estão os fatos:

-   Nem os dados brutos e nem os filtrados revelaram quaisquer diferenças estatísticas significativas de conversão entre os grupos.
-   Nem os dados brutos e nem os filtrados revelaram quaisquer diferenças estatísticas significativas no volume médio de pedidos entre os grupos.
-   O gráfico mostrando a diferença em conversão entre os grupos nos informa que os resultados do grupo B são piores e não parecem estar melhorando significativamente.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_18_1587743586.png)

-   O gráfico mostrando a diferença no volume médio de pedidos entre os grupos nos informa que os resultados do grupo B estão ficando piores a cada dia, e atualmente estão 20% menores do que os do grupo A:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_19_1587743610.png)

Com base nesses fatos, podemos concluir que o teste não está sendo bem sucedido e deveria ser interrompido. Não há razão para dar continuidade a ele, já que a probabilidade de que aquele segmento B se torne melhor do que o segmento A é quase inexistente.

Análise Passo a Passo de um Teste A/B

Tarefa5 / 5

1.

Estude a estrutura da tabela `data`. Tomando como modelo o código amostral da lição, crie as variáveis `ordersByUsersA` e `ordersByUsersB` com as colunas `['userId', 'orders']`, nas quais a quantidade de pedidos por usuários com pelo menos um pedido será especificada.

Declare as variáveis `sampleA` e `sampleB`, com usuários que realizaram pedidos e a quantidade de pedidos correspondente. Usuários sem pedidos terão 0. Faça referência ao código amostral da lição.

Calcule a significância estatística da diferença de conversão com base nos resultados obtidos após duas semanas de testes. Aplique o teste Mann-Whitney.

Imprima o valor-p para comparar a conversão dos grupos. Arredonde-o para cinco casas decimais.

Calcule e imprima a diferença relativa de conversão entre os grupos. Arredonde o resultado para três casas decimais.

2.

Calcule a significância estatística da diferença do volume médio de pedidos entre os grupos.

Imprima o valor-p para comparar os volumes médios de pedidos dos grupos. Arredonde-o para três casas decimais.

Imprima o ganho relativo para o grupo B, arredondando o valor mais uma vez para três casas decimais.

3.

Agora vamos remover as anomalias de nossos dados.

Lembre-se de que o 95º e 99º percentis para o volume médio de pedidos foram $7,729 e $43,536. Para quantidade de usuários, o 95º e 99º percentis foram dois e três pedidos.

Vamos considerar como usuários anômalos aqueles que realizaram três ou mais pedidos, ou que fizeram um com valor superior a $10,000. Para isso, você deverá remover 1% dos usuários com a maior quantidade de pedidos e entre 1% e 5% dos pedidos mais caros.

São os usuários que você irá remover, mas antes você deverá identificar os pedidos anômalos. Depois você irá adicionar os usuários que realizaram esses pedidos à lista de anomalias.

Faça fatias de dados para usuários com três ou mais pedidos (`usersWithManyOrders`) e usuários com pedidos com valores superiores a $10,000 (`usersWithExpensiveOrders`). Junte-os na tabela `abnormalUsers`, remova os valores duplicados e ordene os dados em ordem crescente.

Imprima as cinco primeiras linhas de `abnormalUsers`.

4.

Remova os visitantes anômalos e calcule a significância estatística da diferença de conversão.

Crie as variáveis `sampleAFiltered` e `sampleBFiltered` para armazenar os dados após as anomalias terem sido removidas pela filtragem.

Imprima o valor-p para comparar as taxas de conversão dos grupos filtrados. Arredonde-o para cinco casas decimais. Imprima o ganho relativo do grupo B e arredonde-o para três casas decimais.

5.

Remova as anomalias e calcule a significância estatística para a diferença no volume médio de pedidos.

Imprima o valor-p para comparar os volumes médios de pedidos dos grupos filtrados. Arredonde-o para três casas decimais. Imprima o ganho relativo do grupo B e arredonde-o para três casas decimais.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

import pandas as pd

import scipy.stats as stats

import datetime as dt

import numpy as np

  

visitors \= pd.read\_csv('/datasets/data\_for\_tasks\_3\_visitors.csv')

visitors\['date'\] \= visitors\['date'\].map(

lambda x: dt.datetime.strptime(x, '%d/%m/%Y')

)

  

orders \= pd.read\_csv('/datasets/data\_for\_tasks\_3.csv')

orders\['date'\] \= orders\['date'\].map(

lambda x: dt.datetime.strptime(x, '%d/%m/%Y')

)

  

ordersByUsersA \= (

orders\[orders\['group'\] \== 'A'\]

.groupby('userId', as\_index\=False)

.agg({'orderId': pd.Series.nunique})

)

ordersByUsersA.columns \= \['userId', 'orders'\]

  

ordersByUsersB \= (

orders\[orders\['group'\] \== 'B'\]

.groupby('userId', as\_index\=False)

.agg({'orderId': pd.Series.nunique})

)

ordersByUsersB.columns \= \['userId', 'orders'\]

  

usersWithManyOrders \= pd.concat(

\[

ordersByUsersA\[ordersByUsersA\['orders'\] \> 2\]\['userId'\],

ordersByUsersB\[ordersByUsersB\['orders'\] \> 2\]\['userId'\],

\],

axis\=0,

)

usersWithExpensiveOrders \= orders\[orders\['revenue'\] \> 10000\]\['userId'\]

abnormalUsers \= (

pd.concat(\[usersWithManyOrders, usersWithExpensiveOrders\], axis\=0)

.drop\_duplicates()

.sort\_values()

)

print(abnormalUsers.head(5))

print('{0:.3f}'.format(stats.mannwhitneyu(

orders\[np.logical\_and(

orders\['group'\]\=='A',

np.logical\_not(orders\['userId'\].isin(abnormalUsers)))\]\['revenue'\],

orders\[np.logical\_and(

orders\['group'\]\=='B',

np.logical\_not(orders\['userId'\].isin(abnormalUsers)))\]\['revenue'\])\[1\]))

  

print('{0:.3f}'.format(

orders\[np.logical\_and(orders\['group'\]\=='B',np.logical\_not(orders\['userId'\].isin(abnormalUsers)))\]\['revenue'\].mean()/

orders\[np.logical\_and(

orders\['group'\]\=='A',

np.logical\_not(orders\['userId'\].isin(abnormalUsers)))\]\['revenue'\].mean() \- 1))

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-46-481Z.md
### Última modificação: 2025-05-28 20:03:46

# As Tão Esperadas Conclusões do Teste A/B - TripleTen

Capítulo 6/9

Analisando os Resultados de um Teste A/B

# As Tão Esperadas Conclusões do Teste A/B

Você fez um trabalho duro neste capítulo. Você acabou de aprender que:

-   Existe uma diferença estatística significativa na conversão entre os grupos, tanto de acordo com os dados brutos como com os filtrados.
-   Os dados brutos não apresentaram uma diferença estatística significativa entre os grupos, em termos de volume médio de compra. Porém, após as anomalias terem sido removidas, na verdade havia uma diferença estatística significativa.
-   O gráfico da diferença de conversão entre os grupos evidencia que os resultados do grupo B são melhores que aqueles no grupo A: eles possuem uma tendência ao crescimento ou a se estabilizarem em torno da média.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.6.8.2PT.png)

-   O gráfico da diferença no volume médio de compra mostra flutuações: foi esse gráfico que permitiu que você detectasse as anomalias. Você não pode chegar a nenhuma conclusão definitiva a partir desse gráfico.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.6.8.3PT.png)

Pergunta

Com base nos fatos listados acima, o que deveríamos fazer com o teste?

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/PT/moved_8.6.8PT.png)

Continuar testando

Interrompa o teste, considere-o um sucesso e passe para sua próxima hipótese.

É isso aí. Parabéns! Você terminou sua primeira análise de um teste A/B, e ela foi um sucesso.

Que teste? O que aconteceu com ele?

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-47-782Z.md
### Última modificação: 2025-05-28 20:03:48

# Erros Comuns na Análise do Teste A/B - TripleTen

Capítulo 6/9

Analisando os Resultados de um Teste A/B

# Erros Comuns na Análise do Teste A/B

### Dividindo incorretamente o tráfego do teste

Esse é um problema comum. Por exemplo, usuários são divididos em segmentos de modo incorreto.

Por exemplo, você não pode considerar a divisão do tráfego correta caso o grupo A utilize a versão móvel de um site e o grupo B a versão para desktop.

Trabalhar com grupos de tamanhos diferentes também distorce os resultados. Se os tamanhos forem 49 e 51, ao invés de 50 e 50, a diferença relativa na quantidade de usuários será `49/51-1=0.0392`, que é quase 4%. Se, por acaso, devido a isso, você determinar que a receita caiu em 4%, você provavelmente irá interromper o teste e abandonar sua hipótese.

### Ignorando a significância estatística

Decisões sobre as diferenças nos resultados dos testes são geralmente tomadas exclusivamente com base em mudanças relativas.

"Um segmento é 5% melhor do que outro"—é realmente melhor, ou trata-se apenas de uma flutuação estatística? A longo prazo, todas as decisões erradas resultam em prejuízos ou em diminuição de receita para o negócio.

### O problema com os picos

Nós já discutimos isso. Faça um esforço para evitar isso, e também tente proteger os outros: muitas vezes solicitam que analistas elaborem decisões baseadas em resultados provisórios. Não ceda à pressão!

### A amostra é muito pequena.

Eles vêm e pedem para você conduzir um teste A/B em uma amostra de 10 ou 20 usuários. Você poderia aceitar, mas os resultados que você irá obter não serão confiáveis. O impacto de cada observação individual será muito forte, então não haverá significância e nem precisão.

### O teste foi muito curto

Você calculou o tamanho requisitado da amostra usando uma calculadora. Então você inicia o teste, obtém sua amostra e toma uma decisão. Como você deve ter compreendido após este capítulo, os resultados dos testes podem variar muito nas condições da vida real.

Tome decisões apenas quando você tiver certeza de seus resultados.

### O teste foi muito longo

Você também não deve ir para o outro extremo. Às vezes, os resultados ainda não se estabilizaram, eles flutuam, mas continuar o experimento não irá produzir nenhum tipo de efeito em sua decisão.

Por exemplo, a significância estatística não foi atingida, e os resultados do grupo B estão flutuando, mas é evidente que eles são menores que os resultados do grupo A. Então não há motivo para prosseguir com o teste: o grupo B não terá como assumir a liderança, e não haverá aumento na receita.

### Cometendo falhas na análise de anomalias

Nunca se esqueça das anomalias, e sempre as leve em consideração ao analisar os resultados.

Dados da vida real contém muito mais anomalias do que os dados deste capítulo. Com o passar do tempo e com mais experiência, você irá se tornar um mestre na caça às anomalias.

### Negligenciando a correção da significância estatística com comparações múltiplas

Quanto mais grupos você tem em seu teste, mais frequentemente você irá obter um resultado positivo falso para pelo menos uma comparação.

Pergunta

Como você testa se o tráfego foi bem dividido em grupos para um teste A/B?

Com uma calculadora de tamanho de amostra

Com testes AA

Correto! Testes A/A ajudam a avaliar se os grupos foram bem divididos, determinar suas proporções e confirmar que o tráfego é homogêneo, o que quer dizer que os grupos trabalham com versões absolutamente idênticas do produto e compartilham as mesmas métricas chave.

Com a correção Bonferroni

Excluindo anomalias

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-49-101Z.md
### Última modificação: 2025-05-28 20:03:49

# Conclusão - TripleTen

Capítulo 6/9

Analisando os Resultados de um Teste A/B

# Conclusão

Parabéns! Você analisou seu primeiro teste A/B e aprendeu sobre os erros mais comuns.

### O que você aprendeu:

-   Como aplicar os métodos de análise estatística para a análise de um teste A/B
-   Como analisar os gráficos de métricas diárias e cumulativas para testes A/B
-   Como analisar valores atípicos da amostra
-   Como analisar gráficos e anomalias
-   Como aplicar critérios estatísticos

### Leve isso com você:

Baixe o [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_8/moved_Resumo_do_Captulo_Analisando_os_Resultados_de_Testes_AB_pt.pdf) para poder consultá-lo quando necessário.

### Leitura (Independente) para casa

O livro de Steven Strogatz's _The Joy of x: A Guided Tour of Math from One to Infinity_ possui um excelente capítulo sobre estatística—"The New Normal." Nós recomendamos.

E agora quanto ao projeto independente.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-51-090Z.md
### Última modificação: 2025-05-28 20:03:51

# Fatos, Emoções e Avaliações - TripleTen

Capítulo 7/9

Soft Skills

# Fatos, Emoções e Avaliações

Escute! Alguém está gritando: "A plataforma estúpida está travada, isso me deixa com tanta raiva." Aqui temos fatos, emoções e avaliações.

"A plataforma está travada" é um fato.

"Isso me deixa com tanta raiva" é uma emoção.

"A plataforma estúpida" é uma avaliação.

Como analista iniciante, você precisa saber as diferenças entre essas coisas, porque:

1.  Os verdadeiros especialistas sabem controlar suas emoções e são muito cuidadosos ao fazer avaliações. Eles preferem basear suas ações em fatos.
2.  As pessoas tomam decisões que são afetadas por emoções e avaliações — suas e de outras pessoas. Isso tem um efeito direto nos dados que você obtém (por exemplo, aumenta ou diminui o número de pedidos). Um bom analista, como um detetive, explica isso em hipóteses.

Um **fato** descreve a situação como ela realmente é. Olhando para a sua tela, podemos dizer que a plataforma está realmente travada.

Uma **emoção** é uma reação subjetiva. Um evento (inclusive um pensamento) pode causar várias reações. Alguns alunos podem ficar com raiva porque o problema os impede de trabalhar, enquanto outros podem ficar felizes, pois podem fazer uma pausa.

Uma **avaliação** é sua própria posição, uma opinião carregada de emoção sobre um evento, objeto ou pessoa. Esta não é uma "opinião de especialista". As avaliações são distintas dos fatos. A plataforma online não tem cérebro, então não pode ser realmente estúpida. É apenas um conjunto de algoritmos que não funcionaram.

Pergunta

Selecione todos os exemplos que envolvem avaliações e fatos, mas não emoções:

Escolha quantas quiser

As últimas três horas de trabalho no projeto foram exaustivas.

Correto. "Três horas" é um fato, "exaustivo" é uma avaliação.

Uma nova tarefa surgiu do nada.

Correto. "Uma nova tarefa" é um fato, enquanto "saiu do nada" é uma percepção pessoal, uma avaliação do evento.

A satisfação com nosso serviço caiu 15% no mês passado. Isso é totalmente inaceitável!

Correto. "Caiu 15% no mês passado" é um fato, "totalmente inaceitável" é uma avaliação.

Estou surpreso de que ainda estamos recebendo novos clientes: eu não escolheria este serviço se fosse eles!

O líder da minha equipe sempre diz que faço muita bagunça. Eu não aguento mais isso!

Você conseguiu!

Em suma, ao formular seus pensamentos:

-   Use fatos acima de tudo.
-   Evite emoções quando pedirem para você uma opinião especializada.
-   Seja específico ao fazer suas avaliações e julgamentos.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-52-403Z.md
### Última modificação: 2025-05-28 20:03:52

# Expressar Claramente sua Posição - TripleTen

Capítulo 7/9

Soft Skills

# Expressar Claramente sua Posição

Pergunta

Maria trabalha como analista para uma empresa de TI há três semanas. Recentemente, a gestão da empresa mudou, e foi solicitado que Maria calculasse a taxa de retenção do serviço de rede de contato. Ela ficou muito feliz, pois isso é fácil. Ela recebe a resposta rapidamente e envia um e-mail para seu chefe: "A taxa de retenção é de 56%".

Qual você acha que foi o primeiro pensamento do chefe?

Ótimo, mais de 50% é um bom sinal.

Isso é bom ou ruim?

Exatamente. O chefe esperava que Maria apresentasse não apenas a taxa, mas também conclusões e hipóteses sobre o que essa taxa pode implicar.

Abaixo de 90%? Temos que fazer alguma coisa!

Que resposta clara e concisa a Maria deu! Ela definitivamente é uma boa analista.

Trabalho maravilhoso!

Os analistas são convidados a comentar sobre uma grande variedade de questões. Seus argumentos podem ser aceitos ou rejeitados com base em quão bem você os formula e quão logicamente os apresenta.

No trabalho, você geralmente vai interagir com colegas analistas e clientes internos.

Você pode discutir muitas coisas com seus colegas, expressar preocupações, compartilhar tarefas complexas, buscar soluções e pedir feedback sobre seu código. Para tornar essas discussões mais construtivas, você deve:

-   Perguntar ao seu colega se ele pode reservar algum tempo para você
-   Expressar claramente o objetivo da discussão
-   Conversar sobre os detalhes, começando com os fatos
-   Ser claro ao expressar opiniões e avaliações pessoais

Os clientes internos geralmente são chefes ou colegas de outros departamentos. Eles podem não conhecer todas as sutilezas da análise. Eles estão interessados em obter resultados e fazer planos com base nisso; os detalhes técnicos não são importantes para eles. Então aqui você deve:

-   Reler o pedido inicial que você recebeu do cliente.
-   Preparar os resultados com base nesta requisição; comece com os números-chave e, em seguida, faça breves conclusões, onde
    
    a) se você conhece os valores anteriores de uma métrica, deve mostrar como ela mudou ao longo do tempo.
    
    b) se possível, informar se as conclusões correspondem com à situação geral do negócio.
    
-   Durante a apresentação, deve dar mais atenção às suas conclusões do que às ferramentas e métodos que utilizou.
-   Não dar instruções diretas em suas conclusões (pelo menos enquanto você for um analista júnior); esse não é o seu lugar. Dê recomendações em vez disso.
-   Manter a calma e ficar zen se sua lógica for desafiada durante a discussão. Os profissionais de verdade estão sempre prontos para descobrir porque suas conclusões estão sendo questionadas. Eles confirmam mais uma vez que estão certos ou encontram um espaço para melhorar.

Pergunta

Maria estuda os dados dos últimos dois meses e escreve outra mensagem para seu chefe (o cliente). O que ela deveria escrever?

A taxa de retenção é de 56% este mês e descobri o que está errado. No mês passado, tivemos problemas técnicos com nossa plataforma e muitos usuários saíram por causa disso. Precisamos consertar tudo e reconquistá-los.

Perguntei aos meus colegas sobre o que aconteceu com a plataforma e fiquei sabendo que alguns usuários ficaram descontentes depois que mudamos para os novos servidores. Eles ainda estão enviando e-mails para o suporte e pedindo seu dinheiro de volta. A taxa de retenção diminuiu 2%; estamos perdendo usuários. O mais importante agora é tornar a plataforma estável.

Este mês a taxa de retenção é de 56%; no mês passado, foi de 58%. A métrica está diminuindo. A mudança para os novos servidores causou problemas técnicos. Esses 2% dos usuários provavelmente saíram por causa disso. Se resolvermos esse problema, eles podem voltar.

Sim, esta é a melhor opção. Vemos os números claros e como eles mudam, bem como os fatos sobre questões técnicas. Há uma hipótese livre de qualquer tom diretivo. Isso é o que se espera de um analista júnior.

A taxa de retenção diminuiu 2%; que está dentro dos limites da flutuação regular. Nenhum problema foi encontrado.

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-53-698Z.md
### Última modificação: 2025-05-28 20:03:54

# Sprint 9 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 8/9

Projeto do Curso

# Sprint 9 - Projeto

Parabéns! Você concluiu "Tomando Decisões de Negócios Baseados em Dados". É hora de aplicar o conhecimento e as habilidades que você adquiriu em um projeto: um estudo de caso analítico da vida real que você concluirá por conta própria.

Quando você terminar o projeto, envie seu trabalho para o revisor do projeto para avaliação. Eles te darão feedback dentro de 48 horas. Use o feedback para fazer alterações e, em seguida, envie a nova versão de volta ao revisor do projeto.

Você pode obter mais feedback sobre a nova versão. Isso é completamente normal. Não é incomum passar por vários ciclos de feedback e revisão.

Seu projeto será considerado concluído assim que o revisor do projeto o aprovar.

## Descrição do Projeto

### Contexto

Você é analista em uma grande loja online. Junto com o departamento de marketing, você compilou uma lista de hipóteses que podem ajudar a aumentar a receita.

Você precisa priorizar essas hipóteses, lançar um teste A/B e analisar os resultados.

## Descrição dos dados

**Dados usados na primeira parte do projeto**

`/datasets/hypotheses_us.csv` [Baixe o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/hypotheses_us.csv)

-   `Hypotheses` — breves descrições das hipóteses
-   `Reach` — alcance do usuário, em uma escala de um a dez
-   `Impact` — impacto nos usuários, em uma escala de um a dez
-   `Confidence` — confiança na hipótese, em uma escala de um a dez
-   `Effort` — os recursos necessários para testar uma hipótese, em uma escala de um a dez. Quanto maior o valor de `Effort`, mais recursos são necessários para o teste.

**Dados usados na segunda parte do projeto**

`/datasets/orders_us.csv` [Baixe o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/orders_us.csv)

-   `transactionId` — identificador do pedido
-   `visitorId` — identificador do usuário que fez o pedido
-   `date` — do pedido
-   `revenue` — do pedido
-   `group` — o grupo de teste A/B ao qual o usuário pertence

`/datasets/visits_us.csv` [Baixe o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/visits_us.csv)

-   `date` — data
-   `group` — grupo de teste A/B
-   `visits` — o número de visitas na data especificada para o grupo de teste A/B especificado

Certifique-se de pré-processar os dados. Pode haver erros nos conjuntos de dados originais; por exemplo, alguns dos visitantes podem ter entrado no grupo A e no grupo B.

### Parte 1. Priorizando Hipóteses

O arquivo `hypotheses_us.csv` contém nove hipóteses para aumentar a receita de uma loja online com `Reach`, `Impact`, `Confidence` e `Effort` especificados para cada um.

A tarefa é:

-   Aplicar o framework `ICE` para priorizar hipóteses. Classifique-os em ordem decrescente de prioridade.
-   Aplicar o framework `RICE` para priorizar hipóteses. Classifique-os em ordem decrescente de prioridade.
-   Mostre como a priorização de hipóteses muda quando você usa `RICE` em vez de `ICE`. Dê uma explicação para as alterações.

### Parte 2. Análise de teste A/B

Você realizou um teste A/B e obteve os resultados descritos nos arquivos `orders_us.csv` e `visitors_us.csv`.

**Tarefa**

Analise o teste A/B:

1.  Faça um gráfico da receita acumulada por grupo. Tire conclusões e crie conjecturas.
2.  Faça um gráfico do tamanho médio acumulado do pedido por grupo. Tire conclusões e crie conjecturas.
3.  Faça um gráfico da diferença relativa no tamanho médio acumulado do pedido para o grupo B em comparação com o grupo A. Faça conclusões e crie conjecturas.
4.  Calcule a taxa de conversão de cada grupo como a proporção de pedidos para o número de visitas para cada dia. Trace as taxas de conversão diárias dos dois grupos e descreva a diferença. Tire conclusões e crie conjecturas.
5.  Faça um gráfico da diferença relativa na conversão cumulativa para o grupo B em comparação com o grupo A. Tire conclusões e crie conjecturas.
6.  Calcule os percentis 95 e 99 para o número de pedidos por usuário. Defina o ponto em que um ponto de dados se torna uma anomalia.
7.  Faça um gráfico de dispersão dos preços dos pedidos. Tire conclusões e crie conjecturas.
8.  Calcule os percentis 95 e 99 dos preços dos pedidos. Defina o ponto em que um ponto de dados se torna uma anomalia.
9.  Encontre a significância estatística da diferença na conversão entre os grupos usando os dados brutos. Tire conclusões e crie conjecturas.
10.  Encontre a significância estatística da diferença no tamanho médio do pedido entre os grupos usando os dados brutos. Tire conclusões e crie conjecturas.
11.  Encontre a significância estatística da diferença na conversão entre os grupos usando os dados filtrados. Tire conclusões e crie conjecturas.
12.  Encontre a significância estatística da diferença no tamanho médio do pedido entre os grupos usando os dados filtrados. Tire conclusões e crie conjecturas.
13.  Tome uma decisão com base nos resultados do teste. As decisões possíveis são: 1. Pare o teste, considere um dos grupos o líder. 2. Pare o teste, conclua que não há diferença entre os grupos. 3. Continue o teste.

## Como meu projeto será avaliado?

Seu projeto será avaliado com base nos seguintes critérios. Leia-os cuidadosamente antes de iniciar o projeto.

Isso é o que os revisores do projeto procuram ao avaliar seu projeto:

-   Como você prepara dados para análise
-   Como você prioriza hipóteses
-   Como você interpreta os gráficos resultantes
-   Como você calcula a significância estatística
-   Quais conclusões você tira com base nos resultados do teste A/B
-   Se você segue a estrutura do projeto e mantém o código organizado
-   As conclusões que você faz
-   Se você deixa comentários a cada passo

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-536e3829-1f4a-44de-8d37-f7abc961c710.containerhub.tripleten-services.com/doc/tree/a35cdb86-d3a0-426a-ba86-bf36031ca1ac.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-03-59-594Z.md
### Última modificação: 2025-05-28 20:03:59

# Feedback do Sprint 9 - TripleTen

Capítulo 9/9

Conclusão

# Compartilhe sua opinião

**Olá!**

Você acabou de concluir um sprint e enviar seu projeto. Parabéns por alcançar este marco! 🚀

Agora é o momento perfeito para dar seu feedback sobre o sprint. Sua opinião vai nos ajudar a aprimorar ainda mais o processo de aprendizagem.

Este questionário curto vai levar apenas 2 minutos. Todas as respostas vão ser confidenciais: não vamos compartilhá-las de forma que identifique você.

Agradecemos por nos ajudar a evoluir com você!

![](https://practicum-content.s3.us-west-1.amazonaws.com/common/ok.svg)

Muito obrigado pelas suas respostas!

Já as encaminhamos para os departamentos correspondentes.

9 — 9

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-04-00-904Z.md
### Última modificação: 2025-05-28 20:04:01

# Conclusão - TripleTen

Capítulo 9/9

Conclusão

# Conclusão

Parabéns! Você terminou de tomar Decisões de Negócios com Base em Dados.

### Você aprendeu a:

-   Identificar métricas a serem melhoradas
-   Gerar hipóteses
-   Escolher como testá-los
-   Priorizar hipóteses
-   Analisar os resultados dos testes A/B, as limitações deste método e os erros mais comuns cometidos nos testes A/B
-   Aplicar a análise estatística aos resultados do teste A/B

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-04-02-191Z.md
### Última modificação: 2025-05-28 20:04:03

# Preparação de Carreira 60% - TripleTen

Capítulo 9/9

Conclusão

# Preparação de Carreira

### Olá! Que bom que você está aqui conosco.

Ficamos felizes com seu engajamento e disposição em atingir seus objetivos.

Completados 60% do curso, você ganha a oportunidade de aderir ao CPC - Curso de Preparação de Carreira.

O **CPC, Curso de Preparação de Carreira,** foi desenvolvido com muito empenho pensando em cada etapa da jornada de desenvolvimento de habilidades que te orientem e guiam até atingir seus objetivos de carreira.

O curso **irá guiá-lo desde a construção da estratégia de funil para entender área e sub áreas de atuação, até termos seu kit de empregabilidade desenvolvido nas melhores práticas para área tech.**

O programa tem duração de 2 **semanas** e aborda os seguintes temas:

### **Transição de carreira: passos a seguir**

### **Estratégia de funil**

### **A importância da sua rede de contatos**

### **Github e o seu portifólio;**

### **Como montar um currículo;**

### **Carta de apresentação**

### **Linkedin**

Em cada etapa você terá todo o conteúdo necessário para se sentir ainda mais confiante. **E nunca estará sozinho!**

Abaixo, você encontra um formulário para que a área de carreira, possa conhecer você melhor e entender seus objetivos.

Preencha-o caso queria de fato, adentrar ao curso de preparação de carreira.

[https://forms.gle/6r8DLkfLHsM31RqZ6](https://forms.gle/6r8DLkfLHsM31RqZ6)

Após o preenchimento, dentro de alguns dias você será adicionado ao canal referente ao curso no Discord.

O link para o curso e demais instruções estará neste canal.

Bons estudos!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-06-42-547Z.md
### Última modificação: 2025-05-28 20:06:43

# Introdução a Como Relatar uma História Usando Dados - TripleTen

Capítulo 1/6

Introdução a Como Relatar uma História Usando Dados

# Introdução a Como Relatar uma História Usando Dados

Olá, este é o capítulo sobre como relatar uma história usando dados!

Aqui você aprenderá os princípios básicos da construção de relatórios e da apresentação dos resultados de sua pesquisa. Você também construirá gráficos usando métodos das bibliotecas seaborn e plotly.

**O que você vai aprender**:

-   Os princípios básicos a seguir ao apresentar os resultados de sua pesquisa
-   Como escolher que tipo de gráfico usar para os dados específicos
-   As regras mais importantes para criar uma apresentação
-   Como usar as bibliotecas de visualização seaborn e plotly

O projeto para esta seção envolverá o estudo do mercado de restaurantes. Você usará Python para trabalhar com dados e preparar uma apresentação para investidores. Você pode encontrar uma pequena prévia [aqui](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/moved_Descrio_do_Projeto_PDF_port.pdf).

Neste sprint, vamos nos concentrar em:

![](https://practicum-content.s3.amazonaws.com/resources/Narrativa_de_Dados_1713356010.png)

![](https://practicum-content.s3.amazonaws.com/resources/Pre-processamento_de_Dados_1713356023.png)

### Quanto tempo isso vai levar?

Neste sprint são apresentadas bibliotecas totalmente novas, pode levar entre 30 e 50 horas para concluir este material. Se você sentir que está ficando para trás, sinta-se à vontade para entrar em contato com nossa Equipe de Orientação. Como sempre, estamos empenhados em ajudar você a cada passo do caminho.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-06-43-875Z.md
### Última modificação: 2025-05-28 20:06:44

# O Processo da Análise de Dados - TripleTen

Capítulo 1/6

Introdução a Como Relatar uma História Usando Dados

# O Processo da Análise de Dados

Vamos dar uma olhada no nosso pipeline de análise de dados e ver onde estamos:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_2.1.2PT_1657614412.png)

Este curso é sobre o passo 6: explicar por que e como sua solução resolve o problema do cliente. É crucial não só chegar a boas conclusões, mas comunicá-las de forma convincente.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-06-45-210Z.md
### Última modificação: 2025-05-28 20:06:45

# Introdução - TripleTen

Capítulo 2/6

Preparando Apresentações

# Introdução

Os analistas precisam ser mais que apenas matemáticos que dominam Python e SQL, eles também precisam saber contar histórias. Neste capítulo, você aprenderá a criar uma história a partir de dados e a melhor forma de apresentá-la.

### O que você vai aprender:

-   Como fazer apresentações claras
-   Como escolher gráficos que se adequam aos dados
-   Como construir relatórios no Jupyter Notebook
-   Como tomar decisões que consideram a sazonalidade

### Quanto tempo vai demorar?

10 lições, aproximadamente 5–15 minutos cada

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-06-46-610Z.md
### Última modificação: 2025-05-28 20:06:46

# Como Falar Sobre a Sua Pesquisa - TripleTen

Capítulo 2/6

Preparando Apresentações

# Como Falar Sobre a Sua Pesquisa

Os analistas de dados profissionais transformam os números e gráficos em histórias.

Aqui está um gráfico do número de solicitações de pesquisa por "Argentina" no Google ao longo do tempo (segundo o [Google Trends](https://trends.google.com/trends/explore?date=2022-06-01%202023-02-15&gprop=news&q=Argentina)). Elas quase quadruplicaram em dezembro de 2022.

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/Screenshot_2023-02-22_160850_1677071371.png)

Em dezembro de 2022, a seleção argentina de futebol estava jogando a Copa do Mundo de Futebol (e foi campeã em 18 de dezembro). O desempenho deles atraiu atenção de pessoas em todo o mundo, o que é evidente no gráfico.

A história por trás dos dados explica os números que vemos. Se os analistas são capazes de contar essa história, isso significa que eles têm uma compreensão profunda dos processos de negócios atuais. Também ajudará eles a preverem e apresentarem sugestões para o futuro.

_Por trás de toda boa história ficam os dados precisos e recursos visuais de qualidade. Lembre-se de que são os dados que conduzem a história, e não o contrário._

Ao tirar as conclusões com base em dados, não se limite a números brutos; veja o que está acontecendo ao redor deles.

Fontes para a construção da história:

-   Informação geral sobre a empresa e procedimentos de negócios

O que a sua companhia faz? Que tipo de produtos você oferece?

-   Métricas financeiras, analíticas e outras

Existem as métricas que afetam o índice que você calculou?

-   Fatores externos e internos que afetam a empresa
    
    Quais eventos dentro e fora da empresa podem ter afetado o resultado?
    

_Os dados e análises são apenas peças de um quebra-cabeça até você explicar o que eles significam._

Pergunta

Vá [https://trends.google.com/](https://trends.google.com/), digite `IRS` na barra de pesquisa, selecione "Histórico de consultas" e "Últimos 12 meses" e veja como o número de consultas mudou nos últimos dois anos. Descreva os valores de pico observados e apresente uma hipótese sobre eles.

Vemos picos nos últimos meses do ano porque a maioria das pessoas paga impostos nessa época.

Os valores de pico em abril têm a ver com a data de vencimento dos impostos.

Depois de enviar as inscrições, os estudantes ficam sempre no site da escola para ver se foram aceitos. Talvez algumas pessoas paguem impostos no final do ano; observamos valores altos também nessa época.

Não há um padrão claro; as pessoas pesquisam por "IRS" em momentos aleatórios ao longo do ano.

Os valores são baixos no verão porque as pessoas estão de férias.

Fantástico!

Pergunta

Agora vamos procurar informação sobre a Apple. Veja o histórico de consultas para "apple" em [https://trends.google.com](https://trends.google.com/) Lembre-se de definir "Últimos 12 meses" em vez de "Dia anterior". Sobre por que vemos um pico em setembro.

iPhones defeituosos causaram um grande escândalo nos noticiários da época.

A Apple teve uma mudança de gestão.

Houve uma apresentação de um novo produto nessa época.

Bingo! A Apple apresenta novos produtos todo mês de setembro, gerando um aumento do interesse.

Os iPhones ficaram mais baratos.

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-06-47-947Z.md
### Última modificação: 2025-05-28 20:06:48

# Para Quem Contar Sua História - TripleTen

Capítulo 2/6

Preparando Apresentações

# Para Quem Contar Sua História

Nesta lição, veremos as melhores práticas de como abordar a apresentação de suas descobertas. É importante saber quem as estudará. Por exemplo, o público pode ser:

-   Analistas

Neste caso, prepare um relatório detalhado com código, comentários sobre o código e um link para seus dados. É melhor fazer seu relatório usando o Jupyter Notebook. Esclareça o que exatamente seus colegas querem ver: os métodos que você usou para os cálculos ou resultados os explicando um pouco. Isso ajudará você a afinar seu relatório.

-   Gerentes de produto

Prepare uma apresentação detalhada do seu relatório em PowerPoint ou PDF. Certifique-se de incluir todos os principais resultados e conclusões, bem como suas próprias recomendações.

-   Alta Gerência

O relatório deve ter respostas completas para as perguntas exatas que seus gerentes fizeram. Por exemplo, seu CFO estará principalmente preocupado com as finanças.

Aqui está uma ótima regra que informa qual informação pode adicionar e qual pode apagar: **quanto mais longe o público estiver da análise de dados, mais simples deve ser o relatório**. Não dê ao CEO todos os detalhes do que você fez com os valores ausentes. Eles podem ficar felizes em saber como você faz bem o seu trabalho, mas no final do dia você estará apenas desperdiçando o tempo deles. Liberte-os para poderem se concentrar nas coisas importantes.

Pergunta

MyPeeps é uma grande rede social. Há um mês, a equipe de publicidade introduziu banners promocionais nos feeds de notícias dos usuários. O gerente de produto solicitou os dados de como os usuários interagem com os banners. E agora, o que você deveria fazer?

Pergunte para que eles precisam da informação, porque essa instrução de tarefa é muito vaga.

Bom trabalho! Você se salvou de um potencial refazer mais tarde.

Calcule o CTR e envie os resultados para o gerente de produto.

Vou enviar os cálculos que fiz no Jupyter Notebook e dizer como abri-lo.

Farei uma apresentação completa com todas as métricas do banner (sou bom nisso!).

Você conseguiu!

Pergunta

Seus colegas de equipe realizaram alguns cálculos enquanto estudavam os índices de cancelamento de usuários de um serviço de música. Mas eles não conseguem descobrir algumas anomalias nos dados, então eles pedem sua ajuda compartilhando alguns gráficos sobre a atividade da campanha publicitária. E agora, o que você deveria fazer?

De jeito nenhum, deixe-os descobrir por conta própria.

Vou enviar os cálculos que fiz no Jupyter Notebook, assim como os dados.

Vou enviar apenas os gráficos que acredito que ajudarão a responder suas perguntas.

Vou perguntar-lhes sobre suas hipóteses, falar dos gráficos e enviar tudo o que eles precisam.

Exatamente. É melhor perguntar primeiro para saber o que enviar.

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-06-49-291Z.md
### Última modificação: 2025-05-28 20:06:50

# Sazonalidade e Fatores Externos - TripleTen

Capítulo 2/6

Preparando Apresentações

# Sazonalidade e Fatores Externos

Imagine que você está trabalhando para um agregador de notícias. A principal fonte de receita da empresa são os anúncios incorporados nos artigos que publica. Recentemente, você notou um aumento na CTR de artigos com a tag "esportes". A investigação posterior revelou que os artigos esportivos mais populares tinham as tags "patinação artística", "Shibutani" e "Chen". Sua tarefa é descobrir porque os esportes estão recebendo mais atenção este mês. Você também precisa ver se vale a pena vender publicidade relacionada com os artigos com essas tags ou não.

Pergunta

O que você faz?

A patinação artística é popular atualmente, então vamos lá e vendemos a publicidade.

Vou descobrir quais eventos estão acontecendo ao redor do mundo, pois isso pode ser uma coisa sazonal.

É, ler as notícias e descobrir o que está acontecendo é uma ótima ideia.

Se os números forem altos, há algum potencial lá.

Vamos perguntar a Alex Trebek.

Você conseguiu!

O estudo aconteceu durante as Olimpíadas de Inverno, e as patinadoras americanas conquistaram medalhas de bronze. Precisamos saber se a tendência continuará assim que os jogos terminarem.

O exemplo mais simples de tendências sazonais é a forma como o interesse em determinados produtos aumenta e diminui dependendo da época do ano. Por exemplo, as pessoas tendem a comprar shorts quando está quente e papel de embrulho logo antes do Natal.

Aqui está o histórico de consultas para "papel de embrulho":

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_2_1588767325.png)

Os analistas usam dados históricos para verificar tendências sazonais. Se você vir picos acentuados nos dados, verifique se algo semelhante aconteceu antes. Você pode estudar históricos de consultas com os serviços similares [https://wordstat.yandex.com](https://wordstat.yandex.com/) (os materiais estão em inglês). Isso o ajudará a criar hipóteses sobre a sazonalidade que terão de ser testadas posteriormente.

Não são somente as vendas de mercadorias que dependem da estação; muitos serviços são necessários apenas em épocas específicas do ano (por exemplo, colocar pneus de inverno no carro).

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_3_1588767354.png)

Para ver se um determinado produto segue um padrão sazonal, veja se existem diferenças significativas no comportamento do consumidor em diferentes fases do ano. Depois de encontrar uma mudança em um mês específico que você acredite que sinaliza uma tendência sazonal, pare. Formule uma hipótese e verifique a significância estatística na diferença que você está observando.

Aqui está mais um exemplo, desta vez vindo do mundo do varejo. Precisamos ajudar o departamento de compras no planejamento. Você traçou a demanda de açúcar nos últimos meses:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_4_1588767427.png)

O que é esse aumento repentino em setembro? Todos correram para a loja para comprar açúcar depois de notícias sobre a escassez. O que um analista deveria fazer? Investigar:

-   Se há realmente uma crise na produção de açúcar ou não. Se for assim, podemos prever o comportamento do consumidor?
-   Por que as notícias saíram se realmente não há uma crise do açúcar.

Depois de responder a essas perguntas, poderemos ajudar melhor o departamento de compras a montar uma estratégia adequada.

O ambiente e suas mudanças são outro fator importante para suas descobertas.

Você não precisa necessariamente ler resumos de notícias regularmente, mas se encontrar uma anomalia causada por fatores externos, veja o que está acontecendo no mundo e informe seu gerente.

Leia o estudo do TripleTen [Y.Direct and the Four Seasons](https://yandex.com/adv/news/yandex-direct-and-the-four-seasons-how-demand-changes-over-the-course-of-the-year) (os materiais estão em inglês) e compartilhe quaisquer dúvidas que você tenha ou pontos que achou interessantes com os outros estudantes no chat.

Pergunta

Você está trabalhando para uma startup que fabrica minigeladeiras móveis. Descubra em qual mês a empresa deve anunciar seu lançamento. Faça alguma pesquisa em [https://wordstat.yandex.com](https://wordstat.yandex.com/) (os materiais estão em inglês) e tente resumir o que você encontrar.

Nosso produto está em demanda durante todo o ano!

O outono é a melhor época — novembro, por exemplo.

O verão, seja junho ou julho, é o melhor.

Exatamente. A demanda está no auge e os clientes estão procurando qualquer coisa que os ajude a se refrescar.

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-06-50-818Z.md
### Última modificação: 2025-05-28 20:06:51

# Vamos Comprar as Passagens de Avião - TripleTen

Capítulo 2/6

Preparando Apresentações

# Vamos Comprar as Passagens de Avião

Você está comprando as passagens de avião. Elas geralmente custam $450, mas existem duas maneiras diferentes de economizar:

1.  Usar um código promocional para obter $60 de desconto.
2.  Comprar as passagens com cartão de um banco parceiro e ganhar 15% de desconto.

Você não pode combinar essas duas ofertas. Rápido! Qual você escolhe?

Pergunta

Rápido! Qual você escolhe?

O código promocional! $60 de desconto é ótimo

O desconto do banco. Eu vou ter que pagar só 85% do preço total.

15% de $450 é $67,50, o que economiza $7,50 extras.

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-06-52-182Z.md
### Última modificação: 2025-05-28 20:06:52

# Valores Absolutos e Relativos - TripleTen

Teoria

# Valores Absolutos e Relativos

As pessoas geralmente preferem trabalhar com os valores absolutos: volume, tamanho, escala de um evento observado. Mas você não pode se guiar só por eles em seu trabalho.

Por exemplo, digamos que você esteja analisando como o número de pedidos incompletos de uma loja virtual muda por mês. Em setembro foram 2.600 pedidos, mas 5.000 em outubro. O número absoluto de pedidos incompletos quase dobrou! Parece que a loja tem grandes problemas.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.6PT.png)

Lembre-se de que um valor relativo é a razão de dois outros valores.

Vamos calcular um valor relativo: a proporção de pedidos incompletos entre o total de pedidos. Houve um total de 5.000 pedidos em setembro, enquanto em outubro teve 10.000. No primeiro caso, a porcentagem de pedidos incompletos é de `2600 / 5000 × 100% = 52%`; e no segundo, é `5000 / 10000 × 100% = 50%`.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.6.2PT.png)

As coisas não estão ótimas, mas parece que estão gradualmente indo na direção certa. Foram os valores relativos que nos permitiram ver isso.

Valores Absolutos e Relativos

Tarefa3 / 3

1.

O arquivo `/datasets/auto.csv` contém dados sobre as vendas de carros das concessionárias no segmento de tamanho médio (faixa de preço médio) no quarto trimestre. Leia a tabela e salve-a na variável `auto`. Calcule o total de vendas para cada marca de carro e armazene o resultado na coluna `total`. Imprima a tabela.

2.

Calcule e imprima o número total de veículos vendidos de tamanho médio.

3.

Sabemos que 53.158 veículos no segmento premium foram vendidos neste trimestre, enquanto o segmento "Caminhões e outros" registrou 44.067. Em seguida, calcule a proporção de cada segmento, dividindo o número de veículos em cada segmento por `total_segments`, e salve essas proporções nas variáveis `premium`, `trucks` e `midsize.` Crie um gráfico, nomeando suas colunas `Premium`, `Trucks and other`, `Midsize`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

import pandas as pd

import matplotlib.pyplot as plt

  

auto \= pd.read\_csv('/datasets/auto.csv')

  

auto\['total'\] \= auto\['october'\] + auto\['november'\] + auto\['december'\]

  

total\_segments \= auto\['total'\].sum() + 44067 + 53158

  

premium \= 53158 / total\_segments

trucks \= 44067 / total\_segments

midsize \= auto\['total'\].sum() / total\_segments

  

keys \= \['Premium', 'Trucks and other', 'Midsize'\]

vals \= \[premium, trucks, midsize\]

  

plt.bar(keys, vals)

plt.show()

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-06-53-493Z.md
### Última modificação: 2025-05-28 20:06:53

# Paradoxo de Simpson - TripleTen

Capítulo 2/6

Preparando Apresentações

# Paradoxo de Simpson

Às vezes, você pode fazer tudo certo, mas obter os resultados **contra-intuitivos**, que parecem desafiar o senso comum. Isso não significa que é a hora de pegar seu ursinho de pelúcia e chorar! Simplesmente defina o fenômeno como um paradoxo e chame ele como tal.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.7PT.png)

Um exemplo de fenômeno contra-intuitivo é o **paradoxo de Simpson** (também chamado de **efeito Yule-Simpson** ou **paradoxo da amálgama**). Um dos lugares onde se observou o paradoxo foi em um estudo de preconceito de gênero em relação às admissões aos programas de pós-graduação da Universidade da Califórnia, Berkeley, em 1973. A proporção de admissões para as inscrições mostrou que os homens tinham uma chance muito maior de entrar em Berkeley que as mulheres:

```
import pandas as pd

data = pd.read_csv('data_simpson.csv', delimiter=';')
gender_stat = pd.pivot_table(
    data, aggfunc=sum, index='gender', columns='param', values='number'
)
gender_stat['ratio'] = round(
    gender_stat['accepted'] / gender_stat['applied'] * 100
)
print(gender_stat)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.7.2PT.png)

Essa grande diferença de 16% gerou uma controvérsia, então Berkeley investigou isso.

Primeiro, a administração da escola pediu a vários departamentos os números de inscrição e admissão:

```
faculty_gender_stat = pd.pivot_table(data, index = 'faculty', values = 'number', columns = ['gender','param'])
print(faculty_gender_stat)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_8_1588768377.png)

Ainda não está claro o que aconteceu, então vamos dar uma olhada na proporção de inscrições para admissões:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_9_1588768412.png)

Você deveria dar uma olhada nisso! Para quatro dos seis departamentos, você realmente tinha mais chances de ser aceito se fosse uma mulher. Vamos tentar descobrir como isso é possível.

Aqui está a essência do paradoxo de Simpson: uma tendência observada em dois grupos diferentes de dados desaparece ou se inverte quando você combina os grupos. Isso geralmente é o resultado de ignorar os dados ocultos ou ter grupos de controle que não são representativos.

Vamos ver quais dados foram perdidos na hora da compilação:

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/9.2.7.5PT_1665063466.png)

Não havia muita concorrência para o departamento A: apenas 108 candidatas do sexo feminino queriam entrar. A situação relativa ao departamento B foi semelhante.

Agora vamos olhar para o departamento F. Aceitou mais mulheres que homens, mas essa não é a única característica interessante. As chances de entrar eram consideravelmente menores, e o número de inscrições enviadas por mulheres era muito maior que para o departamento A.

As mulheres se inscreveram com mais frequência em departamentos competitivos com baixas taxas de admissão. Os homens, por outro lado, tendiam a se candidatar a departamentos menos competitivos com taxas mais altas de admissão. Essa diferença de abordagem distorceu os resultados, mas isso ficou despercebido até que os analistas de dados começaram a trabalhar.

Para descobrir o que está por trás do fenômeno, você precisa:

-   Ser cético em relação aos dados que você obtém e compará-los com a realidade
-   Examinar outras fatias de dados e ver se há algo oculto que possa afetar o resultado geral
-   Continuar cavando até encontrar uma resposta sobre a relação verdadeira nos dados.

Existem outros paradoxos bem conhecidos que os empregadores adoram perguntar durante as entrevistas. Por exemplo:

-   **O paradoxo de Menino ou Menina**
-   **O problema de Monty Hall**

Sinta-se à vontade para estudá-los por conta própria. Estar ciente de vários paradoxos ajuda você a se mostrar um pouco durante as entrevistas e apresentar seu conhecimento quando se trata de resultados contra-intuitivos.

Pergunta

Uma empresa que fabrica bolsas de snowboard está se preparando para lançar uma nova cor antes do inverno. Mostra aos clientes as duas opções (Azul Desafiante e Fúcsia Feliz) e pergunta se eles gostam da cor. Mil pessoas veem cada cor. Aqui estão os resultados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.7.6PT.png)

Parece que é hora de começar a pedir a Fúcsia Feliz. Mas podemos realmente confiar nos resultados?

Podemos sim, tudo parece estar bem. Mil pessoas foram entrevistadas e coletamos suas respostas.

Vou verificar com a minha equipe para ver o que eles pensam sobre o que os clientes responderam.

Eu gostaria de ver o que as pessoas que gostam de snowboard têm a dizer.

Vou ver se houve diferença nas respostas entre homens e mulheres.

Isso mesmo! Os homens e mulheres tendem a ter preferências de cores diferentes. Seria interessante ver se ambos gostam da cor Fúcsia Feliz.

Muito bem!

Pergunta

Veja uma divisão por gênero dos dados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.7.7PT.png)

O que podemos concluir?

A cor Azul Desafiante é a vencedora inesperada! Foi apreciada por uma quantidade maior de homens e mulheres.

Se você calcular a proporção entre o total de respondentes e os respondentes que escolheram a cor Azul Desafiante, sim. Você descobriu um exemplo do paradoxo de Simpson!

A cor Fúcsia Feliz ainda é o favorito. Os homens escolheram a cor Azul Desafiante com mais frequência, e o fato de poucas mulheres terem sido entrevistadas não importa muito.

Precisamos refazer a sondagem, desta vez certificando-nos de pedir números iguais de homens e mulheres.

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-06-55-599Z.md
### Última modificação: 2025-05-28 20:06:55

# Quando os Gráficos são os Inimigos - TripleTen

Capítulo 2/6

Preparando Apresentações

# Quando os Gráficos são os Inimigos

Antes de darmos uma olhada nos exemplos de bons gráficos, vamos falar sobre como não fazê-los.

Os gráficos podem ser enganosos quando:

1.  Há um erro nos cálculos.
2.  Você escolheu o tipo errado de gráfico.

Corrigir os números é fácil; corrigir a apresentação não é.

Pergunta

Vamos começar com um exemplo. O que há de errado com o gráfico abaixo?

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.8PT.png)

Os cálculos podem estar certos, mas as seções estão desproporcionais.

É preciso e informativo.

As ações não somam 100.

Exatamente. Eles somam apenas 80%, o que nos diz que pode haver algo errado com a matemática...

Você não pode realmente dizer sobre o que é o gráfico.

Fantástico!

Vamos tirar nossas primeiras conclusões:

-   Você precisa verificar seus cálculos e dados de origem para garantir que tudo esteja correto.
-   Você deve sempre certificar-se de que sua visualização não seja enganosa.

Aqui está um exemplo de um gráfico de barras "informativo" (mas enganoso):

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.8.3PT.png)

Todos correram aproximadamente no mesmo ritmo. A questão é que "aproximadamente" não é suficiente; os 100 metros rasos são questão de segundos. Precisamos de um eixo-y diferente, a menos que queiramos que um Usain Bolt irritado nos ligue e reclame.

Outro erro comum é focar nas aparências. Após dominar os gráficos simples, você terá vontade de torná-los mais complexos. Certifique-se de estudar o gráfico e pergunte a si mesmo o que ele está tentando dizer. Ele responde à pergunta que está sendo feita? Se adicionar elementos torna o gráfico menos informativo, mantenha-o simples.

Que pergunta você acha que este gráfico ([de um artigo da Wired](https://www.wired.com/2013/04/tedtalk/) (o artigo está em inglês)) deveria responder?

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.8.2PT.png)

Se você não olhou para a legenda, pode pensar que é um plano de arredondamento ou a proporção de diferentes metais em alguma nova escada com a espiral inovadora. Na verdade, é um gráfico de pizza 3D.

Menos é mais quando se trata de gráficos. Você sempre pode dividir a informação em várias visualizações, mostrando diferentes aspectos dela. Se você tentar mostrar tudo de uma vez, poderá obter algo assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_15_1588769480.png)

Opa. Isso é lindo, mas não diz nada. O olho treinado pode identificar o que é importante, mas como seus colegas se sentirão quando o virem pela primeira vez?

Outra conclusão importante é que você não pode ser preguiçoso; faça várias visualizações de dados para ilustrar várias características. Isso ajudará você e seu grupo de detetives a encontrar respostas mais rapidamente.

Pergunta

O que há de errado com este gráfico?

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_17_1588769753.png)

O que está acontecendo aqui? Eu não tenho ideia do que está acontecendo, e fica em preto e branco.

Nós também não entendemos, e poderia usar um pouco de cor.

Está tudo ótimo, os eixos são rotulados e a informação é fácil de ler.

Muito bem!

Pergunta

O que podemos fazer com este gráfico?

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.8.4PT.png)

Adicione um nome e legenda para deixar mais claro o que está acontecendo.

Precisamos fazer diferentes visualizações dos dados.

Separar os produtos individuais definitivamente ajudaria.

Brinque com a paleta de cores para facilitar o trabalho dos olhos.

Alguém realmente fez isso? Certamente você não pode estar falando sério!

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-06-56-911Z.md
### Última modificação: 2025-05-28 20:06:57

# Diferentes Gráficos para Diferentes Tipos de Informação - TripleTen

Teoria

# Diferentes Gráficos para Diferentes Tipos de Informação

Nesta lição, você aprenderá qual gráfico escolher para representar seus dados da melhor forma. Como você decide que tipo de gráfico usar?

Uma maneira é fazer um de cada e escolher a melhor opção. Mas uma alternativa ainda mais rápida é determinar exatamente o que você precisa do gráfico para ilustrar e selecionar aquele que se encaixa na conta.

Para que servem os gráficos geralmente?

**Comparações ao longo do tempo**

Quantos períodos de tempo você precisa mostrar?

-   Se houver muitos, use um gráfico de linhas:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.9PT.png)

-   Se houver apenas alguns (vários meses, por exemplo), um gráfico de barras é a melhor escolha:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.9.2PT.png)

**A distribuição de um conjunto de dados**

Quantas variáveis você precisa mostrar?

-   Uma? O gráfico de barras pode funcionar e você pode sobrepor um gráfico de linhas que mostra, por exemplo, a distribuição normal.
-   Duas? Vá com um gráfico de dispersão, que também é ótimo para mostrar inter-relações nos dados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_20_1588776571.png)

**Distribuição de dados por categoria**

Se você quiser acompanhar as alterações ao longo do tempo, crie um gráfico de barras. Se não, vá com um gráfico de pizza. Apenas tome cuidado com a forma como você exibe várias proporções.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.9.3PT.png)

**Funis e outros gráficos especializados**

Embora estes não sejam os gráficos tradicionais, muitas vezes você também precisará fazê-los. Os mapas de calor são um exemplo de gráficos especializados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.9.4PT.png)

Depois de escolher o diagrama, você pode relaxar, certo? Ainda não. Os gráficos são compostos de vários elementos, portanto, certifique-se que:

-   **a cor**
    -   é selecionada de acordo com a paleta de cores
    -   não dificultará a compreensão se o gráfico for impresso em preto e branco
    -   também é visível para as pessoas daltônicas
    -   contrasta bem com o fundo
-   **o texto**
    -   é legível
    -   é horizontal
    -   inclui uma legenda
    -   Os dados no gráfico são rotulados.
-   **as linhas**
    -   da grade (se houver) estão ocultas
    -   não tem marcas irrelevantes nos eixos
    -   a cor é selecionada de acordo com a paleta de cores
    -   contrasta bem com o fundo.
-   **o layout geral**
    -   é proporcional
    -   tem eixos uniformemente espaçados
    -   é bidimensional
    -   mantém os adornos extras ao mínimo

Salve e revise esta lista de verificação ao revisar suas visualizações. Os gráficos devem apoiar suas conclusões, bem como os resultados de sua análise.

Aqui está uma lista de métodos que ajudarão você a traçar gráficos informativos no Matplotlib.

1.  Use o método **set\_title()** para dar um título aos gráficos:

```
import matplotlib.pyplot as plt
 fig, test = plt.subplots()

 test.set_title('Distribuição normal')
```

2.  Para os rótulos de eixo, use os métodos **set\_xlabel()** e **set\_ylabel()**:

```
import matplotlib.pyplot as plt
 fig, test = plt.subplots()

 test.set_xlabel('Rótulo para o eixo X')
 test.set_ylabel('Rótulo para o eixo Y')
```

3.  O método **legend()** é para a legenda:

```
import matplotlib.pyplot as plt
 fig, test = plt.subplots()

 test.plot(x, y1, label = 'Rótulo 1')
 test.plot(x, y2, label = 'Rótulo 2')
 test.plot(x, y3, label = 'Rótulo 3')

 test.legend()
```

4.  Para modificar o tamanho do gráfico, temos os métodos **set\_figheight()** e **set\_figwidth()**:

```
import matplotlib.pyplot as plt
fig, test = plt.subplots()

fig.set_figheight(10) # altura 
fig.set_figwidth(8) # largura
```

Diferentes Gráficos para Diferentes Tipos de Informação

Tarefa

Há um gráfico no pré-código. Adicione os seguintes elementos a ele:

-   Cabeçalho da tabela: `'Linha reta'`
-   Rótulos de eixo: `'Rótulo para o eixo X'` e `'Rótulo para o eixo Y'`
-   Legenda
-   Altere o tamanho para que a altura e a largura sejam cinco

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

import matplotlib.pyplot as plt

  

fig, test \= plt.subplots()

test.scatter(\[0, 1, 2, 3, 4, 5\], \[0, 1, 2, 3, 4, 5\], label \= 'Rótulo 1')

\# liste os elementos aqui

  

test.set\_title('Linha reta')

  

fig, test \= plt.subplots()

  

test.set\_xlabel('Rótulo para o eixo X')

test.set\_ylabel('Rótulo para o eixo Y')

  

  

test.legend()

fig.set\_figheight(5) \# altura

fig.set\_figwidth(5) \# largura

plt.show()

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-06-58-287Z.md
### Última modificação: 2025-05-28 20:06:58

# Noções Básicas de Fazer Apresentações - TripleTen

Capítulo 2/6

Preparando Apresentações

# Noções Básicas de Fazer Apresentações

Após terminar sua pesquisa, tirar suas conclusões e traçar seus gráficos, é hora de preparar a apresentação final.

**Ferramentas**

Os usuários do Windows costumam fazer apresentações no PowerPoint, parte do Microsoft Office. As alternativas incluem o PowerPoint Online e o Libre Office.

Os usuários do macOS têm o Keynote.

A maioria das apresentações são arquivos **.pptx** (PowerPoint) ou **PDF**.

Se você estiver trabalhando em sua apresentação em equipe ou se ela for para o uso interno, salve-a como um arquivo .pptx.

Se você mesmo vai entregar a apresentação, salve-a como PDF. Dessa forma, o estilo, as fontes e as imagens aparecerão da maneira que você deseja. Enquanto os PDFs podem ser abertos de praticamente qualquer dispositivo, os arquivos .pptx não podem.

**Diretrizes de apresentação**

Vejamos como podemos apresentar os resultados de um teste A/B.

Especifique seu tópico e as datas de início e término do estudo no slide de introdução.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.10PT.png)

As descobertas gerais vão ao início.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.10.2PT.png)

Uma ideia por slide. Não deveria haver nada irrelevante ou confuso. O que exatamente significa "loja online" aqui?

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.10.3PT.png)

Não está claro. O título deveria indicar as principais descobertas, para que o público saiba no que prestar atenção e quais perguntas serão respondidas no slide. Isso é melhor:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.10.4PT.png)

Mas você não pode simplesmente jogar um gráfico para o seu público; você também precisa descrever suas conclusões.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.10.5PT.png)

Conselho geral:

1.  Uma apresentação é o melhor formato para se comunicar com partes interessadas empresariais. _**Nunca apresente sua análise em um notebook Jupyter**_.
2.  Atenha-se a um único enredo.
    
    Resolva os problemas um por um. Complete cada conjunto de slides de forma que seus leitores sejam levados às conclusões corretas.
    
3.  Certifique-se de que os dados são precisos e que seus gráficos não se contradizem.
4.  Cite as fontes das quais você tirou os dados (como no slide acima).
5.  Coloque as recomendações no final.
    
    Elas não precisam ser detalhadas, mas precisam ser relevantes e claras.
    

Sua empresa provavelmente possui diretrizes de apresentação com suas próprias nuances e requisitos. Pergunte aos seus colegas ou supervisor sobre isso antes de preparar uma apresentação.

Pergunta

O que você achou deste slide?

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_28_1588776922.png)

Está tudo ótimo. Este é o gráfico de uma distribuição com as escalas dos eixos especificados.

O gráfico parece bom, embora esteja faltando uma legenda e rótulos.

Os eixos do gráfico precisam ser rotulados para sabermos o que estamos vendo. O slide também precisa de um título e de resultados.

Isso mesmo! O slide realmente precisa de conclusões indicando também a pergunta que o gráfico está tentando responder.

Pode não haver informação suficiente, mas posso somente preencher os espaços em branco quando fizer a apresentação.

Você conseguiu!

Pergunta

O que você diria sobre este slide?

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.2.10.6PT.png)

Faltam conclusões: quais indicadores têm impacto e quais não.

Teria sido melhor construir uma matriz de correlação apoiada em conclusões.

Com certeza. Seus gerentes provavelmente não se importam com todos os detalhes dos dados, embora as conclusões e gráfico legível sejam cruciais.

Uma ótima opção para uma apresentação; tem tudo o que falamos na lição.

Só precisamos adicionar um gráfico e conclusões para torná-lo super informativo.

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-06-59-415Z.md
### Última modificação: 2025-05-28 20:06:59

# Relatórios no Jupyter Notebook - TripleTen

Capítulo 2/6

Preparando Apresentações

# Relatórios no Jupyter Notebook

Você já estudou os casos nos quais seus colegas de equipe precisavam de relatórios como arquivos de notebook Jupyter. Você pode salvá-los em um dos dois formatos;

-   **ipynb**, o formato de arquivo padrão para o Jupyter Notebook. Seu colega só precisa de um computador para abrir seu relatório.
-   **HTML** é bom quando você não tem como abrir o Jupyter Notebook (no seu celular, por exemplo). A remarcação é gerada automaticamente. Salvar seu caderno no formato que você precisa é fácil: `File -> Download as... -> HTML`

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_30_1588777240.png)

Aqui está um chacklist que pode usar quando você estiver preparando relatórios no Jupyter Notebook:

1.  Se você estiver enviando um arquivo ipynb, certifique-se de anexar os dados de origem.
2.  Se estiver enviando um arquivo HTML, certifique-se de que todas as células estejam preenchidas corretamente para que apareçam as tabelas e o gráfico.
3.  Especifique a finalidade do arquivo em uma célula _markdown_.
4.  Marque os passos principais. Indique onde os dados são pré-processados, onde as questões são colocadas e onde as principais descobertas são apresentadas.
5.  Insira um índice no início do arquivo.
6.  Liste os principais resultados e constatações; isso ajudará os leitores a encontrar rapidamente as respostas para suas perguntas.
7.  Deixe comentários no código, certificando-se de indicar o significado e a função de cada variável.

Vamos ver como criamos tabelas no caderno. Elas ajudam o usuário a navegar pelo documento e ilustram o curso do estudo.

Selecione o tipo de célula _markdown_ e insira todos os pontos principais de sua pesquisa da seguinte forma:

```
1. [Abrindo dados]
2. [Dados de pré-processamento]
    * [Processando valores ausentes]
    * [Processando duplicatas]
```

Adicione um link para cada célula markdown com o nome do ponto correspondente:

```
<a id="some_id"></a>
```

Indique o ID exclusivo da célula. Adicione links à lista que você criou:

```
1. [Abrindo dados](#início)
2. [Dados de pré-processamento](#pré-processamento)
    * [Processando valores ausentes](#null)
    * [Processando duplicatas](#duplicados)
```

Use `Shift + Enter` para ver a lista com links.

É uma boa prática evitar caminhos de arquivo referentes ao seu computador. Por exemplo, `C://Users//Kitty//research.csv` provavelmente não abrirá para seu companheiro de equipe.

Para evitar confusão, crie uma variável chamada `path` que armazene um caminho de arquivo exclusivo e adicione o nome do arquivo:

```
import pandas as pd
path = 'C://Users//Kitty//'
df = pd.read_csv(path + 'auto.csv')
```

Pergunta

Dê uma olhada nesta parte de um relatório do Jupyter notebook. É adequado para ser enviado a um colega analista?

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_31_1588777350.png)

Ainda não, pois ainda precisa de um cabeçalho e comentários no código.

É difícil saber se você está onde deveria estar se não houver nenhum cabeçalho.

Sim, eles sabem o que está acontecendo, e vão descobrir isso rapidamente.

Não, esqueci de adicionar comentários no código.

Eles só precisam dos meus métodos de cálculo, então o código é suficiente.

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-07-00-754Z.md
### Última modificação: 2025-05-28 20:07:01

# Conclusão - TripleTen

Capítulo 2/6

Preparando Apresentações

# Conclusão

Parabéns! Você está cada vez mais perto de poder contar histórias convincentes com base em dados.

### O que você aprendeu:

-   Como fazer apresentações claras.
-   Como escolher o tipo de gráfico que se adequa aos seus dados.
-   Como construir relatórios no Jupyter Notebook.
-   Como tomar decisões que considerem a sazonalidade..

### Leve isso com você

Faça o download da [folha conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/moved_Folha_de_concluses_Preparando_Apresentaes_pt.pdf) e do [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/moved_Resumo_do_Captulo_Preparando_Apresentaes_pt.pdf) para que você possa consultá-los quando necessário.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-07-02-067Z.md
### Última modificação: 2025-05-28 20:07:02

# Introdução - TripleTen

Capítulo 3/6

A biblioteca Seaborn

# Introdução

Apenas a biblioteca Matplotlib não é o suficiente! Neste capítulo, você irá aprender sobre as capacidades básicas da biblioteca Seaborn.

### O que você vai aprender:

-   Como aplicar estilos embutidos em Matplotlib
-   Como construir gráficos combinados usando o método `jointplot()`
-   Como selecionar boas cores para os gráficos
-   Como construir gráficos para variáveis categóricas
-   Como usar os métodos `distplot()`, `pairplot()`, `violinplot()` e `stripplot()`

### Quanto tempo irá demorar:

7 lições, com aproximadamente 15-20 minutos cada uma

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-07-03-552Z.md
### Última modificação: 2025-05-28 20:07:04

# Por que apenas o Matplotlib não é o suficiente - TripleTen

Teoria

# Por que apenas o Matplotlib não é o suficiente

Matplotlib é uma biblioteca de baixo nível para a visualização de dados em Python, um alicerce para bibliotecas mais avançadas. Veja a biblioteca Seaborn, por exemplo. Matplotlib possui muitos parâmetros úteis para a construção de gráficos customizados. Mas não é incomum que analistas precisem construir apenas os gráficos padrão. Mais do que isso, eles precisam construí-los rapidamente, então não é possível ficar escrevendo códigos longos para cada um deles.

Uma opção é salvar um script para construir gráficos. Mas escrever um script como esse em Matplotlib para um mapa de calor (por exemplo) consome muito tempo. É por isso que os mapas de calor, assim como distribuições e diagramas de dispersão, são construídos em seaborn.

Além disso, há quem ache que o Matplotlib é "feio". Mas há duas maneiras de fazer gráficos mais agradáveis para os olhos:

-   Use estilos embutidos em Matplotlib.
-   Importe e utilize uma biblioteca como a seaborn.

Para descobrir quais estilos estão disponíveis, confira o valor do atributo **available** no módulo **style**:

```
import matplotlib.pyplot as plt

print(plt.style.available) # chamando conjuntos diferentes de cores
```

```
['bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark-palette', 'seaborn-dark', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'seaborn', 'Solarize_Light2', 'tableau-colorblind10', '_classic_test']
```

Às vezes você terá necessidade de que apenas alguns dos gráficos de um projeto tenham um estilo particular. Passe o nome do estilo para o gerenciador de contexto **with** no método **context()**. Então indique a área dentro da qual as mudanças devem ser restritas:

```
with plt.style.context('seaborn-pastel'):
    plt.bar([10, 20, 30, 40],[3, 9, 18, 7])
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_32_1588843482.png)

Se todos os gráficos no projeto precisarem ter o mesmo estilo, chame o método **plt.style.use()** quando você iniciar o projeto:

```
plt.style.use('ggplot') # o estilo ggplot é selecionado aqui
```

Você pode estudar como aplicar os estilos embutidos do Matplotlib por conta própria com esses documentos: [https://matplotlib.org/tutorials/introductory/customizing.html](https://matplotlib.org/tutorials/introductory/customizing.html) _(o artigo está em inglês)_.

Por que apenas o Matplotlib não é o suficiente

Tarefa2 / 2

1.

Você é um analista de uma rede de restaurantes. A rede possui um restaurante principal e quatro filiais menores. Entre elas, a filial #3 aparentemente foi a mais lucrativa no mês passado. A gerência achou isso duvidoso, já que esse restaurante está localizado em um bairro que não é muito movimentado. Analise os nomes das colunas do DataFrame. Então construa um gráfico de barras da quantidade de pedidos feita em cada restaurante. Use o estilo `'seaborn-talk'`.

Caminho para o conjunto de dados: `'/datasets/rest_us.csv'`

2.

Construa um gráfico de barras para exibir a valor médio de consumo (a coluna `Rest['avg_bill']`) para cada filial. Desta vez, use o estilo `'seaborn'`.

9

1

2

3

4

5

6

7

8

9

import matplotlib.pyplot as plt

import pandas as pd

  

rest \= pd.read\_csv('/datasets/rest\_us.csv')

  

with plt.style.context('seaborn'):

plt.bar(rest\['name\_rest'\],rest\['avg\_bill'\])

plt.show()

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-07-04-855Z.md
### Última modificação: 2025-05-28 20:07:05

# O Método jointplot() - TripleTen

Teoria

# O Método jointplot()

No curso de Análise Exploratória de Dados você construiu um gráfico de distribuição conjunta e um gráfico de correlações. E se nós combinarmos eles?

A função **jointplot** da biblioteca seaborn nos permite combinar duas distribuições distintas em um gráfico. Esse método é uma vantagem importante que o seaborn possui em relação ao Matplotlib. Basta uma linha de código, e voilà! Seu gráfico de distribuição conjunta está pronto!

Vamos observar um exemplo da Yandex.Taxi. Motivar os motoristas ajuda a assegurar que eles vão trabalhar bem. Considere as gorjetas dos passageiros, por exemplo. Passageiros podem dar gorjetas de até 15% do valor da corrida. Vamos ver se a gorjeta possui relação com as avaliações dos motoristas:

```
import pandas as pd

taxi = pd.read_csv('/datasets/taxi.csv')
print(taxi.head())
```

```
driver_id  rating  tips
0       3116     4.6     9
1       4742     6.1    12
2       3482     6.8     8
3       4155     8.0    12
4       3425     6.4     8
```

Vamos chamar o método `jointplot` com os argumentos `x`,`y`, e `data`:

```
import seaborn as sns
import pandas as pd

taxi = pd.read_csv('/datasets/taxi.csv')

sns.jointplot(x="rating", y="tips", data=taxi)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.3PT.png)

Vamos acrescentar mais algumas informações: uma distribuição de densidade e uma regressão. (Talvez você se lembre que no curso de estatística de que em histogramas de densidade, a densidade dos intervalos é representada pela altura das colunas. Regressão é um método que nos traz informações sobre a relação entre as variáveis.) Vamos atribuir o valor `"reg"` (regressão) para o argumento `kind`:

```
sns.jointplot(x="rating", y="tips", data=taxi, kind='reg')
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.3.2PT.png)

O gráfico mostra que existe uma correlação linear positiva. A avaliação de um motorista e o valor da gorjeta costumam ser interdependentes.

Esse gráfico pode ser anexado a um relatório direcionado a seus colegas, mas ele não ficará muito claro para a gerência sem explicações adicionais.

O Método jointplot()

Tarefa3 / 3

1.

Vamos trocar o exemplo do táxi por outro. O gerente de uma rede de restaurantes está criando um programa motivacional para os garçons. Pediram para você analisar dados relativos a gorjetas do arquivo `/datasets/restaurant_data_us.csv`. Leia esse conjunto de dados, armazene-o em `rest_data`, e analise as informações gerais sobre os dados.

2.

Converta os tipos de dados da coluna de `'bill_datetime'` para `datetime` e verifique as cinco primeiras linhas.

3.

Construa um gráfico para exibir a relação entre o valor da conta e as gorjetas. Defina `kind="reg"` para adicionar uma regressão linear que se encaixe.

9

1

2

3

4

5

6

7

8

import seaborn as sns

import pandas as pd

  

rest\_data \= pd.read\_csv('/datasets/restaurant\_data\_us.csv')

rest\_data\['bill\_datetime'\] \= pd.to\_datetime(rest\_data\['bill\_datetime'\])

sns.jointplot(x\='tips', y\='restaurant\_bill', data\=rest\_data, kind\='reg')

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-07-06-196Z.md
### Última modificação: 2025-05-28 20:07:06

# Paletas de cores - TripleTen

Teoria

# Paletas de cores

Possuir um senso estético é importante para um analista. De verdade.

Mesmo um gráfico perfeitamente correto não será útil se estiver feio.

Visualizações como esta com certeza não vão deixar seus colegas boquiabertos:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_35_1588849713.png)

Serviços online com paletas de cores especialmente compiladas podem te ajudar a evitar esses tipos de problemas. Você deverá apenas aplicar as cores que você precisa ao gráfico (os materiais estão em inglês):

-   [https://colorhunt.co/](https://colorhunt.co/)
-   [http://fabianburghardt.de/swisscolors/](http://fabianburghardt.de/swisscolors/)
-   [https://flatuicolors.com/](https://flatuicolors.com/)
-   [https://uxpro.cc/toolbox/visual-design/colors/](https://uxpro.cc/toolbox/visual-design/colors/)

A biblioteca seaborn também possui suporte para paletas de cores. Você pode acessá-las usando o método **color\_palette()**:

```
current_palette = sns.color_palette("coolwarm", 20)
print(sns.palplot(current_palette))
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_36_1588849760.png)

Você pode definir uma paleta padrão para todos os gráficos usando o método **set\_palette()**:

```
sns.set_palette('dark')
```

Se você definir a paleta `'dark'`, o gráfico da lição anterior irá ficar assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.4.1PT.png)

Aprenda sobre os tipos de paletas na [documentação](https://seaborn.pydata.org/tutorial/color_palettes.html#building-color-palettes) _(o artigo está em inglês)_. Estas são as paletas padrão:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.4.2PT.png)

A saturação das cores aumenta de acordo com o eixo X e a luminosidade, com o eixo Y.

Para selecionar uma cor para um gráfico específico, adicione o argumento `color` e o nome da cor. Vamos modificar o gráfico da lição anterior:

```
sns.jointplot(x="rating", y="tips", data=taxi, kind='reg', color='blue')
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.4.3PT.png)

Além de usar nomes padronizados, como `'blue'`, você pode indicar uma cor usando o formato **hex**, que é composto por seis dígitos hexadecimais. Por exemplo, `#00FF00` é lima (lime) e `#FF6347` é tomate (tomato). Você não precisa decorar os códigos (com exceção de `#FFFFFF`, branco, e `#000000`, que é o preto), porque há uma [tabela](https://www.color-hex.com/) que você pode consultar.

Paletas de cores

Tarefa

Defina a cor `'#006400'` para o gráfico da aula anterior.

9

1

2

3

4

5

6

7

import seaborn as sns

import pandas as pd

  

rest\_data \= pd.read\_csv('/datasets/restaurant\_data\_us.csv')

rest\_data\['bill\_datetime'\] \= pd.to\_datetime(rest\_data\['bill\_datetime'\])

sns.jointplot(x\='tips', y\='restaurant\_bill', data\=rest\_data, kind\='reg', color \= '#006400')

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-07-08-483Z.md
### Última modificação: 2025-05-28 20:07:08

# Estilos de Gráfico - TripleTen

Teoria

# Estilos de Gráfico

Aqui estão alguns métodos Matplotlib que também funcionam para seaborn:

-   **set\_title()**
-   **set\_xlabel()** and **set\_ylabel()**

Podemos mudar o tamanho de um gráfico usando o método `figure()` com o argumento `figsize`, que configura tanto a altura quanto a largura em polegadas:

```
plt.figure(figsize=(12, 3)) # Observação! Escreva este código antes de criar o gráfico
ax = sns.lineplot(x='timepoint', y='signal', hue='event', style='event', data=fmri)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_40_1588850526.png)

Você pode configurar o estilo do gráfico usando `set_style()`. Há cinco temas pré-definidos que você pode selecionar como o valor de seu único argumento: `'darkgrid'`, `'whitegrid'`, `'dark'`, `'white'`, e `'ticks'`. `'darkgrid'` é o tema padrão. Se você não precisa de grades em seu gráfico, escolha os temas `'dark'`, `'white'`, ou o tema `'ticks'`. O tema `'whitegrid'` é a melhor opção para gráficos complexos.

```
sns.set_style('dark')
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_41_1588850588.png)

Gráficos são geralmente construídos com dois eixos: X e Y. Para exibir apenas os eixos, use o método `despine()`. O método `despine()` remove as linhas do topo e da direita do gráfico e deixa apenas os eixos x e y. Ele possui uma boa aparência e funciona muito bem para apresentações.

```
sns.despine()
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_42_1588850607.png)

Estilos de Gráfico

Tarefa

Altere o estilo do gráfico para `'dark'`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

import seaborn as sns

import pandas as pd

  

rest\_data \= pd.read\_csv('/datasets/restaurant\_data\_us.csv')

rest\_data\['bill\_datetime'\] \= pd.to\_datetime(rest\_data\['bill\_datetime'\])

  

\# escreva seu código aqui

  

  

  

sns.set\_style('dark')

sns.jointplot(

x\='tips', y\='restaurant\_bill', data\=rest\_data, kind\='reg', color\='#006400'

)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-07-09-821Z.md
### Última modificação: 2025-05-28 20:07:10

# Dados Categóricos - TripleTen

Teoria

# Dados Categóricos

A biblioteca seaborn permite que você teste gráficos com conjuntos de dados embutidos. Aprenda mais sobre eles na [documentação](https://seaborn.pydata.org/tutorial/categorical.html) _(o artigo está em inglês)_. O método **load\_dataset()** te dá acesso aos conjuntos de dados embutidos:

```
import seaborn as sns

iris = sns.load_dataset('iris')
print(iris.head())
```

```
   sepal_length  sepal_width  petal_length  petal_width species
0           5.1          3.5           1.4          0.2  setosa
1           4.9          3.0           1.4          0.2  setosa
2           4.7          3.2           1.3          0.2  setosa
3           4.6          3.1           1.5          0.2  setosa
4           5.0          3.6           1.4          0.2  setosa
```

Você já está familiarizado com gráficos de barras. Você pode criá-los usando o método `barplot()` com os seguintes argumentos:

-   `x` — os dados no eixo X
-   `y` — os dados no eixo Y
-   `data` — o conjunto de dados para a construção
-   `color` ou `palette`

Dê uma olhada em todos os argumentos possíveis para o método `barplot()` na [documentação](https://seaborn.pydata.org/generated/seaborn.barplot.html#seaborn.barplot) _(o artigo está em inglês)_. Vamos construir um gráfico de barras usando dados do conjunto de dados `flights` (a quantidade de passageiros mensais em voos de 1949 a 1960):

```
import seaborn as sns

flights = sns.load_dataset('flights')
ax = sns.barplot(x='year', y='passengers', data=flights)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.6PT.png)

`barplot()` agrega dados por conta própria; por rotina, ele calcula a média. Altere esta opção no argumento **estimator**:

```
import seaborn as sns
from numpy import median

flights = sns.load_dataset('flights')
ax = sns.barplot(x='year', y='passengers', data=flights, estimator=median)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.6.2PT.png)

Na seção de Análise Exploratória de Dados, você estudou diagramas de extremos e quartis (diagramas de caixa). Em seaborn, podêmos fazê-los usando `boxplot()`:

```
import seaborn as sns

sport = sns.load_dataset('exercise')
ax = sns.boxplot(x='diet', y='pulse', data=sport)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.6.3PT.png)

Além dos eixos x e y, você pode adicionar uma terceira dimensão para o gráfico. Por exemplo, você pode analisar a adesão de pessoas a diferentes dietas em termos tanto de sua pulsação quanto do tipo de atividade física. Observe o parâmetro **hue** do método `boxplot()`...

```
import seaborn as sns

sport = sns.load_dataset('exercise')
ax = sns.boxplot(x='kind', y='pulse', hue='diet', data=sport)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.6.4PT.png)

Dados Categóricos

Tarefa2 / 2

1.

Faça um gráfico do valor médio da conta pela quantidade de pessoas na festa. Talvez você precise redimensionar sua janela de resultados para visualizar o gráfico completo.

2.

Adicione uma coluna `'tips_percent'` e calcule razões entre as gorjetas e as contas. Construa um diagrama de caixa para a quantidade de clientes e descubra quais grupos deixam as gorjetas mais altas.

99

1

2

3

4

5

6

7

8

9

10

11

import seaborn as sns

import pandas as pd

  

rest\_data \= pd.read\_csv('/datasets/restaurant\_data\_us.csv')

rest\_data\['bill\_datetime'\] \= pd.to\_datetime(rest\_data\['bill\_datetime'\])

rest\_data\['tips\_percent'\] \= rest\_data\['tips'\] / rest\_data\['restaurant\_bill'\]

ax \= sns.boxplot(x\='persons', y\='tips\_percent', data\=rest\_data)

  

  

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-07-11-199Z.md
### Última modificação: 2025-05-28 20:07:11

# Visualizando Distribuições - TripleTen

Teoria

# Visualizando Distribuições

Você construiu gráficos de distribuição usando o método `hist()`. A biblioteca seaborn possui gráficos pré-prontos para a visualização de distribuições de uma variável e distribuições conjuntas.

O método **distplot()** (distribution plot) apresenta a distribuição do valor e sua densidade e funde um histograma com um gráfico de densidade.

```
import seaborn as sns

sport = sns.load_dataset("exercise")
sns.distplot(sport['pulse'])
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.7PT.png)

Assim como nos gráficos `hist()`, você define as barras utilizando o argumento `bins`:

```
sns.distplot(sport['pulse'], bins=10)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.7.2PT.png)

Construa gráficos de distribuição conjunta usando o método `pairplot()`:

```
sns.pairplot(sport)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.7.3PT.png)

Uma terceira medição também pode ser exibida. Ela é declarada no argumento `hue`:

```
sns.pairplot(sport, hue='diet')
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.7.4PT.png)

Visualizando Distribuições

Tarefa2 / 2

1.

Contrua um gráfico de distribuição conjunta para os dados em `rest_data`.

2.

Adicione uma terceira dimensão, `'waiter_gender'`.

99

1

2

3

4

5

6

7

8

9

10

import seaborn as sns

import pandas as pd

  

rest\_data \= pd.read\_csv('/datasets/restaurant\_data\_us.csv')

rest\_data\['bill\_datetime'\] \= pd.to\_datetime(rest\_data\['bill\_datetime'\])

  

sns.pairplot(rest\_data, hue\='waiter\_gender')

  

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-07-13-450Z.md
### Última modificação: 2025-05-28 20:07:13

# Gráficos Especiais em Seaborn - TripleTen

Teoria

# Gráficos Especiais em Seaborn

Esses são gráficos difíceis de produzir sem o seaborn.

### violinplot()

Assim como o `boxplot()`, este gráfico descreve o formado da distribuição. Sua aparência incomum se deve à combinação de dois gráficos de distribuição de densidade. A principal vantagem do `violinplot()` em relação ao `boxplot()` é que ele permite que você analise a distribuição e determine seu tipo.

Uma comparação entre o `violinplot()` e o `boxplot()`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.8PT.png)

Em seaborn, construímos esse tipo de gráfico utilizando o método `violinplot()`:

```
sns.violinplot(x="kind", y="pulse", data=sport, palette='rainbow')
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.8.2PT.png)

### stripplot()

`stripplot()` é uma outra maneira de exibir dados categóricos. Nós pegamos um gráfico de dispersão para cada categoria. Nós recomendamos a construção desses tipos de gráficos em conjunto com outros, como o `violinplot()`.

Em seaborn, nós construímos esse tipo de gráfico utilizando o método `stripplot()`:

```
sns.stripplot(x="diet", y="pulse", data=sport)
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.3.8.3PT.png)

Gráficos Especiais em Seaborn

Tarefa4 / 4

1.

Recupere o mês e o dias da semana dos dados da data do pedido no restaurante. Armazene o mês na coluna `'bill_month'` e o dia da semana na coluna `'bill_weekday'`. Imprima as cinco primeiras linhas de `rest_data`.

2.

Use o método `stripplot()` para construir um gráfico de variância para o percentual de gorjetas por mês.

3.

Utilize o método `stripplot()` para construir um gráfico de variância para o percentual de gorjetas para cada dia da semana.

4.

Utilize o método `stripplot()` para contruir um gráfico de variância para a quantidade de gorjetas para cada dia da semana. Adicione a dimensão `'waiter_gender'`

99

1

2

3

4

5

6

7

8

9

10

11

import seaborn as sns

import pandas as pd

  

rest\_data \= pd.read\_csv('/datasets/restaurant\_data\_us.csv')

rest\_data\['bill\_datetime'\] \= pd.to\_datetime(rest\_data\['bill\_datetime'\])

rest\_data\['tips\_percent'\] \= rest\_data\['tips'\] / rest\_data\['restaurant\_bill'\]

rest\_data\['bill\_month'\] \= rest\_data\['bill\_datetime'\].dt.month

rest\_data\['bill\_weekday'\] \= rest\_data\['bill\_datetime'\].dt.weekday

sns.stripplot(x\="bill\_weekday", y\="tips\_percent", data\=rest\_data, palette\='rainbow', hue \= 'waiter\_gender')

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-07-14-743Z.md
### Última modificação: 2025-05-28 20:07:15

# Conclusão - TripleTen

Capítulo 3/6

A biblioteca Seaborn

# Conclusão

Com a biblioteca seaborn, você teve contato com uma vasta gama de gráficos. Ela permite que você os construa rapidamente, diferente do Matplotlib.

### O que você aprendeu:

-   Como utilizar estilos embutidos em Matpotlib
-   Como construir gráficos combinados usando o método `jointplot()`
-   Como selecionar boas cores para os gráficos
-   Como construir gráficos para variáveis categóricas
-   Como usar os métodos `distplot()`, `pairplot()`, `violinplot()` e `stripplot()`

### Leve isso com você

Faça o download da [folha conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/Folha_de_Concluses_A_Biblioteca_Seaborn.pdf) e do [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/moved_DA_9_sprint_Resumo_do_Captulo_A_Biblioteca_Seaborn_ptbr.pdf) para que você possa consultá-los quando necessário.

### Leitura independente _(os materiais estão em inglês)_:

[Correlações ilegítimas](http://www.tylervigen.com/spurious-correlations)

[Encontrando as paletas de cores certas para visualizações de dados](https://www.dreamerux.com/articles/9bpclyvtuylf5g287onh90efz9avka)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-07-16-054Z.md
### Última modificação: 2025-05-28 20:07:16

# Home

Análise de Dados

83%

![](https://practicum-content.s3.amazonaws.com/resources/Final_Project_2_1700743144.svg)

### Sprint 14

## Projeto Final

### Capítulo 4

## Entregando Projetos

Projeto Final. Decomposição

[Avançar](/trainer/data-analyst/lesson/feaa4b83-c847-44c3-9187-c1a4ec45e577/)

## Conteúdo

1.  ![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/what_to_expect_1679489901_1681843066.svg)
    
    Módulo Adicional
    
    ### Curso demonstrativo
    
    5 capítulos
    
2.  ![](https://practicum-content.s3.amazonaws.com/resources/what_to_expect_1_1700743172.svg)
    
    Onboarding
    
    ### É bom ver você na TripleTen!
    
    14 aulas
    
3.  ![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1._Advanced_Spreadsheets_1676362144_1681842191.svg)
    
    Módulo Adicional
    
    ### Excel: Trabalhando com Dados
    
    3 capítulos restantes
    
4.  ![](https://practicum-content.s3.amazonaws.com/resources/5._Python_Fundamentals_1700742947.svg)
    
    Sprint 1
    
    ### Python básico
    
    9 capítulos
    
5.  ![](https://practicum-content.s3.amazonaws.com/resources/5._Python_Fundamentals_1700743842.svg)
    
    Sprint 2
    
    ### Continuação de Python básico
    
    7 capítulos
    
6.  ![](https://practicum-content.s3.amazonaws.com/resources/data_prep_1700742967.svg)
    
    Sprint 3
    
    ### Manipulação de dados
    
    11 capítulos
    
7.  ![](https://practicum-content.s3.amazonaws.com/resources/1._Advanced_Spreadsheets_1700742977.svg)
    
    Sprint 4
    
    ### Análise Estatística de Dados
    
    6 capítulos
    
8.  ![](https://practicum-content.s3.amazonaws.com/resources/6._Software_Development_Tools_1700742989.svg)
    
    Sprint 5
    
    ### Ferramentas de Desenvolvimento de Software
    
    8 capítulos
    
9.  ![](https://practicum-content.s3.amazonaws.com/resources/Final_Project_2_1700742997.svg)
    
    Sprint 6
    
    ### Projeto Integrado 1
    
    5 capítulos
    
10.  ![](https://practicum-content.s3.amazonaws.com/resources/sandbox_1679490946_1681843095_1682758313_1685704627_1687875109.svg)
    
    Módulo Adicional
    
    ### Sandbox
    
    2 capítulos restantes
    
11.  ![](https://practicum-content.s3.amazonaws.com/resources/2._SQL_Databases_1700743030.svg)
    
    Sprint 7
    
    ### Coleta e Armazenamento de Dados (SQL)
    
    8 capítulos
    
12.  ![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/10._Integrated_Project_2_1681844030.svg)
    
    Módulo Adicional
    
    ### Introdução ao HTML, CSS
    
    3 capítulos
    
13.  ![](https://practicum-content.s3.amazonaws.com/resources/9._Business_Analytics_1_1700743059.svg)
    
    Sprint 8
    
    ### Análise de Negócio
    
    9 capítulos
    
14.  ![](https://practicum-content.s3.amazonaws.com/resources/B_Testing_1700743072.svg)
    
    Sprint 9
    
    ### Tomando Decisões de Negócios Baseadas em Dados
    
    9 capítulos
    
15.  ![](https://practicum-content.s3.amazonaws.com/resources/4._Storytelling_with_Data_1_1700743084.svg)
    
    Sprint 10
    
    ### Como Relatar uma História Usando Dados
    
    6 capítulos
    
16.  ![](https://practicum-content.s3.amazonaws.com/resources/Final_Project_2_1700743102.svg)
    
    Sprint 11
    
    ### Projeto Integrado 2
    
    2 capítulos
    
17.  ![](https://practicum-content.s3.amazonaws.com/resources/11.__Intro_to_Machine_Learning_1700743112.svg)
    
    Sprint 12
    
    ### Automação
    
    6 capítulos
    
18.  ![](https://practicum-content.s3.amazonaws.com/resources/11.__Intro_to_Machine_Learning_1700743126.svg)
    
    Módulo Adicional
    
    ### Suplementos de Automação
    
    5 capítulos restantes
    
19.  ![](https://practicum-content.s3.amazonaws.com/resources/8.Data_Visualization_with_Python_1700743135.svg)
    
    Sprint 13
    
    ### Previsões e Predições
    
    6 capítulos
    
20.  ![](https://practicum-content.s3.amazonaws.com/resources/Final_Project_2_1700743144.svg)
    
    Sprint 14
    
    ### Projeto Final
    
    2 capítulos restantes
    

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-08-52-746Z.md
### Última modificação: 2025-05-28 20:08:53

# Introdução - TripleTen

Capítulo 4/6

A biblioteca Plotly

# Introdução

Seguindo em frente e aprendendo mais! Gráficos não são totalmente estáticos; eles também podem ser interativos. Por isso, este é o momento perfeito para introduzir a plotly, uma biblioteca muito interessante para visualização de dados.

### O que você vai aprender:

-   Como determinar quando uma visualização interativa é necessária e quando você não terá utilidade
-   Como construir gráficos básicos em plotly
-   Como construir um gráfico de pizza
-   Como construir um funil de vendas

### Quanto tempo irá demorar:

5 aulas, aproximadamente 10-20 minutos cada

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-08-54-027Z.md
### Última modificação: 2025-05-28 20:08:54

# Gráficos Interativos - TripleTen

Capítulo 4/6

A biblioteca Plotly

# Gráficos Interativos

Vamos passar para o próximo nível: gráficos interativos!

Como o nome sugere, um **gráfico interativo** permite que você interaja com ele. Isso quer dizer que o usuário pode selecionar elementos e obter informações adicionais ao posicionar o cursor sobre eles.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_64d0e1e66359371e393cdc05a8bf986768f27a32_2_1035x472_1570391623.gif)

Parece legal, não é? Mas cuidado para não usar demais esse recurso. Você sempre poderá fazer um gráfico como esse, o que não significa que você deveria.

Gráficos interativos são úteis quando:

-   Você está trabalhando com um gráfico de linhas ou histograma muito complexo, com uma grande quantidade de dados. A possibilidade de interagir com o gráfico irá facilitar a análise dos valores de componentes essenciais com um detalhamento muito maior.
-   Você precisa subdividir o gráfico em segmentos independentes para trabalhar melhor com ele (por exemplo, dando um zoom para uma visualização mais detalhada).
-   Você precisa colocar muita informação em um gráfico (você pode esconder as informações em elementos pop-up).

Um gráfico interativo é um tipo de protótipo para outra visualização alternativa: um **dashboard**. Um dashboard é uma visualização de dados agrupada tematicamente, que possibilita uma elaboração rápida de respostas para dúvidas sobre um produto ou negócio. Dashboards são interativos e exibem seus dados em tempo real. Você irá aprender a criar dashboards quando estiver estudando automação, em breve.

A biblioteca **plotly** permite que você construa gráficos interativos em Python. Ela é baseada em **plotly.js**, que, por sua vez, é baseada na famosa biblioteca de visualização de dados **d3.js** da linguagem JavaScript, de programação para a internet. Você pode aprender mais sobre d3.js [aqui](https://d3js.org/) _(o artigo está em inglês)_.

Com a plotly, você não precisa perder horas criando gráficos complexos; basta chamar os métodos. Rotacione os gráficos conforme suas necessidades. Você pode selecionar elementos, visualizar valores, deixar as imagens maiores ou menores.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-08-55-332Z.md
### Última modificação: 2025-05-28 20:08:55

# Instalando o Python e o Jupyter Notebook - TripleTen

Capítulo 4/6

A biblioteca Plotly

# Instalando o Python e o Jupyter Notebook

Você já está trabalhando com o Python e com o Jupyter Notebook por um bom tempo. Caso você ainda não tenha realizado o download deles, chegou a hora de fazer isso.

É possível instalar os programas separadamente, e então adquirir as bibliotecas que você precisa de modo manual, mas é muito mais fácil simplesmente instalar o Anaconda, que já inclui as bibliotecas pandas e Matplotlib, além de outras bem comuns. Caso você ainda não tenha feito isso, siga as seguintes instruções:

Baixe o Anaconda [aqui](https://www.anaconda.com/distribution/). Durante a instalação, verifique as seguintes opções nas configurações avançadas:

-   `Adicione o Anaconda à variável de ambiente PATH do sistema`
-   `Registre o Anaconda como o Python do sistema 3.n` (a versão 3.5 será substituída pela versão inclusa no download do Anaconda, caso não seja a mesma)

Existem duas maneiras de abrir o Jupyter Notebook:

-   Encontre o Jupyter Notebook no Anaconda Navigator e inicie-o clicando no ícone
-   Escreva o comando _jupyter notebook_ no Prompt de Comando (Windows) ou Terminal (macOS)

Excelente! Agora você pode completar tarefas e criar relatórios para colegas de equipe no Jupyter Notebook em seu próprio computador.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-08-57-847Z.md
### Última modificação: 2025-05-28 20:08:58

# Gráficos Básicos da Biblioteca plotly - TripleTen

Capítulo 4/6

A biblioteca Plotly

# Gráficos Básicos da Biblioteca plotly

Você pode criar diversos tipos de visualização com a biblioteca plotly, de simples gráficos de linha a gráficos complexos com dados geoespaciais.

Você vai encontrar uma lista completa dos gráficos na [documentação](https://plot.ly/python/#fundamentals) _(a documentação está em inglês)_. Vamos começar construindo algo com que já estamos familiarizados: um gráfico de linhas e um histograma.

Se você está criando um gráfico baseado em dados de um Dataframe, utilize **Plotly Express**, uma API (Application Programming Interface) desenvolvida para oferecer acesso rápido aos principais métodos da biblioteca. Para importá-la, devemos fazer o seguinte:

```
import plotly.express as px
```

Assim como no caso da seaborn, a biblioteca plotly possui conjuntos de dados embutidos. Vamos construir um gráfico dos dados da eleição:

```
data = px.data.election()
print(data.head())
```

```
                distrito  Coderre  Bergeron  Joly  total    vencedor     resultado
0     101-Bois-de-Liesse     2481      1829  3024   7334      Joly  plurality
1  102-Cap-Saint-Jacques     2525      1163  2675   6363      Joly  plurality
2   11-Sault-au-Récollet     3348      2770  2532   8650   Coderre  plurality
3           111-Mile-End     1734      4782  2514   9030  Bergeron   majority
4         112-DeLorimier     1770      5933  3044  10747  Bergeron   majority
```

Como podemos observar nessa tabela, temos os distritos de votação, a quantidade de votos para cada um dos três candidatos, a quantidade total de votos, quem foi o vencedor, e qual o tipo de vitória. Vamos observar o gráfico e encontrar dependências.

### Gráficos de linha

Uma característica importante do gráfico de linhas interativo é que ele permite que você observe os valores de uma linha simplesmente passando o cursor sobre ela. Nós construímos gráficos de linhas chamando o método **line()** com os seguintes argumentos:

-   `data` — dados
-   `X` — dados no eixo X
-   `Y` — dados no eixo Y
-   `title` — o título do gráfico

```
import plotly.express as px

data = px.data.election()
fig = px.line(data, x='distrito', y='Coderre', title='Resultados de Coderre por distrito')
fig.show()
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.4.4PT.png)

Parece que as legendas do eixo X não estão corretas. Vamos girá-las usando o método **update\_xaxes()**. Vamos passar um ângulo de rotação em graus utilizando o argumento **tickangle**.

```
fig = px.line(data, x='distrito', y='Coderre', title='Resultados de Coderre por distrito')
fig.update_xaxes(tickangle=45) # Gire as legendas de marcação 45% no eixo X
fig.show()
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.4.4.2PT.png)

### Gráfico de barras

Construímos gráficos de barras utilizando o método `bar()` com os mesmos argumentos que usamos para os gráficos de linha:

```
import plotly.express as px

data = px.data.election()
fig = px.bar(data, x='distrito', y='Coderre', title='Resultados de Coderre por distrito')
fig.update_xaxes(tickangle=45)
fig.show()
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.4.4.3PT.png)

Vamos adicionar o parâmetro `color` e separar os dados por tipo de sistema de eleição:

```
fig = px.bar(data, x='distrito', y='Coderre', color='result', title='Resultados de Coderre por distrito')
fig.update_xaxes(tickangle=45)
fig.show()
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.4.4.4PT.png)

### Estudo autônomo

Abra o Jupyter Notebook em seu computador. Importe `plotly`, habilite o conjunto de dados embutido `data.election` e responda às questões do questionário.

(Para tornar o `plotly` disponível no Jupyter Notebook, escreva `!pip install plotly` em uma célula e depois execute. Agora você já pode começar a trabalhar com o código).

-   Construa um gráfico de barras para o candidato Joly. Qual região registrou a maior quantidade de votos para esse candidato?
-   Construa um histograma para o candidato Joly, reagrupando os dados pelo tipo de vitória. Qual região de cada um desses tipos de sistema de eleição proporcionou a maior quantidade de votos para esse candidato?

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-08-59-181Z.md
### Última modificação: 2025-05-28 20:08:59

# Gráficos de Pizza - TripleTen

Capítulo 4/6

A biblioteca Plotly

# Gráficos de Pizza

Há muita discussão em relação à utilização de gráficos de pizza, já que geralmente as pessoas cometem erros ao construí-los.

Mas eles são insubstituíveis quando precisamos representar proporções. Nós construímos gráficos de pizza utilizando o método **Pie()** com **labels** (as legendas das proporções) e **values** (seus valores).

Vamos importar o conjunto de métodos **graph\_objects**:

```
from plotly import graph_objects as go
```

Agora, vamos construir um gráfico de pizza de pedidos com pedidos de uma rede de restaurantes de acordo com o local:

```
from plotly import graph_objects as go

name_rest = ['Local 1', 'Local 2', 'Local 3', 'Local 4', 'Principal']
values = [18538, 12098, 8353, 15043, 23450]

fig = go.Figure(data=[go.Pie(labels=name_rest, values=values)])
fig.show()
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.4.5PT.png)

À primeira vista já é possível verificar qual foi a contribuição de cada local.

### Estudo autônomo

1.  Construa o gráfico de pizza da lição.
2.  Dê um nome para ele.
3.  Dados para mais um mês acabaram de chegar:
    -   O local 2 construiu um novo salão e expandiu seu menu, pelo que o número de pedidos aumentou 30% desde o último mês.
    -   O restaurante principal foi fechado por duas semanas para uma inspeção feita pelo Ministério da Saúde. O número de pedidos caiu 50% desde o último mês.
    -   O local 3 viu um pequeno aumento nos pedidos—só 3%.
    -   O número de pedidos para os locais 1 e 4 ficou mais ou menos na mesma: 18.650 e 15.100 pedidos, respectivamente.
        
        Construa um gráfico de pizza com esses novos dados.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-09-00-495Z.md
### Última modificação: 2025-05-28 20:09:00

# Gráficos de funis de vendas - TripleTen

Capítulo 4/6

A biblioteca Plotly

# Gráficos de funis de vendas

Você estudou os funis de vendas no curso em Análise de Negócios. Naquela seção, o funil de vendas se parecia com uma tabela. As bibliotecas de plotagem o transformaram em um gráfico visual e interativo.

Nós usamos o método **Funnel()** para construir funis. Seus argumentos são:

-   y: os nomes dos funis de preparação
-   x: o número de usuários de um estágio específico

```
from plotly import graph_objects as go

fig = go.Figure(go.Funnel(
    y = ["Visitantes","Adicionaram um produto ao carrinho", "Iniciaram o check-out", "Pagaram"],
    x = [583, 258,  120, 56]
    ))
fig.show()
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/PT/moved_9.4.6PT.png)

Nós podemos conseguir informações mais detalhadas passando o cursor sobre cada parte do funil:

-   A porcentagem de usuários nessa parte comparada com usuários na primeira parte
-   A porcentagem de usuários nessa parte comparada com usuários na parte anterior
-   A porcentagem de usuários nessa parte comparada com o total de todos os valores.

Nenhum cálculo extra é necessário. Apenas passe o cursor sobre o gráfico.

### Estudo autônomo

Agora, você pode pedir comida do restaurante online. A propaganda trouxe 30.000 visitantes ao site, e 19.589 usuários adicionaram um produto ao seu carrinho. De todos os nossos usuários, 14.694 começaram o processo de checkout. A janela da ordem de pagamento recebeu 12.586 visitantes, 0% deles acabaram clicando no botão Pedido Completo abaixo.

Construa um funil e crie suas próprias hipóteses sobre onde pode estar o problema.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-09-02-356Z.md
### Última modificação: 2025-05-28 20:09:02

# Conclusão - TripleTen

Capítulo 4/6

A biblioteca Plotly

# Conclusão

Você tem muitas ferramentas à sua disposição. O importante é saber qual delas usar.

### Você aprendeu:

-   Como determinar quando uma visualização interativa é necessária e quando não terá utilidade
-   Como construir gráficos básicos em plotty
-   Como fazer gráficos de pizza
-   Como construir funis de vendas

### Guarde isso com você:

Faça o download da [folha conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/Folha_de_Concluses_A_Biblioteca_Plotly_pt.pdf?etag=a96319e039b9c9bc4d69302827273751) e do [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_9/Resumo_do_Captulo_A_Biblioteca_plotly.pdf?etag=d8969959a7ab7a8c37f3761ef2127cc6) para que você possa consultá-los quando necessário.

### Leitura independente _(os materias estão em inglês)_:

[https://datavizcatalogue.com/blog/top-10-charts-in-2019/](https://datavizcatalogue.com/blog/top-10-charts-in-2019/)

[O próximo nível da Visualização de Dados em Python](https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-09-03-637Z.md
### Última modificação: 2025-05-28 20:09:04

# Sprint 10 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 5/6

Projeto do Curso

# Sprint 10 - Projeto

Parabéns! Você concluiu a sessão sobre como relatar uma história usando dados. É hora de aplicar o conhecimento e as habilidades que você adquiriu em um projeto: um estudo de caso analítico da vida real que você concluirá por conta própria.

Quando você terminar o projeto, envie seu trabalho para o revisor do projeto para avaliação. Ele te dará feedback dentro de 48 horas. Use o feedback para fazer alterações e, em seguida, envie a nova versão de volta ao revisor do projeto.

Você pode obter mais feedback sobre a nova versão. Isso é completamente normal. Não é incomum passar por vários ciclos de feedback e revisão.

Seu projeto será considerado concluído assim que o revisor do projeto o aprovar.

## Descrição do Projeto

Você decidiu abrir uma pequena cafeteria com garçons robôs em Los Angeles. É um projeto bem promissor, mas caro, então você e seus parceiros decidem tentar atrair investidores. Eles estão interessados nas condições atuais do mercado e querem saber se você conseguirá manter seu sucesso quando a novidade de garçons robôs esmorecer.

Você é um guru de análise, então seus parceiros te pediram para preparar uma pesquisa do mercado. Você tem dados de código aberto sobre restaurantes em LA.

### Instruções para completar o projeto

**Passo 1. Carregue os dados e prepare-os para a análise**

Carregue os dados sobre restaurantes em LA. Certifique-se de que os tipos de dados para cada coluna estão corretos e não há valores ausentes ou duplicados. Processe-os, se necessário.

Caminho do arquivo: _/datasets/rest\_data\_us\_upd.csv_. [Carregar o conjunto de dados](https://practicum-content.s3.amazonaws.com/datasets/rest_data_us_upd.csv)

**Passo 2. Análise de dados**

-   Investigue as proporções de vários tipos de estabelecimentos. Construa um gráfico.
-   Investigue as proporções de estabelecimentos de rede e não. Construa um gráfico.
-   Qual tipo de estabelecimento é típico para redes?
-   O que caracteriza redes: muitos estabelecimentos com um pequeno número de assentos ou poucos estabelecimentos com muitos assentos?
-   Determine o número médio de assentos para cada tipo de restaurante. Em média, qual tipo de restaurante tem o maior número de assentos? Construa gráficos.
-   Coloque os dados dos nomes das ruas da coluna `address` em uma coluna separada.
-   Construa um gráfico de dez ruas com o maior número de restaurantes.
-   Encontre o número de ruas que têm apenas um restaurante.
-   Para as ruas com muitos restaurantes, olhe para a distribuição de número de assentos. Quais tendências você consegue notar?

Tire uma conclusão geral e apresente recomendações sobre o tipo mais apropriado de restaurante e o número de assentos. Comente sobre a possibilidade de desenvolver uma rede.

**Passo 3. Preparando uma apresentação**

Faça uma apresentação da sua pesquisa e compartilhe-a com os investidores. Você pode usar qualquer ferramenta que você quiser para criá-la, mas você **deve** converter sua apresentação para o formato PDF para avaliação. Inclua um link para a apresentação em uma célula Markdown no seguinte formato:

```
Presentation: <link para um armazenamento em nuvem>
```

Siga as diretrizes de formatação do capítulo "Preparando Apresentações".

**Formato:** Complete a tarefa em um notebook Jupyter. Insira o código nas células 'code' e as explicações de texto nas células 'markdown'. Aplique formatação e cabeçalhos.

### Descrição de dados

Tabela `rest_data`:

-   _object\_name_ — nome de estabelecimento
-   _chain_ — estabelecimento de rede (TRUE/FALSE)
-   _object\_type_ — tipo de estabelecimento
-   _address_ — endereço
-   _number_ — número de assentos

## Como meu projeto será avaliado?

Seu projeto será avaliado com base nos seguintes critérios. Leia-os cuidadosamente antes de iniciar o projeto.

Isso é o que os revisores do projeto procuram ao avaliar seu projeto:

-   Como você prepara dados para análise
-   Que tipos de gráficos você constrói
-   Se você escolhe tipos corretos de gráficos para os dados
-   Se você usa a biblioteca seaborn para trabalhar com gráficos
-   Como você interpreta os gráficos resultantes
-   Como você calcula e interpreta cada parâmetro
-   Se você prepara uma apresentação estruturada
-   Se você segue os princípios para criar apresentações dados neste curso.
-   Como você transmite a mensagem principal da sua apresentação
-   Se você segue a estrutura do projeto e mantém o código organizado
-   As conclusões que você tirou
-   Se você deixa comentários a cada passo

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-299d25f9-ec03-4777-9baf-f0b62a81b995.containerhub.tripleten-services.com/doc/tree/5ced7d00-fbec-46e8-a636-7ca436c5f9b1.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-09-08-707Z.md
### Última modificação: 2025-05-28 20:09:09

# Feedback do Sprint 10 - TripleTen

Capítulo 6/6

Conclusão

# Compartilhe sua opinião

**Olá!**

Você acabou de concluir um sprint e enviar seu projeto. Parabéns por alcançar este marco! 🚀

Agora é o momento perfeito para dar seu feedback sobre o sprint. Sua opinião vai nos ajudar a aprimorar ainda mais o processo de aprendizagem.

Este questionário curto vai levar apenas 2 minutos. Todas as respostas vão ser confidenciais: não vamos compartilhá-las de forma que identifique você.

Agradecemos por nos ajudar a evoluir com você!

![](https://practicum-content.s3.us-west-1.amazonaws.com/common/ok.svg)

Muito obrigado pelas suas respostas!

Já as encaminhamos para os departamentos correspondentes.

7 — 7

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-09-10-050Z.md
### Última modificação: 2025-05-28 20:09:10

# Conclusão - TripleTen

Capítulo 6/6

Conclusão

# Conclusão

Parabéns! Você concluiu a seção "Como Relatar uma História Usando Dados".

**Você aprendeu:**

-   Como tirar conclusões com base em pesquisa
-   Como fazer relatórios de vários tipos para seus clientes
-   Como determinar qual tipo de gráfico é o melhor para seus dados
-   Como criar gráficos informativos sem elementos supérfluos
-   Como usar a biblioteca de visualização de dados seaborn
-   Como escolher uma paleta de cores para um gráfico
-   Como usar a biblioteca de visualização de dados plotly

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-09-11-389Z.md
### Última modificação: 2025-05-28 20:09:11

# Você é um Mestre em Narrativa de Dados! - TripleTen

Capítulo 6/6

Conclusão

# Você é um Mestre em Narrativa de Dados!

A partir de agora você pode desenvolver projetos completamente sozinho, desde a formulação de hipóteses e realização de testes até a criação de apresentações impressionantes de seus resultados. Com essas habilidades dominadas, você agora pode mostrar um portfólio completo aos empregadores, comprovando sua paixão pelos dados e demonstrando seu desejo de crescer profissionalmente.

Dê os próximos passos da sua jornada profissional, atualize sua proposta de valor com as suas novas habilidades e veja o que o mercado reserva para você! E, claro, a melhor maneira de fazê-lo é compartilhar suas conquistas em seu perfil do LinkedIn. É assim que você se torna visível para potenciais colegas e recrutadores!

### Atualize seu perfil do LinkedIn

Você desbloqueou uma nova conquista! Sim, essa é a medalha de Narrador de Dados, e neste ponto você já sabe o que fazer com ela!

Fique à vontade para copiar a medalha e o texto do bloco abaixo para o seu post. Fique a vontade para modificá-lo e tornar o texto mais pessoal!

Não se esqueça de marcar [@TripleTenBrasil](https://www.linkedin.com/school/tripleten-brasil/).

Jonathas Martins da Rocha

Agora eu consigo criar excelentes visualizações de dados, e seaborn e plotly são os meus melhores amigos! #TripleTen #TripleTenBrasil

![](https://practicum-content.s3.amazonaws.com/resources/Data_Storyteller_PT_1687160078.png)

Copiar textoSalvar imagem[Criar um post no LinkdIn](https://www.linkedin.com/feed/)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-09-56-464Z.md
### Última modificação: 2025-05-28 20:09:56

# A Essência de Análise Baseada em Eventos - TripleTen

Capítulo 1/2

Análise Baseada em Eventos

# A Essência de Análise Baseada em Eventos

A **Análise Baseada em Eventos** envolve rastreamento e análise de vários eventos (ações realizadas por usuários) em aplicativos, serviços e outros sistemas digitais.

Um ótimo exemplo de métodos de análise baseada em eventos em ação é decodificação de dados de voo de um gravador de acesso rápido de uma aeronave. Esses dispositivos são parecidos com caixas-pretas que são utilizadas para investigar acidentes e incidentes, mas seu objetivo é aumentar a segurança e a eficiência do avião. Eles gravam unidades simples de dados, como valores de sensores, sons dentro da cabine e as conversas dos membros da equipe. Os dados são transmitidos para um centro de controle que analisa as ações da equipe e os indicadores de desempenho técnico.

Você já está familiarizado com a análise web que, ao contrário da análise baseada em eventos, baseia-se em sessões de usuários (normalmente, visualizações de uma página).

Digamos que estamos trabalhando com uma loja online de produtos para animais. Usuários vão para o site e procuram os produtos de que precisam, enquanto o sistema de análise (por exemplo, Google Analytics) armazena dados sobre visualizações das páginas do site e une-os para criar sessões. A unidade maior aqui será uma visualização de página. Sistemas de análise web podem armazenar os seguintes dados sobre um usuário:

1.  Visualizou a página principal
2.  Foi para a página de pesquisa (o usuário estava procurando alguma coisa; para descobrirmos o que, precisaríamos ver o URL na interface do Metrica)
3.  Acessou a página do produto
4.  Acessou a página de outro produto
5.  Acessou a página de carrinho
6.  Acessou a página de checkout
7.  Acessou a página de pagamento
8.  Acessou a página de finalização do pedido

A precisão necessária de dados depende de sua tarefa. Se você quiser saber a taxa de conversão, análise web será o suficiente. Apenas calcule o número de usuários cujas sessões incluíam visitas para a página de pagamento e divida-o pelo número total de usuários. Mas e se nós precisamos responder a perguntas mais complexas? Tais como estas:

-   A cor de um item afeta a probabilidade de que uma compra será realizada?
-   Quantos usuários desistem de usar o site após falhar em encontrar o produto que eles estavam procurando?

A análise baseada em eventos nos dá uma imagem mais detalhada do comportamento do usuário. Veja como podemos usá-la para descrever o mesmo caminho de um usuário:

1.  Visitou a página principal
2.  Clicou na caixa de pesquisa na página principal
3.  Procurou "ração para gato"
4.  Visualizou a página de Catterbury Tails Cat Food
5.  Olhou para imagens ampliadas de Catterbury Tails Cat Food
6.  Clicou em Divine Feline Cat Food na seção de itens recomendados
7.  Visualizou a página de Divine Feline Cat Food
8.  Olhou para imagens ampliadas de Divine Feline Cat Food
9.  Alterou o tamanho de pacote de Divine Feline Cat Food de 9 a 12 kg
10.  Adicionou o produto ao carrinho
11.  Clicou no botão Carrinho
12.  Visualizou a página do Carrinho
13.  Clicou no link de Entrega e examinou as opções na janela pop-up
14.  Clicou no botão Checkout
15.  Visualizou a página de Checkout
16.  Preencheu um certo número de campos (podem existir muitos e cada campo preenchido pode ser rastreado como um evento separado)
17.  Clicou o botão Pagar
18.  Pagou pelo produto na página de Pagamento
19.  Visualizou a página de finalização do pedido

Se nós focamos no comportamento do usuário, em vez de visualizações de páginas, podemos tirar muito mais conclusões. Mais do que isso, podemos criar ou até mesmo provar hipóteses relacionadas a visualizações de páginas inteiras e suas seções separadas (produtos semelhantes, opções de cores etc.).

Sistemas de análise web têm opções embutidas para rastrear tipos populares de eventos (um usuário realizou uma pesquisa ou adicionou um produto ao carrinho). Também é possível adicionar eventos criando manualmente suas condições.

No entanto, sistemas como esses ainda são baseados em sessões de usuários, em vez de análise detalhada de seu comportamento. Isso não significa que sistemas de análise web são piores que os de análise baseada em eventos; eles apenas têm diferentes objetivos. Análise web trata-se de origens de tráfego, enquanto análise baseada em eventos se preocupa com comportamento. Se você tiver um grande volume de informações estatísticas, você pode tirar conclusões inesperadas examinando ações de muitos usuários que até poderiam parecer triviais. Esse tipo de análise é a função principal de sistemas baseados em eventos.

Análise baseada em eventos é amplamente utilizada com aplicativos móveis, cujos usuários realizam muitas ações pequenas - apenas pense em quantas pequenas coisas você faz no Instagram ou Facebook! Você provavelmente nem as repara, já que elas se misturam perfeitamente em sua experiência. Cada uma dessas ações relata muito sobre seu comportamento. Ao usar essa informação, o aplicativo "está aprendendo" a te oferecer conteúdo e anúncios relevantes.

Você pode rastrear eventos para qualquer produto digital que tenha uma forma complexa de interação de usuários. Peguemos o exemplo de serviços web ou telas interativas em museus. Quando clientes "se comunicam" com um produto através de uma interface, análise baseada em eventos é uma forma de coletar dados sobre essa interação. Esses dados podem então ser usados para tornar o produto mais personalizado.

Pergunta

Qual das opções abaixo descreve o processo de assinatura de um guia online do Texas Hold'em em termos análise baseada em eventos?

Visualizou a página principal > Visualizou a página "Preços" > Visualizou a página "Regras do Jogo" > Visualizou a página "Vídeos Tutoriais" > Visualizou a página "Assinar" > Visualizou a página "Pagar sua assinatura" > Visualizou a página "Seu pagamento foi bem-sucedido"

Visualizou a página principal > Reproduziu um vídeo na página principal > Acesse a página "Tarifas" > Abriu o bloco da página "Tarifas" com detalhes do plano básico > Acessou a página "Regras do jogo" > Acessou a página " Página de vídeos tutoriais" > Reproduziu o vídeo "As melhores estratégias de Texas Hold'em" na página "Vídeos tutoriais" > Reproduziu 20 segundos do vídeo e o interrompeu > Foi para a página "Inscrever-se" > Preencheu o campo "E-mail" > Escolha a opção "Pagar com MasterCard" > Acessou a página "Pagar assinatura" > Pague com sucesso o plano básico > Acesse a página "Seu pagamento foi efetuado com sucesso"

Correto! Aqui há muitos eventos, mas nem todos eles levaram a visualizações de outras páginas (por exemplo, reprodução de vídeos). Esse fã de poker precisou de apenas 20 segundos para decidir que o guia vale o dinheiro.

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-09-57-756Z.md
### Última modificação: 2025-05-28 20:09:58

# Como Eventos São Rastreados - TripleTen

Capítulo 1/2

Análise Baseada em Eventos

# Como Eventos São Rastreados

Antes de podermos começar a analisar eventos de usuários, precisamos saber onde iremos gravá-los. Existem vários serviços para rastrear eventos, como [AppMetrica](https://appmetrica.yandex.ru/), [Amplitude](https://amplitude.com/) e [MixPanel](https://mixpanel.com/) _(os materiais estão em inglês)_. Algumas empresas criam seus próprios sistemas. Nesta lição, focaremos em como gravar eventos, em vez de onde fazê-lo.

Como esses eventos se parecem?

Digamos que temos um jogo chamado MiniMarket 3000 onde jogadores criam pequenas lojas online e competem entre si para obter o maior lucro. O jogador GoodLucker2083 executa a ação "Aquisição de 20 kg de beterraba do armazém de legumes do jogador Artichoke23 por 500 moedas." Como essa ação será refletida em um sistema de análise baseada em eventos?

Um evento pode ser representado como um dicionário onde a chave é o parâmetro do evento e o valor é (surpresa!) o valor deste parâmetro.

Eventos têm parâmetros primários ou básicos que podem ser usados para uma análise básica. Para um evento em MiniMarket 3000, os parâmetros terão o seguinte aspecto:

```
{'id': 4242618, # Identificador do evento
'event_datetime': '2019-10-02 14:25:34', # Data e hora do evento
'event_type': 'in_game_purchase', # Tipo do evento - compra dentro do jogo 
'user_id': 129540 # Identificador do jogador
}
```

Esses parâmetros são suficientes para responder às seguintes perguntas bastante simples:

-   Quantos jogadores fazem compras dentro do jogo? Dica: você precisa calcular o número de `user_id unívocos` onde `event_type` = `in_game_purchase`.
-   Quais são as primeiras 10 ações que usuários executam com maior frequência? Dica: para cada usuário, selecione os primeiros 10 eventos, junte-os em event\_type e então conte o número de eventos de cada tipo.

Mas e se quisermos saber como o tamanho médio de uma compra dentro do jogo varia ao longo do tempo? Não podemos obter informações desse tipo usando os parâmetros básicos, então precisamos de parâmetros adicionais! Vamos listar os parâmetros que caracterizam uma compra:

```
{'purchase_amount': 20, # O jogador comprou 20 unidades de um produto no jogo
'purchase_product': 'beetroot', # Que produto o jogador comprou
'purchase_price_per_unit': 25, # O preço por item, em moedas
'purchase_total_cost': 500 # O custo total da compra dentro do jogo
}
```

Esses dados serão suficientes para responder às seguintes perguntas relacionadas a compras dentro do jogo:

-   Que produtos do jogo os jogadores compram com maior frequência? Dica: você precisa selecionar o produto `purchase_product` comprado com maior frequência.
-   Que produto do jogo é o mais caro?Dica: selecione o evento para o qual o preço de compra purchase\_price\_per\_unit é o maior.

Então, o evento de uma compra tem o seguinte aspecto:

```
{'id': 4242618, # Identificador do evento
'event_datetime': '2019-10-02 14:25:34',# Data e hora do evento
'event_type': 'in_game_purchase', # Tipo do evento - compra dentro do jogo
'user_id': 129540, # Identificador do jogador
'purchase_amount': 20, # O jogador comprou 20 unidades
'purchase_product': 'topinambur', # Que produto o jogador comprou
'purchase_price_per_unit': 25, # O preço por unidade, em moedas
'purchase_total_cost': 500 # O custo total da compra
}
```

Cada nova hipótese expande a lista dos parâmetros que precisam ser rastreados. Provavelmente, o analista de MiniMarket 3000 precisa responder às seguintes perguntas:

-   Como o comportamento dos usuários mudou após a atualização do jogo? Dica: você também precisará rastrear a versão do aplicativo para vários eventos.
-   Quais jogadores vendem itens para outros jogadores com maior frequência? Dica: você também precisará rastrear os IDs dos jogadores que vendem itens dentro do jogo.
-   Que percentagem de suas poupanças no jogo os jogadores estão dispostos a gastar com uma compra? Dica: você precisa registrar o saldo do pagador no momento de cada evento. Isso permite calcular a que percentagem da poupança do jogador a compra corresponde.

Ao desenvolver um sistema de rastreamento de eventos, o analista deve projetar uma estrutura de parâmetros que considere todos os principais cenários do uso do aplicativo ou serviço e permita testar um certo número de hipóteses analíticas.

Para fazer isso, o analista identifica as principais jornadas do usuário e ações sugeridas pelas mecânicas do aplicativo e então define um **plano de rastreamento**. Isso é uma lista de eventos discriminados por categoria, junto com descrições dos parâmetros que são registrados para cada evento. Aqui está como uma parte do plano de rastreamento para MiniMarket 3000 pode ficar:

[Tracking\_plan\_sample\_us.xlsx](https://practicum-content.s3.us-west-1.amazonaws.com/data-analyst-eng/moved_Tracking_plan_sample_us.xlsx)

Vamos dar uma olhada no plano de rastreamento:

-   Cada evento tem uma breve descrição e um nome. Esse é o nome do evento que depois será guardado no parâmetro `event_type`.
-   Recomendamos preencher o campo "Why" (porquê). Indicar precisamente o objetivo te ajuda a evitar registrar eventos desnecessários.
-   Cada propriedade é descrita separadamente. Indique os valores que ela pode ter.
-   Você não precisa identificar parâmetros para cada evento. Você pode guardá-los em um documento separado.
-   Os campos "Location" (localização) e "Conditions" (condições) ajudarão os desenvolvedores a decidir em que momento seria melhor registrar um determinado evento.

Rastrear planos e corrigir estruturas de eventos são passos fundamentais de uma boa análise baseada em eventos. Qual o sentido de uma análise que não coleta dados úteis para o negócio?

Pergunta

Escolhe um sistema de rastreamento de eventos que irá ajudar a determinar como o número de compras dentro do jogo por usuário varia ao longo do tempo.

{'id': 5892, 'event\_type': 'in\_game\_purchase', 'purchase\_amount': 250}

{'id': 5892, 'event\_type: 'in\_game\_purchase', 'user\_id': 18934, 'purchase\_amount': 250}

{'id': 5892, 'event\_datetime': '2019-10-01 22:12:34', 'event\_type: 'in\_game\_purchase', 'user\_id': 18934}

Certo! E não se preocupe que você não tem o custo das compras dentro do jogo, purchase\_amount.Você quer saber o número de compras, e não quanto elas custam. Este conjunto de parâmetros te dá tudo o que precisamos para descobrir isso.

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-10-00-104Z.md
### Última modificação: 2025-05-28 20:10:00

# Métodos de Análise Baseada em Eventos - TripleTen

Capítulo 1/2

Análise Baseada em Eventos

# Métodos de Análise Baseada em Eventos

Coletar eventos é apenas o primeiro passo. Agora precisamos analisá-los!

Você frequentemente precisará analisar funis de eventos que têm semelhanças com os funis de marketing e de produtos que você já conhece. Em análise baseada em eventos, um funil é uma determinada série de ações que usuários devem efetuar em um site, jogo ou aplicativo para alcançar um certo resultado.

Funis para lojas online são bastante simples: você não pode fazer checkout até que adicione um produto ao carrinho e você não pode realmente comprar algo sem concluir o processo de checkout.

Aplicativos móveis também têm funis. Digamos que haja um aplicativo chamado Kittygram. Aqui usuários podem publicar fotos de seus gatos e ver fotos de gatos de outros usuários. Vamos construir um funil:

-   O usuário baixa o aplicativo
-   O usuário se registra
-   O usuário preenche seu perfil
-   O usuário realiza sua primeira ação significativa ao publicar sua primeira foto ou começar a seguir outro usuário
    
    Kittygram rastreia várias métricas importantes:
    
-   A proporção de usuários registrados para o número total de pessoas que baixam o aplicativo
-   A proporção de usuários registrados que preenchem seus perfis para todos os usuários registrados
-   A proporção de usuários que realizam sua primeira ação significativa (postam uma foto ou começam a seguir outro usuário) para o número de usuários que preenchem seus perfis

A última etapa desse funil tem duas ramificações. Seria interessante descobrir quantos usuários primeiro começam a seguir outros usuários e quantos primeiro postam fotos de gatos. Isso poderia potencialmente revelar a existência de dois tipos de usuários: criadores de conteúdo e consumidores de conteúdo.

Mais uma métrica importante é o período de tempo entre eventos. Por exemplo, nós sabemos que o tempo médio até a primeira publicação é sete dias. Se quisermos que esse período seja mais curto, pode ser uma boa ideia enviar notificações push lembrando os usuários de que eles podem compartilhar fotos de gatos no aplicativo. Mas tenha cuidado para não enviar muitas notificações, ou os usuários irão desativar notificações e seu plano irá por água abaixo.

É importante não apenas calcular métricas para avaliar o progresso pelo funil (taxas de conversão de etapa para etapa), mas também rastrear aqueles usuários que não chegaram a uma determinada etapa. Pode haver uma classe inteira de usuários que não seguem outras contas ou não publicam nada. Você deve estar curioso para saber o que eles estão fazendo no aplicativo. Talvez estejam admirando secretamente gatos de outras pessoas?

Pergunta

Qual das seguintes sequências de eventos não representa o funil correto de eventos para um aplicativo de notas?

Registro > Início do tutorial > Final do tutorial/Pulando o tutorial > A primeira nota

Registro > A primeira nota > Compartilhando a nota com outro usuário

Registro > A primeira nota > Favoritando uma nota > Escrevendo um comentário sobre o aplicativo

Correto, essa sequência de eventos dificilmente pode ser chamada de um funil de eventos. Os eventos aqui não são elementos de um encadeamento lógico, e os eventos 3 e 4 poderiam facilmente acontecer em uma ordem reversa (para escrever um comentário, um usuário não tem que criar ou favoritar uma nota).

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-10-01-415Z.md
### Última modificação: 2025-05-28 20:10:01

# O Momento "Aha!" - TripleTen

Capítulo 1/2

Análise Baseada em Eventos

# O Momento "Aha!"

Um dos objetivos de análise baseada em eventos é identificar o momento "aha!". É o momento quando um usuário vê claramente o valor de um aplicativo ou serviço particular e se torna fiel a ele. Tais usuários são mais propensos a usar o serviço regularmente e pagar por ele. Dizem que o momento "aha!" para os usuários do Facebook acontece quando eles adicionam sete amigos dentro de 10 dias.

A identificação do momento "aha!" está intimamente ligada a taxa de retenção que pode revelar o desenvolvimento da lealdade dos clientes. Você se lembra de ter encontrado o número dos clientes que ficaram viciados em café quando você estava estudando coortes comportamentais?

No Kittygram o momento "aha!" poderia acontecer quando o usuário receber a 31ª curtida ou começar a seguir o 13º gato. Você precisaria fazer uma pesquisa para encontrar os números exatos.

Análise baseada em eventos também é útil em testes A/B; ela te permite a avaliar mudanças do comportamento dos usuários causadas por alterações feitas a seu aplicativo.

Análise baseada em eventos também pode ser utilizada para fazer previsões sobre a perda de usuários. Por exemplo, quando você estiver analisando dados sobre os usuários que pararam de usar um aplicativo ou serviço, você pode construir um modelo matemático para identificar o momento quando eles decidiram a deixar de usá-lo.

Então você pode comparar o comportamento dos usuários que param e dos que permanecem. Provavelmente, se um usuário está gastando menos e menos tempo no aplicativo e realizando menos ações significativas, isso pode ser um sinal de que ele vai parar de usar o aplicativo em breve. Analistas desenvolvem modelos de aprendizado de máquina para fazer previsões desse tipo.

Análise baseada em eventos é uma ferramenta poderosa para avaliar o comportamento dos usuários. Ela torna possível melhorar a experiência de utilização de um aplicativo ou serviço e elevar métricas de negócio.

Pergunta

Qual das seguintes opções se parece mais com um momento "aha!" para um serviço de delivery de sorvete?

O usuário faz seu primeiro pedido de sorvete e escreve um comentário sobre o serviço

O usuário coloca seu endereço de entrega no aplicativo

O usuário fez três pedidos de sorvete durante um mês

Isso definitivamente se parece com um momento "aha!" Podemos ver que o cliente usava o serviço várias vezes durante um curto período de tempo. Isso significa que existe uma alta probabilidade de ele se tornar leal, pelo menos por um certo período de tempo. E também pode significar algumas espinhas.

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-10-02-727Z.md
### Última modificação: 2025-05-28 20:10:03

# Conclusão - TripleTen

Capítulo 1/2

Análise Baseada em Eventos

# Conclusão

Análise baseada em eventos pode ser estranhamente divertida. Você aprendeu muito nesta lição:

-   O que são eventos e análise baseada em eventos
-   Como eventos são rastreados
-   A estrutura de um plano de rastreamento
-   Que métodos são usados para analisar eventos

### Leitura adicional _(os materiais estão em inglês)_:

[https://segment.com/academy/collecting-data/how-to-create-a-tracking-plan/](https://segment.com/academy/collecting-data/how-to-create-a-tracking-plan/)

[https://help.amplitude.com/hc/en-us/articles/115000465251-Data-Taxonomy-Playbook](https://help.amplitude.com/hc/en-us/articles/115000465251-Data-Taxonomy-Playbook)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-10-04-950Z.md
### Última modificação: 2025-05-28 20:10:05

# Sprint 11 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 2/2

Materiais para revisão

# Sprint 11 - Projeto

Você trabalha em uma startup que vende produtos alimentícios. Você precisa analisar o comportamento do usuário para o aplicativo da empresa.

Primeiro, estude o funil de vendas. Descubra como os usuários chegam à etapa de compra. usuários realmente chegam a essa etapa? Quantos ficam presos nas fases anteriores? Quais etapas em particular?

Em seguida, veja os resultados do teste A/A/B. (Continue lendo para obter mais informação sobre os testes A/A/B). Os designers gostariam de alterar as fontes de todo o aplicativo, mas os gerentes temem que os usuários achem o novo design intimidador. Eles decidem tomar a decisão com base nos resultados de um teste A/A/B.

Os usuários são divididos em três grupos: dois grupos de controle recebem as fontes antigas e um grupo de teste recebe as novas. Descubra qual conjunto de fontes produz melhores resultados.

A criação de dois grupos A tem certas vantagens. Podemos adaptar um princípio segundo o qual só estaremos confiantes na precisão de nossos testes quando os dois grupos de controle forem semelhantes. Se houver diferenças significativas entre os grupos A, isso pode nos ajudar a descobrir fatores que podem distorcer os resultados. A comparação de grupos de controle também nos informa de quanto tempo e dados precisaremos ao executar outros testes.

Você usará o mesmo conjunto de dados para análise geral e análise A/A/B. Em projetos reais, os experimentos se realizam constantemente. Os analistas estudam a qualidade de um aplicativo usando dados gerais, sem prestar atenção na participação dos usuários em experimentos.

### Descrição dos dados

Cada linha do registro (também referido como log) é uma ação do usuário ou um evento.

-   `EventName` — nome do evento
-   `DeviceIDHash` — identificador de usuário exclusivo
-   `EventTimestamp` — hora do evento
-   `ExpId` — número do experimento: 246 e 247 são os grupos de controle, 248 é o grupo de teste

### Instruções para completar o projeto

**Passo 1. Abra o arquivo de dados e leia a informação geral**

Caminho do arquivo:_`/datasets/logs_exp_us.csv`_ [Baixe o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/logs_exp_us.csv)

**Passo 2. Prepare os dados para análise**

-   Renomeie as colunas de uma maneira que seja conveniente para você
-   Verifique se há valores ausentes e tipos de dados. Corrija os dados, se necessário
-   Adicione uma coluna de data e hora e uma coluna separada para datas

**Passo 3. Estude e verifique os dados**

-   Quantos eventos ficam nos registros?
-   Quantos usuários ficam nos registros?
-   Qual é o número médio de eventos por usuário?
-   Qual é o período de tempo que os dados cobrem? Encontre as datas máxima e mínima. Desenhe um histograma por data e hora. Você pode ter certeza de que possui os dados igualmente completos para todo o período? Os eventos mais antigos podem acabar aparecendo nos diários de alguns usuários por motivos técnicos, e isso pode distorcer o quadro geral. Encontre o momento em que os dados começam a ser completos e ignore a seção anterior. Qual período os dados realmente representam?
-   Você perdeu muitos eventos e usuários ao excluir os dados mais antigos?
-   Certifique-se de ter usuários de todos os três grupos experimentais.

**Passo 4. Estude o funil de eventos**

-   Veja quais eventos estão nos diários e sua frequência de ocorrência. Classifique-os por frequência.
-   Encontre o número de usuários que executaram cada uma dessas ações. Ordene os eventos pelo número de usuários. Calcule a proporção de usuários que executaram a ação pelo menos uma vez.
-   Em que ordem você acha que as ações ocorreram? Todos elas fazem parte de uma única sequência? Você não precisa levá-las em consideração ao calcular o funil.
-   Use o funil de eventos para encontrar a parcela de usuários que passam de uma etapa para a próxima (por exemplo, para a sequência de eventos A → B → C, calcule a proporção de usuários na etapa B para o número de usuários na etapa A e a proporção de usuários na etapa C para o número na etapa B).
-   Em qual fase você perde mais usuários?
-   Qual é a parcela de usuários que faz o caminho inteiro, desde o primeiro evento até o pagamento?

**Passo 5. Estude os resultados do experimento**

-   Quantos usuários há em cada grupo?
-   Temos dois grupos de controle no teste A/A, no qual verificamos nossos mecanismos e cálculos. Veja se há uma diferença estatisticamente significativa entre as amostragens 246 e 247.
-   Selecione o evento mais popular. Em cada um dos grupos de controle, encontre o número de usuários que realizaram essa ação. Encontre a proporção deles. Verifique se a diferença entre os grupos é estatisticamente significativa. Repita o procedimento para todos os outros eventos (economizará tempo se você criar uma função especial para este teste). Você pode confirmar se os grupos foram divididos corretamente?
-   Faça a mesma coisa para o grupo com fontes alteradas. Compare os resultados com os de cada um dos grupos de controle para cada evento isoladamente. Compare os resultados com os resultados combinados para os grupos de controle. Quais conclusões você pode tirar do experimento?
-   Qual nível de significância você definiu para testar as hipóteses estatísticas mencionadas acima? Calcule quantos testes de hipóteses estatísticas você realizou. Com um nível de significância estatística de 0,1, um de cada 10 resultados pode ser falso. Qual deveria ser o nível de significância? Se você quiser alterá-lo, execute as etapas anteriores novamente e verifique suas conclusões.

Ao concluir este projeto, você irá provar suas habilidades:

![](https://practicum-content.s3.amazonaws.com/resources/Pre-processamento_de_Dados_1713356126.png)

![](https://practicum-content.s3.amazonaws.com/resources/Analise_Exploratoria_1713356139.png)

![](https://practicum-content.s3.amazonaws.com/resources/Analise_Estatistica_1713356150.png)

![](https://practicum-content.s3.amazonaws.com/resources/Analise_de_Negocio_1713356270.png)

![](https://practicum-content.s3.amazonaws.com/resources/Narrativa_de_Dados_1713356279.png)

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-cb7b95e8-e437-412c-a4b3-412a343692a1.containerhub.tripleten-services.com/doc/tree/e3e80813-c474-4f51-b738-53523a6e5fdd.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-10-08-938Z.md
### Última modificação: 2025-05-28 20:10:09

# Feedback do Sprint 11 - TripleTen

Capítulo 2/2

Materiais para revisão

# Compartilhe sua opinião

**Olá!**

Você acabou de concluir um sprint e enviar seu projeto. Parabéns por alcançar este marco! 🚀

Agora é o momento perfeito para dar seu feedback sobre o sprint. Sua opinião vai nos ajudar a aprimorar ainda mais o processo de aprendizagem.

Este questionário curto vai levar apenas 2 minutos. Todas as respostas vão ser confidenciais: não vamos compartilhá-las de forma que identifique você.

Agradecemos por nos ajudar a evoluir com você!

![](https://practicum-content.s3.us-west-1.amazonaws.com/common/ok.svg)

Muito obrigado pelas suas respostas!

Já as encaminhamos para os departamentos correspondentes.

7 — 7

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-10-10-241Z.md
### Última modificação: 2025-05-28 20:10:10

# Preparação de Carreira 80% - TripleTen

Capítulo 2/2

Materiais para revisão

# Preparação de Carreira 80%

Olá!

Sabemos que seguir com toda a agenda do curso pode ser desafiador e talvez, **você não tenha aderido ao CPC** - Curso de Preparação de Carreira anteriormente. Não se preocupe!

O **CPC, Curso de Preparação de Carreira,** foi desenvolvido com muito empenho pensando em cada etapa da jornada de desenvolvimento de habilidades que te orientem e guiam até atingir seus objetivos de carreira.

O curso **irá guiá-lo desde a construção da estratégia de funil para entender área e sub áreas de atuação, até termos seu kit de empregabilidade desenvolvido nas melhores práticas para área tech.**

O programa tem duração de 2 **semanas** e aborda os seguintes temas:

### **Transição de carreira: passos a seguir**

### **Estratégia de funil**

### **A importância da sua rede de contatos**

### **Github e o seu portifólio;**

### **Como montar um currículo;**

### **Carta de apresentação**

### **Linkedin**

Em cada etapa você terá todo o conteúdo necessário para se sentir ainda mais confiante. **E nunca estará sozinho!**

Abaixo, você encontra um formulário para que a área de carreira, possa conhecer você melhor e entender seus objetivos.

Preencha-o caso queria de fato, adentrar ao curso de preparação de carreira.

[https://forms.gle/6r8DLkfLHsM31RqZ6](https://forms.gle/6r8DLkfLHsM31RqZ6)

Após o preenchimento, dentro de alguns dias você será adicionado ao canal referente ao curso no Discord.

O link para o curso e demais instruções estará neste canal.

Bons estudos!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-11-53-837Z.md
### Última modificação: 2025-05-28 20:11:54

# Introdução - TripleTen

Capítulo 1/6

Introduções à Automação

# Introdução

Neste capítulo, você descobrirá o que são dashboards e como criá-los.

### O que você irá aprender:

-   Como coletar requerimentos dos seus clientes para dashboards
-   Como construir os gráficos mais comuns com a biblioteca Dash
-   Como adicionar elementos interativos ao seu dashboard
-   Como projetar um dashboard com HTML
-   Como definir a lógica de um dashboard

Neste sprint, você adicionará ao seu conjunto de habilidades uma habilidade muito importante para um analista de dados:

![](https://practicum-content.s3.amazonaws.com/resources/Dashboards_1713356373.png)

### Quanto tempo irá demorar:

_8 lições, aproximadamente de 20 a 25 minutos cada_

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-11-55-123Z.md
### Última modificação: 2025-05-28 20:11:56

# Introdução - TripleTen

Capítulo 2/6

Pipelines de Dados e Para Que Servem

# Introdução

Neste capítulo você vai aprender o que são pipeline de dados e como eles são construídos.

### O que você irá aprender:

-   Como pipelines funcionam
-   Como escrever um script pipeline

### Quanto tempo irá demorar:

_2 lições, aproximadamente de 15 a 20 minutos cada_

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-11-56-429Z.md
### Última modificação: 2025-05-28 20:11:57

# Automatizando Dashboards com Dados Pipelines - TripleTen

Capítulo 2/6

Pipelines de Dados e Para Que Servem

# Automatizando Dashboards com Dados Pipelines

**Pipeline de dados** são a fundação para automação. Eles são programas especiais que são chamados de acordo com cronogramas. Eles coletam, juntam, transformam e armazenam dados automaticamente.

Pipelines também tornam possível:

-   Analisar dados na internet e armazenar os resultados em um banco de dados
-   Coletar informações sobre visitas e compras de vários usuários de sistemas corporativos e fazer relatórios para futuras análises de coorte
-   Detectar anomalias no comportamento do usuário
-   Analisar teste A/B
-   Mandar toneladas de spam para seus colegas de equipe sobre crescimento em vendas e visitas

E tudo isso sem nenhum ser humano precisar levantar um dedo! Apenas dedos de robôs.

Os estágios de preparação de um pipeline podem ser descritos com o acrônimo **ETL**: extract, transform, load (extrair, transformar, carregar).

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.2.2PT_1658736559.png)

Na etapa de extração, pipelines coletam dados de várias fontes, tais como sites, bases de dados da companhia e seus parceiros, e APIs externos.

Na etapa de transformação, os dados coletados são padronizados. Por exemplo, textos podem ser convertidos para números ou datas. E depois são categorizados. Após a categorização, os dados são transformados, ou convertidos para o formato conveniente para a construção de relatórios ou armazenamento de informações agregadas. Por exemplo, nessa fase dados sobre visitas e vendas podem ser transformados em tabelas, onde o LTV é calculado para uma eventual análise de coorte.

Na última etapa de armazenamento, pipelines salvam dados agregados em tabelas de bancos de dados, fazem relatórios e emails em massa.

Pergunta

Henry é um jovem analista que quer comprar um carro usado. Para evitar pagar demais, ele decidiu coletar todas as ofertas na seção de veículos dos sites populares de classificados, e escolheu a melhor opção em termos da relação preço- qualidade. Sua colega de equipe Bella é mais experiente em análise. Ela sugeriu que ele processe os dados com um Super ETL e depois tire umas férias. Henry não sabe exatamente como definir o programa corretamente, apesar de ele estar ciente dos passos que ele precisa seguir:

-   Baixar páginas HTML com propagandas
-   Extrair marcas de carros, preços e números de telefone de vendedores dos layouts HTML
-   Armazenar esses três parâmetros no banco de dados

Ajude Henry a se tornar proprietário de um carro.

Conectar-se com os sites de classificados (Load), obter as informações necessária das propagandas (Transform), armazenar os dados em uma base de dados (Extract).

Conectar-se com sites de classificados (Extract), obter as informações necessárias das propagandas (Transform), armazenar os dados no banco de dados (Load).

Isso mesmo!! Henry já consegue até escutar o ronco do motor.

Conectar-se com sites de classificados (Transform), obter as informações necessárias das propagandas (Load), armazene os dados no banco de dados (Extract).

Fantástico!

Pipelines de dados, são projetados de forma que todas as suas etapas podem ser efetuadas repetidas vezes, produzindo resultados consistentes. Entretanto, sistemas que servem como fontes de dados podem apresentar falhas. Se isso ocorrer, o pipeline não terá dados completos. É isso porque cada programa dentro do pipeline deve ser capaz de coletar dados não apenas na data atual, mas também para as anteriores. Isso torna possível recomeçar scripts de pipelines depois que os sites forem consertados a fim de coletar os dados para os dias em que houve problemas.

Por exemplo, a analista Annie escreveu um pipeline que coleta dados do Google Analytics a cada hora. Ele funciona perfeitamente bem, mas três meses depois problemas internos interferem: o centro empresarial onde ela trabalha ficou sem energia elétrica. Demora uma semana para resolver o problema, então Annie acaba tendo que rodar o script por 24 horas durante 7 dias para compensar as horas perdidas. Mas a longo prazo, não há nenhum prejuízo; ela consegue completar os dados.

No mundo real, pipelines são bibliotecas prontas como **[Luigi](https://github.com/spotify/luigi)**(os materiais estão em inglês) ou [**Bubbles**](https://github.com/stiivi/bubbles) (os materiais estão em inglês). Elas deixam você construir sistemas pipelines inteiros que coletam dados de uma infinidade de fontes, com dependência mútua e uma programação complexa. Nós não vamos olhar para elas nessa seção; ao invés disso, nós vamos estudar o básico de construção de pipelines. Isso irá te preparar para trabalhar com sistemas mais complexos no futuro.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-11-58-728Z.md
### Última modificação: 2025-05-28 20:11:59

# Criando um Script de Pipeline - TripleTen

Capítulo 2/6

Pipelines de Dados e Para Que Servem

Opcional

# Criando um Script de Pipeline

Você sempre tem a opção de receber arquivos de despejo dos seus colegas de equipe, mas é sempre bom tentar ter acesso direto a bancos de dados. Por quê?

-   Isso vai agilizar seu trabalho, já que você não terá que esperar até que um arquivo de despejo esteja pronto.
-   Automação é impossível sem acesso direto aos dados. Se seus colegas de equipe ficarem doentes ou saírem de férias sem criar um arquivo de despejo para você, o processo de automação vai falhar.
-   Você pode encontrar muitas coisas interessantes nos bancos de dados. A biblioteca **SQLAlchemy** permite que você leia os dados de tabelas de bancos de dados dentro de DataFrames do Pandas e armazene dados do Pandas em um conjunto de dados com apenas um comando. Para aprender como instalar o SQLAlchemy, veja o curso opcional ("Trabalhar com o SGBD Postgres a partir de um script Python").

Por enquanto, vamos ver como isso funciona.

Vamos conectar-nos ao banco de dados usando `sqlalchemy`. Vamos criar um script em Sublime Text:

```
#!/usr/bin/python

# Importar bibliotecas
import pandas as pd
from sqlalchemy import create_engine

# Definir parâmetros para conectar-se ao banco de dados;
# você pode solicitá-los ao administrador do banco de dados.
db_config = {'user': 'my_user',         # nome de usuário
             'pwd': 'my_user_password', # senha
             'host': 'localhost',       # endereço do servidor
             'port': 5432,              # porta de conexão
             'db': 'games'}             # nome do banco de dados

# Criar a string de conexão do banco de dados. 
connection_string = 'postgresql://{}:{}@{}:{}/{}'.format(db_config['user'],
                                                                     db_config['pwd'],
                                                                       db_config['host'],
                                                                       db_config['port'],
                                                                       db_config['db'])
# Conectar-se ao banco de dados.
engine = create_engine(connection_string)

# Criar uma consulta SQL.
query = ''' SELECT game_id, name, platform, year_of_release
            FROM data_raw
        '''

# Executar a consulta e armazenar o resultado
# no DataFrame.
# O SQLAlchemy vai nomear automaticamente as colunas
# com o mesmo nome que elas são chamadas na tabela do banco de dados. Vamos ter apenas que
# especificar a coluna do índice usando index_col.
data_raw = pd.io.sql.read_sql(query, con = engine, index_col = 'game_id')

print(data_raw.head(5))
```

Vamos estudar o código em detalhes. Para conectar-se a um banco de dados, você precisa especificar:

-   um SGBD
-   a localização do banco de dados: o endereço de IP do servidor onde o banco de dados está implementado, e a porta a ser conectada
-   Nome de usuário e senha para a conexão
-   O nome do banco de dados que você precisa (um servidor pode armazenar vários bancos de dados)

Normalmente, esses parâmetros são agrupados como uma string de conexão. Dentro do código, essa string é como uma hidra com muitas cabeças responsável por gerar os parâmetros:

```
connection_string = 'postgresql://{}:{}@{}:{}/{}'.format(db_config['user'],
                                                                     db_config['pwd'],
                                                                       db_config['host'],
                                                                       db_config['port'],
                                                                       db_config['db'])
```

Este é o resultado:

```
connection_string = 'postgresql://my_user:my_user_password@localhost:5432/games'
```

Vamos traduzir esse comando para inglês: "Connect to PostgreSQL DBMS on the server _localhost_ through port 5432 (Conectar-se ao SGBD PostgreSQL no servidor _localhost_ pela porta 5432). Entre no banco de dados `games` com o nome de usuário `my_user`(a senha é `my_user_password`)." A construção `'postgresql://{}:{}@{}:{}/{}'.format(...)` armazena os parâmetros para conectar-se ao banco de dados no dicionário `db_config`. Isso será útil se você quiser armazenar o conteúdo do dicionário em um arquivo separado de configuração, assim você não precisa escrever isso manualmente em cada novo script de automação.

Vamos automatizar a conexão ao banco de dados usando a função **create\_engine()** de `sqlalchemy`. Vamos passar a essa função a string de conexão:

```
engine = create_engine(connection_string)
```

Agora vamos fazer uma consulta SQL. As aspas `'''` nos vão permitir organizar nossa consulta ordenadamente e dividi-la em várias linhas para torná-la legível:

```
query = ''' SELECT game_id, name, platform, year_of_release
            FROM data_raw
        '''
```

No final do script, vamos chamar o comando para executar a consulta:

```
data_raw = pd.io.sql.read_sql(query, con = engine, index_col = 'game_id')
```

Aqui, `pd.io.sql.read_sql` é uma função do Pandas que recebe a conexão ao banco de dados `con = engine`, executa a `query` (consulta) dentro desse banco de dados e armazena o DataFrame resultante em `data_raw`. Definimos a coluna `game_id` como índices do DataFrame.

Temos informações básicas sobre cinco jogos:

![](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/2.3.a.png)

Vamos descobrir como armazenar dados de um DataFrame em um banco de dados. Podemos usar SQL para criar uma tabela, `agg_game_year`, armazenando as seguintes informações:

```
CREATE TABLE agg_games_year(record_id SERIAL PRIMARY KEY,
                    year_of_release TIMESTAMP,
                    avg_critic_score REAL,
                    avg_user_score REAL,
                    total_copies_sold INT);
```

Vamos agrupar os dados sobre jogos no DataFrame `data_raw` e armazená-los em `agg_games_year`:

```
# Criar uma consulta SQL.
# Dessa vez nós vamos obter todas as colunas de data_raw. 
query = ''' SELECT *
            FROM data_raw
        '''

# Executar a consulta e armazenar o resultado
# no DataFrame.
data_raw = pd.io.sql.read_sql(query, con = engine, index_col = 'game_id')

# Converter os dados para os tipos necessários.
columns_numeric = ['na_players', 'eu_players', 'jp_players', 'other_players',
                   'critic_score', 'user_score']
columns_datetime = ['year_of_release']

for column in columns_numeric: data_raw[column] = pd.to_numeric(data_raw[column], errors='coerce')
for column in columns_datetime: data_raw[column] = pd.to_datetime(data_raw[column])

# Descobrir o número total de vendas para todas as regiões.
data_raw['total_copies_sold'] = data_raw[['na_players',
                      'eu_players',
                      'jp_players',
                      'other_players']].sum(axis = 1)

# Criar uma tabela agregada.
agg_games_year = data_raw.groupby('year_of_release').agg({'critic_score': 'mean',
                              'user_score': 'mean',
                              'total_copies_sold': 'sum'})

# Renomear as colunas do DataFrame para que
# seus nomes sejam diferentes daqueles das colunas do banco de dados.
# Isso irá ajudar o SQLAlchemy a armazenar os dados automaticamente. 
agg_games_year = agg_games_year.rename(columns = {'critic_score': 'avg_critic_score',
                                                  'user_score': 'avg_user_score'})

# Armazenar o DataFrame na tabela agg_games_year no banco de dados.
agg_games_year.to_sql(name = 'agg_games_year', con = engine, if_exists = 'append', index = False)
```

O SQLAlchemy analisa automaticamente se as chaves primárias da tabela de um banco de dados e os índices do DataFrame são os mesmos. O SQLAlchemy é guiado pelo parâmetro **if\_exists**. Se você especificar `if_exists = 'replace'`, o SQLAlchemy vai limpar os dados existentes da tabela e salvar os novos dados. Se você indicar `if_exists='append'`, o SQLAlchemy vai adicionar novas linhas no final da tabela. No nosso caso, a tabela e o DataFrame são indexados por colunas diferentes (`record_id` para a tabela e `'year_of_release'` para o DataFrame), então nós temos que passar `'append'` para o parâmetro `if_exists`. Vamos cuidar das duplicatas um pouco mais tarde.

O script, que lê os dados de uma tabela, os modifica e os armazena em uma tabela agregada está pronto! Para transformá-lo em um verdadeiro pipeline, temos de definir os parâmetros de entrada. Vamos supor que sua tarefa é ler as atualizações da tabela `data_raw` todo ano e colocar os resultados da agregação na tabela `agg_games_year`. Para fazer isso, é preciso passá-la dois parâmetros: as datas inicial e final do período de tempo. As duas datas são necessárias para casos em que você for iniciar o script para vários anos de uma só vez, para evitar inseri-las manualmente várias vezes em uma linha.

```
#!/usr/bin/python

import sys
import getopt
from datetime import datetime
import pandas as pd
from sqlalchemy import create_engine

if __name__ == "__main__":

    # Parâmetros de entrada
    unixOptions = "sdt:edt"
    gnuOptions = ["start_dt=", "end_dt="]
    
    fullCmdArguments = sys.argv
    argumentList = fullCmdArguments[1:] # excluindo o nome do script
    
    try:
        arguments, values = getopt.getopt(argumentList, unixOptions, gnuOptions)
    except 
        getopt.error as err:
        print (str(err))
        sys.exit(2)
    
    start_dt = ''
    end_dt = ''
    for currentArgument, currentValue in arguments:
        if currentArgument in ("-sdt", "--start_dt"):
            start_dt = currentValue
        elif currentArgument in ("-edt", "--end_dt"):
            end_dt = currentValue
    
    db_config = {'user': 'my_user',         
                 'pwd': 'my_user_password', 
                 'host': 'localhost',       
                 'port': 5432,              
                 'db': 'games'}             
    
    connection_string = 'postgresql://{}:{}@{}:{}/{}'.format(db_config['user'],
                                     db_config['pwd'],
                                 db_config['host'],
                                 db_config['port'],
                                 db_config['db'])
    engine = create_engine(connection_string)
    
    # Selecionar apenas jogos que foram lançados
    # entre start_dt e end_dt
    query = ''' SELECT *
            FROM data_raw
            WHERE year_of_release::TIMESTAMP BETWEEN '{}'::TIMESTAMP AND '{}'::TIMESTAMP
        '''.format(start_dt, end_dt)
    
    data_raw = pd.io.sql.read_sql(query, con = engine, index_col = 'game_id')

    columns_numeric = ['na_players', 'eu_players', 'jp_players', 'other_players',
                       'critic_score', 'user_score']
    columns_datetime = ['year_of_release']
    for column in columns_numeric: data_raw[column] = pd.to_numeric(data_raw[column], errors='coerce')
    for column in columns_datetime: data_raw[column] = pd.to_datetime(data_raw[column])
    
    data_raw['total_copies_sold'] = data_raw[['na_players',
                          'eu_players',
                          'jp_players',
                          'other_players']].sum(axis = 1)
    
    agg_games_year = data_raw.groupby('year_of_release').agg({'critic_score': 'mean',
                                  'user_score': 'mean',
                                  'total_copies_sold': 'sum'})
    
    agg_games_year = agg_games_year.rename(columns = {'critic_score': 'avg_critic_score',
                                                      'user_score': 'avg_user_score'})
    
    agg_games_year.to_sql(name = 'agg_games_year', con = engine, if_exists = 'append', index = False))
```

O script recebeu dois parâmetros de entrada: `start_dt` e `end_dt`, e uma condição no bloco WHERE foi adicionada ao comando SQL: `year_of_release:: TIMESTAMP BETWEEN '{}'::TIMESTAMP AND '{}'::TIMESTAMP`. Ela converte o campo de texto `year_of_release` e as strings `start_dt` e `end_dt` para o tipo `TIMESTAMP` e possibilita selecionar apenas os jogos lançados entre `start_dt` e `end_dt`.

Nosso pipeline está quase pronto. Resta apenas um pequeno problema: se o iniciarmos 10 vezes seguidas no ano de 2005, por exemplo, ele vai adicionar 10 novas linhas a `agg_games_year`, enquanto que precisamos de apenas uma. Para fazer com que o pipeline atualize os dados automaticamente, precisamos ensiná-lo um comando SQL especial chamado **DELETE:**

```
DELETE FROM table_name WHERE conditions_for_finding_records_to_be_deleted;
```

Por exemplo, se você precisar excluir todos os jogos com classificação E de `data_raw`, insira o seguinte comando em `psql` :

```
DELETE FROM data_raw WHERE rating = 'E';
```

Agora vamos adicionar uma função que vai excluir automaticamente as linhas mais antigas do pipeline:

```
#!/usr/bin/python

import sys
import getopt
from datetime import datetime
import pandas as pd
from sqlalchemy import create_engine

if __name__ == "__main__":

    # Parâmetros de entrada
    unixOptions = "sdt:edt"
    gnuOptions = ["start_dt=", "end_dt="]
    
    fullCmdArguments = sys.argv
    argumentList = fullCmdArguments[1:] # excluindo o nome do script
    
    try:
        arguments, values = getopt.getopt(argumentList, unixOptions, gnuOptions)
    except 
        getopt.error as err:
        print (str(err))
        sys.exit(2)
    
    start_dt = ''
    end_dt = ''
    for currentArgument, currentValue in arguments:
        if currentArgument in ("-sdt", "--start_dt"):
            start_dt = currentValue
        elif currentArgument in ("-edt", "--end_dt"):
            end_dt = currentValue
    
    db_config = {'user': 'my_user',         
                 'pwd': 'my_user_password', 
                 'host': 'localhost',       
                 'port': 5432,              
                 'db': 'games'}             
    
    connection_string = 'postgresql://{}:{}@{}:{}/{}'.format(db_config['user'],
                                     db_config['pwd'],
                                 db_config['host'],
                                 db_config['port'],
                                 db_config['db'])
    engine = create_engine(connection_string)
    
    # Selecionar apenas jogos que foram lançados
    # entre start_dt e end_dt
    query = ''' SELECT *
            FROM data_raw
            WHERE year_of_release::TIMESTAMP BETWEEN '{}'::TIMESTAMP AND '{}'::TIMESTAMP
        '''.format(start_dt, end_dt)
    
    data_raw = pd.io.sql.read_sql(query, con = engine, index_col = 'game_id')

    columns_numeric = ['na_players', 'eu_players', 'jp_players', 'other_players',
                       'critic_score', 'user_score']
    columns_datetime = ['year_of_release']
    for column in columns_numeric: data_raw[column] = pd.to_numeric(data_raw[column], errors='coerce')
    for column in columns_datetime: data_raw[column] = pd.to_datetime(data_raw[column])
    
    data_raw['total_copies_sold'] = data_raw[['na_players',
                          'eu_players',
                          'jp_players',
                          'other_players']].sum(axis = 1)
    
    agg_games_year = data_raw.groupby('year_of_release').agg({'critic_score': 'mean',
                                  'user_score': 'mean',
                                  'total_copies_sold': 'sum'})
    
    agg_games_year = agg_games_year.rename(columns = {'critic_score': 'avg_critic_score',
                                                      'user_score': 'avg_user_score'})
    
    # Excluir registros antigos entre start_dt e end_dt
    query = '''DELETE FROM agg_games_year 
                   WHERE year_of_release BETWEEN '{}'::TIMESTAMP AND '{}'::TIMESTAMP
            '''.format(start_dt, end_dt)
    engine.execute(query)

    agg_games_year.to_sql(name = 'agg_games_year', con = engine, if_exists = 'append', index = False))
```

Agora está tudo pronto! Todos os elementos básicos do pipeline estão preparados. Vamos verificar mais uma vez se os dados das tabelas de origem foram:

-   Lidos
-   Processados
-   Armazenados em tabelas agregadas

É importante mencionar que todas as operações desses pipelines podem ser executadas com um comando SQL. Mas o seu objetivo agora é criar um pipeline simples em Python, em vez de escrever o algoritmo otimizado.

## Tarefa

Você vai precisar trabalhar em seu computador local. Você é responsável por carregar todos os registros anuais sobre saúde do Chile no banco de dados do Ministério da Saúde. Essa tarefa é muito importante, já que as políticas públicas de saúde se baseiam nessas informações.

Como os registros são armazenados anualmente em arquivos CSV individuais, você vai precisar criar um pipeline capaz de receber esses arquivos e armazená-los no banco de dados central. A maneira como os dados são armazenados não é ideal; portanto, seu pipeline vai ter que realizar algumas verificações de integridade dos dados sobre saúde antes de carregá-los no servidor.

A consistência e a integridade de dados são princípios fundamentais dos bancos de dados. Portanto, é extremamente importante inspecionar esses conjuntos de dados para ver quais correções devem ser feitas. Por exemplo, os caracteres “ñ” e “ó” podem ser armazenados como �. Se esse for o caso, você terá que substituir esses caracteres pelos caracteres corretos.

Verifique a consistência da estrutura dos dados. Por exemplo, preste atenção a números inteiros, linhas vazias e inconsistências nos nomes de colunas entre arquivos diferentes. Seu pipeline deve ser capaz de armazenar três arquivos de dados diferentes e evitar armazenar dados duplicados.

Observação: como esta tarefa inclui análise de argumentos, você vai precisar usar o Visual Studio Code. O Jupyter Notebook não funciona para análise de argumentos.

**Os dados**

Você vai precisar baixar os seguintes conjuntos de dados de um repositório de dados públicos.

-   [Egresos Hospitalarios por Comuna de Residencia (disociados) Año 2020](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/EGRE_DATOS_ABIERTOS_2020.csv)
-   [Egresos Hospitalarios por Comuna de Residencia (disociados) Año 2019](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/EGRE_DATOS_ABIERTOS_2019.csv)
-   [Egresos Hospitalarios por Comuna de Residencia (disociados) Año 2018](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/EGRE_DATOS_ABIERTOS_2018.csv)

Além disso, aqui está um artigo que contém informações sobre as divisões administrativas do Chile:

[https://es.wikipedia.org/wiki/Anexo:Comunas\_de\_Chile](https://es.wikipedia.org/wiki/Anexo:Comunas_de_Chile) (consulte o artigo se achar útil).

**Comece por aqui**

Use o seguinte script como uma referência para entender o que você precisa implementar.

```
def parse_arguments():
    """
    Analisar argumentos da linha de comando para retornar o caminho do arquivo.

    Retorna:
        file_path (str): o caminho para o arquivo fornecido pelo usuário.
    """
   
    return file_path

def extract_year_from_path(file_path):
    """
    Extrair o ano do nome do caminho.

    Retorna:
        o ano do documento com os dados.
    """
    
    return year

def data_already_exists(engine, table_name, year):
        """ Valida se os dados que queremos armazenar já existem no banco de dados. Para fazer isso:
        1. Use o objeto "engine" para se conectar ao banco de dados.
        2. Crie uma consulta SQL para verificar se há algum registro em "table_name" correspondente ao "year" (ano) fornecido.
        3. Execute a consulta SQL.
        
        Retorna:
            True se os dados já estão armazenados no seu banco de dados. Caso contrário, retorna False. Analisar
            outros resultados possíveis ao consultar a tabela.
        """
    return exists

def load_data(file_path):
    # Carregar os dados em um DataFrame para operações futuras
    return df

def preprocess_data(df, engine, threshold=0.5):
    """
    Args:
        df (pd.DataFrame): o DataFrame de entrada.
        threshold (float): a proporção de colunas que podem conter '*' antes que 
                                            a linha seja removida.
        
        Pré-processar o DataFrame removendo as linhas em que a maioria das colunas contêm o 
    caractere '*'. Você também precisa padronizar os tipos de dados de colunas de acordo com a
    estrutura de dados apropriada. Inspecionar arquivos CSV diferentes para analisar possíveis problemas. Para fazer isso:
            
        1. Calcule o número de colunas.
        2. Determine quantos caracteres '*' são permitidos por linha com base no limiar.
        3. Filtre as linhas que ultrapassaram o número permitido de '*'.
        4. Converta colunas específicas para o tipo inteiro.
        5. Renomeie as colunas usando uma lista predefinida de nomes novos.
        6. Retorne o DataFrame limpo e formatado.
                
    Retorna:
        pd.DataFrame: o DataFrame limpo.
    """
    # Calcular o número de colunas
     num_columns = # Aqui fica seu código
    
    # Determinar o número de caracteres '*' permitidos com base no limiar
    allowed_stars = int(num_columns * threshold)

    # Filtrar as linhas em que o número de caracteres '*' excede o limiar permitido
    cleaned_df = df[df.apply(lambda x: (x == '*').sum() <= allowed_stars, axis=1)]
       
        ## Aqui fica seu código
    
    return cleaned_df

def create_db_engine(db_name): 
    """
    Criar uma conexão ao banco de dados com "sqlite:///"
    1. Crie uma string de conexão usando um nome de banco de dados conveniente.  Por exemplo,
    "sqlite:///{nome do seu banco de dados}.db". Armazene-a em uma variável.
        2. Inicie o mecanismo do SQLAlchemy para o banco de dados chamando create_engine(). Passe
        a variável anterior como um parâmetro dessa função.
        3. Imprima uma mensagem de confirmação de conexão.
        4. Retorne o mecanismo do SQLAlchemy para mais interações com o banco de dados.
    """
    ###### AQUI FICA SEU CÓDIGO ######
    return conection 

def save_to_database(df, engine, table_name):
    """
    Salvar o DataFrame (df) limpo no seu banco de dados.
    1. O DataFrame é inserido em uma tabela SQL especificada por table_name.
        2. Se a tabela já existir, novos dados serão anexados.
        3. O índice do DataFrame é excluído das colunas da tabela.
        4. A conexão ao banco de dados é gerenciada usando o mecanismo fornecido.
    """
```

Use o seguinte código para testar seu pipeline.

```
def validate_data(engine, table_name):  
    with engine.connect() as connection:
        # Consultar a nova tabela
        query = text(f'SELECT ANO_EGRESO, count(*) FROM {table_name} GROUP BY ANO_EGRESO')
        result = connection.execute(query)

        rows = result.fetchall()
        for row in rows[:100]:
            print(row)                    
      
if __name__ == "__main__":
    # Analisar o caminho do arquivo usando os argumentos da linha de comando
    file_path = parse_arguments()
    table_name = 'egresos_pacientes'
    
    # Carregar o banco de dados no seu sistema
    engine = create_db_engine('database/ministerio_de_salud_chile.db')   
    print('[INFO]: Conexão com o banco de dados')
    
    if file_path:    
        print(f"Caminho do arquivo: {file_path}")
        year = extract_year_from_path(file_path)
         
        # Carregar seu arquivo CSV em um DataFrame do Pandas      
        raw_data = load_data(file_path)
        print('[INFO]: Carregar os dados como um DataFrame do Pandas')
        
        # Verificar se os dados já estão no banco de dados
        if data_already_exists(engine, table_name, year):
            print(f"Os dados já existem no banco de dados. Nenhuma ação deve ser tomada.")   
        else:
            # Pré-processar os dados
            raw_data = preprocess_data(raw_data)
            print('[INFO]: Pré-processar os dados')
            
            # Salvar os dados em uma nova tabela dentro do banco de dados existente    
            save_to_database(raw_data, engine, table_name)
            print('[INFO]: Carregar os dados no banco de dados')
    else:
        print('Nenhum caminho foi fornecido')
    
    validate_data(engine, table_name)
```

Você pode usar a seguinte linha de comando para executar seu código e armazenar um arquivo. Por exemplo:

python _test.py_ -f dbs/EGRESOS\_2020/EGRE\_DATOS\_ABIERTOS\_2020.csv

**Resultado**

```
(2020, 1292935)
```

Assim que você carregar os últimos arquivos, o resultado deve ser mais ou menos isso

```
(2018, 1620450)
(2019, 1623335)
(2020, 1292935)
```

Além disso, se você tentar carregar um arquivo já armazenado, você verá algo assim

```
The data already exists in the database. No action taken.
(2018, 1620450)
(2019, 1623335)
(2020, 1292935)
```

Solução

```
import sys
import getopt
import pandas as pd
import re
from sqlalchemy import create_engine, text
from sqlalchemy.exc import OperationalError

def parse_arguments():
    """
    Analisar argumentos da linha de comando para retornar o caminho do arquivo.
    
    Retorna:
        file_path (str): O caminho para o arquivo fornecido pelo usuário.
    """
    unixOptions = "f:"
    gnuOptions = ["file="]

    fullCmdArguments = sys.argv
    argumentList = fullCmdArguments[1:]  # excluir o nome do script

    try:
        arguments, values = getopt.getopt(argumentList, unixOptions, gnuOptions)
    except getopt.error as err:
        print(str(err))
        sys.exit(2)

    file_path = ''

    for currentArgument, currentValue in arguments:
        if currentArgument in ("-f", "--file"):
            file_path = currentValue
    
    return file_path

def extract_year_from_path(file_path):
    # Dividir o caminho do arquivo em partes
    year = file_path.split('/')[-1].split('.')[0][-4:]
    
    return year

def data_already_exists(engine, table_name, year):
    try:
        with engine.connect() as connection:
            # Consultar a nova tabela
            query = text(f'SELECT * FROM {table_name} WHERE ANO_EGRESO={year}')
            result = connection.execute(query)
            exists = result.fetchone() is not None
    except OperationalError as e:
            exists = False
    return exists

def load_data(file_path):
    df = pd.read_csv(file_path, encoding='latin1', delimiter=';')
    return df

def preprocess_data(df, threshold=0.5):
    """
    Pré-processar o DataFrame removendo as linhas em que a maioria das colunas contêm o caractere '*'.

    Args:
        df (pd.DataFrame): O DataFrame de entrada.
        threshold (float): A proporção de colunas que podem conter '*' antes que a linha seja removida.

    Retorna:
        pd.DataFrame: O DataFrame limpo.
    """
    ## Excluir linhas vazias
    ## Calcular o número de colunas
    num_columns = df.shape[1]

    # Determinar o número de caracteres '*' permitidos com base no limite
    allowed_stars = int(num_columns * threshold)

    # Filtrar as linhas em que o número de caracteres '*' excede o limite permitido
    cleaned_df = df[df.apply(lambda x: (x == '*').sum() <= allowed_stars, axis=1)]
    
    # Formato de dados
    cleaned_df.loc[:,'COMUNA_RESIDENCIA'] = cleaned_df['COMUNA_RESIDENCIA'].astype(int)
    cleaned_df.loc[:,'REGION_RESIDENCIA'] = cleaned_df['REGION_RESIDENCIA'].astype(int)
    cleaned_df.loc[:,'ANO_EGRESO'] = cleaned_df['ANO_EGRESO'].astype(int)
    
    # Renomear as colunas
    new_column_names = ['PERTENENCIA_ESTABLECIMIENTO_SALUD', 'SEXO', 'GRUPO_EDAD', 'ETNIA',
       'GLOSA_PAIS_ORIGEN', 'COMUNA_RESIDENCIA', 'GLOSA_COMUNA_RESIDENCIA',
       'REGION_RESIDENCIA', 'GLOSA_REGION_RESIDENCIA', 'PREVISION',
       'GLOSA_PREVISION', 'ANO_EGRESO', 'DIAG1', 'DIAG2', 'DIAS_ESTADA',
       'CONDICION_EGRESO', 'INTERV_Q', 'PROCED']
    old_column_names = cleaned_df.columns
    
    column_mapping = dict(zip(old_column_names, new_column_names))
    cleaned_df.rename(columns=column_mapping, inplace=True)

    return cleaned_df

def create_db_engine(db_name):
    connection_string = f'sqlite:///{db_name}'
    engine = create_engine(connection_string)
    print(f'[INFO]: Conexão verificada: {connection_string}')
    return engine

def save_to_database(df, engine, table_name):
    df.to_sql(name=table_name, con=engine, if_exists='append', index=False)
    
def validate_data(engine, table_name):  
    with engine.connect() as connection:
        # Consultar a nova tabela
        query = text(f'SELECT ANO_EGRESO, count(*) FROM {table_name} GROUP BY ANO_EGRESO')
        result = connection.execute(query)

        rows = result.fetchall()
        for row in rows[:100]:
            print(row)                    
      
if __name__ == "__main__":
    # Analisar o caminho do arquivo usando os argumentos da linha de comando
    file_path = parse_arguments()
    table_name = 'egresos_pacientes'
    
    # Carregar o banco de dados no seu sistema
    engine = create_db_engine('database/ministerio_de_salud_chile.db')   
    print('[INFO]: Conexão com o banco de dados')
    
    if file_path:    
        print(f"Caminho do arquivo: {file_path}")
        year = extract_year_from_path(file_path)
         
        # Carregar seu arquivo CSV em um DataFrame do Pandas      
        raw_data = load_data(file_path)
        print('[INFO]: Carregar os dados como um DataFrame do Pandas')
        
        # Verificar se os dados já estão no banco de dados
        if data_already_exists(engine, table_name, year):
            print(f"Os dados já existem no banco de dados. Nenhuma ação deve ser tomada.")    
        else:
            # Pré-processar os dados
            raw_data = preprocess_data(raw_data)
            print('[INFO]: Pré-processar os dados')
            
            # Salvar os dados em uma nova tabela dentro do banco de dados existente  
            save_to_database(raw_data, engine, table_name)
            print('[INFO]: Carrega os dados no banco de dados')
    else:
        print('Nenhum caminho foi fornecido')
    
    validate_data(engine, table_name)
```

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-12-00-062Z.md
### Última modificação: 2025-05-28 20:12:01

# Conclusão - TripleTen

Capítulo 2/6

Pipelines de Dados e Para Que Servem

# Conclusão

Você aprendeu sobre pipelines e escreveu o seu primeiro script!

\--- Faça o download da [folha conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/Folha_de_Concluses_Pipelines_de_Dados_e_Porque_Us-los.pdf?etag=74e80cb3f577834d5903c1a40e66ca69) e do [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/Resumo_do_Captulo_Pipelines_de_Dados_e_Porque_Us-los.pdf?etag=a32b94a71f6d320eaa5d5331289f52b4) para que você possa consultá-los quando necessário.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-12-01-347Z.md
### Última modificação: 2025-05-28 20:12:01

# Introdução - TripleTen

Capítulo 3/6

Desenhando e Criando Dashboards com Dash

# Introdução

Neste capítulo, você descobrirá o que são dashboards e como criá-los.

### O que você irá aprender:

-   Como coletar os requisitos do seu cliente para o dashboard
-   Como construir os gráficos mais comuns com a biblioteca Dash
-   Como adicionar elementos interativos ao seu dashboard
-   Como projetar um dashboard com HTML
-   Como definir a lógica de um dashboard

### Quanto tempo irá demorar:

_8 lições, aproximadamente de 20 a 25 minutos cada_

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-12-10-255Z.md
### Última modificação: 2025-05-28 20:12:10

# Dashboards - TripleTen

main.py

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

\# !/usr/bin/python

\# -\*- coding: utf-8 -\*-

  

import dash

import dash\_core\_components as dcc

import dash\_html\_components as html

  

import plotly.graph\_objs as go

  

import pandas as pd

  

\# defina os dados a serem exibidos

from sqlalchemy import create\_engine

  

\# código de exemplo para conectar ao banco de dados com PostgreSQL

\# db\_config = {'user': 'my\_user',

\# 'pwd': 'my\_user\_password',

\# 'host': 'localhost',

\# 'port': 5432,

\# 'db': 'games'}

\# engine = create\_engine('postgresql://{}:{}@{}:{}/{}'.format(db\_config\['user'\],

\# db\_config\['pwd'\],

\# db\_config\['host'\],

\# db\_config\['port'\],

\# db\_config\['db'\]))

\# código de exemplo para conectar-se ao banco de dados com SQLite

engine \= create\_engine('sqlite:////db/games.db', echo\=False)

  

\# obtenha os dados brutos

query \= '''

SELECT \* FROM data\_raw;

'''

games\_raw \= pd.io.sql.read\_sql(query, con\=engine)

  

\# defina os dados para o relatório

games\_grouped \= (

games\_raw.groupby(\['genre', 'year\_of\_release'\])

.agg({'name': 'count'})

.reset\_index()

.rename(columns\={'name': 'Games Released'})

)

  

\# defina os gráficos a serem exibidos

data\_games\_by\_year \= \[\]

for genre in games\_grouped\['genre'\].unique():

current \= games\_grouped.query('genre == @genre')

data\_games\_by\_year += \[

go.Scatter(

x\=current\['year\_of\_release'\],

y\=current\['Games Released'\],

mode\='lines',

stackgroup\='one',

name\=genre,

)

\]

\# defina o layout

external\_stylesheets \= \['https://codepen.io/chriddyp/pen/bWLwgP.css'\]

app \= dash.Dash(

\_\_name\_\_, external\_stylesheets\=external\_stylesheets, compress\=False

)

app.layout \= html.Div(

children\=\[

\# html

html.H1(children\='Games released by year'),

dcc.Graph(

figure\={

'data': data\_games\_by\_year,

'layout': go.Layout(

xaxis\={'title': 'Year'}, yaxis\={'title': 'Games released'}

),

},

id\='games\_by\_year',

),

\]

)

Dashboards

Tarefa4 / 4

A tabela `data_raw` do banco de dados de `games` contém dados sobre os jogos de computador ordenados por ano de lançamento.

1.  Escreva uma consulta para recuperar todos os dados de `data_raw`.
2.  No dashboard, exiba um gráfico de áreas empilhadas no qual cada gênero terá uma área correspondente; os anos estarão no eixo X e o número de jogos lançados ficarão no eixo Y.

Janela do navegador

1912px × 1686px

<iframe allow="fullscreen;geolocation;camera;microphone;publickey-credentials-get" class="trainer-result__content" name="frame"></iframe>

## Embedded Content

---

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-12-11-550Z.md
### Última modificação: 2025-05-28 20:12:11

# Coletando Requisitos ao Construir um Dashboard - TripleTen

Capítulo 3/6

Desenhando e Criando Dashboards com Dash

# Coletando Requisitos ao Construir um Dashboard

A criação de dashboards exige que você se torne um analista de negócios ou redator técnico por um curto período de tempo. Você terá que se comunicar com os clientes do dashboard, coletar requisitos funcionais e elaborar **requisitos técnicos**.

Antes de criar um dashboard, esclareça os seguintes detalhes com o cliente:

-   Qual problema de negócios o dashboard deve resolver e quem será o usuário principal
-   Com que frequência se espera que o dashboard seja usado
-   Estrutura de dados do dashboard: quais métricas (KPI) ele deveria exibir, quais parâmetros deveriam ser agrupados, quais parâmetros deveriam ser usados para formar coortes de usuários
-   O tipo de dados a serem exibidos no painel (valores absolutos ou relativos, ou ambos)
-   As fontes de dados
-   O banco de dados que armazenará dados agregados
-   Com que frequência os dados deveriam ser atualizados
-   Quais gráficos deveria exibir e em que ordem
-   Quais controles o dashboard deveria ter

No mundo ideal, os clientes forneceriam os requisitos técnicos explícitos, mas na vida real os analistas geralmente precisam elaborá-los por conta própria. Vamos falar sobre a melhor maneira de fazê-lo.

Primeiro, você deveria descobrir o problema de negócios exato que o dashboard pretende resolver. Você pode ouvir algo como "Gostaríamos de ver como as campanhas publicitárias influenciam o número de usuários adquiridos" ou "Gostaríamos de ver quais produtos são comprados com mais frequência". Nesta fase, você precisa encontrar as respostas para duas perguntas:

1.  A construção de um dashboard é uma boa maneira de resolver esse problema? Vale a pena construir um dashboard se o cliente o consultar regularmente para tomar decisões. Se eles pretendem estudar a situação uma vez, é melhor preparar um relatório periódico.
2.  Quantos dashboards você deve construir? Pode ser muito difícil determinar se a tarefa em questão requer vários dashboards. Você terá que confiar em sua experiência e bom senso. Se você achar que os requisitos do cliente incluem vários problemas de negócios ou precisa de muitos dados heterogêneos, provavelmente terá que criar vários dashboards.

Em seguida, descubra de quais dados você precisa. Por exemplo, se o cliente quiser um dashboard mostrando LTV e taxas de retenção e conversão, você precisará de dados sobre as sessões e compras do usuário. Mas se o cliente quiser um dashboard que reflita a lucratividade das estratégias de investimento, você precisará de dados sobre as transações e estratégias.

Nesta fase, determine quais métricas precisam ser adicionadas aos dados do dashboard. Por exemplo, o cliente pode precisar de um dashboard que ilustre o LTV e as taxas de retenção e conversão com o detalhamento por país, tipo de dispositivo e canal de anúncio.

Depois de fazê-lo, fale com o cliente sobre os tipos de gráficos que eles desejam que apareçam no dashboard. Você precisa ter uma ideia clara de quais dados o cliente considera mais importantes. Quanto mais importante é o gráfico, mais espaço ele deveria ocupar. Para cada gráfico, descubra se ele deveria exibir os valores relativos (por exemplo, porcentagens de vendas) ou absolutos (por exemplo, vendas diárias em dólares).

Nesta fase, você também precisa saber quais controles e filtros deveriam ser adicionados ao dashboard. Quase todos os dashboards exigem os filtros de tempo. Os outros filtros e controles quase sempre espelham as métricas de dados necessárias para o dashboard. Por exemplo, se o cliente quiser um dashboard que mostre as vendas divididas por área residencial, seria lógico supor que o dashboard precisasse de um filtro correspondente.

Esboce um **rascunho do dashboard** com o cliente, uma espécie de mapa ou modelo que mostre os tipos de gráficos e seus tamanhos e posições relativas. Você pode criar uma tabela simples para seu rascunho. Por exemplo, o modelo de dashboard mostrando o lançamento de programas de TV ao longo do tempo pode ser assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.3.3PT_1658736729.png)

Suas comunicações com o cliente deveriam levar a um conjunto de requisitos técnicos. Eles podem ter a forma de um documento oficial, notas ou uma apresentação. Geralmente, não há necessidade de burocratizar as coisas. O importante é que todas as partes interessadas possam dar uma breve olhada a esse documento e concordar que ele tem tudo do que precisam.

Depois de preparar um rascunho do dashboard e dos requisitos técnicos, é hora de começar a parte técnica do projeto. Nesta fase, você deveria perguntar aos administradores do banco de dados quais fontes você pode usar. Juntamente com os engenheiros de dados, você decidirá qual banco de dados armazenará dados agregados e em qual será executado o script que serve para o seu dashboard.

Aqui estão os elementos básicos de qualquer dashboard:

-   **Cabeçalho** — deveria informar ao usuário o que o dashboard ilustra
-   **Descrição do dashboard** — um breve texto descrevendo o problema que o dashboard resolve e notas sobre como ele funciona (se houver algo fora do comum)
-   **Gráficos e diagramas**
-   **Controles**

Chega de teoria, vamos praticar!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-12-12-864Z.md
### Última modificação: 2025-05-28 20:12:13

# Criando Gráficos Básicos no Dash - TripleTen

main.py

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

#!/usr/bin/python

\# -\*- coding: utf-8 -\*-

  

import dash

import dash\_core\_components as dcc

import dash\_html\_components as html

  

import plotly.graph\_objs as go

  

import pandas as pd

  

\# configurando os dados a serem exibidos

from sqlalchemy import create\_engine

  

\# código de exemplo para conectar ao banco de dados com PostgreSQL

\# db\_config = {'user': 'my\_user',

\# 'pwd': 'my\_user\_password',

\# 'host': 'localhost',

\# 'port': 5432,

\# 'db': 'games'}

\# engine = create\_engine('postgresql://{}:{}@{}:{}/{}'.format(db\_config\['user'\],

\# db\_config\['pwd'\],

\# db\_config\['host'\],

\# db\_config\['port'\],

\# db\_config\['db'\]))

\# código de exemplo para conectar-se ao banco de dados com SQLite

engine \= create\_engine('sqlite:////db/games.db', echo\=False)

  

\# obtenção de dados brutos

query \= '''

SELECT \* FROM data\_raw

'''

games\_raw \= pd.io.sql.read\_sql(query, con\=engine)

  

\# tipos de conversão

games\_raw\['year\_of\_release'\] \= pd.to\_datetime(games\_raw\['year\_of\_release'\])

columns \= \['na\_players', 'eu\_players', 'jp\_players', 'other\_players'\]

for column in columns:

games\_raw\[column\] \= pd.to\_numeric(games\_raw\[column\], errors\='coerce')

games\_raw\['total'\] \= (

games\_raw\[\['na\_players', 'eu\_players', 'jp\_players', 'other\_players'\]\]

.sum(axis\=1)

.round(2)

)

  

\# definindo os gráficos a serem exibidos

games\_raw \= (

games\_raw\[\['name', 'platform', 'genre', 'total'\]\]

Criando Gráficos Básicos no Dash

Tarefa8 / 8

Desenhe uma tabela com as seguintes colunas:

-   `name` (jogo);
-   `platform` (plataforma);
-   `genre` (gênero);
-   `total` (vendas por região);

Armazene a tabela em `games_raw,` ordene esta variável em ordem decrescente dos valores da coluna `'total'`. Imprima as primeiras 10 linhas.

Transponha a tabela `games_raw` e armazene seus valores (o parâmetro `values`) no parâmetro `cells` do elemento `go.Table`.

Defina `id = 'games_by_genre'` para a tabela.

Janela do navegador

1912px × 1686px

<iframe allow="fullscreen;geolocation;camera;microphone;publickey-credentials-get" class="trainer-result__content" name="frame"></iframe>

## Embedded Content

---

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-12-17-353Z.md
### Última modificação: 2025-05-28 20:12:17

# Noções Básicas de Trabalho com Controles - TripleTen

main.py

99

1

2

3

4

5

6

7

8

9

10

11

#!/usr/bin/python

\# -\*- coding: utf-8 -\*-

  

import dash

  

  

\# seu código aqui

  

\# lógica do dashboard, não altere as linhas abaixo

if \_\_name\_\_ \== '\_\_main\_\_':

app.run\_server(host\='0.0.0.0', port\=3000)

Noções Básicas de Trabalho com Controles

Tarefa

Esta não é uma tarefa real, é apenas um espaço no qual você pode brincar com o código que ensinamos nesta lição.

Janela do navegador

1912px × 1686px

<iframe allow="fullscreen;geolocation;camera;microphone;publickey-credentials-get" class="trainer-result__content" name="frame"></iframe>

## Embedded Content

---

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-12-18-668Z.md
### Última modificação: 2025-05-28 20:12:19

# Controles Básicos no Dash - TripleTen

main.py

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

#!/usr/bin/python

  

import dash

import dash\_core\_components as dcc

import dash\_html\_components as html

from dash.dependencies import Input, Output

import plotly.graph\_objs as go

import pandas as pd

  

\# Recuperando e transformando dados

urbanization \= pd.read\_csv('/datasets/urbanization.csv')

urbanization\['Year'\] \= pd.to\_datetime(urbanization\['Year'\], format\='%Y-%m-%d')

  

\# Layout

external\_stylesheets \= \['https://codepen.io/chriddyp/pen/bWLwgP.css'\]

app \= dash.Dash(\_\_name\_\_, external\_stylesheets\=external\_stylesheets)

app.layout \= html.Div(children\=\[

html.H1(children\='Max urbanization, top-25'),

html.Label('Time range:'),dcc.DatePickerRange(start\_date \= '2010',

end\_date \= '2019',

display\_format \= 'YYYY',

id \= 'dt\_selector', ),

dcc.Graph(id\='urbanization\_by\_year'),

dcc.Checklist( options \= \[{'label': 'Africa', 'value': 'afr'},

{'label': 'Eurasia', 'value': 'eur'},

{'label': 'Australia', 'value': 'au'},

{'label': 'Americas', 'value': 'am'}\],

value \= \['afr', 'eur', 'au', 'am'\],

id \= 'continent\_selector' )\# Figura estática removida

\])

  

\# Retorno de chamada para atualizar o gráfico com base nas datas selecionadas.

@app.callback(

Output('urbanization\_by\_year', 'figure'),

\[Input('dt\_selector', 'start\_date'),

Input('dt\_selector', 'end\_date')\]

)

def update\_graph(start\_date, end\_date):

\# Filtrar dados com base no intervalo de datas selecionado

filtered\_data \= urbanization\[

(urbanization\['Year'\] \>= pd.to\_datetime(start\_date)) &

(urbanization\['Year'\] <= pd.to\_datetime(end\_date))

\]

\# Calcular a urbanização máxima para cada Entity (entidade) a partir dos dados filtrados

max\_urbanization \= (filtered\_data.groupby('Entity')

.agg({'Urban': 'max'})

.reset\_index()

.sort\_values(by\='Urban', ascending\=False)

Controles Básicos no Dash

Tarefa

Esta não é uma tarefa real, é apenas um espaço no qual você pode brincar com o código que ensinamos nesta lição.

Janela do navegador

1912px × 1686px

<iframe allow="fullscreen;geolocation;camera;microphone;publickey-credentials-get" class="trainer-result__content" name="frame"></iframe>

## Embedded Content

---

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-12-19-980Z.md
### Última modificação: 2025-05-28 20:12:20

# Controles e Interatividade - TripleTen

main.py

999

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

#!/usr/bin/python

\# -\*- coding: utf-8 -\*-

  

import dash

import dash\_core\_components as dcc

import dash\_html\_components as html

from dash.dependencies import Input, Output

  

import plotly.graph\_objs as go

  

from datetime import datetime

  

import pandas as pd

  

\# configurando os dados a serem exibidos

from sqlalchemy import create\_engine

  

\# código de exemplo para conectar ao banco de dados com PostgreSQL

\# db\_config = {'user': 'my\_user',

\# 'pwd': 'my\_user\_password',

\# 'host': 'localhost',

\# 'port': 5432,

\# 'db': 'games'}

\# engine = create\_engine('postgresql://{}:{}@{}:{}/{}'.format(db\_config\['user'\],

\# db\_config\['pwd'\],

\# db\_config\['host'\],

\# db\_config\['port'\],

\# db\_config\['db'\]))

\# código de exemplo para conectar-se ao banco de dados com SQLite

engine \= create\_engine('sqlite:////db/games.db', echo\=False)

  

\# obtenção de dados brutos

query \= '''

SELECT \* FROM data\_raw

'''

games\_raw \= pd.io.sql.read\_sql(query, con\=engine)

  

\# tipos de conversão

games\_raw\['year\_of\_release'\] \= pd.to\_datetime(games\_raw\['year\_of\_release'\])

columns \= \['na\_players', 'eu\_players', 'jp\_players', 'other\_players'\]

for column in columns:

games\_raw\[column\] \= pd.to\_numeric(games\_raw\[column\], errors\='coerce')

\# definindo os dados para o relatório

games\_grouped \= (

games\_raw.groupby(\['year\_of\_release', 'genre'\])

.agg({'name': 'nunique'})

.reset\_index()

.rename(columns={'name': 'games\_launched'})

)

  

\# definindo o layout

Controles e Interatividade

Tarefa4 / 4

Conclua a tarefa de compilar o conjunto de gêneros de jogos de maneira mais concisa: crie uma lista dropdown.

Adicione o elemento `dcc.Dropdown` ao dashboard para compilar gêneros de jogos. Especifique `multi = True` e `id = 'genre_selector'`.

Para `dcc.Dropdown`, defina os parâmetros `options` (opções disponíveis) e `value` (opções selecionadas).

Armazene o conjunto de gêneros de jogos exclusivos do conjunto de dados `games_grouped` em ambos os parâmetros. Aplique os métodos `list comprehension` e `unique()` para formar `options`.

Crie `value` com o método `unique().tolist()`. Indique todos os gêneros como opções selecionadas.

Lembre-se que `options` é um vetor de dicionários com a seguinte estrutura: `[{'label': gênero_name1, 'value': gênero_name1}, ...]`.

Janela do navegador

1912px × 1686px

<iframe allow="fullscreen;geolocation;camera;microphone;publickey-credentials-get" class="trainer-result__content" name="frame"></iframe>

## Embedded Content

---

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-13-03-721Z.md
### Última modificação: 2025-05-28 20:13:04

# Elementos do Dashboard - TripleTen

main.py

999

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

#!/usr/bin/python

\# -\*- coding: utf-8 -\*-

  

import dash

import dash\_core\_components as dcc

import dash\_html\_components as html

from dash.dependencies import Input, Output

  

import plotly.graph\_objs as go

  

from datetime import datetime

  

import pandas as pd

  

\# configurando os dados a serem exibidos

from sqlalchemy import create\_engine

  

\# código de exemplo para conectar ao banco de dados com PostgreSQL

\# db\_config = {'user': 'my\_user',

\# 'pwd': 'my\_user\_password',

\# 'host': 'localhost',

\# 'port': 5432,

\# 'db': 'games'}

\# engine = create\_engine('postgresql://{}:{}@{}:{}/{}'.format(db\_config\['user'\],

\# db\_config\['pwd'\],

\# db\_config\['host'\],

\# db\_config\['port'\],

\# db\_config\['db'\]))

\# código de exemplo para conectar-se ao banco de dados com SQLite

engine \= create\_engine('sqlite:////db/games.db', echo\=False)

  

\# obtenção de dados brutos

query \= '''

SELECT \* FROM agg\_games\_year\_genre\_platform

'''

agg\_games\_year\_genre\_platform \= pd.io.sql.read\_sql(query, con\=engine)

agg\_games\_year\_genre\_platform\["year\_of\_release"\] \= pd.to\_datetime(

agg\_games\_year\_genre\_platform\["year\_of\_release"\]

)

  

note \= '''

Este dashboard ajudará você a aprender as noções básicas de composição.

'''

  

\# definindo o layout

external\_stylesheets \= \['https://codepen.io/chriddyp/pen/bWLwgP.css'\]

app = dash.Dash(

\_\_name\_\_, external\_stylesheets=external\_stylesheets, compress=False

Elementos do Dashboard

Tarefa5 / 5

Organize os elementos do dashboard assim:

-   O `launches_by_year` deve ocupar `eight` colunas de layout e sua altura deve ser de 50% da largura do dashboard.
-   À direita, exiba o `sales_by_platform` e o `sales_by_genre` em uma coluna. A coluna deve ocupar `four` colunas de layout. Cada um dos gráficos deve ter uma altura igual a 25% da largura do dashboard.

Janela do navegador

1912px × 1686px

<iframe allow="fullscreen;geolocation;camera;microphone;publickey-credentials-get" class="trainer-result__content" name="frame"></iframe>

## Embedded Content

---

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-13-37-056Z.md
### Última modificação: 2025-05-28 20:13:37

# Elementos do Dashboard - TripleTen

main.py

999

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

#!/usr/bin/python

\# -\*- coding: utf-8 -\*-

  

import dash

import dash\_core\_components as dcc

import dash\_html\_components as html

from dash.dependencies import Input, Output

  

import plotly.graph\_objs as go

  

from datetime import datetime

  

import pandas as pd

  

\# configurando os dados a serem exibidos

from sqlalchemy import create\_engine

  

\# código de exemplo para conectar ao banco de dados com PostgreSQL

\# db\_config = {'user': 'my\_user',

\# 'pwd': 'my\_user\_password',

\# 'host': 'localhost',

\# 'port': 5432,

\# 'db': 'games'}

\# engine = create\_engine('postgresql://{}:{}@{}:{}/{}'.format(db\_config\['user'\],

\# db\_config\['pwd'\],

\# db\_config\['host'\],

\# db\_config\['port'\],

\# db\_config\['db'\]))

\# código de exemplo para conectar-se ao banco de dados com SQLite

engine \= create\_engine('sqlite:////db/games.db', echo\=False)

  

\# obtenção de dados brutos

query \= '''

SELECT \* FROM agg\_games\_year\_genre\_platform

'''

agg\_games\_year\_genre\_platform \= pd.io.sql.read\_sql(query, con\=engine)

agg\_games\_year\_genre\_platform\["year\_of\_release"\] \= pd.to\_datetime(

agg\_games\_year\_genre\_platform\["year\_of\_release"\]

)

  

note \= '''

Este dashboard ajudará você a aprender as noções básicas de composição.

'''

  

\# definindo o layout

external\_stylesheets \= \['https://codepen.io/chriddyp/pen/bWLwgP.css'\]

app = dash.Dash(

\_\_name\_\_, external\_stylesheets=external\_stylesheets, compress=False

Elementos do Dashboard

Tarefa5 / 5

Organize os elementos do dashboard assim:

-   O `launches_by_year` deve ocupar `eight` colunas de layout e sua altura deve ser de 50% da largura do dashboard.
-   À direita, exiba o `sales_by_platform` e o `sales_by_genre` em uma coluna. A coluna deve ocupar `four` colunas de layout. Cada um dos gráficos deve ter uma altura igual a 25% da largura do dashboard.

Janela do navegador

1912px × 1686px

<iframe allow="fullscreen;geolocation;camera;microphone;publickey-credentials-get" class="trainer-result__content" name="frame"></iframe>

## Embedded Content

---

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-13-38-395Z.md
### Última modificação: 2025-05-28 20:13:38

# Desenvolvimento do Dashboard e o Básico da Composição - TripleTen

main.py

999

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

#!/usr/bin/python

\# -\*- coding: utf-8 -\*-

  

import dash

import dash\_core\_components as dcc

import dash\_html\_components as html

from dash.dependencies import Input, Output

  

import plotly.graph\_objs as go

  

from datetime import datetime

  

import pandas as pd

  

\# definindo os dados a serem exibidos

from sqlalchemy import create\_engine

  

\# código de exemplo para conectar ao banco de dados com PostgreSQL

\# db\_config = {'user': 'my\_user',

\# 'pwd': 'my\_user\_password',

\# 'host': 'localhost',

\# 'port': 5432,

\# 'db': 'games'}

\# engine = create\_engine('postgresql://{}:{}@{}:{}/{}'.format(db\_config\['user'\],

\# db\_config\['pwd'\],

\# db\_config\['host'\],

\# db\_config\['port'\],

\# db\_config\['db'\]))

\# código de exemplo para conectar ao banco de dados com PostgreSQL

engine \= create\_engine('sqlite:////db/games.db', echo\=False)

  

\# obtenção de dados brutos

query \= '''

SELECT \* FROM agg\_games\_year\_genre\_platform

'''

agg\_games\_year\_genre\_platform \= pd.io.sql.read\_sql(query, con\=engine)

agg\_games\_year\_genre\_platform\['year\_of\_release'\] \= pd.to\_datetime(

agg\_games\_year\_genre\_platform\['year\_of\_release'\]

)

  

query \= '''

SELECT \* FROM agg\_games\_year\_score

'''

agg\_games\_year\_score \= pd.io.sql.read\_sql(query, con\=engine)

agg\_games\_year\_score\['year\_of\_release'\] \= pd.to\_datetime(

agg\_games\_year\_score\['year\_of\_release'\]

)

\# ignorando registros sem pontuação

agg\_games\_year\_score = agg\_games\_year\_score.query(

Desenvolvimento do Dashboard e o Básico da Composição

Tarefa7 / 7

Retorne os conjuntos de gráficos sugeridos de `update_figures`.

Dê outra olhada em como o conjunto de parâmetros de saída é definido no decorador.

Janela do navegador

1912px × 1686px

<iframe allow="fullscreen;geolocation;camera;microphone;publickey-credentials-get" class="trainer-result__content" name="frame"></iframe>

## Embedded Content

---

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-13-39-730Z.md
### Última modificação: 2025-05-28 20:13:40

# Conclusão - TripleTen

Capítulo 3/6

Desenhando e Criando Dashboards com Dash

# Conclusão

Parabéns! Você foi ótimo!

### O que você aprendeu:

-   Como determinar o que seu cliente quer de um dashboard
-   Como construir os gráficos mais comuns usando a biblioteca Dash
-   Como adicionar os elementos interativos a um dashboard
-   Como projetar um dashboard com HTML
-   Como definir a lógica de um dashboard

### Leve isso com você

Faça download do [sumário do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/Captulo_Sumrio_Designando_e_Desenvolvendo_Dashboards_com_Dash.pdf?etag=27ebab8f77392d51ce2dc88be8d69534) e [folha de conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/Folha_de_Concluses_Projetando_e_Desenvolvendo_Dashboards_com_Dash.pdf?etag=cf2df2d1a6c04f08722da4f72f051348) da Base de Conhecimento para que você possa consultá-los quando necessário.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-26-298Z.md
### Última modificação: 2025-05-28 20:15:27

# Introdução - TripleTen

Capítulo 4/6

Tableau

# Introdução

Neste capítulo você irá aprender como usar **Tableau**, outra ferramenta para análise de dados e visualização. Os produtos da companhia americana Tableau Software são considerados estar entre as melhores no mercado para negócios de inteligência de software. Empresas como Apple, Cisco e Paypal usam Tableau e em breve você saberá como usar também!

### O que você irá aprender:

-   Como instalar Tableau Public, a versão livre do Tableau
-   Como preparar e exportar dados em um formato adequado para Tableau
-   Como fazer vários tipos de gráficos
-   Como fazer dashboards
-   Como publicar seu portfólio dashboard online

### Quanto tempo irá demorar:

10 lições, de aproximadamente 25 a 30 minutos

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-28-917Z.md
### Última modificação: 2025-05-28 20:15:29

# O Básico do Tableau - TripleTen

Capítulo 4/6

Tableau

# O Básico do Tableau

Nesta lição, você irá aprender as etapas principais dos projetos Tableau.

Todos os exemplos nesse capítulo foram feitos com Tableau Public, mas no seu trabalho como analista, você provavelmente usará a versão para empresas, então vamos dar uma olhada nele também.

## Como a versão para empresas funciona

Em linhas gerais, a versão para empresas consiste em dois componentes principais:

-   Tableau Desktop — o programa instalado no computador, que permite que você se conecte com dados, analisá-los, e construir dashboards.
-   Tableau Server — o servidor de programas responsável por publicar dashboards como páginas web.

Para trabalhar com Tableau Desktop, siga os seguintes passos:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.4.3PT_1658737906.png)

1.  Conecte-se às fontes de dados usando Tableau Desktop. Essas fontes podem ser documentos de bancos de dados; Tableau pode conectar-se a praticamente qualquer armazenamento de dados e sistemas de processamento. Cada documento Tableau conecta-se a várias fontes e as une a cada vetor.
2.  Os dados da fonte são visualizados como gráficos e tabelas. Então os gráficos e tabelas podem ser colocados nos dashboards. Mais do que isso, Tableau pode criar vários filtros para gerenciar os conteúdos dos graficos, tabelas, e dashboards.
3.  Depois que você editou os documentos Tableau Desktop, você como armazená-los no seu computador e colocá-los do servidor Tableau. O Tableau Server conecta-se às fontes de dados pretendidos automaticamente, e gerencia as configurações de privacidade e segurança para que apenas usuários autorizados tenham acessos a dados internos.

Diferentes versões do Tableau incluem componentes adicionais. Por exemplo:

-   Tableau Prep Builder — uma ferramenta especial para preparar dados para processamento com Tableau Desktop
-   Tableau Reader — um programa para visualização de dashboards e visualizações

No entanto, esses componentes são usados raramente. Tableau desktop é a maior ferramenta para analistas trabalharem com Tableau.

## Como Tableau Public funciona

A maneira como Tableau Public funciona é um pouco mais simples:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.4.3.2PT_1658737937.png)

As etapas básicas de organização são como na versão empresarial: conectar dados aos dados → criar visualizações → criar dashboards → publicar.

As maiores diferenças são as seguintes limitações:

-   Você não pode conectar-se com bancos de dados. No Tableau Public, você pode usar apenas arquivos como fontes de dados.
-   Você não pode salvar os resultados no seu computador; você pode apenas armazená-los na nuvem do Tableau Public.
-   O Tableau Public tem apenas duas opções de privacidade: você pode tornar seu dashboard acessível para toda a internet ou negar acesso para todo mundo exceto você mesmo.

Caso contrário, Tableau Public tem essencialmente as mesmas características do que a versão para empresas.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-31-222Z.md
### Última modificação: 2025-05-28 20:15:31

# Preparando Dados - TripleTen

Capítulo 4/6

Tableau

# Preparando Dados

Nesta lição, você vai aprender:

-   Quais formatos de dados Tableau prefere e como obtê-los usando Python.
-   Como exportar dados para o Tableau de um banco de dados implantado em uma máquina virtual.
-   Como carregar dados de um arquivo de texto para Tableau.

## Preparando dados de arquivos externos

Primeiro nós vamos aprender a conseguir dados no formato que é melhor para Tableau. Nós vamos mostrar como fazer isso usando dados sobre populações urbanas e rurais em vários países desde de 1950. Os arquivos de dados contém os seguintes campos:

-   `País`
-   `Ano` — o ano em que os dados foram coletados (AAAA-01-01)
-   `Urbana` — a população urbana
-   `Rural` — a população rural

Vamos olhar para isso:

```
import pandas as pd

urbanization = pd.read_csv('urbanization_countries.csv')
urbanization.head(10)
```

Aqui está o resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-15T205957.734_1594836021.png)

Em Tableau-speak, esse tipo de estrutura de dados é chamada de **crosstab**. Nós podemos ver duas colunas descrevendo populações: uma contém dados sobre população rural, a outra sobre população urbana. É melhor transformar crosstabs então eles ficarão assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-15T210001.363_1594836042.png)

Isso é chamado de "row-oriented table" (tabelas orientadas para linhas). Tableau por si só é um sistema row-oriented, então é fácil processar dados dessa forma.

Vamos olhar como podemos transformar uma crosstab em uma tabela row-oriented usando Python:

```
urbanization_rot = urbanization.melt(id_vars = ['País', 'Ano'], 
                                       value_vars = ['Urbana', 'Rural'])
urbanization_rot
```

Aqui está o resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-15T210054.516_1594836069.png)

Para transformar a tabela, nós usamos a função `melt`. Isso nos permite transformar as colunas do DataFrame em linhas. Os parâmetros básicos dessa função são:

-   `id_vars` — colunas para serem deixadas inalteradas
-   `value_vars` — colunas para serem convertidas em colunas em linhas

Quando a função é executada, os nomes das colunas são postos em uma nova coluna chamada`variável`, enquanto os valores das colunas vão dentro da coluna `valores`.

Os nomes `variável` e `valor` não são informativos, então vamos deixar nossa moldura row-oriented um pouco mais limpa e armazená-la em Tableau para trabalhos futuros:

```
urbanization_rot = urbanization.melt(id_vars = ['País', 'Ano'], 
                                       value_vars = ['Urbana', 'Rural'])
urbanization_rot = urbanization_vertical.rename(columns = {'variable': 'Tipo de População', 
                                                             'value': 'População'})
urbanization_rot.to_csv('urbanization_rot.csv', index = False)
urbanization_rot
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-15T210118.269_1594836098.png)

Agora nós temos dados com que podemos trabalhar.

## Preparando dados de um banco de dados implementados em uma máquina virtual na nuvem

Geralmente, Tableau é capaz de conectar bancos de dados diretamente, sem qualquer passo adicional, mas a versão livre não é. Felizmente, ainda existe um jeito de conectar-se com bancos de dados; só dá um pouco mais de trabalho. Veja o curso extra sobre Automação para aprender como ler dados de um banco de dados e baixá-lo de uma máquina vitual.

## Conectando com os dados em Tableau Public

Para conectar-se com os dados em Tableau, siga os seguintes passos:

-   Inicie Tableau Public.
-   Dê uma olhada na tela para gerenciar o projeto (nós vamos discutir isso depois) e conexão de dados.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6_1662569970.png)

-   Selecione "Arquivo de texto" da área "Conectar" no lado esquerdo da tela. Na janela que aparecer, encontre e selecione o arquivo que você quer usar como fonte de dados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/7_1662569986.png)

-   Tableau irá carregar os dados e imprimir uma amostra na tela:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/8_1662570006.png)

-   Os dados foram carregados com sucesso; agora nós podemos passar a analisá-los. Pressione "Sheet 1." Tableau irá abrir a tela para trabalhar com uma nova folha:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/9_1662570029.png)

No canto superior esquerdo você pode ver os parâmetros e valores do arquivo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/10_1662570060.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/1/moved_11_2.mp4"></iframe>

Pratique por si só: baixe o arquivo dessa lição e carregue-a no Tableau.

[urbanization\_rot.csv](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/urbanization_rot.csv)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-32-535Z.md
### Última modificação: 2025-05-28 20:15:32

# Tabelas e Cálculos Simples - TripleTen

Capítulo 4/6

Tableau

# Tabelas e Cálculos Simples

Nessa lição, você irá aprender a criar tabelas simples com Tableau e descubra como nosso cálculo interno funciona.

## Interface da folha de edição

Você pode criar tabelas e desenhar gráficos com a interface da folha de edição. A ilustração abaixo mostra a interface de elementos básicos:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/12_redaktirovat_1662570201.png)

Uma **área de exibição de dados (1)** mostra o gráfico ou a tabela que você criou.

**Areas** **2 e 3** ajudam você a selecionar e declarar novas dimensões e medidas. No nosso caso, o país, ano e tipo de população são dimensões, enquanto o tamanho da população é a medida. As dimensões são todas colunas não numéricas da tabela original. O Tableau categoriza automaticamente todas as colunas com valores de números inteiros e números de pronto flutuante como medidas.

As **área de controle de páginas (4)**. Páginas são um tipo especial de filtro; você pode usá-las para criar visualizações dinâmicas.

As **área de controle de filtros (5)** permitem que você crie filtros baseados nas suas dimensões e medidas. Nós vamos discutir filtros daqui a pouco.

As **área de controle de aparência (6)** permitem que você modifique como gráficos e tabelas serão mostrados na área de exibição dos dados. Aqui você pode definir cores, fonte, tipos de gráficos, e outros parâmetros.

As **área de controle de linhas e colunas (7)** permitem que você defina as dimensões horizontal (eixo X) e vertical (eixo Y) dos dados.

## Criando um crosstab simples

Vamos começar com a operação mais simples: criar tabelas. Nós vamos fazer uma tabela onde as colunas contêm anos, e as linhas contêm os países. Células irão conter a população de cada país em cada ano.

Para criar uma tabela:

1.  Arraste a dimensão Ano para a área das Colunas.
2.  Arraste a dimensão Países para a área das Linhas.
3.  Arraste a as medidas de População para a área de exibição.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/13_1662570217.png)

Nós vamos obter o seguinte crosstab:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/14_1662570240.png)

Perceba que a população de cada combinação país/ano é indicada em uma célula. O Tableau automaticamente soma os dados para todos os valores da dimensão Tipo da População (que nós não adicionamos na tabela). Vamos adicionar isso e ver o que obtemos. Arraste a dimensão Tipo de População para a área das linhas:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/15_1662570258.png)

Resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/16_1662570281.png)

Agora as células mostram as populações rural e urbana separadamente.

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/1/moved_17.mp4"></iframe>

## Criando um campo calculado simples

Em Tableau você pode usar as dimensões e medidas originais para criar suas próprias medidas e dimensões. Vamos dizer que você queira exibir a população como milhões de pessoas. Nós vamos introduzir medida de cálculo, População Milhão:

1.  Clique com o botão direito do mouse na área de controle de medidas e selecione "Create Calculated Field"(Criar um Campo Calculado):

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/17_1662570363.png)

2.  No formato que aparece, insira o nome da medida e a fórmula do seu cálculo, depois pressione "OK":

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/18_1662570376.png)

3.  A nova medida irá aparecer na lista de medidas. Perceba que um sinal de igual aparece antes do nome; esse é um identificador especial para medidas calculadas e dimensões.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/19_1662570440.png)

Agora, se nós arrastarmos "População Milhão" para o quadrado "Text" da área de controle aparência, nós vamos obter o seguinte:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/20_1662570475.png)

A linguagem Tableau para campos calculados tem todos os comandos básicos que você precisará para trabalhar expressões numéricas e strings. Será fácil aprender já que você já sabe Python. Você pode encontrar a lista completa de comandos aqui: [https://help.tableau.com/current/pro/desktop/en-us/functions\_all\_alphabetical.htm](https://help.tableau.com/current/pro/desktop/en-us/functions_all_alphabetical.htm) _(os materias estão em inglês)_

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/1/moved_21.mp4"></iframe>

## Campos calculados para dimensões

Você pode criar dimensões calculadas de forma similar. Vamos dizer que queremos agrupar países por região. Nós gostaríamos de começar criando a região AN (América do Norte) para Canadá, EUA, e México. Para fazer isso, vamos criar a medida calculada chamada Região:

1.  Clique com o botão direito do mouse na área de controle de medidas e selecione Criar Campo Calculado.
    
2.  Insira o nome da medida e a fórmula para o seu cálculo e pressione OK:
    

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/22_1662570539.png)

3.  Tableau irá mostrar a nova dimensão na lista de dimensões:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/23_1662570562.png)

Agora, se criarmos uma tabela em que as linhas contêm as dimensões País e Região, obteremos o seguinte:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/24_1662570583.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/1/moved_39.mp4"></iframe>

## Tabelas, painéis e células e cálculos usando-as:

A coisa mais difícil sobre Tableau é descobrir como cálculos internos acontecem. O Tableau vê todos os dados como crosstabs e os divide em três níveis de cálculos:

-   tabela
-   painel
-   célula

Aqui está o que queremos dizer:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.4.5.2PT_1658738370.png)

Como você pode ver, as tabelas incluem todas as células. Um painel consiste em células representando as dimensões mais básicas (Ano, País, Tipo de População).

Vamos olhar como isso funciona. Vamos criar outra medida calculada, que irá encontrar o tamanho total da população, e chamá-la de População Total:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/27_1662570881.png)

Como você pode ver, a fórmula tem duas funções, SUM (SOMA) e TOTAL:

-   SUM é uma função **agregada** que funciona no nível da célula.
-   TOTAL é uma função da **tabela** que nos permite juntar os resultados das muitas células ou mesmo painéis.

Em outras palavras, a expressão `TOTAL(SUM([Population]))` afirma: "Some o tamanho da população em cada célula e depois junte os resultados." Vamos ver como Tableau as junta. Arraste a medida População Total para a área de rotulagem de dados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/28_1662570901.png)

Resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/29_1662570916.png)

Como você, nós temos dois números bem grandes que não parecem nem um pouco com os dados sobre o Afeganistão. Parece que Tableau automaticamente sumarizou as populações rurais e urbanas para todos os anos em cada país. Vamos conferir: encontre a medida População Total da área de controle de aparência, clique no botão direito sobre ela e selecione Compute Using (Cálculo de Uso). Aqui você ver um número de opções afetando a forma como a tabela de função TOTAL é computada:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/30_redaktirovat_1662570934.png)

Primeiro, vamos estudar o nível da tabela:

-   `Table (across)` define o cálculo pelo eixo horizontal da tabela. Para cada país e tipo de população, os valores de todos os anos é somado (esse modo de cálculo padrão).
-   `Table (down)` faz o mesmo ao longo do eixo vertical. Se definirmos assim, vamos ver o total anual da população mundial em cada ano:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-15T220907.120_1594840159.png)

-   `Table (across then down)` zig-zags em volta da tabela ao longo do eixo horizontal, e `table (down then across)` faz o mesmo ao longo do eixo vertical. O resultado de SUM será o mesmo para ambos os modos e representa a soma de todas células:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-15T220939.813_1594840194.png)

Agora vamos olhar as opções de cálculo para painéis:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/33_1662571113.png)

-   `Pane (down)` adiciona as células dentro do painel do topo para baixo ao longo do eixo vertical, para nós obtermos a população total para um país para um determinado ano:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.4.5.4PT_1658738458.png)

-   `Pane (across then down)` zig-zags sentro de um painel separado ao longo do eixo horizontal, e `table (down then across)` faz o mesmo ao longo do eixo vertical. Em ambos os casos, o resultado irá representar a soma dos dois tipos de população para um determinado país para cada ano:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.4.5.5PT_1658738487.png)

Agora vamos olhar para cálculos no nível da célula. Como você deve esperar, o resultado iguala o valor de uma célula:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-15T221050.518_1594840261.png)

Agora só precisamos descobrir o que os modos de computação nomeados após as dimensões fazem:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/37_1662571152.png)

-   `Country` funciona de forma parecida com `table (down)`, mas ele preserva o detalhamento por tipo de população:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-15T221125.770_1594840296.png)

`Population Type` é parecido com `cell (down)`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-15T221143.732_1594840314.png)

`Year` funciona como `table (across)`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-15T221201.832_1594840333.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/1/moved_39.mp4"></iframe>

## Porcentagens de cálculo

No seu trabalho como analista você frequentemente terá que calcular porcentagens usando os dados originais. Vamos ver como isso é feito:

1.  Declare a medida `% of Total` e a defina:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/40_1662571223.png)

Perceba que não definimos a soma para a medida População Total, como já está presente na medida em si.

A fórmula que iremos somar a população no nível da célula usando `SUM (Population)` e depois a divida pelo resultado do TOTAL (dependendo do modo de cálculo).

2.  Agora vamos definir o modo padrão de exibição para `% do Total`. Para fazer isso, clique com o botão direito do mouse em `% of Total` na área de controle de medidas e selecione Default Properties (Propriedades Padrão) > Number Format (Formato do Número) do dropdown:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/41_1662571249.png)

Na nova janela, selecione Percentage (Porcentagem), indique o número de casas decimais, e pressione "OK":

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/42_1662571272.png)

Agora `% of Total` será mostrado como porcentagem em qualquer tipo de visualização.

3.Arraste a % do Total de medidas para controle da área de rotulagem de dados, depois clique com o botão direito do mouse e selecione o modo de cálculo `Pane (down)`.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/41_1662571331.png)

Agora `% do Total` irá mostrar a parte das populações urbana e rural de cada país para cada ano:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-15T222651.407_1594841223.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/1/moved_46.mp4"></iframe>

## Gerenciando as dicas de ferramentas

Como muitos de vocês devem ter notado.Tableau tem **toolltips**: abrem com dicas que aparecem quando o cursor está sobre a célula da tabela ou ponto do gráfico:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/45_1662571588.png)

Às vezes a dica é concisa, mas Tableau permite que você adicione muitas informações se você quiser. Para fazer isso, clique área de controle tooltip na área de visualização de aparência:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/47_1662571608.png)

Tableau irá mostrar uma janela onde você pode editar o texto:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/48_1662571625.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/1/moved_49.mp4"></iframe>

## Tabelas de destaque

Tabelas de destaque são convenientes quando você precisa mostrar tabela de dados ou cor das células de acordo com uma certa regra.

Para criar uma tabela de destaque, siga os passos seguintes:

1.  Arraste a dimensão para áreas das colunas de controle, Países para a área de controle de linhas, e a medida População para a área de controle de rotulagem de dados. Selecione o tipo de gráfico Tabela de destaque do dropdown Show Me:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/50_1662571678.png)

2.  Tableau irá fazer a seguinte tabela, cujos elementos irá ser da mesma cor:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/52_1662571705.png)

3.  Agora vamos mudar a paleta de cores. Para fazer isso, clique na área de controle de cor e depois de Edit Colors:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/53_1662571723.png)

Na nova janela, selecione a paleta de cores e o número de passos de cores que você gostaria que estivessem no seu diagrama:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/54_1662571741.png)

Ordene as linhas por tamanho de população em ordem decrescente e você obterá o seguinte:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/55_1662571755.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/1/moved_56.mp4"></iframe>

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-33-864Z.md
### Última modificação: 2025-05-28 20:15:34

# Filtros - TripleTen

Capítulo 4/6

Tableau

# Filtros

Nessa lição, você irá aprender a criar filtros e usá-los para gerenciar dados.

## Criando um filtro simples

Nós criamos uma tabela mostrando dados sobre populações urbanas e rurais decompostas por país e ano. Vamos dizer que você queira mostrar apenas certos anos e países; depois você precisará usar os filtros Tableau. Vamos começar por países. Para filtrar os dados por país, faça o seguinte:

1.  Arraste a dimensão País para a área de edição de filtros:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1_1662576304.png)

2.  O sistema irá mostrar a janela de filtros. Nós podemos selecionar os países pretendidos aqui mesmo, mas é melhor fazer uma tabela mais interativa, então nós só vamos pressionar OK:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/2_1662576318.png)

3.  Na área de edição de filtros, encontre o filtro necessário e clique com botão direito do mouse nele. Selecione Show Filter do menu:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/3_1662576332.png)

4.  Tableau irá mostrar todos os valores na parte direita da tabela:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/4_1662578161.png)

5.  Agora você pode selecionar os países e, que te interessam (vamos dizer que eles são `Montenegro`, `Portugal`, e `United States`):

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/5_1662578489.png)

O filtro é enorme, e não parece muito bom. Nós podemos fazer com que ele fique melhor, por exemplo, o transformando em um menu dropdown de várias opções. Para fazer isso, clique o botão no canto direito superior do nosso filtro e selecione `Multiple Values (dropdown)`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6_1662578509.png)

TABLEAU irá imediatamente mudar a aparência do filtro, tornando-o mais compacto:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/7_1662578543.png)

Agora nós podemos selecionar os países que quisermos do dropdown:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/8_1662578571.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/2/moved_9.mp4"></iframe>

## Filtrando por tempo

Definindo um filtro de tempo em Tableau é ainda mais simples. Para fazer isso:

1.  Arraste a dimensão Ano para a área de edição de filtros:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/10_1662578642.png)

2.  O sistema irá mostrar a janela de filtros. Você não precisa mudar nada, apenas pressione Next:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11_1662578659.png)

3.  Na próxima tela, você pode selecionar o alcance da data. Nós não vamos mudar nada aqui também, apenas pressione OK:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/12_1662578677.png)

4.  Selecione o filtro da área de controle de filtros e o defina para ser exibido próximo à tabela:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/13_1662578705.png)

Assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-16T230932.851_1594930180.png)

Vamos selecionar o período de tempo de 1965 a 1968 ao invés de todo o intervalo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-16T230946.361_1594930195.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/2/moved_16.mp4"></iframe>

## Aplicando filtros a folhas com a mesma fonte de dados

Na maioria dos casos você irá criar várias tabelas ou gráficos, ou até mesmo dashboards, usando a mesma fonte de dados. Quando você cria um filtro em Tableau, por definição ele é aplicado na folha onde foi criado. O que faremos se quisermos fazer um filtro "universal" que funciona em diferentes folhas que compartilham uma fonte de dados?

Para fazer isso, selecione o filtro da área de controle de filtros e clique com o botão direito do mouse nele.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/17_1662578826.png)

Depois selecione Apply to Worksheets (Aplicar em Folhas de Trabalho)→ All Using This Data Source (Tudo Usando Essa Fonte de Dados). O filtro será aplicado para todas as folhas que usam a fonte.

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/2/moved_18.mp4"></iframe>

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-35-797Z.md
### Última modificação: 2025-05-28 20:15:36

# Publicando Dashboards - TripleTen

Capítulo 4/6

Tableau

# Publicando Dashboards

Nós já fizemos bastante coisa: fizemos upload dos dados do arquivo, criamos uma tabela, criamos alguns campos calculados, e definimos os filtros. É bom termos certeza de salvarmos o arquivo para não perdermos nosso trabalho.

Como você sabe, você não pode armazenar arquivos na sua máquina pessoal, com Tableau Public, mas você pode salvá-las na nuvem Tableau Public.

Não publique dados confidenciais e/ou sensíveis online. Recomendamos o uso de dados fictícios, falsos ou anônimos - será o suficiente para criar a Prova de Conceito para seus clientes. No entanto, você ainda pode usar a nuvem do Tableau Public para criar um portfólio com conjuntos de dados públicos.

Para fazer isso:

1.  Abra o menu File e selecione `Save to Tableau Public`, ou pressione `Ctrl + S`.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/19_1662578983.png)

2.  Depois você precisará se logar. Se você não em uma conta no Tableau Public, pressione "Create one now for free."

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/20_1662578998.png)

3.  Tableau irá abrir uma janela onde você pode editar o nome do seu documento. Entre nela e pressione Save.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/21_1662579016.png)

Tableau irá começar a carregar seu documento na nuvem. O progresso será mostrado em uma janela separada. Não deve demorar mais menos de 5 a 10 segundos.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/22_1662579066.png)

4.  Quando o documento é carregado, a janela irá fechar automaticamente. Tableau irá abrir seu navegador padrão e exibir seu trabalho publicado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/23_1662579089.png)

5.  Pressione "Edit details" para editar os parâmetros do documento publicado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/24_1662579105.png)

O sistema irá mostrar uma forma onde você pode editar os parâmetros de visualização:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/25_1662579145.png)

Aqui você pode renomear sua visualização e adicionar uma descrição ou abas. Se você quiser prover acesso público ao seu trabalho, tique "Allow others to download or explore e copy this workbook and its data." (Permitir outros a explorar e copiar esse workbook e os dados). Lembre-se de que toda a internet terá acesso a esse projeto, incluindo os dados originais.

Nós recomendamos também que você tique "Show workbook sheets as tabs."(Mostrar folhas do workbook como abas). Todas as folhas estarão disponíveis na versão web e vão abrir em abas separadas. Isso os tornará mais fáceis de navegar.

6.  Ótimo! Seu trabalho foi salvo. Agora quando você abrir o Tableau, você o verá na lista de documentos disponíveis.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/27_1662579169.png)

A seguir, você irá aprender como fazer gráficos e criar dashboards.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-37-131Z.md
### Última modificação: 2025-05-28 20:15:37

# Gráficos Simples - TripleTen

Capítulo 4/6

Tableau

# Gráficos Simples

Nessa lição, você vai aprender a fazer gráficos simples e descobrir como editar seu tamanho, cor e legendas.

## Mapa de variações (Com marcas de densidade) (Heatmaps)

Um mapa de variação (ou gráfico de densidade) é uma tabela de contingência (crosstab) cujas células mostram elementos gráficos coloridos de acordo com a paleta que você escolher, ao invés de valores numéricos.

Como fazer um mapa de variação:

1.  Arraste a dimensão Year para a área de controle de colunas, a dimensão Country para a área de controle de linhas, e a medida Population para a área de controle de legendas. Abra o mapa suspenso "Show Me" e selecione o tipo de gráfico mapa de calor:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1_1662579432.png)

2.  O Tableau irá fazer a seguinte tabela, onde todos os elementos serão da mesma cor e seu tamanho vai variar de acordo com o valor de Population. A tabela não parece muito ilustrativa, então vamos ordená-la por tamanho da população em ordem decrescente (para que os líderes estejam nas linhas do topo) e adicione alguma cor. Para fazer isso, arraste a medida Population para a área de controle de cor:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/2_1662579448.png)

Resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/3_1662579463.png)

3.  Legal! Mas vamos mudar a paleta de cores. Clique na área de controle de cores e selecione "Edit colors":

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/4_1662579481.png)

Depois selecione a nova paleta de cores e o número dos passos da cor que você gostaria de ver no seu diagrama:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/5_1662579499.png)

Resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6_1662579648.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_7.mp4"></iframe>

## Bolhas em pacotes (Packed bubbles)

Gráficos de bolhas em pacotes são uma opção perfeita quando você precisar indicar o tamanho das categorias em comparação de umas com as outras. Por exemplo, nós podemos usá-los para mostrar as populações relativas de vários países.

Para fazer um gráfico de bolhas:

1.  Arraste a dimensão Country para a área de controle de colunas e a medida Population para a área de rótulos de dados. Abra o mapa suspenso Show Me e selecione o tipo de gráfico packed bubbles:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/8_1662579672.png)

2.  Tableau irá mostrar um gráfico de bolhas simples em pacotes. Vamos adicionar um pouco de cor, como fizemos com gráficos anteriores. Nós também adicionaremos valores das populações às legendas:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/9_1662579692.png)

3.  Parece bastante informativo, mas parece que temos o total de países para cada ano. Nós podemos consertar isso usando filtros, mas existe outra solução, bem mais empolgante: páginas. Arraste a dimensão Year para a área de controle de páginas:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/10_1662579736.png)

Agora o gráfico ficou assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11_1662579764.png)

Perceba que o Tableau selecionou automaticamente o primeiríssimo ano de um dado intervalo e mostrou o tamanho da população apenas para 1950. Páginas são essencialmente filtros dinâmicos para desenhar gráficos animados. Clique em Reproduzir na área de controle das páginas para assistir a uma curta animação que ilustra a população global ao longo do tempo. Os botões à direita do Play permitem você ajustar a velocidade que as páginas vão mudar:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/12.2_1662579785.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_13.mp4"></iframe>

## Mapas em árvore (Treemaps)

Mapas em árvore foram criados originalmente para mostrar o tamanho dos arquivos em pastas. Vamos ver como eles funcionam.

Vamos dizer que um estudante chamado Noah tem uma pasta chamada "Noah" com alguns arquivos e duas subpastas. Elas formam uma hierarquia, ou estrutura de pastas, que vai ser assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.4.8.3PT_1658738690.png)

Cada pasta tem um mapa de árvore separado. Você pode clicar em um quadrado específico e ver o diagrama do próximo nível da árvore (no caso de Noah, a próxima pasta).

Nossos dados não têm uma hierarquia clara, então vamos praticar construindo um treemap com apenas um nível:

1.  Arraste a dimensão Country para a área de controle de colunas e a medida Population para a área de rótulos de dados. Abra o mapa suspenso Show Me e selecione o tipo de gráfico treemap:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/14_1662579860.png)

2.  Especifique o rótulo dos dados, cor, e controle de páginas, como fizemos mais cedo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/15_1662579877.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_16_1.mp4"></iframe>

## Gráfico de pizza

Você já aprendeu sobre gráficos de pizza. Para construir um em Tableau:

1.  Arraste a dimensão Country para a área de controle de colunas e a medida Population para a área de rótulos de dados. Abra o mapa suspenso Show Me e selecione o tipo de gráfico de pizza:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/17_1662579941.png)

2.  Tableau irá mostrar um gráfico de pizza sem nenhuma legenda ou ordenação. Nós podemos adicionar legendas movendo as dimensões pretendidas e medidas para a área de controle de rótulos de dados. Vamos ordenar os setores por população total em ordem crescente passando pelo botão correspondente:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/18_1662580001.png)

3.  O novo diagrama indica que a população total para todos os anos está sendo mostrada. Vamos adicionar um filtro ao ano 2015:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/19_1662580018.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_20.mp4"></iframe>

Ficou ótimo, mas vamos aprender como destacar os 'leaders' (líderes) e coloque os países remanescentes na categorias "Others". Vamos considerar "líderes" países com populações maiores do que 150 milhões. Vamos criar um novo campo calculado, Países com 150M+ usando a fórmula:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/21_1662580465.png)

Para países com grandes populações, essa fórmula irá retornar o nome deles; para outros países ele vai gritar "Others"(Outros).

Aqui está o que obtemos:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/23_1662580484.png)

E aqui está como nós o obtemos:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_24.mp4"></iframe>

## Barras horizontais

Esse é um outro tipo de gráfico que você já conhece bem. Construí-lo em Tableau é muito simples:

1.  Arraste a dimensão Countries para a área de controle de colunas e a medida População para área de controle de rótulos de dados. Abra o mapa suspenso e selecione o tipo de gráfico de barras:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/25_1662580592.png)

2.  Ordene os resultado em ordem decrescente e adicione a medida População à area de controle de rótulos dos dados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/26_1662580610.png)

3.  O gráfico resultante aparentemente mostra a soma total para todos os anos, então adicione um filtro de ano e selecione 2015:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/27_1662580631.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_28.mp4"></iframe>

## Gráfico de barras empilhadas (Stacked bars)

1.  Para evitar adicionar muitos detalhes para o gráfico, pegue o campo 150M+ Countries ao invés de Countries e arraste para a área de controle de colunas. Arraste a medida Population para a área de controle de rótulo de dados, como sempre. Abra o mapa suspenso Show Me e selecione o tipo de gráfico stacked bars (gráfico de barras empilhadas):

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/29_1662580969.png)

2.  Adicione o campo 150M+ Countries e a medida Population para rótulos no gráfico resultante. Crie um filtro de ano e selecione 2015:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/30_1662580984.png)

3.  Agora vamos fazer o gráfico ficar um pouco mais informativo e divida a população em urbana e rural:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/31_1662580999.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_32.mp4"></iframe>

## Gráfico de barras agrupadas (Side-by side bar)

1.  Arraste 150M+ Countries e Population Type para a área de controle de área de controle de colunas, e a medida Population para a área de controle de rótulo de dados. Abra o mapa suspenso Show Me e selecione o tipo de gráfico side-by-side (gráfico de barras agrupadas):

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/33_1662581032.png)

2.  Ordene o gráfico resultante em ordem decrescente. Crie um filtro de ano e selecione 2015:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/34_1662581054.png)

3.  Defina a variável calculada `%of Total` como rótulo de dados. Perceba que Tableau define o mode de computação correto para essa variável por padrão — Table Across. Nesse caso, o gráfico inteiro é a tabela, então a porcentagem encontrada relativa à população total pelas dimensões.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/35_1662581073.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_36.mp4"></iframe>

## Histogramas

Arrate a medida Population para a área de controle de rótulos, abra o mapa suspenso Show Me, e selecione o tipo de gráfico histograma:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/37_1662581124.png)

O único problema é que o resultado não é muito informativo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/38_1662581142.png)

Isso é porque temos dados em vários países, com população rural e urbana misturadas (uma única observação representa populações rural e urbanas em um país em um determinado ano). Vamos dividir a população de acordo com o tipo e construir histogramas para dois países: EUA e Rússia. Para fazer isso, arraste a dimensão Population Type para a área de edição de colunas e a dimensão Country para a área de edição de colunas e linhas. No filtro de países, selecione Estados Unidos e Federação Russa. Aqui está o resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/39_1662581165.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_40.mp4"></iframe>

## Gráfico de caixa (Box-and-whisker)

1.  Arraste a dimensão Country para a área de controle de colunas e a medida Population para a área de rótulos de dados. Abra o mapa suspenso "Show Me" e selecione tipo de gráfico de caixa:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/41_1662581201.png)

2.  O Tableau irá te dar um conjunto com muitos valores atípicos:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/42_1662581221.png)

Nesse gráfico, uma única observação (dot) é o total de populações urbana e rural no país em todos os anos. Primeiro vamos dividir a população e colocar cor usando o campo 150M+ Countries:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/43_1662581245.png)

3.  Agora vamos arrastar Population para rótulos de dados e limitar as observações ao ano 2015 usando um filtro:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/44_1662581267.png)

4.  Agora nós podemos ver claramente que Índia e China têm os principais valores atípicos em ambas as categorias. Tableau fornece uma maneira fácil de se livrar dos valores atípicos: clique com o botão direito do mouse no país de interesse na legenda colorida e clique em Exclude:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/45_1662581289.png)

Aqui está o resultado depois da exclusão de Índia e China:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/46_1662581697.png)

Quando os dados são excluídos, eles não são removidos. Tableau automaticamente adiciona um filtro à área de controle de filtros, e se você o deletar, o dados aparecerão de novo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/46_--_kopiia_1662581309.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_42.mp4"></iframe>

## Gráfico de dispersão (Scatterplot)

Vamos arredondar essa lição olhando para gráficos de dispersão. Para construí-los, nós vamos precisar de duas medidas numéricas, mas temos apenas uma. O que faremos? Vamos dividir Population em duas medidas: Urban e Rural, declarando duas novas medidas calculadas:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/47_1662581601.png) ![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/48_1662581716.png)

Para ver como eles funcionam, vamos criar uma tabela cujas células contenham a dimensão Country e que as colunas contenham a dimensão Population Type. As medidas Urban e Rural estarão nas intersecções.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/49_1662581798.png)

Como você pode ver, Urban soma apenas as linhas em que o tipo de população é urbana, enquanto Rural soma todo o resto.

Agora vamos construir o gráfico de dispersão em si:

1.  Arraste as medidas Urban e Rural para a área de edição de rótulos de dados e o menu suspenso Show Me, e selecione gráfico de dispersão:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/50_1662581819.png)

Nós vamos obter o seguinte gráfico:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/51_1662581837.png)

Todos o dados foram unidos em um único ponto, representando o número total de moradores rurais de todos os países em todos os anos no eixo X, e a soma de todos os moradores no eixo Y.

2.  Nós realmente não queremos o total de dos totais, então vamos dividir tudo por país. Arraste a dimensão Country para área de edição de detalhes:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/52_1662581860.png)

3.  Depois vamos mudar o tipo de marcadores (pontos mostrados no gráfico):

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/53_1662581880.png)

4.  Vamos definir o tamanho e suas legendas de acordo com a medida Population:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/54_1662581912.png)

5.  Vamos arrastar o campo 150M+ Countries para a área de edição de cores e Year para a área de controle de páginas para que possamos discernir dados de diferentes anos:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/55_1662581946.png)

Agora você pode usar páginas para assistir uma animação mostrando como a população urbana e rural mudou ao longo dos anos em diferentes países.

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_56all.mov"></iframe>

## Embedded Content

---

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-38-463Z.md
### Última modificação: 2025-05-28 20:15:38

# Gráficos Lineares e Gráficos de Área Empilhada - TripleTen

Capítulo 4/6

Tableau

# Gráficos Lineares e Gráficos de Área Empilhada

Nesta lição, você vai aprender a construir gráficos lineares e gráficos de área empilhada e descobrir como trabalhar com os parâmetros de data e hora.

Como começar pelos parâmetros de hora.

Tableau analisa automaticamente os tipos de dados no conjunto de dados original, e se um dos campos parecer conter datas e hora, automaticamente é designada um rótulo especial:

Se você colocar tal dimensão nas linhas e área de controle de colunas, você será capaz de ajustar o nível de detalhes (por exemplo, mostrar apenas anos, trimestres, meses, etc.). Para fazer isso, clique com o botão direito em dimensão:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/57_1662582779.png)

A data exata é uma opção particularmente importante. É normalmente usado para gerenciar o nível de detalhes para eixos de hora nos gráficos.

Vamos ver como conseguimos usar um eixo X baseado em hora quando construímos gráficos lineares e de área. O eixo X não tem que ser temporal; ele também é usado para indicar outras dimensões.

## Gráficos lineares

Para fazer um gráfico linear:

1.  Arraste a dimensão Year para a área de controle de colunas e a medida Population para a área de controle de rótulos de dados. Para Year, defina o nível de detalhes como Exact Date. Abra o menu suspenso e selecione o tipo de gráfico Lines:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/58_1662582800.png)

2.  Obtivemos um gráfico mostrando como a população global total mudou ao longo dos anos. Vamos torná-lo mais informativo dividindo a população em urbana e rural e adicionando rótulos de dados:
    
    -   Arraste a dimensão Population Type para a área de controle de cor e dados.
    -   Adicione a medida à área de controle de rótulos de dados.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/59_1662582833.png)

3.  Você verá um gráfico sem qualquer rótulo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/60_1662582851.png)

Para adicionar rótulos ao final das linhas, clique em área de controle de rótulos de dados e tique "Allow labels to overlap other marks" e "Label end of line":

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/61_1662582899.png)

O resultado é esse gráfico limpo, a partir do qual podemos concluir que só em 2007 o número de moradores urbanos ultrapassou o de moradores rurais:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/62_1662582927.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_79.mp4"></iframe>

## Gráficos de duas linhas

Às vezes é bom usar dois eixos Y quando construir gráficos. Em Tableau, tais visualizações são chamadas de duas linhas. Vamos construir um gráfico que ten Urban ao longo do eixo Y e Rural pelo ao longo do direito. Nós queremos que ele se pareça da mesma forma do que na última visualização.

1.  Arraste a dimensão Year para a área de controle de colunas e as medidas Urban e Rural para área de controle de rótulos de dados. Para Year, defina o nível de detalhes como Exact Date. Abra o mapa suspenso e selecione o tipo de gráfico Duas Linhas:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/63_1662582956.png)

2.  Tableau irá mostrar um gráfico com quase os mesmos gráficos sobre população urbana e rural. Mas alguma coisa realmente deu errado: os gráficos não têm mais intersecção no ano 2006. Isso é porque por definição, Tableau estabelece diferentes escalas para o eixo Y. Para sincronizar a escala do eixo, clique com o botão direito no eixo direito e selecione Synchronize Axis:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/64_1662582978.png)

3.  Agora o gráfico se parece com o anterior:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/65_1662583005.png)

4.  Tudo o que precisamos é adicionar legendas ao final das linhas. Nesse caso temos duas medidas, ao invés de apenas uma divisão baseada na dimensão Population Type. Vamos usar a dimensão especial Measure Names para as legendas. Isso é criado automaticamente pelo Tableau. Vamos editar todas as medidas de uma vez e arrasa Measure Names para o controle de área de rótulos de dados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-17T133050.891_1594981864.png)

Nós vamos terminar seguindo as duas linhas:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/67_1662583053.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_80.mp4"></iframe>

## Gráficos de área

Agora vamos ver como fazer gráficos de área em Tableau:

2.  Arraste a dimensão Year para a área de controle de colunas, e a medida Population para áreas de controle de rótulos de dados. Para Year, defina o nível de detalhes como Exact Date. Abra o menu suspenso e selecione o tipo de gráfico Gráficos de Área:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/68_1662583077.png)

2.  Tableau criará um gráfico contendo uma única área. Para tornar isso mais interessante, vamos adicionar a dimensão 150M+ Coutries para a cor e área de controle de legendas:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/69_1662583099.png)

3.  O gráfico resultante é bem mais informativo, mas é uma boa ideia ordenar as áreas logicamente — por exemplo, por população total, em ordem crescente:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/70_1662583119.png)

Para ordenar as áreas, mova o cursor para o canto superior da legenda e pressione o botão pra que você possa ver. Selecione Sort do mapa suspenso:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/71_1662583141.png)

Na nova janela, selecione Field (ordenando pelo valor da medida) e Ascending, e defina Population como medida e Sum como função de agregação. Feche a janela pressionando X.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/72_1662583159.png)

Resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/73_1662583176.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_81.mp4"></iframe>

## Combinação dupla

Às vezes você pode precisar construir um gráfico de barras e um linear em vez de dois gráficos lineares. Nesse caso, use o tipo de gráfico combinação dupla.

1.  Arraste a dimensão Year para a área de controle de colunas, e as medidas Population e Urban para a área de controle de rótulos de dados. Para Year, defina o nível de detalhes como Exact Date. Abra o menu suspenso Show Me e selecione Dual Combination:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/74_1662583197.png)

2.  Tableau irá mostrar um gráfico de combinação dupla com duas medidas:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/75_1662583216.png)

Você pode trocar os eixos das medidas mudando a ordem das linhas na área de edição:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/76_1662583233.png)

Você pode também definir o tipo de gráfico para cada medida usando o mapa suspenso na área de controle de visualização:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/77_1662583255.png)

Resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/78_1662583296.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/3/moved_82.mp4"></iframe>

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-40-634Z.md
### Última modificação: 2025-05-28 20:15:40

# Gráficos Especiais - TripleTen

Capítulo 4/6

Tableau

# Gráficos Especiais

Nesta seção você vai aprender como construir tipos de gráficos especiais: mapas, visualizações circulares, gráficos de Gantt (schedules), e gráficos de marcador.

## Mapas de símbolos

Quando você baixar dados para Tableau, ele automaticamente reconhece o mapa de dados e o marca com uma legenda especial: ![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-07-17T140041.234_1594983653.png)

Você pode criar visualizações de mapa com campos como esse. A versão atual do Tableau oferece duas opções: símbolos e mapas comuns. Mapas de símbolos mostram um símbolo (círculo) para cada país, ilustrando a medida selecionada. Você pode ajustar o tamanho e a cor desses símbolos.

Para criar um mapa de símbolos:

1.  Arraste o campo ''País'' para a área de controle de colunas, e a medida 'População' para a área de exibição de dados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/1_1662583418.png)

2.  Selecione o mapa de símbolos do menu Show Me:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/2_1662583542.png)

3.  O sistema irá mostrar um mapa de símbolos:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/3_1662583576.png)

4.  Você pode ajustar a cor e o tamanho dos círculos:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/4_1662583598.png)

Defina a medida `% of Total` como a legenda:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/5_1662583629.png)

Perceba que por definição, a porcentagem será calculada usando a medida ''País''. Defina o modo de cálculo da célula (Cell) para calcular a porcentagem da população mundial total:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/6_1662583648.png)

Resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/7_1662583770.png)

5.  Observe que no mapa, os valores de ''População'' serão calculados como a soma de todos os anos nos dados originais, portanto, precisamos filtrar os dados pela dimensão ''Ano'' ou colocar ''Ano'' na área de controle das páginas:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/8_1662583788.png)

Use elementos de páginas de controle para ver como a população mundial mudou:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/9_1662583809.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/4/moved_10.mp4"></iframe>

## Mapas

Mapas são criados de maneira similar:

1.  Arraste o campo ''País'' para a área de controle de colunas, e a medida População para a área de exibição de dados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11_1662583863.png)

2.  Selecione a opção de mapa do menu Show Me:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/12_1662583888.png)

3.  Edite a paleta de cores e os rótulos dos dados como fizemos com o mapa de símbolos. Arraste a dimensão 'Ano' para a área de controle de páginas. Resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/13_1662583925.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/4/moved_14.mp4"></iframe>

## Exibição em círculo

Exibições em círculo são um tipo de visualização auxiliar frequentemente usado em combinações com outros gráficos (por exemplo, um gráfico de caixa, "box-and-whisker").

Vamos construir uma exibição em círculo simples:

1.  Arraste o campo ''País'' para a área de controle de colunas, e a medida ''População'' para a área de exibição de dados. Abra o menu suspenso 'Show Me' e selecione a opção de exibição em círculo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/15_1662583993.png)

O Tableau vai criar o seguinte (cada círculo corresponde a um país):

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/16_1662584014.png)

2.  Vamos adicionar alguns detalhes:
    
    -   Arraste a dimensão ''População' Type' para as colunas e áreas de controle de símbolo.
    -   Arraste a dimensão 150M+ Countries para as áreas de controle de cor e de rótulos de dados.
    -   Arraste a medida ''População'' para a área de controle do tamanho do símbolo.

Resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/17_1662584049.png)

3.  Os valores de ''População'' serão somados para todos os anos nos dados, então, precisamos filtrar pela dimensão 'Ano', ou colocar 'Ano' na área de controle de páginas.

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/4/moved_18_1.mp4"></iframe>

## Gráfico de marcador

Gráficos de marcador são perfeitos quando você tiver um certo KPI como objetivo e precisar analisar se se encaixarão.

Imagine que você é um analista na empresa de jogos PeaceGaming, que criou o super sucesso World of Tractors. Antes do jogo ter sido lançado, o diretor do projeto fez a promessa de que o jogo renderia 100 milhões de dólares no seu primeiro ano. Após seis meses, o jogo rendeu 70 milhões de dólares. O gráfico de marcador é assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.4.10.2PT_1658739898.png)

Olhando pra esse gráfico, nós podemos concluir que o jogo alcançou 75% da sua receita alvo seis meses após ter sido lançado. O diretor de projetos parece que fez uma estimativa razoável sobre a receita potencial do jogo.

Para construir um gráfico de marcador, vamos usar uma nova fonte de dados: o arquivo `sales_managers_us.csv`. Ele contém dados sobre objetivos para o ano 2020 as vendas reais que eles obtiveram: [sales\_managers\_us.csv](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/sales_managers_us.csv)

Estrutura de arquivo:

-   Gestor de vendas
-   Posição
-   Objetivo — a objetivo do gestor de vendas para 2020
-   Vendas 2020 — as vendas reais do gestor

Para construir um gráfico de marcadores, siga os seguintes passos:

1.  Crie um novo documento Tableau usando `sales_managers_us.csv` como dados fonte:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/18_1662584135.png)

2.  Na primeira folha de um documento, arraste a dimensão Sales Manager (gestor de vendas) para a área de controle de colunas, e a medida Goal and Sales 2020 (objetivos e vendas 2020) para a área de controle de rótulos de dados. Abra o menu suspenso Show Me e selecione gráfico de marcadores:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/20_1662584204.png)

3.  Tableau irá mostra um gráfico de marcadores. Às vezes Tableau troca os campos de objetivo e de métrica. Para trocar o lugar deles, clique com o botão direito no eixo horizontal e selecione os campos Swap Reference Line. Também adicione informações sobre posições de gestores para o diagrama e arraste os dados sobre vendas para a área de controle de rótulo dos dados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/21_1662584275.png)

4.  Agora o gráfico está mais interessante, mas seria ainda melhor se pudermos colorir a escala de funcionários bem sucedidos de verde e os menos bem sucedidos de vermelho. Para fazer isso, vamos criar um novo campo de cálculo e chamá-lo de Goal Reached (Objetivo Alcançado):

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/22_1662584300.png)

Vamos arrastar para a área de controle de cor e ajustar as cores:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/23_1662584326.png)

Agora nós podemos ver que Jack, Jeannie e Olivia alcançaram seus objetivos, enquanto os outros ainda têm trabalho a fazer.

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/4/moved_24_1.mp4"></iframe>

## Gráfico de Gantt

O gráfico de Gantt transmite informações sobre linhas do tempo e cronograma do projeto. Eles têm o tempo ao longo horizontal, e tarefas ao longo do eixo vertical. Cada tarefa no gráfico corresponde a linha horizontal, cujo comprimento representa a duração das tarefas. Tarefas são normalmente organizadas de cima para baixo.

Dê uma olhada neste exemplo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.4.10.3PT_1658739934.png)

Para construir um gráfico de Gantt, vamos usar ainda uma outra fonte de dados: `tableau_schedules_us_fixed.csv`. Ela contém dados sobre tarefas para dois projetos: "Domestication of an elephant" (Domesticação de um elefante) e "Learn new skills at TripleTen" (Aprenda novas habilidades no TripleTen):

[tableau\_schedules\_us\_fixed.csv](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/tableau_schedules_us_fixed.csv)

Estrutura de arquivo:

-   Projeto
-   Tarefa
-   Número da Tarefa — o número da tarefa dentro da sequência do projeto
-   Data de início
-   Duração

Para construir um gráfico de Gantt:

1.  Crie um novo documento Tableau que usa `tableau_schedules_us.csv` como fonte de dados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/25_1662584396.png)

2.  Na primeira folha do documento, arraste a dimensão Start Date para a área de controle de colunas e escolha o modo Exact Date. Arraste a dimensão Task para a área de controle de linhas e a medida Duration para a área de controle de rótulos de dados. Abra o manu suspenso Show Me e selecione o tipo gráfico de Gantt:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/26_1662584413.png)

3.  Tableau vai mostrar um gráfico com todas as fases embaralhadas. Para ordená-las, arraste o nome do projeto e seu número de sequência para a área de controle de linhas:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/27_1662584430.png)

4.  Agora o gráfico está muito mais bonito:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/28_1662584478.png)

5.  Vamos usar a dimensão Task e a medida Duration para fazer rótulos legais para a tarefa:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/29_1662584453.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/4/moved_30.mp4"></iframe>

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-41-963Z.md
### Última modificação: 2025-05-28 20:15:42

# Construindo um Dashboard - TripleTen

Capítulo 4/6

Tableau

# Construindo um Dashboard

Agora você vai aprender a fazer um dashboard usando gráficos que construiu nas lições anteriores.

## Rascunhos de dashboards

Como você sabe, qualquer dashboard começa com um rascunho:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.4.11PT_1658740020.png)

O dashboard que gostaríamos de construir consiste em três partes horizontais:

-   A parte superior contém o título e filtro de ano do dashboard.
-   A parte do meio contém o gráfico de crescimento da população (atualmente tem o mesmo nome "gráficos de área empilhados").
-   A parte de baixo deve conter um gráfico mostrando mudanças nos percentuais da população (nós ainda precisamos fazer isso) e um gráfico mostrando população urbana versus rural (atualmente "gráfico de linha").

## Gráficos adicionais e um pouco de decoração

Antes de continuarmos sobre como construir um dashboard, vamos criar mais um gráfico: um gráfico mostrando como populações de países mudaram como porcentagem da população mundial. Ele será construído da mesma forma que o gráfico de área empilhada para crescimento da população:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.4.11.2_1_1662584962.png)

A diferença é que para a medida Population(População), nós devemos indicar Quick Table Calculation and Percent of Total (Tabela Rápida de Cálculo e Percentual Total) na área de controle de linhas:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/31_1662585010.png)

À primeira vista, o gráfico não é diferente:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.4.11.3_1662585057.png)

Isso porque nós ainda precisamos indicar `Compute Using > Table (down)` para Population nas linhas de controle de área. Porcentagens serão calculadas baseadas na população total de cada ano, ao invés do total para a tabela toda. Aqui está o resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.4.11.4_1662585105.png)

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/4/moved_32_1.mp4"></iframe>

Vamos preparar o gráfico de população que será mostrado no dashboard. Arraste-o para mais perto da lista de folhas, o renomeie Population, e adicione alguns detalhes para dica de ferramenta:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/4/moved_33.mp4"></iframe>

Nós vamos fazer o mesmo com o gráfico de população urbana versus rural (e vamos renomear de acordo):

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/4/moved_34.mp4"></iframe>

Agora estamos prontos para construir nosso dashboard.

## Construindo um dashboard

Para construir um dashboard:

1.  Clique em New Dashboard ao longo do canto direito da tela Tableau:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/33_1662585354.png)

2.  Você verá a tela de edição do dashboard:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/35_redaktirovat_1662585373.png)

-   A área de edição do dashboard (1) é o lugar onde podemos editar a aparência do seu dashboard. Aqui você irá localizar gráficos, tabelas e outros elementos que você precisa.
-   Ajuste o tamanho do dashboard na área 2.
-   A área de seleção de visualização (3) contém a lista de visualizações que você já criou. Cada um pode ser arrastado para a área de edição do dashboard.
-   Na área de objetos adicionais (4), você pode selecionar elementos adicionais para adicionar cabeçalhos, textos explicativos, imagens, e outras imagens o dashboards.
    
-   Nós vamos começar editando o tamanho do nosso dashboard. Na área de controle de tamanho do dashboard, clique menu suspenso Size:
    

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/36_1662585472.png)

Depois clique no menu suspenso Range e selecione Automatic:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/37_1662585487.png)

Agora seu dashboard irá se tornar full-screen automaticamente quando exibido.

4.  Agora vamos construir alguns andaimes para nosso dashboard. Na área de elementos adicionais, selecione o objeto Vertical e arraste-o para a área de edição do dashboard duas vezes:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/38_1662585507.png)

5.  Nós, assim, dividimos a área de edição em duas partes. Vamos adicionar o gráfico crescimento da população ("Population") para a parte superior:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/39_1662585537.png)

6.  Vamos adicionar o gráfico percentual dinâmico da população ("%of Population") e o gráfico população urbana versus rural ("Urban vs Rural") para a parte de baixo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/40_1662585554.png)

7.  Algumas legendas foram adicionadas ao dashboard junto com os gráficos. Vamos removê-los. Para fazer isso, clique com o botão direito e depois clique 'Remove from Dashboard' (Remover do Dashboard) no canto superior direito:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/41_1662585579.png)

8.  Agora vamos editar os eixos X dos gráficos; seus cabeçalhos parecem estranhos ("Year of Year"). Nós vamos transformá-lo em apenas "Year." Para fazer isso, clique com o botão direito no cabeçalho e selecione "Edit Axis":

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/42_1662585598.png)

Tableau irá mostrar uma janela onde você pode editar os parâmetros do eixo. Mude o nome do eixo para "Year" e feche a janela pressionando X no canto superior direito.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/43_1662585631.png)

Faça o mesmo com todos os gráficos do dashboard.

9.  Agora vamos adicionar um título ao nosso dashboard. Primeiro nós adicionamos outro recipiente vertical acima dos gráficos e coloque um objeto Text ali:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/44_1662585659.png)

Tableau irá mostrar uma janela onde você pode editar o texto. Nós vamos nomear o dashboard "Dashboard da População Mundial." Escreva em negrito e ajuste o tamanho da fonte para 16.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/45_1662585679.png)

Resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/46_1662585706.png)

10.  Vamos adicionar um elemento de controle, um filtro de ano. Para fazer isso, clique com o botão direito em qualquer gráfico e pressione o botão mais baixo.

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/47_1662585733.png)

Selecione Filtros > Ano do ano no menu suspenso:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/48_1662585753.png)

Tableau vai adicionar o filtro escolhido ao seu dashboard. Arraste-o para o recipiente vertical para que ele fique do lado esquerdo do nome do dashboard:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/49_1662585775.png)

Resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/50_1662585790.png)

11.  Pressione o botão modo de apresentação para ver como seu dashboard funciona (pressione Esc pra sair do modo apresentação):

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/51_1662585809.png)

12.  Salve seu documento para publicar seu dashboard no seu perfil público Tableau.

Vídeo:

<iframe class="base-markdown-iframe__iframe" allowfullscreen="" src="https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_11/PT/Screen/4/moved_35.mp4"></iframe>

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-43-278Z.md
### Última modificação: 2025-05-28 20:15:43

# Conclusão - TripleTen

Capítulo 4/6

Tableau

# Conclusão

Parabéns! Você agora sabe o básico sobre Tableau Public.

### Você aprendeu a:

-   Instalar a versão livre do Tableau, Tableau Public
-   Preparar e exportar dados em um formato compatível com Tableau
-   Fazer vários tipos de gráficos
-   Construir dashboards
-   Publicar seu dashboard online

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-45-321Z.md
### Última modificação: 2025-05-28 20:15:45

# Projeto do Curso - TripleTen

Capítulo 5/6

Projeto do Curso

# Projeto do Curso

Parabéns! Você concluiu o curso de Automação. É hora de aplicar o conhecimento e as habilidades que você adquiriu em um projeto: um estudo de caso analítico da vida real que você concluirá por conta própria.

Quando você terminar o projeto, envie seu trabalho para o revisor do projeto para avaliação. Ele te dará feedback dentro de 48 horas. Use o feedback para fazer alterações e, em seguida, envie a nova versão de volta ao revisor do projeto.

Você pode obter mais feedback sobre a nova versão. Isso é completamente normal. Não é incomum passar por vários ciclos de feedback e revisão.

Seu projeto será considerado concluído assim que o revisor do projeto o aprovar.

### Como enviar seu projeto para revisão

Envie seu trabalho para revisão como um arquivo ZIP. O arquivo não deve exceder 9 megabytes de tamanho.

O arquivo deve conter o seguinte:

-   Um arquivo contendo os dados carregados no dashboard (`trend_by_time.csv`)
-   Um link para o dashboard no servidor do Tableau Public
-   Um arquivo de apresentação em PDF
-   Instruções sobre como iniciar todos os arquivos (nomeie como readme.txt) (opcional)

Você também pode incluir arquivos adicionais conforme necessário.

Coloque todos os seus arquivos (o total não deve exceder 9 megabytes) em um arquivo ZIP e envie-o para revisão.

Você receberá um arquivo em troca, com os comentários do revisor dentro de cada arquivo.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-46-651Z.md
### Última modificação: 2025-05-28 20:15:46

# Parte 1. Elaboração de Requisitos Técnicos - TripleTen

Capítulo 5/6

Projeto do Curso

# Parte 1. Elaboração de Requisitos Técnicos

Você trabalha como analista de anúncios em vídeo na agência de publicidade Sterling & Draper. Você dedica muito tempo analisando vídeos de tendências no YouTube para determinar qual conteúdo merece atenção de marketing.

Cada vídeo tem uma categoria específica (entretenimento, música, notícias e política etc.), região e data de tendência.

Um vídeo pode estar na seção de tendências por vários dias seguidos.

Toda semana, os novos funcionários Melanie e Ashok fazem as mesmas perguntas:

-   Quais categorias de vídeo foram tendências na semana passada?
-   Como foram distribuídos entre as várias regiões?
-   Quais categorias foram especialmente populares nos Estados Unidos?

Em sua sexta semana de trabalho, você decide que está na hora de automatizar o processo. Você vai fazer um dashboard para Melanie e Ashok.

Pergunta

Quais etapas precisam ser tomadas para projetar e construir o dashboard?

Converse com os administradores de banco de dados e descubra quais dados eles realmente coletam. Em seguida, trabalhe com os engenheiros de dados para projetar tabelas agregadas e a estrutura do pipeline e espere que eles desenvolvam esse pipeline. Em seguida, construa o dashboard. Finalmente, anuncie solenemente que Melanie e Ashok agora podem estudar os dados por conta própria.

Você precisa desenvolver o código do dashboard imediatamente. Ele deve aceitar dados brutos diretamente. Nenhuma tabela agregada ou pipelines. Vamos poupar o tempo dos engenheiros de dados. Quando o dashboard estiver pronto, avise Melanie e Ashok.

Você precisa conversar com Melanie e Ashok e descobrir onde obter os dados e em quais bancos de dados armazenar as informações agregadas. Você cuidará do design dos elementos do dashboard. Não há necessidade de discutir nada com os administradores de banco de dados e engenheiros de dados, eles estão fora de contato com problemas analíticos.

Você precisa discutir o conteúdo do dashboard, seu layout e os dados que precisam ser exibidos com Melanie e Ashok. Em seguida, converse com os administradores de banco de dados e engenheiros de dados e descubra onde e como os dados necessários são coletados e como podem ser transformados. Não se esqueça de perguntar onde armazenar as tabelas agregadas. Por fim, desenvolva o pipeline e o dashboard.

Isso mesmo! Primeiro, discutimos o rascunho do dashboard e os requisitos técnicos com os gerentes. Então vamos aos administradores. O código vem bem no final.

Você conseguiu!

Depois de conversar com os gerentes e administradores de banco de dados, você elaborou breves requisitos técnicos:

-   Objetivo dos negócios: analisar o histórico de vídeos de tendências no YouTube
-   Com que frequência o dashboard será usado: pelo menos uma vez por dia
-   Usuário do dashboard de destino: gerentes de planejamento de anúncios em vídeo
-   Conteúdo de dados do dashboard:
    -   Vídeos de tendências do passado, divididos por dia e categoria
    -   Vídeos de tendências, divididos por países
    -   Uma tabela de correspondência entre categorias e países
-   Parâmetros segundo os quais os dados devem ser agrupados:
    -   Data e hora da tendência
    -   Categoria de vídeo
    -   País
-   Os dados:
    -   Histórico de tendências — valores absolutos com divisão por dia (dois gráficos: números absolutos e proporção percentual)
    -   Eventos, discriminados por países — valores relativos (% de eventos)
    -   A correspondência entre as categorias e os países — valores absolutos (uma tabela)
-   Importância: todos os gráficos são igualmente importantes
-   Fontes de dados para o dashboard: os engenheiros de dados prometeram criar uma tabela agregada chamada `trending_by_time`. Segue sua estrutura:
    -   `record_id` — chave primária
    -   `region` — país / região geográfica
    -   `trending_date` — data e hora
    -   `category_title` — a categoria de vídeo
    -   `videos_count` — o número de vídeos na seção de tendências
-   A tabela fica armazenada no banco de dados do `youtube` , criado especialmente para suas necessidades
-   Intervalo de atualização de dados: uma vez a cada 24 horas, à meia-noite UTC
-   Gráficos, controles do dashboard e sua disposição:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.5.2PT_1658740194.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-47-963Z.md
### Última modificação: 2025-05-28 20:15:48

# Parte 2. Construindo o dashboard - TripleTen

Capítulo 5/6

Projeto do Curso

# Parte 2. Construindo o dashboard

Vamos dar outra olhada no projeto:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/11.5.3PT_1658740238.png)

O filtro de data e hora e o filtro de país devem modificar todos os gráficos do dashboard. Observe que os gráficos de histórico de interação "Histórico de tendências" e "Histórico de tendências, %" devem ter data e hora ao longo do eixo X. "Histórico de tendências" deve ter o número de vídeos na seção de tendências (o campo `videos_count` ) ao longo do eixo Y, e o outro gráfico deve ter a porcentagem lá.

Para construir o dashboard, conclua as etapas seguintes:

1.  No Tableau Public, use `trending_by_time.csv` (consulte a parte inferior desta página para fazer download) para criar um dashboard modelado no rascunho.
    
2.  Publique o dashboard no servidor do Tableau Public. Certifique-se de que seja acessível para todos: tente abri-lo em vários navegadores. Se não estiver acessível, o revisor não poderá verificá-lo.
    
3.  Use seu dashboard para responder às perguntas que os gerentes fizeram:
    
    -   Quais categorias de vídeo estão em alta com mais frequência?
    -   Como foram distribuídos entre as regiões?
    -   Quais categorias foram especialmente populares nos Estados Unidos? Houve alguma diferença entre as categorias populares nos EUA e as populares em outros lugares?

Prepare uma breve apresentação contendo um relatório (respostas a estas questões e gráficos).

Baixe o arquivo aqui:

[trending\_by\_time.csv](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/trending_by_time.csv)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-49-899Z.md
### Última modificação: 2025-05-28 20:15:50

# Sprint 12 - Projeto - TripleTen

Descrição

Revisar

História

# Sprint 12 - Projeto

### Como entregar o projeto para revisão

Você deve extrair dados para seu dashboard localmente.

Crie o dashboard localmente e publique-o no Tableau Public.

Coloque todos os seus arquivos (incluindo arquivos adicionais conforme necessário – apenas se certifique de que o tamanho total não seja maior que 9 megabytes) em um arquivo ZIP e envie-o para revisão.

Você receberá um arquivo em troca, com os comentários do revisor dentro de cada arquivo.

O arquivo deve conter o seguinte:

-   Um arquivo contendo os dados carregados no dashboard (nomeie `trending_by_time.csv`)
-   Um link para o dashboard no servidor do Tableau Public
-   Um arquivo de apresentação em PDF

[Avançar](/trainer/data-analyst/lesson/cac725a6-ba05-43a8-809d-0bcc6dd30da9/)

Passo 2/2

Sua atribuição foi revisada e aceita

A atribuição foi enviada

Revisão completa

Olá Jonathas

os slides agora estão ótimos. Parabéns. Há tópicos, análises, recomendações, conclusão.

Desejo sucesso na jornada.

Lopes Ramonrevisor

Como foi a avaliação?

## Seu projeto com feedback

Baixar projeto

[Avançar](/trainer/data-analyst/lesson/cac725a6-ba05-43a8-809d-0bcc6dd30da9/)

## Sprint 12 - Projeto

2025-05-15 20:12

2025-05-14 22:22

O que você achou do projeto?

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-51-240Z.md
### Última modificação: 2025-05-28 20:15:51

# Feedback do Sprint 12 - TripleTen

Capítulo 6/6

Conclusão

# Compartilhe sua opinião

**Olá!**

Você acabou de concluir um sprint e enviar seu projeto. Parabéns por alcançar este marco! 🚀

Agora é o momento perfeito para dar seu feedback sobre o sprint. Sua opinião vai nos ajudar a aprimorar ainda mais o processo de aprendizagem.

Este questionário curto vai levar apenas 2 minutos. Todas as respostas vão ser confidenciais: não vamos compartilhá-las de forma que identifique você.

Agradecemos por nos ajudar a evoluir com você!

![](https://practicum-content.s3.us-west-1.amazonaws.com/common/ok.svg)

Muito obrigado pelas suas respostas!

Já as encaminhamos para os departamentos correspondentes.

7 — 7

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-52-531Z.md
### Última modificação: 2025-05-28 20:15:53

# Conclusão - TripleTen

Capítulo 6/6

Conclusão

# Conclusão

Parabéns! Você terminou a seção sobre Automação.

**O que você aprendeu:**

-   Como usar os serviços Amazon Web
-   Como agendar scripts
-   Como agregar dados
-   Como criar e remover tabelas
-   Como escrever um script pipeline
-   Como coletar os requerimentos dos clientes para dashboards
-   Como fazer um gráfico básico usando a biblioteca Dash
-   Como adicionar controles interativos ao dashboard
-   Como projetar um dashboard usando HTML
-   Como definir a lógica de um dashboard

Se você quer saber mais sobre esse tópico, confira o curso opcional Automation Extras. Nele, você vai aprender sobre a interface de linha de comando, Python scripts e máquinas virtuais.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-15-54-292Z.md
### Última modificação: 2025-05-28 20:15:54

# Agora você é um Gênio dos Dashboards - TripleTen

Capítulo 6/6

Conclusão

# Agora você é um Gênio dos Dashboards

💡 Parabéns por alcançar este novo marco em sua jornada de aprendizagem! Você dominou as habilidades de criação de dashboards e automação de processamento de dados. Verifique seu progresso na [barra de progresso pessoal](https://tripleten.com/profile/da/#skillset)!

Nesta etapa, você poderá ajudar uma empresa a otimizar e automatizar seus processos analíticos. Você provou sua habilidade de projetar e construir dashboards interativos de acordo com necessidades comerciais, escrever pipeline scripts, trabalhar com Amazon Web Services e muito mais.

### Mostre seu projeto no LinkedIn

Agora você tem a medalha "Gênio de Dashboards" em seu arsenal. Então, mais uma vez, sugerimos que você compartilhe seu progresso em suas páginas de mídia social.

Fique à vontade para copiar a medalha e o texto do bloco abaixo para o seu post (claro, você pode personalizar o texto como quiser)! Adicione um link para o dashboard no servidor público do Tableau e marque [@TripleTenBrasil](https://www.linkedin.com/school/tripleten-brasil/).

Jonathas Martins da Rocha

Oi! Eu acabei de criar um dashboard no Tableau! Agora eu posso escrever pipeline scripts, vincular meus dashboards a bancos de dados e trabalhar com Amazon Web Services. Dê uma olhada: \[link para seu dashboard no servidor público do Tableau\] #TripleTen #TripleTenBrasil #tableau #automation #dashboard #dash

![](https://practicum-content.s3.amazonaws.com/resources/Dashboards_Whiz_PT_1687160161.png)

Copiar textoSalvar imagem[Criar um post no LinkdIn](https://www.linkedin.com/feed/)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-23-04-042Z.md
### Última modificação: 2025-05-28 20:23:04

# Introdução a Suplementos de Automação - TripleTen

Capítulo 1/6

Introdução a Suplementos de Automação

Opcional

# Introdução a Suplementos de Automação

Olá, este é o curso opcional Automation Extras!

A parte mais importante desse curso é o primeiro capítulo, "The Basics of Launching Scripts." (O Básico Sobre Começar Scripts). Os outros capítulos se ampliam sobre o que você aprendeu no curso principal.

## O que você vai aprender:

-   Como usar o interface da linha de comando
-   Como escrever a programação de scripts de Python
-   Como trabalhar com máquinas virtuais em AWS
-   Como agregar dados e criar tabelas em bancos de dados
-   Como iniciar dashboards em máquinas locais e virtuais
-   Como acessar bancos de dados usando a versão livre do Tableau

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-23-05-370Z.md
### Última modificação: 2025-05-28 20:23:05

# Introdução - TripleTen

Capítulo 2/6

O Básico de Como Executar Scripts

Opcional

# Introdução

Automação é uma questão de **scripts**, pequenos programas iniciados da acordo com o previsto, ou manualmente. Você vai aprender a escrevê-los em Python.

Scripts de Python são iniciados a partir da linha de comando. Você vai aprender a trabalhar com o console em Windows, Linux, e MacOs.

Todos os exemplos dessa seção podem ser rodadas com o sistema operacional de sua escolha, mas a melhor opção automação é o Linux. Esse OS é estável, e é fácil definir um script de cronograma em Linux do que em Windows. Além disso, administradores da maioria das empresas irá fornecer a você um pequeno servidor Linux.

Uma boa maneira de se familiarizar com Linux é implementar uma máquina virtual usando um serviço de nuvem como DigitalOcean, Amazon Web Services, ou Microsoft Azure. **Serviços em nuvem** fornecem recursos de computação remotos, ou **máquinas virtuais**. Você precisa desses computadores "imaginários" para completar suas tarefas.

Dados continuam se acumulando, e eles não podem ser processados usando apenas um servidor. Hoje estamos testemunhando uma transição para serviços em nuvem. Isso é verdade tanto para Big Data quanto para analistas. A habilidade de trabalhar com serviços em nuvem é uma habilidade útil para analistas.

### O que você vai aprender:

-   Como escrever scripts Python sem o Jupyter
-   Como trabalhar com linhas de comando
-   Como rodar scripts da linha de comando
-   Como programar scripts
-   Como usar os serviços Amazon Web

### Quanto tempo irá demorar:

_5 lições, de aproximadamente 10 a 15 minutos cada_

_3 tarefas individuais, de aproximadamente 15 a 20 minutos cada_

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-23-06-679Z.md
### Última modificação: 2025-05-28 20:23:07

# Linhas de Comando Básicas - TripleTen

Capítulo 2/6 · Faltam 10 lições

O Básico de Como Executar Scripts

Opcional

# Linhas de Comando Básicas

Você vai trabalhar bastante com **linhas de comando** nessa parte do curso. Esses são os mesmos programas especiais para gerenciar sistemas operacionais usando comandos de texto. Às vezes eles são chamados **terminais** ou interface de **linhas de comando**.

Cada sistema operacional tem ao menos uma interface de linha e comando:

-   `cmd` e `powershell` no Windows
-   `baseados` na família de sistemas operacionais Linux
-   `Terminal` em MacOS

Por exemplo, aqui está como a janela `powershell` fica no Windows 10:

![](https://practicum-content.s3.amazonaws.com/resources/Untitled_1575118580_1591265417_1699945246.png)

Quando trabalha na linha de comando você precisa saber essas três coisas:

-   Quem? — Sobre qual nome de usuário você está rodando o comando
-   Onde? — qual sistema operacional você está usando
-   Por quê? — qual comando você quer rodar e porquê

## Quem

Por definição, quando você abrir a interface, você vai receber o mesmo nome de usuário sob o qual você estava logado no sistema operacional. Mas você pode também chamar comandos e conectar a máquinas virtuais sobre outros nomes de usuários. Você pode geralmente encontrar o nome de usuário em uma linha com o formato `USER_NAME@COMPUTER_NAME` na parte esquerda da linha de comando. O Windows tem um formato ligeiramente diferente: `C:\\Users\\USERNAME`.

Amostra:

![](https://practicum-content.s3.amazonaws.com/resources/11.2.2_1699945078.png)

Aqui o usuário fantasia se conecta a uma máquina virtual e obtém o nome `yandex_test`, como a máquina virtual é registrada ao usuário `yandex_test`. Depois `yandex_test` se torna `postgres` (um banco de dados PostgreSQL usuário-administrador), adquirindo as características e permissões dos últimos para que ele comece a gerenciar o anco de dados sem qualquer restrição.

Você pode tirar a máscara do usuário com o comando de `exit`. Por exemplo:

![](https://practicum-content.s3.amazonaws.com/resources/11.2.2.2_1699945129.png)

O usuário `postgres` se torna o usuário `yandex_test`, que, por sua vez, é um usuário `stilgar` disfarçado! Um filme de espião em três partes.

## Onde

Oh, os lugares para os quais a interface da linha de comando irá te levar. O usuário fantasia pode começar no Windows 10 e terminar em uma máquina rodando Ubuntu.

Comandos se diferem em cada sistema operacional. Ubuntu não vai entender a maioria dos comandos do Windows e vice versa. Para identificar o SO (sistema operacional) que você está trabalhando, imprima sua versão atual:

-   No Windows rode o comando `[System.Environment]::OSVersion.Version`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_3_1575118714_1591265549.png)

-   Em Ubuntu, `lsb_release -a`:

![](https://practicum-content.s3.amazonaws.com/resources/11.2.2.3_1_1699945160.png)

-   Em MacOS, `sw_vers -productVersion`.

Agora que sabemos qual é o SO, vamos descobrir em qual pasta você está. Isso vai determinar quais arquivos você vê. Algumas pastas são apenas para leitura ou simplesmente inacessíveis. Aqui está o que fazer para descobrir em qual pasta você está:

-   No Windows, olhe para o caminho mostrado na parte esquerda da linha de comando:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_5_1575118740_1591265623.png)

-   Em Ubuntu, rode o comando `pwd`:

![](https://practicum-content.s3.amazonaws.com/resources/11.2.2.4_1699945183.png)

-   Em MacOS, rode o comando `echo $PWD`.

## Por quê?

Antes de rodar um comando, pergunte a você:

-   O que você quer fazer?
-   Você está na máquina certa, na pasta correta, sob o nome de usuário correto?

Por exemplo, para gerenciar um banco de dados em uma máquina local, você precisa rodar comandos sob o nome de usuário `postgres` no seu computador, ao invés de `cabeça_nas_nuvens` em algum lugar em um serviço de nuvem.

Pergunta

Aqui está uma checklist de auto-avaliação:

Eu sei que máquina estou usando

Sim

Eu sei em qual SO estou

Sim

Eu sei o exato nome de usuário que estou usando atualmente

Sim

Eu sei em que pasta estou

Sim

Eu sei quais comandos eu vou rodar

Sim

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-23-08-883Z.md
### Última modificação: 2025-05-28 20:23:09

# Acessando a Linha de Comando da sua Máquina Local - TripleTen

Capítulo 2/6 · Faltam 9 lições

O Básico de Como Executar Scripts

Opcional

# Acessando a Linha de Comando da sua Máquina Local

Nesta lição você vai aprender a rodar linhas de comandos em vários sistemas operacionais.

## Linux

Usuários de Linux têm acesso automaticamente à linha de comando. Pressione `Ctrl+Alt+T`, e o sistema irá mostrar a janela do terminal.

## macOS

MacOS é baseado em Unix, um processador Linux, e tem quase todas suas funcionalidades. Se você está trabalhando em MacOS, encontre o programa `Terminal` na pasta `Applications/Utilities` (Aplicações/Utilidades); essa é a linha de comando. Comece-o e insira o comando:

```
echo "Hello world!"
```

O sistema irá imprimir a mensagem `Hello world` na tela.

## Windows 7

Apesar de o Windows 7 ter uma interface chamada PowerShell, nós recomendamos que você instale o Ubuntu baseado em Linux como sistema operacional paralelo e totalmente funcional, já que Ubuntu é melhor para trabalhar com linhas de comando. Siga [essas instruções](https://ubuntu.com/blog/how-to-upgrade-from-windows-7-to-ubuntu-installation) _(os materiais estão em inglês)_ para criar uma máquina virtual (o cenário "Virtualização") com Ubuntu.

**Windows 7 agora é considerado um sistema ultrapassado. Recomendamos fortemente que você faça o upgrade para Windows 10.**

## Windows 10

Se você trabalha em Windows 10, nós recomendamos que você instale o sistema **Ubuntu** baseado em Linux como sistema operacional paralelo e totalmente funcional, já que Ubuntu é melhor para trabalhar com linhas de comando. Para fazer isso:

1.  Abra o menu Start (Iniciar) e encontre a Microsoft Store (Loja Microsoft):

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_72_1591266401.png)

2.  Encontre Ubuntu:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_73_1591266443.png)

3.  Você verá a janela do aplicativo Ubuntu. Pressione Install. Ubuntu será instalado no seu computador.
    
4.  Reinicie e depois encontre `Ubuntu` no menu Start (Iniciar).
    

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_74_1591266477.png)

Inicie Ubuntu e aguarde enquanto o sistema termina seu setup inicial. Depois você verá a janela do terminal. Insira o comando:

```
echo "Hello world!"
```

O sistema irá imprimir a mensagem `Hello world!`.

Você pode receber uma mensagem de erro durante a instalação:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_75_1591266516.png)

Se acontecer, você precisa abrir uma janela especial de diálogo:

-   Clique com o botão direito no botão do Windows e escolha “Apps and Features”
-   Procure por “Turn Windows features on or off” e abra-o
-   Na nova janela tique `Windows Subsystem for Linux`

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_76_1591266558.png)

-   Pressione OK, aguarde que os arquivos de sistema sejam instalados, e reinicie o computador.

Uma peculiaridade de trabalhar com Ubuntu no Windows 10 é que Ubuntu tem acesso ao sistema de arquivos, mas seus caminhos diferem daqueles do Windows. Por exemplo, no Windows, o caminho para a origem do driver C é `C:\`. Mas em Ubuntu, o caminho para essa pasta é `/mnt/c/`.

Se você tem um SO diferente (digamos, uma versão anterior do Windows), conecte ao Ubuntu usando uma máquina virtual.

## Observe bem!

Nas próximas lições nós vamos indicar onde precisamente você deve inserir cada comando. Se or para rodar na interface, nós vamos escrever, "Rode na linha de comando."

Esses são as interfaces das linhas de comando que você vai trabalhar:

-   Linux: Terminal
-   macOS: Terminal
-   Windows: Ubuntu

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-23-10-244Z.md
### Última modificação: 2025-05-28 20:23:10

# Interface de Comando Básica - TripleTen

Capítulo 2/6 · Faltam 8 lições

O Básico de Como Executar Scripts

Opcional

# Interface de Comando Básica em Linux

Como já dito, automação é mais estável em Linux. Aqui está como a interface da linha de comando é:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1575119459_1591266809.png)

A última linha está esperando que o comando seja inserido. Os nomes dos comandos diferem para cada sistema. As diferenças entre os comandos de Linux e Windows são maiores que aquelas entre outros sistemas operacionais. Comandos em MacOS são identicos aos do Linux.

Dê uma olhada nos comandos mais frequentes:

-   **whoami** diz para você o usuário atual.
-   **cd** (change directory) muda a pasta atual (diretório). Por exemplo, em Linux `cd /home/YOUR_USERNAME` vai te retornar ao principal diretório de trabalho; em MacOS o mesmo pode ser feito com `cd /Users/YOUR_USERNAME`.
-   **mkdir** (make directory) cria uma pasta. `mkdir /home/YOUR_USER_NAME/logs` irá criar uma sub pasta no diretório de trabalho `logs`.
-   **rm** (remove) deleta um arquivo ou uma pasta vazia. Por exemplo, `rm /home/YOUR_USERNAME/logs` vai deletar a pasta `logs` se não tiver nenhum arquivo.
-   **rm -r** (recursive deletion) deleta uma pasta com arquivos nela. `rm -r /home/YOUR_USER_NAME/logs` irá deletar `logs`, mesmo se ela conter arquivos.
-   **cat**(concatenate) irá imprimir o conteúdo dos arquivos. Por exemplo, `cat /home/YOUR_USERNAME/logs/test.log` vai imprimir os conteúdos de `test.log`.
-   **echo** vai imprimir textos ou os conteúdos da variável, `echo "Hello world!"` vai imprimir a string `Hello world!`.

Cada um desses comandos básicos irá funcionar igualmente bem em Linux e PowerShell.

Você pode experimentar esses comandos na sua máquina, mas tome cuidado com comandos que deletam arquivos e pastas.

Os comandos acima são o suficiente para fazer automação básica. Mas se você quer aprofundar e se tornar guru de verdade, você pode ler sobre isso [aqui](https://www.hostinger.com/tutorials/linux-commands) _(os materiais estão em inglês)_.

Agora você é capaz de aprender como iniciar scripts Python da linha de comando e ajustar a automação da programação em Linux.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-23-11-552Z.md
### Última modificação: 2025-05-28 20:23:11

# Tarefa Independente 1: Definindo uma conta AWS - TripleTen

Capítulo 2/6 · Faltam 7 lições

O Básico de Como Executar Scripts

Opcional

# Definindo uma conta AWS

## Mamãe, para que servem as nuvens?

Como já dissemos, uma boa maneira de se familiarizar com Linux é implementar uma máquina virtual usando o serviço de nuvem: DigitalOcean, Amazon Web Services, Microsoft Azure, ou Google Cloud. **Cloud services** oferecem recursos de computação remota, ou **máquinas virtuais**. Você precisa desses computadores "imaginários" para completar suas tarefas. Nós os chamamos de "imaginários" porque um dispositivo físico pode rodar uma infinidade de computadores virtuais ao mesmo tempo. Eles são frequentemente incorporados em **agrupamentos de cálculos**, um conjunto de máquinas fazendo cálculos paralelos para resolver um único problema.

Dados continuam se acumulando, e eles não podem ser processados usando apenas um servidor. Hoje estamos testemunhando uma transição para serviços em nuvem. Isso é verdade tanto para Big Data quanto para analistas. A habilidade de trabalhar com serviços em nuvem é uma habilidade útil para analistas.

Máquinas virtuais são suas amigas. Se você tiver algum problema para completar tarefas individuais no seu próprio computador, fique à vontade para usar uma máquina virtual.

Nesta seção vamos de dizer como criar uma máquina virtual em Amazon Web Services (AWS). Uma das vantagens da Amazon é que esse serviço de nuvem oferece 12 meses de serviços básicos (incluindo a criação de pequenas máquinas virtuais) de graça.

**Perceba que será solicitado que você faça o pagamento das informações e incorra em cobrança se exceder o limite do Tempo Gratuito.** Isso não vai ocorrer se você completar as tarefas que indicarmos nesse capítulo.

Você pode ver mais detalhes aqui: [https://aws.amazon.com/free/](https://aws.amazon.com/pt/free/?nc1=h_ls&all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=*all&awsf.Free%20Tier%20Categories=*all).

Para criar uma máquina virtual, você precisa fazer o seguinte:

1.  Crie um par de chaves para fornecer suas credenciais na conexão.
2.  Crie uma máquina virtual.

Vídeo de instrução (texto a seguir):

<iframe class="base-markdown-iframe__iframe" id="player-lFLZkRZCXRU" frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" title="Data_11.1_sprint NM" width="640" height="360" src="https://www.youtube.com/embed/lFLZkRZCXRU?rel=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Ftripleten.com&amp;widgetid=1&amp;forigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2Fc22fe31d-0074-48b4-aeaf-d0305392a7f7%2F%3Ffrom%3Dprogram&amp;aoriginsup=1&amp;gporigin=https%3A%2F%2Ftripleten.com%2Ftrainer%2Fdata-analyst%2Flesson%2F0361a0a3-d94c-4cbc-bc36-cf26e8e16ff9%2F%3Ffrom%3Dprogram&amp;vf=1"></iframe>

# Criando um par de chaves

Os primeiros dois passos são solicitados apenas quando você criar a sua primeira máquina virtual. Todas as máquinas subsequentes serão capazes de usar os dados do usuário e Identity and Access Management (IAM) existente e o par de chaves existente.

Vamos criar um par de chaves. Para fazer isso, encontre E2C ("Elastic Compute Cloud" que é Nuvem Elástica Computacional) no menu Services e clique nele.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_77_1591268583.png)

O sistema vai abrir o painel de controle E2C. Clique em "Keypairs":

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_78_1591268626.png)

Insira o nome do seu par de chaves, escolha o formato .pem, e pressione "Create key pair":

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_79_1591268649.png)

Salve o arquivo .pem como `test_pair.pem` no seu diretório pessoal. Por exemplo, no Windows, se o nome de usuário for `Username`, essa pasta será `C:\Users\Username`. Usuários de Mac devem baixar as chaves para a pasta `Downloads`.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_1_1605795350.png)

Abra seu console Ubuntu o macOSe complete os seguintes passos.

1.  Copie o arquivo que acabou de baixar para seu arquivo pessoal.

**Windows**: Abra o console Ubuntu e insira esse comando: `cp /mnt/c/Users/Username/test_pair.pem ~/`.

**macOS**: Abra o terminal macOS e insira `cp ~/Downloads/test_pair.pem ~/`.

Lembre-se de mudar `Username` para o SEU nome de usuário. Perceba que se você esta usando VirtualBox no Windows 7, essa parte do caminho do arquivo `/mnt/c/Users/Username/` será diferente, dependendo de como você definir suas pastas compartilhadas na configuração do seu VirtualBox.

2.  Agora vá para sua pasta pessoal usando o comando para mudar o diretório: apenas digite `cd` e pressione enter.
    
3.  Defina as permissões necessárias para o arquivo. Por exemplo, se o nome da sua chave é `test_pair.pem`, você precisa rodar o seguinte comando `chmod 0400 test_pair.pem`.
    

Se você tiver erro de permissão negada, tente `sudo chmod 0400 test_pair.pem`. **Não se preocupe se você não vir nenhum resultado — isso significa que você ajustou as permissões com sucesso.**

## Criando uma máquina virtual

Agora vamos criar uma máquina virtual. De novo, encontre E2C no menu Services.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_81_1591268739.png)

O sistema vai abrir o painel de controle E2C. Clique em "Running instances":

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_82_1591268759.png)

Depois escolha "Launch Instance":

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_83_1591268777.png)

Na nova janela, escolha a imagem (Amazon Machine Image) que o sistema vai usar como fundo da sua futura máquina. Uma imagem é um sistema operacional e defina programas adicionais que serão instalados durante a criação da sua máquina virtual. Amazon oferece milhares (talvez centenas de milhares) de imagens, criadas tanto pela Amazon como por usuários. Você precisa selecionar "Ubuntu Server 18.04 LTS (HVM), SSD Volume Type" para o seu projeto. Essa imagem é de nível gratuito qualificado, o que significa máquinas virtuais baseadas nela não usarão recursos extras. Para escolher a imagem, clique em Select:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_84_1591268801.png)

Depois escolha os parâmetros técnicos da sua máquina virtual (tipo de instância). Nós recomendamos t2.micro. Esse conjunto de parâmetros também são de nível gratuito qualificado. Após selecionar o tipo de instância, clique em "Next: Configure Instance Details"(Próximo: Configure Detalhes da Instância):

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_85_1591268824.png)

Não modifique nada na próxima janela. Apenas clique "Next: Add Storage" (Próximo: Adicionar Armazenamento):

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_86_1591269072.png)

Não modifique nada na próxima janela. Apenas pressione "Next: Add Tags" (Próximo: Adicione Tags):

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_87_1591269099.png)

E novamente, não modifique nada na próxima janela. Apenas clique "Next: Configure Security Groups" (Próximo: Configure a Segurança dos Grupos):

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_88_1591269120.png)

Nós abrimos a port para acesso externo em vez de fazer conexões SSH possíveis e então o dashboard inicia para usuários externos. Como você pode ver, por definição sua máquina virtual já tem a regra SSH, que permite que você se conecte à máquina pelo `ssh`. Para adicionar a nova regra, pressione "Add Rule." Certifique-se de não deletar a regra antiga.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_89_1591269189.png)

Insira os seguintes parâmetros à regra:

-   Type (Tipo): Custom TCP Rule
-   Port Range (Intervalo de Portas): 8050
-   Source (Fonte): Anywhere (Qualquer lugar)

Feito isso, pressione ''Review and Launch" (Revisão e Início):

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_90_1591269270.png)

Selecione "Launch" na nova janela:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_91_1591269344.png)

Na última janela, selecione o par de chaves que você criou no começo dessa lição. Depois pressione "Launch Instances" (Iniciar Instâncias):

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_92_1591269359.png)

Agora vamos conectar à nossa máquina virtual.Primeiro, encontre seu endereço DNS público. Procure EC2:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_93_1591269381.png)

Clique: "Running instances":

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_94_1591269403.png)

A princípio o status da máquina virtual será "pending":

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_95_1591269426.png)

Espere que ele mude para "running" e para uma marca verde para aparecer abaixo de "Status Checks."

Depois selecione sua máquina virtual e copie seu DNS público, que pode ser encontrado no canto inferior direito:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_97_1591269479.png)

## Conectando a uma máquina virtual

Agora é hora de se conectar à sua máquina virtual!

Se você está usando Ubuntu ou macOS, insira o seguinte comando à interface:

```
ssh -i <path_to_private_key> ubuntu@<public_dns>
```

Exemplo:

```
ssh -i /mnt/c/cloud_keys/test_pair.pem ubuntu@ec2-3-126-84-83.eu-central-1.compute.amazonaws.com
```

(O caminho para a chave privada é simplesmente `test_pair.pem` porque nossa pasta atual é a que o contém. Se o arquivo está em uma pasta diferente, escreva o caminho completo.)

Não se esqueça de incluir `ubuntu@` antes o nome do domínio da sua máquina virtual (DNS público).

Se você tiver sucesso, você verá essa janela (se não, siga as instruções abaixo):

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_2_1605795734.png)

O **prompt** verde mostra seu nome de usuário e o nome da máquina. Perceba que na primeira linha, nós vê `vl@probook`; essa era a máquina local. Depois que nós nos conectamos com sucesso à máquina virtual, o prompt mudou.

Parabéns! Agora você sabe como trabalhar com máquinas virtuais e é capaz de estudar os comando básicos de Linux.

### E se eu não for capaz de conectar?

Se você não for capaz de conectar, certifique-se que a Instância AWS está funcionando ("rodando" com um check verde abaixo de "Status Checks") e que seu IP endereço/DNS não mudaram. Isso é possível se parou e depois reiniciou uma instância.

## Voltando para sua máquina local

Digite `exit` e aperte enter.

Perceba que o prompt muda de volta para aquela da máquina local.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_3_1605795865.png)

**Observação**

Não inicie várias máquinas virtuais simultaneamente.

Cada máquina é paga separadamente. Ter várias máquinas rodando irá esgotar seus recursos Amazon Free Tier mais rápido.

Livre-se de máquinas que você não precisa pressionando "Stop." Se você pressionar "Terminate", você vai perder qualquer arquivo copiado para a máquina, então só faça isso quando tiver certeza que não precisará dessa máquina novamente.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_4_1605795937.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-23-13-472Z.md
### Última modificação: 2025-05-28 20:23:13

# Tarefa Independente 2: Instalando Python em uma Máquina Local - TripleTen

Capítulo 2/6 · Faltam 6 lições

O Básico de Como Executar Scripts

Opcional

# Instalando Python em uma Máquina Local

Para completar tarefas independentes, você precisará instalar Python na sua máquina local.

Verifique a versão atualmente implementada do Python rodando o seguinte comando:

```
python -V
python3 -V
```

Você provavelmente verá algo assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1575119664_1591270124.png)

## Linux e Ubuntu

Linux e Ubuntu têm Python 3 pré instalado. Tudo o que você tem que fazer é instalar `pip`. Rode o seguinte comando na shell:

```
sudo apt update
sudo apt install python3-pip
```

Em Ubuntu e Linux, programas são instalados através de um programa especial chamado **apt** ("advanced package tool" - pacote avançado de ferramentas). apt tem uma biblioteca no seu computador onde ele armazena todos os programas e versões que conhece. O comando `sudo apt update` atualiza a biblioteca.

O comando `sudo apt install python3-pip` instala a versão do pip para Python 3. `sudo` ("substitute user and do" substitua o usuário e rode) significa que o comando irá rodar em nome do administrador do sistema. Esse comando ajuda a evitar problemas técnicos relacionados, por exemplo, ao acessar pastas do sistema do SO.

Agora você é capaz de instalar as bibliotecas necessárias. Rode os seguintes comandos na interface da linha de comando:

```
sudo pip3 install pandas
sudo pip3 install numpy
```

Dessa forma pip vai instalar automaticamente Pandas e NumPy. No entanto, para resolver algumas tarefas você precisará de outras bibliotecas. Se você não tem uma instalada, você receberá a seguinte mensagem:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1_1575119717_1591270189.png)

Isso significa que uma determinada biblioteca (nesse exemplo, a imaginária `silence_in_the_library` não está instalada. Para instalá-la rode o seguinte comando:

```
sudo pip3 install silence_in_the_library
```

## macOS

Infelizmente, será muito difícil instalar os programas e bibliotecas necessários para este sprint no MacOS. Recomendamos que você conclua todas as tarefas independentes em uma máquina virtual. Use os comandos da seção sobre Linux e Ubuntu.

## Conclusão

Agora você sabe como instalar o Python e bibliotecas em uma máquina local. Na próxima lição, você vai aprender a iniciar o scripts Python da linha de comando.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-23-14-772Z.md
### Última modificação: 2025-05-28 20:23:15

# Iniciando um Script a partir da Linha de Comando - TripleTen

Teoria

# Iniciando um Script a partir da Linha de Comando

Para automatizar processos, você precisa ensinar seu computador a iniciar programas com parâmetros de entrada por programação. Aqui é onde scripts de Python entram. Você pode automatizar o início do Jupyter Notebook também, mas isso é bem mais complexo. Vamos focar em scripts em particular.

Baixe um editor de texto (por exemplo, [Insira o Texto](https://www.sublimetext.com/) ou [Notepad++](https://notepad-plus-plus.org/downloads/)) para editar seus scripts Python. Crie um arquivo vazio no editor de texto e salve-o como `test.py`. Nós sugerimos que você use Sublime Text. Neste curso, nós simplesmente escrevemos "Create a script in Sublime Text (Crie um Script em Sublime Text)," que significa que você deve fazer isso com o editor que baixou.

Coloque um esqueleto de script Python no arquivo (ele roda em qualquer sistema operacional):

```
#!/usr/bin/python

if __name__ == "__main__":
    print('Hello world.')
```

Não se esqueça onde você salvou `test.py`. Abra a linha de comando e vá para aquela pasta usando o comando `cd`. Depois rode o seguinte:

```
python3 test.py
```

O sistema vai mostrar a mensagem: `'Hello world.'`

Você iniciou seu primeiro script! Vamos ver no que ele consiste:

-   `#!/usr/bin/python` diz ao SO em qual língua o script está escrito
-   `# -*- coding: utf-8 -*-` comunica que o script usa **UTF-8** caractere de codificação**.** A rigor, você não precisa indicar isso em Python 3 — é seu padrão.
-   `if __name__ == "__main__"` é a condição contendo a parte principal do script executado.
    
    Em Python, scripts podem ser chamados de duas maneiras:
    
    -   Como programa principal, se o script for iniciado diretamente da linha de comando
    -   Como um módulo importado: através do comando `import` dentro de outro arquivo

Vamos considerar um exemplo. Nós temos dois scripts: `test_A.py` e `test_B.py` (eles devem estar na mesma pasta). Crie `test_A.py` em Sublime Text:

```
#!/usr/bin/python
def test():
    print('Ei, eu sou s função do módulo test_A!')
if __name__ == "__main__":
    print('Eu fui chamado de programa principal!')
else:
    print('Eu fui chamado de módulo importante.')
```

Crie `test_B.py` em Sublime Text:

```
#!/usr/bin/python

import test_A

if __name__ == "__main__":
    print('Eu importo test_A.py')
    test_A.test()
```

Chame `test_A.py` diretamente na linha de comando:

```
python test_A.py
```

```
Eu fui chamado de programa principal!
```

Na linha de comando, chame `test_A.py` importando-o em `test_B.py`:

```
python test_B.py
```

```
Eu fui chamado de módulo importante.
Eu importo test_A.py
Ei, eu sou uma função do módulo test_A!
```

Executar um script da linha de comando é fácil:

```
python /script_path/script_name.py
```

Tudo o que você tem que fazer é aprender a chamar um script com os parâmetros de entrada. Vamos importar as bibliotecas **getopt** e **sys**.

**getopt** (get options - obter opções) lê parâmetros de entrada, ou **options**. Eles podem ser data, parâmetros para conectar a bancos de dados, ou nomes de arquivos. Qualquer coisa!

A biblioteca **sys** importa funções de sistemas. Você precisa da sua função `sys.exit()` nos scripts: ela para a execução de scripts se for iniciada sem parâmetros de entrada. Crie o seguinte script em Sublime Text:

```
#!/usr/bin/python

# Importe as bibliotecas necessárias
import sys
import getopt

if __name__ == "__main__":

    # Defina o formato dos parâmetros de entrada
    unixOptions = "s:e:"  
    gnuOptions = ["start_dt=", "end_dt="]

    # Obtenha uma string com os parâmetros de entrada
    fullCmdArguments = sys.argv
    argumentList = fullCmdArguments[1:]

    # Verifique se os parâmetros de entrada combinam com o formato
    # Indique em unixOptions e gnuOptions
    try:  
        arguments, values = getopt.getopt(argumentList, unixOptions, gnuOptions)
    except getopt.error as err:  
        print (str(err))
        sys.exit(2)      # Pare a execução se os parâmetros de entrada estiverem incorretos

    # Leia os valores dos parâmetros de entrada da string
    start_dt = ''
    end_dt = ''   
    for currentArgument, currentValue in arguments:  
        if currentArgument in ("-s", "--start_dt"):
            start_dt = currentValue                                   
        elif currentArgument in ("-e", "--end_dt"):
            end_dt = currentValue        

    # Imprima o resultado
    print(start_dt, end_dt)
```

Agora vamos estudar o código em detalhes. Aqui está como importar as bibliotecas que precisamos:

```
import sys
import getopt
```

Depois nós definimos os nomes dos parâmetros de entrada:

```
unixOptions = "s:e:"  
gnuOptions = ["start_dt=", "end_dt="]
```

-   `unixOptions = "s:e:"` define os nomes dos parâmetros em estilo clássico **Unix** (Unix é uma família de sistemas operacionais desenvolvidos na década de 1970). Embora o método esteja desatualizado, ele se tornou uma espécie de tradição. O script não vai rodar sem ele.
-   `gnuOptions = ["start_dt=", "end_dt="]` define os nomes dos parâmetros de entrada no estilo **GNU**, um sistema operacional do tipo Unix.

Os simbolos `=` e `:` são elementos indispensáveis dos nomes dos parâmetros. Você pode escolher os nomes dos parâmetros por si só.Por exemplo, você pode escrever algo como `"dt_begin=", "dt_end="]` ao invés de `["start_dt=", "end_dt="]`. Não se esqueça de adicionar `=`.

Você pode chamar o script de duas maneiras diferentes, dependendo do estilo dos nomes dos parâmetros de entrada:

```
python params_test.py -s '2019-01-01' -e '2019-09-01'
# ou
python params_test.py --start_dt='2019-01-01' --end_dt='2019-09-01'
```

Sugerimos que você opte pela segunda opção: ele lê melhor e garante a compatibilidade de parâmetros de cada programa, para que você não confunda os nomes das variáveis.

Depois o script armazena o conjunto de parâmetros de entrada em `argumentList`.

Na nossa amostra, os parâmetros de entrada são `start_dt` e `end_dt`. O sistema os lê automaticamente dentro de `sys.argv`, que nós armazenamos na variável `fullCmdArguments`. Quando o armazenamos em `argumentList`, nós pegamos todos os parâmetros, exceto o primeiro (index 0). Esse é o nome do script, que não vamos precisar:

```
fullCmdArguments = sys.argv
argumentList = fullCmdArguments[1:]
```

Depois o script checa se o conjunto de parâmetros de entrada está vazio. Se não existirem parâmetros, o comando `sys.exit(2)` irá parar a execução do programa. `(2)` o que significa que o erro que causou essa parada foi cometido nos parâmetros de linha de comando. Por exemplo, se os parâmetros de entrada foram indicados incorretamente ou não indicados:

```
try:  
   arguments, values = getopt.getopt(argumentList, unixOptions, gnuOptions)
except getopt.error as err:  
   print (str(err))
   sys.exit(2)
```

Depois o script roda através de todos os parâmetros de entrada e distribui seus valores entre suas variáveis internas:

```
start_dt = ''
end_dt = ''   
for currentArgument, currentValue in arguments:  
   if currentArgument in ("-s", "--start_dt"):
      start_dt = currentValue                                   
   elif currentArgument in ("-e", "--end_dt"):
      end_dt = currentValue
```

Por fim, o script imprime os valores `start_dt` e `end_dt`.

Vamos iniciar o script na linha de comando e passá-lo nos parâmetros de teste:

```
python params_test.py --start_dt='2019-01-01' --end_dt='2019-09-01'
```

Resultado:

```
['--start_dt=2019-01-01', '--end_dt=2019-09-01']
('2019-01-01', '2019-09-01')
```

Você pode passar qualquer parâmetro de entrada para o script, incluindo números, valores lógicos, datas e horários, vetores, e nomes de arquivos. Perceba que parâmetros só podem ser entradas como strings. Cabe a você convertê-los para o formato correto.

Iniciando um Script a partir da Linha de Comando

Tarefa1 / 3

1.

O arquivo `/datasets/urbanization.csv` contém dados sobre a dinâmica de **urbanização** (ou o crescimento da parcela de pessoas vivendo em cidades) pela história, divididos por país e região geográfica.

A estrutura do arquivo como se segue:

-   `Entity` _—_ o nome do país ou região
-   `Year` — o ano de observação
-   `Urban` — porcentagem da população vivendo em cidades

Escreva um script iniciando da linha de comando que leia os dados sobre urbanização no DataFrame `Urbanization` e imprima as 5 primeiras linhas.

Lembre-se da variável `__main__`.

2.

Escreva um script `urbanization.py`, onde a entrada é a variável `regions`: uma string com nomes de países divididos por vírgulas. Dentro do script leia `urbanization` de `data/urbanization.csv`. Filtre apenas aqueles países indicados no parâmetro de entrada e determine o nível máximo de urbanização do histórico de observação.

Formato do parâmetro de entrada:

-   Unix name: `r`
-   GNU name: `regions`

Seu script será chamado dessa forma:

`python urbanization.py --regions='Germany,France,Russia'`

3.

Adicione mais dois parâmetros ao script:

-   `start_dt` — a data quando a análise começou. Unix name: `sdt`, nome GNU: `start_dt`
-   `end_dt` — a data quando a análise terminou. Unix name: `edt`, nome GNU: `end_dt`

Adicione a condição filtrando quais observações devem cair entre `start_dt` e `end_dt`.

Ordene o DataFrame `urbanization` por todos os campos `'Entity'` e `'Year'` em ordem crescente.

Seu script será chamado dessa forma:

`python urbanization.py --regions='Germany,France,Russia' --start_dt='1998-01-01 00:00:00' --end_dt='1999-12-31 23:59:59'`

9

1

2

3

4

5

6

7

8

...

  

import pandas as pd

  

\# o código script a ser executado

if ...:

urbanization \= ...

print(...)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-23-16-101Z.md
### Última modificação: 2025-05-28 20:23:16

# Programação de Scripts - TripleTen

Capítulo 2/6 · Faltam 4 lições

O Básico de Como Executar Scripts

Opcional

# Programação de Scripts

Um script é um programa que não só pode aceitar a entrada de dados, mas também ser programado. Vamos descobrir como fazer um script aderir a uma programação de trabalho.

O sistema operacional Unix-like, tal como Linux, Ubuntu e macOS tem um programa especial de programação chamado **cron** (do grego χρόνος, _chronos_, "tempo"). Ele opera de forma invisível no background e lê uma programação especial **crontab** (tab=tabela).

Você pode adicionar qualquer comando que a interface da linha de comando aceita para a programação. Vamos focar em iniciar scripts do Python.

Aqui está uma amostra da programação `cron`no terminal:

```
5 6 * * 1 python -u -W ignore /home/my_user/script_A.py --start_dt=$(date +\%Y-\%m-\%d\ 00:00:00 -d "1 week ago") >> /home/my_user/logs/script_A_$(date +\%Y-\%m-\%d).log 2>&1
#15 7 * * * python -u -W ignore /home/my_user/script_B.py --start_dt=$(date +\%Y-\%m-\%d\ 00:00:00 -d "1 week ago") >> /home/my_user/logs/script_B_$(date +\%Y-\%m-\%d).log 2>&1
```

Vamos ver o que acontece aqui. Primeiro, a primeira linha:

-   `5 6 * * 1` — indica o tempo que o comando deve executar. Seu formato é como se segue:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_Sprint_11.1/Images/PT/12.2.8PT.png?etag=35fc7dc497a536c970beacd8c7a77099)

`*` — qualquer valor.

Para que a primeira linha de especialistas especifique que o comando deve começar às 6:05 AM todas as manhãs.

-   `python -u -W ignore /home/my_user/script_A.py` — é o comando em si. Aqui:
    
    -   O marcador `-u` significa que os resultados da execução dos scripts não será **tampada** (acumulada na memória do computador). Eles vão ser imediatamente armazenados no arquivo log, por sua vez (mais a respeito disso depois). Por exemplo, se seu script tem comandos `print()`, seus resultados não serão armazenados na memória, mas irão direto para os logs.
    -   `-W ignore` (ignorar avisos) significa que qualquer aviso gerado enquanto o script roda não serão armazenados no arquivo log.
    -   `/home/my_user/script_A.py` — o nome do script a ser programado.
    
-   `--start_dt=$(date +\\%Y-\\%m-\\%d\\ 00:00:00 -d "1 week ago")` — o parâmetro de entrada do script. Aqui:
    
    -   `$(date +\\%Y-\\%m-\\%d\\ 00:00:00 … )` — a linha de expressão de comando que permite que você obtenha os dados atuais no formato `'%Y-%m-%d 00:00:00'`. Para ter uma ideia melhor de como isso funciona, execute o seguinte comando na interface da linha de comando:
    
    ```
      echo $(date +\%Y-\%m-\%d\ 00:00:00)
      
    ```
    

Você vai ver os dados atuais na tela.

-   `-d "1 week ago"` especifica o tempo de intervalo que queremos que diminuir a data atual. `script_A.py` parece ser designado para que sua entrada seja a data em que a semana anterior começou. Ele provavelmente coleta certos dados para os sete dias anteriores. Também existem outras maneiras de definir intervalos de tempo. Por exemplo:
    -   `-d "yesterday"` ou `-d "1 day ago"`
    -   `-d "N days ago"`
    -   `-d "N weeks ago"`
    -   `-d "1 month ago"`
    -   `-d "N months ago"`
    -   `-d "1 year ago"`
    -   `-d "N years ago"`
-   `>> /home/my_user/logs/script_A_$(date +\\%Y-\\%m-\\%d).log` significa que todos os dados do script de impressão serão armazenados no arquivo `script_A_complianceDate.log` em `/home/my_user/logs/`. É muito importante salvar logs se o script executa automaticamente; isso ajuda você a detectar qualquer erro que ocorrer. Sem logs você não tem uma ideia clara sobre como seu sistema de automação realmente funciona.
-   `2>&1` significa que todos os resultados da execução do script (incluindo erros) serão impressos no mesmo lugar que o arquivo log:

Vamos colocar a programação de código em palavras humanas: "Começar `script_A.py` às 6:05 a.m. toda segunda feira; passe o procedimento segunda-feira como parâmetro `start_dt`."

O tempo atual é determinado pela área de fuso horário na máquina onde você vai alinhar o cron. A maioria dos servidores trabalha com o fuso horário `UTC+0`. Esse fuso horário é geralmente usado no registro de data e hora das operações e transações bancárias. Tente manter o `UTC+0` quando analisar e fizer a programação de scripts para evitar discrepâncias no registro dos resultados.

Vamos ver como editar o cronograma `cron`. Você pode apenas fazer isso na sua máquina local com Linux, Ubuntu, ou macOS.

Importante:

**Se você está usando Ubuntu no Windows 10, `cron` não começa automaticamente.** Para ver se ele está rodando, digite:

```
sudo service cron status
```

Se ele está, você verá "running"(rodando). Se não está, inicie-o com esse comando:

```
sudo service cron start
```

1.  Primeiro, crie um arquivo vazio em Sublime Text que irá imprimir `'Hello world'` e a a data e hora atuais:
    
    ```
     #!/usr/bin/python
    
     from datetime import datetime
    
     if __name__ == "__main__":
         print('Hello world: {}'.format(datetime.now()))
     
    ```
    
2.  Vamos salvá-lo como `/home/YOUR_USERNAME/cron_test.py`.
3.  Depois vamos executar o seguinte comando na linha de comando:

```
python cron_test.py
```

Resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_Sprint_11.1/Images/PT/12.2.8.2PT.png?etag=d80e572a7d164ed1bba33d9ca42ee789)

Na linha de comando vamos criar o diretório `/home/YOUR_USERNAME/logs` para armazenar logs:

```
mkdir /home/YOUR_USERNAME/logs
```

Depois chamamos o editor de programação `cron`:

```
crontab -e
```

Você tem que escolher um editor de texto na primeira vez que você for editar sua programação:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_pasted_image_0-2_1575122234_1591271150.png)

Digite "1" e aperte Enter para escolher nano como editor de texto para crontab.

Depois você verá o texto crontab:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_pasted_image_0-3_1575122271_1591271183.png)

E a linha seguinte ao final do arquivo:

```
*/5 * * * * python -u -W ignore /home/YOUR_USERNAME/cron_test.py >> /home/YOUR_USERNAME/logs/cron_test.log 2&>1
```

Aqui `*/5 * * * *` significa "rodar a cada cinco minutos."

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_pasted_image_0-4_1575122306_1591271226.png)

Pressione `Ctrl+O` para salvar. Você verá a mensagem do sistema:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_pasted_image_0-5_1575122325_1591271260.png)

Pressione Enter. Depois pressione `Ctrl+X` para sair do editor crontab. Você verá essa mensagem:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_pasted_image_0-6_1575122345_1591271293.png)

Isso significa que novas definições para a tabela cron devem ser adotadas. Tente chamar esse comando na linha de comando a cada cinco minutos como experiência:

```
cat /home/YOUR_USERNAME/logs/cron_test.log
```

Você verá que a cada cinco minutos cron irá rodar `cron_test.py` e armazenar os resultados da sua conformidade (a string `Hello world` com a atualização de tempo) em `cron_test.log`.

A Única forma de ter certeza que o script está funcionando de acordo com a programação é checando a última execução log de tempos em tempos.

Importante: se você não quer um comando para rodar no futuro, coloque # antes dele no arquivo crontab.

Um problema comum no código são ciclos infinitos. Se o cron iniciar um script que fica rodando e você quer que ele pare, a coisa mais simples é reiniciar a instância no Amazon.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-23-17-980Z.md
### Última modificação: 2025-05-28 20:23:18

# Checklist de Depuração de Problemas cron - TripleTen

Capítulo 2/6

O Básico de Como Executar Scripts

Opcional

# Checklist de Depuração de Problemas cron

Quando trabalhar com `cron` você deve encontrar problemas ao iniciar cronogramas. Se `cron` não iniciar:

1.  Certifique-se que você está usando o sistema operacional solicitado (não se esqueça que `cron` não funciona no Windows). Rode o comando `lsb_release -a`.

O sistema deve ter como resposta essa mensagem:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-04T155907.053_1591275555.png)

Se você ver uma mensagem de erro no lugar, você provavelmente está no Windows:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1575123319_1591275599.png)

2.  Cerifique-se de que o Python está instalado. Rode os comandos `python -V` and `python3 -`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_pasted_image_0-11_1575123343_1591275623.png)

Às vezes a m´quina terá diversas versões do Python simultaneamente, por exemplo, 2.7 e 3.6.

3.  Certifique-se que o cron foi iniciado. Rode o comando `service cron status`. Você provavelmente verá a seguinte mensagem:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_cron___1591275657.png)

Isso acontece porque por definição alguns sistemas operacionais proíbem comandos de programação. Portanto, você terá que iniciar o `cron` manualmente. Rode o comando `sudo service cron start`. Você verá a mensagem:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_2_1575123364_1591275699.png)

Isso significa que o cron foi iniciado com sucesso.

Então iniciamos `cron`. Mas após você reiniciar seu computador cron não irá reabrir automaticamente. Se você precisar que o `cron` reabra automaticamente depois de reiniciar sua máquina local, esse artigo vai te ajudar: [https://scottiestech.info/2018/08/07/run-cron-jobs-in-windows-subsystem-for-linux/](https://scottiestech.info/2018/08/07/run-cron-jobs-in-windows-subsystem-for-linux/) _(os materiais estão em inglês)_

As etapas a seguir fornecem um exemplo de como você pode depurar o cron em uma máquina virtual:

1.  Encontre seu nome de usuário com o comando `whoami`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-04T160150.129_1591275725.png)

2.  Rode o comando `crontab -e` e insira a string de programação. Nós marcamos os lugares onde você precisa inserir o nome de usuário que você recebeu na etapa anterior:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-04T160217.555_1591275750.png)

3.  Verifique se existe outra linha completamente vazia depois da sua programação. O `cron` sensível não vai rodar a última linha da sua programação (no caso de sua programação só tiver uma linha):

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-04T160238.628_1591275772.png)

Normalmente essa linha permanece depois da edição, mas às vezes ela é apagada acidentalmente. É fácil determinar se está lá: é só pressionar a seta para baixo até que o cursor alcance a última linha.

A programação não vai rodar:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-04T160301.986_1591275794.png)

Mas isso vai:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-04T160323.422_1591275824.png)

Vê a diferença?

Para adicionar uma linha vazia, use as setas para cima e para baixo para obter a última linha da programação:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-04T160404.279_1591275855.png)

Pressione `End` (isso pode ser Fn + →, dependendo do seu teclado):

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-04T160427.700_1591275877.png)

Pressione `Enter`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_pasted_image_0-19_1575123571_1591275909.png)

Pressione `Ctrl+o` para salvar a programação:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_pasted_image_0-20_1575123590_1591275936.png)

Pressione `Enter`.

Saia da programação pressionando `ctrl+x` e cheque se ela pode ser iniciada com o comando `cat`: `<LOG_FILE_PATH>/<LOG_FILE_NAME>`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_Sprint_11.1/Images/PT/12.2.9PT.png?etag=e20f8d822ceb19f5ae9f3c1c53e2fa73)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-23-19-285Z.md
### Última modificação: 2025-05-28 20:23:19

# Tarefa independente 3: Iniciando um Script a Partir das Linhas de Comando no AWS - TripleTen

Capítulo 2/6

O Básico de Como Executar Scripts

Opcional

# Tarefa independente 3: Iniciando um Script a Partir das Linhas de Comando no AWS

Para fazer um script funcionar em uma máquina virtual, você precisa transferi-lo da sua máquina local.

Você precisa do programa **scp** ("secure copy", cópia segura) para copiar seus arquivos e usá-los na máquina virtual. Da linha de comando, ele é iniciado da seguinte forma (lembre-se da "Tarefa Independente 1: Setting Up an AWS Account" Definindo uma Conta AWS):

```
scp -i <path_to_private_key> <path_to_a_local_file> ubuntu@<public_dns>:
```

Para transmitir o arquivo para a máquina virtual, siga esses três passos:

1.  Crie um arquivo chamado `test.py` e salve-o no seu computador. Abra o arquivo em Sublime Text (ou outro editor de textos) e adicione o seguinte texto:

```
#!/usr/bin/python

if __name__ == "__main__":
    print('Hello world.')
```

2.  Copie esse arquivo na máquina virtual.

Use o comando:

```
scp -i <path_to_private_key> <path_to_a_local_file> ubuntu@<public_dns>:
```

Aqui está uma amostra do comando para Linux, Ubuntu, ou macOS:

````
scp -i test_pair.pem /mnt/c/Users/Username/test.py ubuntu@ec2-3-126-84-83.eu-central-1.compute.amazonaws.com:
```
````

O nome de usuário é o mesmo que você indicou quando registrou a chave SSH na máquina virtual.

O sinal de dois pontos no final é crucial: ele significa "o diretório pessoal dos usuários na máquina virtual." Com outros sistemas operacionais, a única diferença é a forma que o caminho do arquivo é indicada. Certifique-se de que você sabe como encontrar o caminho do arquivo no seu SO.

3.  Conecte-se à máquina virtual:

```
ssh -i <path_to_private_key>/test_pair.pem ubuntu@<public_dns>
```

4.  Na linha de comando da máquina virtual, execute os comandos para instalar o pip (responda 'Yes' para todas as perguntas):

```
sudo apt update
sudo apt install python3-pip
pip3 --version
```

O sistema vai imprimir o seguinte resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-04T155451.364_1591275308.png)

5.  Uma vez que tiver conectado, insira o comando **dir** (directory). Você verá o conteúdo do seu diretório pessoal:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-04T155531.146_1591275362.png)

6.  Comece o teste de script usando esse comando:

```
python3 test.py
```

Depois admire o resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_Sprint_11.1/Images/PT/12.2.10PT.png?etag=4d8fff7b7dd86645f26fd0c6fa8ae909)

E o mundo diz "Você também"!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-23-20-572Z.md
### Última modificação: 2025-05-28 20:23:20

# Conclusão - TripleTen

Capítulo 2/6

O Básico de Como Executar Scripts

Opcional

# Conclusão

Parabéns! Você aprendeu o básico sobre iniciar scripts.

### O que você aprendeu:

-   Como trabalhar com linhas de comando
-   Como iniciar scripts a partir da linha de comando
-   Como programar scripts
-   Como usar os serviços Amazon Web

### Recomendações da sua equipe:

Escreva scripts analíticos e cadernos de maneira que seja fácil automatizar depois. Sempre faça o conjunto de parâmetros de entrada claros no seu programa. Por exemplo, se você precisa calcular LTV, aqueles parâmetros serão:

-   As datas de início e término do período de aquisição
-   O tempo de vida limite para o qual LTV é calculado (por exemplo, LTV no 6º mês de vida)

Determine quais parâmetros do seu script e notebook serão "externos" para a tarefa em questão. Defina-os no começo do código para você poder transformá-los em parâmetros de entrada se você decidir automatizar o processo mais tarde.

---

Faça o download da [folha conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_Sprint_11.1/Folha_de_Concluses_O_Bsico_Sobre_Iniciar_Scripts_PRT_BR.pdf?etag=989af435b8428b68e7ec188505d75450) e do [resumo do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_Sprint_11.1/moved_Resumo_do_Captulo_O_Bsico_de_Iniciar_Scripts.pdf) para você poder consultá-los quando necessário.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-24-46-675Z.md
### Última modificação: 2025-05-28 20:24:47

# Introdução - TripleTen

Capítulo 3/6 · Faltam 8 lições

Mais sobre Pipelines de Dados

Opcional

# Introdução

No curso principal, você aprendeu que pipelines nos ajudam a recuperar dados de bancos de dados. Aqui, você aprenderá mais sobre como trabalhar com bancos de dados nesse contexto.

### O que você vai aprender:

-   Como agregar dados de bancos de dados
-   Como criar tabelas em bancos de dados

### Quanto tempo irá demorar:

_4 tarefas independentes e 2 aulas, aproximadamente 20-30 minutos cada_

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-24-47-995Z.md
### Última modificação: 2025-05-28 20:24:48

# Tarefa Independente 1: Implantando Servidor de Banco de Dados - TripleTen

Capítulo 3/6 · Faltam 7 lições

Mais sobre Pipelines de Dados

Opcional

# Tarefa Independente 1: Implantando Servidor de Banco de Dados

Hora de aprender alguns novos comandos SQL. Todas as tarefas deste capítulo podem ser realizadas em nossa plataforma online, mas recomendamos que você as conclua de forma independente. Isso ensinará como implantar um sistema de gerenciamento de banco de dados (SGBD) em sua máquina local ou virtual.

Lembre-se que um SGBD é um conjunto de programas que permite criar bancos de dados, preenchê-los com novas tabelas, exibir seu conteúdo e editar tabelas existentes. Qualquer SGBD contém pelo menos duas partes:

-   Um servidor de banco de dados, que permite que os bancos de dados funcionem
-   Um cliente de banco de dados, que permite aos usuários gerenciar os bancos de dados

Você estará trabalhando com PostgreSQL. Vamos implantar o servidor deste SGBD em diferentes sistemas operacionais.

## Ubuntu e Linux

Para instalar o PostgreSQL, execute os seguintes comandos na interface de linha de comando:

```
sudo apt update
sudo apt install postgresql postgresql-contrib
sudo apt-get install python3-psycopg2
```

O primeiro comando atualiza a biblioteca `apt`, o segundo instala o próprio PostgreSQL e o último instala a biblioteca `psycopg2`, que precisaremos para acessar o PostgreSQL usando Python.

O servidor PostgreSQL não está ativo quando você o configura, então você precisará iniciá-lo. Digite este comando na interface:

```
sudo service postgresql start
service postgresql status
```

O sistema exibirá a seguinte mensagem:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1575124447_1591352679.png)

Isso significa que o servidor foi ativado com sucesso.

## macOS

Infelizmente, será muito difícil instalar os programas e bibliotecas necessários para este sprint no MacOS. Recomendamos que você conclua todas as tarefas independentes em uma máquina virtual. Use os comandos da seção sobre Linux e Ubuntu.

## Conclusão

Agora você sabe como instalar e iniciar um servidor PostgreSQL em uma máquina local. Observe que o servidor não será iniciado automaticamente após a reinicialização do computador para a maioria dos sistemas operacionais. Lembre-se de verificar se está ativo antes de começar a trabalhar.

Na próxima tarefa independente, você aprenderá a restaurar um banco de dados a partir de um arquivo de backup.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-24-49-275Z.md
### Última modificação: 2025-05-28 20:24:49

# Tarefa independente 2: Trabalhando com Backups de Banco de Dados - TripleTen

Capítulo 3/6 · Faltam 6 lições

Mais sobre Pipelines de Dados

Opcional

# Tarefa independente 2: Trabalhando com Backups de Banco de Dados

Às vezes, os bancos de dados têm problemas técnicos.

Para garantir a segurança, você pode fazer **backups** ou **arquivos de despejo**. Despejo é um formato específico para arquivos que contêm informações:

-   Na estrutura de um banco de dados (suas tabelas e relacionamentos entre elas)
-   Sobre os dados contidos nas tabelas

Os dados nos arquivos de despejo são geralmente arquivados. Isso facilita a cópia e a transferência.

A criação de um banco de dados a partir de um arquivo de despejo é chamada de **restauração**. Baixe o despejo do banco de dados de jogos.

[games\_raw\_us.dump](https://practicum-content.s3.us-west-1.amazonaws.com/data-analyst-eng/moved_games_raw_us.dump)

# Linux e Ubuntu

Abra o terminal e copie o arquivo `games_raw_us.dump` para a pasta `/tmp`. `/tmp` é aberto a todos os usuários, então você não terá problemas com direitos de acesso.

```
cp /PATH_TO_FILE/games_raw_us.dump /tmp
```

Digite o comando para substituir o usuário atual por `postgres`. O usuário `postgres` cumpre as funções de gerenciamento de banco de dados:

```
sudo su postgres
```

O sistema exigirá uma senha, aquela que você usa para entrar no sistema operacional. Você substituiu o usuário atual por `postgres`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1575129649_1591353284.png)

Crie o banco de dados `games`. Digite o seguinte comando na linha de comando:

```
createdb games
```

Aqui está o resultado (não preste atenção ao aviso):

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1_1575129716_1591353324.png)

Você criou um banco de dados! Agora tudo o que você precisa fazer é criar um usuário e conceder as permissões necessárias. Execute o seguinte comando:

```
psql -d games
```

Isso irá conectá-lo ao cliente Postgres SGBD, que é chamado de `psql`. Você poderá executar os comandos SQL necessários lá. No futuro, escreveremos "Executar o comando no `psql`" para todos os comandos que serão executados no cliente `psql`.

Veja quais tabelas contém o banco de dados de `games`. Execute o seguinte comando no `psql`:

```
\dt
```

O sistema imprimirá uma lista de tabelas de `games`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_2_1575129759_1591353584.png)

`Did not find any relations` significa que o banco de dados de `games` não contém nenhuma tabela. Vamos criar um usuário e dar a ele as permissões necessárias para acessar o banco de dados. Digite o seguinte comando no `psql`:

```
CRIAR USUÁRIO my_user COM SENHA ENCRIPTADA 'my_user_password';
CONCEDER TODOS OS PRIVILÉGIOS EM BANCO DE DADOS games PARA my_user;
```

Saia do `psql` usando o comando `\q`. Você se tornará novamente o usuário `postgres`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_3_1575129786_1591353634.png)

Na linha de comando, digite um comando para restaurar o banco de dados do arquivo de despejo:

```
pg_restore -d games /tmp/games_raw_us.dump
```

Nenhuma mensagem do sistema. Nenhuma notícia é boa notícia.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_4_1575129850_1591353692.png)

Vamos olhar para o banco de dados que temos. Execute `psql -d` games na linha de comando para retornar ao cliente de banco de dados. No `psql`, execute o comando `\dt` para exibir a lista de tabelas. Aqui está o resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_5_1575129861_1591353763.png)

Agora temos a tabela `data_raw` em nosso banco de dados. Vamos ver o que tem dentro. No `psql`, digite o seguinte:

```
SELECT * FROM data_raw LIMIT 50;
```

O sistema imprimirá as primeiras 50 linhas da tabela:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_6_1575129901_1591353816.png)

Pressione `q` para sair da exibição da tabela. Parabéns! Nosso banco de dados finalmente tem dados!

## macOS

Infelizmente, será muito difícil instalar os programas e bibliotecas necessários para este sprint no MacOS. Recomendamos que você conclua todas as tarefas independentes em uma máquina virtual. Use os comandos da seção sobre Linux e Ubuntu.

## Conclusão

Na próxima lição, você criará uma cópia de backup do banco de dados por conta própria.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-24-51-278Z.md
### Última modificação: 2025-05-28 20:24:52

# Tarefa Independente 3: Fazendo Backup de um Banco de Dados - TripleTen

Capítulo 3/6 · Faltam 5 lições

Mais sobre Pipelines de Dados

Opcional

# Tarefa Independente 3: Fazendo Backup de um Banco de Dados

Você aprendeu a recriar bancos de dados a partir de arquivos de backup (ou de despejo) feitos por seus colegas. Mas e se você mesmo quiser fazer um backup?

## Linux e Ubuntu

Abra a interface de linha de comando e digite um comando para substituir o usuário atual por `postgres`. Este último cumpre as funções de gerenciamento de banco de dados:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1575130264_1591354542.png)

Na linha de comando, execute o comando para fazer um arquivo de backup ou de despejo:

```
pg_dump -Fc games> /tmp/games.dump
```

O sistema armazenará o arquivo de backup `games.dump` na pasta `/tmp`, compartilhada por todos os usuários.

Desconecte-se do usuário `postgres` executando o seguinte comando na linha de comando:

```
exit
```

Para copiar o arquivo resultante para a pasta que você precisa, execute o comando:

```
sudo cp /tmp/games.dump /PATH_TO_DUMP_FOLDER
```

## macOS

Infelizmente, você terá grandes dificuldades para instalar os programas e bibliotecas necessários para este sprint no MacOS. Sugerimos que você conclua todas as tarefas individuais em uma máquina virtual. Use os comandos da seção sobre Linux e Ubuntu ao trabalhar nele.

## Conclusão

Você aprendeu a fazer backup de bancos de dados e criar arquivos de despejo. Você pode enviá-los para seus companheiros de equipe ou salvá-los para apreciar suas vitórias.

Nas próximas lições, você aprenderá a projetar tabelas e adicioná-las aos bancos de dados.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-24-52-554Z.md
### Última modificação: 2025-05-28 20:24:53

# Agregando Вados e Сriando Tabelas em Bancos de Dados - TripleTen

Capítulo 3/6 · Faltam 4 lições

Mais sobre Pipelines de Dados

Opcional

# Agregando Вados e Сriando Tabelas em Bancos de Dados

Os dados de origem para a análise são geralmente armazenados em bancos de dados. Geralmente são os **logs não processados** (registros de eventos).

Você trabalhou com logs no projeto integrado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled-3a492323-7f8a-44a5-9c90-2a389888ce67_1572609764_1591355301.png)

Os logs são enormes. As requisições constantes deles podem sobrecarregar um banco de dados, tornar o sistema lento e levar muito tempo. Quanto mais frequentemente você acessar os logs brutos diretamente, mais olhadinhas dos administradores de banco de dados vai notar no bebedouro.

Felizmente, os analistas raramente precisam de dados brutos, pois os dados agregados são suficientes para relatórios e conclusões. **Agregação** é o processo de agrupar dados e torná-los menores em tamanho. Por exemplo, os logs do sistema podem armazenar dados dos últimos seis meses: 60 milhões de registros de visitas ao site de usuários provenientes de 10 países. Sua tarefa é criar um relatório mensal sobre o número total de visitantes por mês, repartidos por país.

RLer 60 milhões de registros de um banco de dados pode levar horas!É melhor fazer uma nova tabela na qual você armazenará os dados agrupados por mês e país. O tamanho de uma tabela como esta seria `6 months х 10 countries = 60 records`. Os dados agora são um milhão de vezes menores e a criação de um relatório levará alguns segundos, em vez de algumas horas.

Para armazenar dados agregados, você precisará criar uma tabela no psql usando o comando **CREATE TABLE**. Aqui está sua sintaxe:

```
CREATE TABLE table_name (primary_key data_type, 
                         column_name1 data_type, 
                         column_name2 data_type, 
                          …);
```

O PostgreSQL possui vários tipos de dados. Vamos precisar dos básicos:

-   **INT** — inteiro
-   **REAL** — número de ponto flutuante
-   **VARCHAR(n)** — uma string, onde n é o comprimento máximo. Por exemplo, `VARCHAR(128)` é para um campo contendo uma string com no máximo 128 caracteres
-   **TIMESTAMP** — data e hora
-   **SERIAL** — um tipo de dados especial para valores de chave primária. Lembre-se de que uma chave primária é o número exclusivo de um registro de tabela. A chave primária é definida com a seguinte expressão:

```
CREATE TABLE table_name (primary_key_field_name serial PRIMARY KEY); 
```

Quando você adiciona um novo registro à tabela, o SGBD atribui a ele um número de linha 1 maior que o anterior e coloca esse número em um campo com o tipo SERIAL.

Vejamos os dados sobre a indústria de jogos da lição anterior. Agora ele pode ser encontrado no banco de dados na tabela `data_raw`. Sua estrutura é a seguinte:

Campo

Tipos de dados

Descrição

game\_id

SERIAL

Chave primária

name

VARCHAR(1024)

O nome do jogo

platform

VARCHAR(128)

Plataforma de jogo

year\_of\_release

VARCHAR(128)

Ano de lançamento

genre

VARCHAR(128)

Gênero do jogo

na\_players

VARCHAR(128)

O número de vendas na América do Norte

eu\_players

VARCHAR(128)

O número de vendas na Europa

jp\_players

VARCHAR(128)

O número de vendas no Japão

other\_players

VARCHAR(128)

O número de vendas no resto do mundo

critic\_score

VARCHAR(128)

Pontuação da crítica

user\_score

VARCHAR(128)

Pontuação do usuário

rating

VARCHAR(128)

Classificação ERSB

Todos os campos da tabela neste caso são do tipo string, pois os campos de número par podem conter os dados incorretos. Um dos elementos cruciais da automação é a conversão de dados para os tipos necessários.

Para nosso futuro pipeline de dados, criaremos uma tabela no psql para armazenar os dados agregados por ano:

```
CREATE TABLE agg_games_year(record_id serial PRIMARY KEY, 
                            year_of_release TIMESTAMP, 
                            avg_critic_score REAL, 
                                  avg_user_score REAL, 
                            total_copies_sold INT);
```

Aqui:

-   `record_id` — o número sequencial do registro da tabela
-   `year_of_release` — ano de lançamento do jogo (uma data)
-   `avg_critic_score` — a pontuação média da crítica para todos os jogos lançados no ano em questão
-   `avg_user_score` — a pontuação média do usuário para todos os jogos lançados no ano em questão
-   `total_copies_sold` — o total de todas as vendas no ano atual

A tabela `data_raw` contém atualmente 16.450 registros. Na nova tabela, cada valor exclusivo do campo `year_of_release` terá uma linha correspondente exclusiva. Vamos fazer um agrupamento no psql:

```
SELECT COUNT (DISTINCT year_of_release) FROM data_raw;
```

Acabamos com uma tabela, `agg_games_year`, com 39 registros só.

A tabela de origem não é tão grande; você pode até ler dados diretamente dele sem agregação. No entanto, nosso objetivo é aprender a criar tabelas agregadas e descobrir qual o impacto que elas têm no tamanho dos dados.

Como você criaria uma tabela agregada `agg_games_year_genre_platform` com a seguinte estrutura?

-   `record_id` — chave primária (atribuída automaticamente)
-   `year_of_release` — ano de lançamento do jogo (uma data)
-   `genre` — gênero de jogo (uma string de 128 caracteres)
-   `platform` — plataforma de jogo (uma string de 128 caracteres)
-   `games` — o número de jogos (um número inteiro)
-   `total_copies_sold` — o número de jogos vendidos em todo o mundo (número inteiro)

Descobriu? Aqui está a resposta:

```
CREATE TABLE agg_games_year_genre_platform(record_id serial PRIMARY KEY,
                                           year_of_release TIMESTAMP,
                                           genre VARCHAR(128),
                                           platform VARCHAR(128),                      
                                           games INT,
                                           total_copies_sold INT);
```

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-24-53-857Z.md
### Última modificação: 2025-05-28 20:24:54

# Tabelas Verticais e Horizontais - TripleTen

Capítulo 3/6

Mais sobre Pipelines de Dados

Opcional

# Tabelas Verticais e Horizontais

Ao projetar as tabelas agregadas, sempre tente torná-las verticais, o que significa que cada elemento dos dados tem sua própria coluna. Quando as coisas são organizadas dessa maneira, a automação e o dimensionamento se tornam muito mais fáceis.

Digamos que você tenha dados sobre compras de usuários divididos por mês. Existem duas maneiras de estruturar a tabela:

-   Verticalmente: uma coluna para meses, uma coluna para IDs de usuário, uma coluna para o número de compras

### Tabela vertical

month

user id

number of purchases

Janeiro de 2019

Sebastian

100

Março de 2019

Sebastian

200

Abril de 2019

Sebastian

30

Janeiro de 2019

Stella

10

Abril de 2019

Stella

500

Janeiro de 2019

Natalie

14

-   Horizontalmente: uma coluna para IDs de usuário, uma coluna para cada mês e números de compras onde eles se cruzam. Em outras palavras, isso é o que aconteceria com a primeira tabela se aplicássemos `pivot_table()` a ela e agrupássemos os dados por ID do usuário:

### Tabela horizontal

user id

Janeiro de 2019

Março de 2019

Abril de 2019

Sebastian

100

200

30

Stella

10

500

Natalie

14

O uso de tabelas horizontais traz problemas: toda vez que você adiciona uma nova coluna ao pipeline, bem como aos dashboards e relatórios conectados a ela, você terá que definir o tipo de coluna e as regras para processar os valores ausentes. Você pode fazer isso no código, claro, mas é muito mais simples criar uma tabela do primeiro tipo. Você não terá que redefinir suas estruturas se decidir adicionar novos dados.

Digamos que você precise de uma tabela para armazenar o número agregado de jogos por ano de lançamento e gênero. Seria errado organizá-lo assim: [games\_pivot\_us.xlsx](https://practicum-content.s3.us-west-1.amazonaws.com/data-analyst-eng/moved_games_pivot_us.xlsx)

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/12.3.6PT_1660210083.png)

Melhor fazer assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/12.3.6.2PT_1660210102.png)

A segunda opção é melhor porque as principais bibliotecas e sistemas para criar dashboards e relatórios, como Dash, Tableau e Google Data Studio, percebem somente os dados organizados verticalmente, onde uma característica corresponde a uma coluna. Armazenar os dados verticalmente evitará problemas.

Algo pode dar errado ao criar uma tabela. Pode acontecer, por exemplo, que a tabela precise de uma estrutura diferente. Não se preocupe! Basta excluí-lo no psql com o comando **DROP TABLE**:

```
DROP TABLE table_name;
```

Depois de excluir as tabelas incorretas e deixar as necessárias, é hora de conceder acesso a elas no psql. Para fazer isso, use o comando **GRANT**:

```
GRANT ALL PRIVILEGES ON TABLE agg_games_year TO anthony;
-- aqui anthony é o nome de um usuário que quer acessar a tabela
```

-   **TO** define o usuário a receber permissão
-   **ON TABLE** indica a tabela para a qual o acesso deve ser concedido
-   **TODOS OS PRIVILÉGIOS** significa que o usuário obtém todas as permissões possíveis para trabalhar com a tabela: ele pode ler, gravar e excluir dados

Não entraremos em muitos detalhes sobre como conceder permissões no PostgreSQL. Diremos que é seguro conceder a você direitos totais de acesso às suas tabelas.

Além dos direitos de acesso, você também precisa dar permissões para trabalhar com chaves primárias. Ao criar um campo SERIAL com chaves primárias, o SGBD cria automaticamente um objeto **SEQUENCE**, que armazena os dados sobre a forma como os identificadores de chave primária deveriam ser gerados. Esses objetos são atribuídos automaticamente a nomes assim: `tableName_keyName_seq`. Por exemplo, o objeto SEQUENCE da chave `record_id` na tabela `agg_games_year` terá o nome `agg_games_year_record_id_seq`. Para dar acesso a ele, execute o seguinte comando no psql:

```
GRANT USAGE, SELECT ON SEQUENCE agg_games_year_record_id_seq TO anthony;
```

`GRANT USAGE` significa que o usuário agora pode adicionar novos valores a `agg_games_year_record_id_seq`. `GRANT SELECT` permite que o usuário leia os dados da tabela.

Experimente os comandos que você acabou de aprender!

Como você criaria a tabela agregada `agg_games_year_score` no psql? Ele precisa ter essa estrutura:

-   `record_id` — chave primária (atribuída automaticamente)
-   `year_of_release` — (a data)
-   `genre` — gênero de jogo (uma string de 128 caracteres)
-   `platform` — plataforma de jogo (uma string de 128 caracteres)
-   `avg_critic_score` — a pontuação média da crítica (um número de ponto flutuante)
-   `avg_user_score` — a pontuação média do usuário (um número de ponto flutuante)

Dê ao usuário `my_user` todas as permissões de acesso para trabalhar com a tabela.

Resposta:

```
CREATE TABLE agg_games_year_score(record_id serial PRIMARY KEY,
                                  year_of_release TIMESTAMP,
                                  genre VARCHAR(128),
                                  platform VARCHAR(128), 
                                  avg_critic_score REAL,    
                                  avg_user_score REAL);
GRANT ALL PRIVILEGES ON TABLE agg_games_year_score TO my_user;    
GRANT USAGE, SELECT ON SEQUENCE agg_games_year_score_record_id_seq TO my_user;
```

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-24-55-649Z.md
### Última modificação: 2025-05-28 20:24:55

# Trabalhando com o Postgres SGBD do script Python - TripleTen

Capítulo 3/6

Mais sobre Pipelines de Dados

Opcional

# Trabalhando com o Postgres SGBD do script Python

Para entender exatamente como os scripts Python funcionam com os bancos de dados, você mesmo escreverá um script simples para o PostgreSQL.

Mas antes de iniciar esta tarefa empolgante, vamos instalar algumas bibliotecas adicionais.

## Linux e Ubuntu

Na linha de comando, execute o seguinte:

```
sudo pip3 install sqlalchemy
sudo apt-get install python3-psycopg2
```

O primeiro comando instalará a biblioteca `sqlalchemy`, enquanto o segundo instalará a biblioteca `psycopg2`, que conecta o `sqlalchemy` ao PostgreSQL.

## macOS

Infelizmente, será muito difícil instalar os programas e bibliotecas necessários para este sprint no MacOS. Recomendamos que você conclua todas as tarefas independentes em uma máquina virtual. Use os comandos da seção sobre Linux e Ubuntu.

## Criando e iniciando o script

Em Sublime Text, crie o script `test_db.py`:

```
#!/usr/bin/python

import pandas as pd

from sqlalchemy import create_engine

if __name__ == "__main__":   

    db_config = {'user': 'my_user',
                 'pwd': 'my_user_password',
                 'host': 'localhost',
                 'port': 5432,
                 'db': 'games'}   

    connection_string = 'postgresql://{}:{}@{}:{}/{}'.format(db_config['user'],
                                                             db_config['pwd'],
                                                             db_config['host'],
                                                             db_config['port'],
                                                             db_config['db'])                 

    # requisitando dados brutos
    engine = create_engine(connection_string)    

    query = '''
                select * from data_raw
            '''

    raw = pd.io.sql.read_sql(query, con = engine)
    print('Existem {} registros em data_raw'.format(raw.shape[0]))
```

Na linha de comando, vá para a pasta na qual seu script está salvo. Se você estiver trabalhando no Linux ou Ubuntu, você executará o comando:

```
python3 test_db.py
```

## Conclusão

Você criou um script que pode se conectar ao banco de dados e recuperar os dados de forma independente. Segure seus chapéus, bancos de dados. O trem de automação está saindo da estação.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-24-56-979Z.md
### Última modificação: 2025-05-28 20:24:57

# Conclusão - TripleTen

Capítulo 3/6

Mais sobre Pipelines de Dados

Opcional

# Conclusão

Agora você sabe como agregar dados e criar tabelas em bancos de dados. Esta é uma parte importante do processo de construção do pipeline!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-24-58-295Z.md
### Última modificação: 2025-05-28 20:24:58

# Introdução - TripleTen

Capítulo 4/6 · Faltam 4 lições

Mais sobre Dash

Opcional

# Introdução

No curso principal, você aprendeu a base para trabalhar com o Dash. Aqui, mostraremos como iniciar dashboards nas máquinas locais e virtuais.

### Quanto tempo irá demorar:

_2 tarefas individuais, de aproximadamente 15 a 20 minutos cada_

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-25-00-557Z.md
### Última modificação: 2025-05-28 20:25:00

# Tarefa Independente 1: Iniciando Dashboard em uma Máquina Local - TripleTen

Capítulo 4/6

Mais sobre Dash

Opcional

# Tarefa Independente 1: Iniciando Dashboard em uma Máquina Local

Nesta lição, você aprenderá como iniciar um dashboard em uma máquina local. Se você usar uma máquina virtual, essa tarefa não será relevante, mas as instruções detalhadas sobre como iniciar um dashboard em uma máquina virtual serão apresentadas na próxima lição.

Primeiro você precisa instalar o Dash.

## Linux e Ubuntu

Execute este comando para instalar o `dash` no terminal:

```
sudo pip3 install dash==1.6.1
```

## macOS

Infelizmente, será muito difícil instalar os programas e bibliotecas necessários para este sprint no MacOS. Recomendamos que você conclua todas as tarefas independentes em uma máquina virtual. Use os comandos da seção sobre Linux e Ubuntu.

## Construindo e iniciando um dashboard

Abra o Sublime Text e crie um dashboard simples. Salve-o como `dash_test.py`. Aqui está o código:

```
# !/usr/bin/python

import dash
import dash_core_components as dcc
import dash_html_components as html

# definindo o layout
external_stylesheets = ['<https://codepen.io/chriddyp/pen/bWLwgP.css>']
app = dash.Dash(__name__, external_stylesheets=external_stylesheets)
app.layout = html.Div(children=[

    # formando cabeçalho com uma tag HTML
    html.H1(children = 'Um dashboard simples!'),

])

# lógica do dashboard

if __name__ == '__main__':
    app.run_server(debug=True)
```

Em seguida, na linha de comando, vá para a pasta que contém `dash_test.py`. No Linux ou Ubuntu, você digitará:

```
python3 dash_test.py
```

O sistema retornará algo assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_pasted_image_0_1591623837.png)

Copie a linha `[http://127.0.0.1:8050/](http://127.0.0.1:8050/)` e abra este link no seu navegador. Você verá uma versão local do seu primeiro dashboard:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_Sprint_11.1/Images/PT/12.4.2PT.png?etag=9c2e07d2d011448ff67fa752ba28b95a)

Funcionou! Você lançou seu dashboard no modo de teste.

Atenção! Quando você inicia um dashboard a partir da linha de comando, ele "captura" a janela atual do terminal. Você não poderá executar nenhum outro comando. Para parar o programa do dashboard, pressione `Ctrl+C`.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-25-01-873Z.md
### Última modificação: 2025-05-28 20:25:02

# Tarefa Independente 2: Iniciando Dashboard em uma Máquina Virtual - TripleTen

Capítulo 4/6

Mais sobre Dash

Opcional

# Tarefa Independente 2: Iniciando Dashboard em uma Máquina Virtual

Nesta lição, você aprenderá como iniciar um dashboard em uma máquina virtual.

Primeiro vamos dar uma olhada na estrutura dos endereços IP. Precisamos deles para encontrar um computador na rede de contato.

Cada computador tem pelo menos dois endereços IP:

-   **localhost** (`127.0.0.1`) — o "eu" do computador na rede de contato. A máquina pode encontrar-se usando este endereço.
-   O **IP público** permite que outros computadores encontrem a máquina na rede de contato. Lembre-se de copiar um IP para se conectar a uma máquina virtual? Esse era um endereço IP público.

Na lição anterior, quando você criou seu dashboard e o chamou, obteve este resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_pasted_image_0_1_1591624718.png)

Na primeira linha o sistema está dizendo que o dashboard está sendo exibido no endereço `127.0.0.1`. Isso significa que outros computadores não poderão vê-lo. Se você iniciar esse dashboard na nuvem em uma máquina virtual, ele será executado, mas ficará invisível. Para tornar seu dashboard visível na internet, você precisa alterar seu código.

Em Sublime Text, crie um novo script para o Dashboards `cloud_dash_test.py`:

```
# !/usr/bin/python

import dash
import dash_core_components as dcc
import dash_html_components as html

# definindo o layout
external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']
app = dash.Dash(__name__, external_stylesheets=external_stylesheets)
app.layout = html.Div(children=[

    # formando cabeçalho com uma tag HTML
    html.H1(children = 'Um dashboard simples!'),

])

# lógica do dashboard

if __name__ == '__main__':
    app.run_server(debug = True, host='0.0.0.0')
```

Este script é como `dash_test.py`, mas agora temos o parâmetro `host='0.0.0.0'` na última linha. Isso tornará o dashboard acessível no endereço IP público da máquina virtual.

Vamos mover o dashboard para a máquina virtual. Execute os seguintes comandos na linha de comando:

```
scp -i <path_to_private_key> <PATH_TO_FILE>/cloud_dash_test.py ubuntu@<public_dns>:
```

Conecte-se à máquina virtual. Execute o seguinte comando na linha de comando:

```
ssh -i <path_to_private_key> ubuntu@<public_dns>
```

Veja como é:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_98_1591624749.png)

Instale `pip3` e `dash` na máquina virtual. Ao trabalhar na nuvem, você usa o Ubuntu e ainda não instalou nada para este sistema operacional. Na linha de comando execute os comandos:

```
sudo apt update
sudo apt install python3-pip
sudo pip3 install dash==1.6.1
```

Inicie o dashboard. Execute o seguinte comando na linha de comando:

```
python3 cloud_dash_test.py
```

O sistema iniciará seu dashboard:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_99_1591624779.png)

Agora vamos ver seu dashboard no navegador. Primeiro, encontre o endereço IP público da sua máquina virtual. Abra o menu 'Serviços' e procure por EC2:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_100_1591624858.png)

O sistema abrirá o dashboard de controle do EC2. Vá para "Instâncias em execução":

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-08T170118.618_1591624904.png)

Clique no nome da sua máquina virtual e copie seu DNS público no canto inferior direito:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-08T170157.230_1591624934.png)

Agora abra o navegador em sua máquina local e digite `<public_IP-address>:8050` no campo de endereço. O sistema exibirá seu dashboard, iniciado a partir da máquina virtual. Por exemplo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_Sprint_11.1/Images/PT/12.4.3PT.png?etag=f2d6489aaf3d9bd3940c34ba4943539f)

Como é o caso das máquinas locais, o dashboard "captura" a janela do terminal atual na máquina virtual. Você não poderá executar nenhum outro comando enquanto estiver ativo. Para parar o programa do dashboard, pressione `Ctrl+C`. Seu dashboard será interrompido e não estará mais visível no endereço IP público.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-25-03-167Z.md
### Última modificação: 2025-05-28 20:25:03

# Conclusão - TripleTen

Capítulo 4/6

Mais sobre Dash

Opcional

# Conclusão

Parabéns! Isso foi muito difícil, mas você fez tudo muito bem.

### O que você aprendeu:

-   Como iniciar um dashboard
-   Como determinar o que seu cliente quer de um dashboard
-   Como construir os gráficos mais comuns usando a biblioteca Dash
-   Como adicionar os elementos interativos a um dashboard
-   Como projetar um dashboard com HTML
-   Como definir a lógica de um dashboard

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-25-05-134Z.md
### Última modificação: 2025-05-28 20:25:05

# Preparando Dados de um Banco de Dados Implementados em uma Máquina Virtual - TripleTen

Capítulo 5/6

Mais sobre Tableau

Opcional

# Preparando Dados de um Banco de Dados Implementados em uma Máquina Virtual

Tal como você aprendeu na lição "Preparando Dados" no capítulo do curso principal sobre o Tableau, você não pode simplesmente se conectar a bancos de dados com a versão gratuita do Tableau. Aqui mostraremos como fazer isso manualmente.

Para obter os dados necessários, você precisará de um script de exportação. Vamos dizer que queremos exportar os dados contidos na tabela `trending_vids` da máquina virtual. Execute o seguinte código no Jupyter:

```
import pandas as pd

from sqlalchemy import create_engine

db_config = {'user': 'my_user',
             'pwd': 'my_user_password',
             'host': 'localhost',
             'port': 5432,
             'db': 'youtube'}   

engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(db_config['user'],
                                                            db_config['pwd'],
                                                            db_config['host'],
                                                            db_config['port'],
                                                            db_config['db']))
query = '''
           SELECT *
           FROM trending_vids
        '''

trending_vids = pd.io.sql.read_sql(query, con = engine, index_col = 'record_id')

trending_vids.to_csv('trending_vids.csv', index = False)

trending_vids.head(5)
```

Você já sabe todas as estratégias nesse script: primeiro nós definimos os parâmetros para nos conectarmos com o banco de dados e estabelecemos a conexão em si:

```
import pandas as pd

from sqlalchemy import create_engine

db_config = {'user': 'my_user',
             'pwd': 'my_user_password',
             'host': 'localhost',
             'port': 5432,
             'db': 'youtube'}   

engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(db_config['user'],
                                                            db_config['pwd'],
                                                            db_config['host'],
                                                            db_config['port'],
                                                            db_config['db']))
```

Em seguida, o script usa uma requisição para o banco de dados para recuperar todos os registros da tabela `log_raw`:

```
query = '''
           SELECT *
           FROM trending_vids
        '''

trending_vids = pd.io.sql.read_sql(query, con = engine, index_col = 'record_id')
```

Depois que isso é feito, o script imprime as primeiras cinco colunas da tabela. O DataFrame em si é armazenado no arquivo `trending_vids.csv`, que é localizado no diretório principal na máquina virtual:

```
trending_vids.to_csv('trending_vids.csv', index = False)

log_raw.head(5)
```

Aqui está o resultado:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_Sprint_11.1/Images/PT/12.5.1PT.png?etag=08334f2d7389f337a3828f607e16502e)

Observe que não estamos dizendo nada sobre a transformação de dados. Isso é porque o banco de dados armazena normalmente dados no formato row-oriented, o que é perfeito para o processar em Tableau.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-25-06-454Z.md
### Última modificação: 2025-05-28 20:25:06

# Conclusão - TripleTen

Capítulo 6/6

Conclusão

Opcional

# Conclusão

Parabéns! Você concluiu o curso adicional de Automação.

**O que você aprendeu:**

-   Como usar os serviços Amazon Web
-   Como programar scripts
-   Como agregar dados
-   Como criar e remover tabelas
-   Como iniciar dashboards na máquina local e virtual

## Excluindo máquinas virtuais da AWS

No início da seção, você criou uma máquina virtual na Amazon Web Services. Agora vamos falar de como excluí-lo.

Você não precisa excluir a própria conta da AWS. Se você não usar nenhum serviço ou recurso da AWS, a Amazon não cobrará de você.

Faça login em [aws.amazon.com](http://aws.amazon.com/).

No menu, selecione os Serviços e procure por EC2. Selecione-o.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-08T204457.047_1591638314.png)

O sistema exibirá o dashboard de controle do EC2. Clique em "Executando instâncias" na janela que se abre:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-08T204524.279_1591638347.png)

Selecione a máquina virtual que você deseja excluir:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-08T204600.207_1591638373.png)

No menu "Ações", selecione "Estado da instância" e depois "Terminar":

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-08T204729.229_1591638544.png)

Na nova janela, confirme que deseja excluir sua máquina virtual:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-08T204916.976_1591638567.png)

O sistema começará a excluir a máquina virtual. O status será "Desligando" por um tempo, mas depois mudará para "Encerrado":

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-08T204937.226_1591638594.png)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-26-21-381Z.md
### Última modificação: 2025-05-28 20:26:21

# Introdução a Previsões e Predições - TripleTen

Capítulo 1/6

Introdução a Previsões e Predições

# Introdução a Previsões e Predições

Olá, este é a parte do curso sobre previsões e predições.

Aqui você vai estudar algoritmos comuns e terá uma ideia como aprendizado de máquina (AM) realmente funciona. Além disso, você vai aprender como aplicar os métodos dessa seção para concluir tarefas de negócios.

Aqui nós não vamos focar na matemática; em vez disso, nosso objetivo será obter um entendimento dos princípios básicos e a maneira como os modelos normalmente se desenvolvem.

Esse conhecimento facilita trabalhar ao lado de cientistas de dados especialistas. Isso ainda pode te inspirar a entrar de cabeça em AM!

**O que você vai aprender:**

-   Como, em termos gerais, o aprendizado de máquina funciona
-   Quais tarefas de negócios ele ajuda a realizar
-   Como fazer previsões usando algoritmos comuns de AM
-   Os passos comuns do desenvolvimento de um modelo

No final da seção você vai entregar um projeto independente envolvendo predizer a rotatividade de clientes em uma rede de academias. Você passará por todo o desenvolvimento pipeline do modelo, assim como um especialista de AM faria. Clique [aqui](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/Trabalho_de_Projeto_PRT_BR.pdf?etag=d411ce25b8a61021ff97091ee359d68f) para ver a descrição do projeto.

Neste sprint, você completará seu conjunto de habilidades de analista de dados aprendendo o seguinte:

![](https://practicum-content.s3.amazonaws.com/resources/Aprendizado_de_Maquina_1713356446.png)

### Quanto tempo isso vai levar?

Você vai lidar com um mundo totalmente novo do AM nesse sprint. Vai levar entre 15 e 20 horas, dependendo do seu conhecimento prévio e seus hábitos de estudo. Tente não se perder e não hesite em pedir ajuda.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-26-22-691Z.md
### Última modificação: 2025-05-28 20:26:23

# Introdução - TripleTen

Capítulo 2/6

Tarefas de Negócios Envolvendo Aprendizado de Máquina

## Introdução

Você vai aprender o que é aprendizado de máquina e como usar para fazer previsões e buscas por estrutura em dados.

### O que você irá aprender:

-   Como diferenciar as várias ferramentas do aprendizado de máquina, como classificação, regressão e classificação
-   Como aplicar métodos da biblioteca sklearn
-   Como dividir dados em treinamento, validação e definições de teste
-   Como construir um pipeline de aprendizado de máquina
-   Como saber quando usar aprendizado de máquina

### Quanto tempo irá demorar:

_10 lições, aproximadamente 10 a 15 minutos cada_

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-26-24-008Z.md
### Última modificação: 2025-05-28 20:26:24

# O que é Aprendizado? - TripleTen

Capítulo 2/6

Tarefas de Negócios Envolvendo Aprendizado de Máquina

# O que é Aprendizado?

Aprendizado consiste em procurar por relações. Quando as relações são procuradas por um algoritmo ao invés de um ser humano, nós podemos falar em aprendizado de máquina (AM).

Tanto nos negócios como na vida, é importante ser capaz de prever a ocorrência de um certo evento ou o valor de uma determinada variável.

Vamos dizer que você esteja trabalhando em uma startup que venda cursos de TI. Sua tarefa é otimizar o trabalho do call center que está promovendo o curso "Visualization Tools for Product Analysts"(Ferramentas de Visualização para Analistas de Produtos). Para gerenciar recursos humanos com sabedoria, você precisa aprender a identificar clientes em potencial que provavelmente comprariam o curso. Essa habilidade vai ser muito útil no seu trabalho no futuro.

Ou talvez a empresa em que você trabalha venda plantas de casa e você esteja desenvolvendo sua estratégia de distribuição, , isto é identificar as melhores maneiras de vender o produto. Antes de investir em abrir uma nova loja, você tem que estimar sua receita potencial (por exemplo, você pode prever qual valor terá em um ano).

A probabilidade que alguém compre um curso e a potencial receita são exemplos de **target variables**(variáveis alvo). Eles podem ser **estimados** ou **previstos** baseado em **características** específicas. No exemplo do call center, as características podem ser específicas de um estudante em potencial: idade, grau de escolaridade, gênero, interesses, e experiência de educação online. No exemplo da receita estimada, as características podem ser localização, espaço aproximado, e a presença de lojas concorrentes nas proximidades.

Previsões são baseadas em **observações**: exemplos existentes para cada característica e variáveis alvo já são conhecidas.

Para o call center, as observações seriam informações sobre tentativas passadas de vender o curso. Dados sobre características (características dos compradores, por exemplo, as circunstâncias nas quais recebeu uma oferta) deve ser conhecidos para cada uma das observações. O valor da variável alvo (se eles comprarem o curso ou não) também deve ser providenciado.

Para receitas estimadas, as observações serão informações sobre uma loja existente específica. Seus parâmetros e média mensal de receita um ano depois de abrir deve ser conhecida.

Para fazer previsões bem sucedidas, você precisa aprender a **encontrar inter-relações**. Eles são baseados em experiência e observação, ou **dados empíricos**. Se você cozinhar o mesmo prato um certo número de vezes, você não terá problema em dizer quanto tempo e esforço irá levar para cozinhá-lo novamente.

A habilidade de encontrar certas inter-relações para a previsão de eventos é chamada de **aprendizado**. Vendendo uma dúzia de cursos ou abrindo centenas de floriculturas, você aprenderá por si só através da experiência. Em contraste, no campo do aprendizado de máquina, modelos aprendem por meio da análise de dados.

Pergunta

Quais das opções a seguir é semelhante a uma máquina que está sendo treinada?

Um algoritmo/programa que recebe dados de entrada e encontra dependências dentro dela

Um computador

O DeLorean DMC-12 de _De Volta para o Futuro_

Todas as opções acima

Isso mesmo! O aprendizado de máquina moderno incorpora algoritmos e novas formas de armazenamento e processamento de cálculos para fazer previsões precisas.

Muito bem!

Pergunta

Quais das opções seguintes não são exemplo de dados empíricos?

Dados de aplicações para abertura de contas bancárias

Dados científicos das estações e expedições do Ártico

As soluções para equações lineares nos seus velhos cadernos de escola

Certo, esses não são dados empíricos. O problema não é que as soluções estavam erradas, mas que você chegou a elas através de _análises_ ao invés de _observação_.

O cliente reclama sobre o serviço de entrega de flores

Fantástico!

Pergunta

Você está prevendo o número de visitantes muito rápido para restaurantes. Qual será a variável objetivo?

Dia da semana

O número médio de visitantes por hora no último mês

O número de visitantes a um restaurante específico para toda hora da próxima semana

Correto. Uma ótima tarefa para aprendizado de máquina.

O número sequencial da hora (por exemplo:5 a.m. = 5 e 5 p.m. =17)

Fantástico!

Pergunta

Quais serão suas observações se você está prevendo resgates de empréstimos?

O valor do indicador do resgate (0 por resgate, 1 para falha para resgatar) para um período de tempo

Agências de crédito

O tamanho do empréstimo

Informações sobre cada empréstimo que é emitido: as condições, o tomador do empréstimo, e a situação do resgate por um certo período de tempo

Certo. Observações incluem o resultado (resgate ou perda), bem como as características baseadas em informações da agência de crédito (dados sobre os tomadores de empréstimo).

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-26-27-446Z.md
### Última modificação: 2025-05-28 20:26:27

# Apresentação à Previsão e Aprendizado de Máquina - TripleTen

Capítulo 2/6

Tarefas de Negócios Envolvendo Aprendizado de Máquina

# Apresentação à Previsão e Aprendizado de Máquina

Imagine que você é um ecologista. Sua tarefa prever o número da população de pinguins africanos e descubra como ele vai mudar baseado em vários parâmetros.

A variável objetivo é o número de pinguins em milhares.

Cada obervação corresponde a um ano do calendário. Você tem os dados sobre a população de pinguins de 2000 a 2019, somando 20 observações no total. Vamos dar uma olhada nelas:

```
print(penguins_df['Population'])
```

```
Year
2000    200
2001    187
2002    151
2003    121
2004    119
2005     95
2006     80
2007     84
2008     60
2009     52
2010     51
2011     55
2012     50
2013     48
2014     47
2015     45
2016     39
2017     43
2018     40
2019     42
Name: Population, dtype: int64
```

Vamos definir o valor da variável objetivo da seguinte maneira:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Screen_Shot_2020-01-26_at_17.20.29_1580048442_1592651321.png)

Ele não é um número único, mas sim um conjunto de 20 números. Tais conjuntos são chamados **vetores** (você deve se lembrar de uma pequena seta da sua época de escola). y é o vetor da variável objetivo.

Um ano aqui é **identificador de uma única observação**, ou de um índice.

Você levanta a hipótese que o tamanho da população de pinguins é influenciado por sete fatores:

-   Media da temperatura anual. Reprodução e sobrevivência dependem de condições climáticas.
-   Temperatura média da água. Pinguins amam nadar e pescar em águas geladas.
-   A condição ecológica global (em uma escala de 1 a 10): oferta de comida, espaço disponível, impacto humano nos lugares de reprodução e alimentação, outros fatores humanos.
-   A população de focas. Elas competem com pinguins por comida.
-   A população de gaivotas. Elas roubam ovos.
-   A densidade da população média em áreas vizinhas. A única coisa pior do que gaivotas são o ser humano, os principais inimigos dos pinguins.
-   O índice de proteção ambiental. Classificando o nível de controle legal sobre respeito por direitos dos pinguins. Por exemplo, uma multa pela extração dos seus ovos.

Para cada ano i, você tem um conjunto de valores característicos, ou **vetores característicos**:

xi⃗\=(xi1,xi2,xi3,xi4,xi5,xi6,xi7)\\vec{x\_i} = (x\_{i\_1}, x\_{i\_2},x\_{i\_3},x\_{i\_4},x\_{i\_5},x\_{i\_6},x\_{i\_7} )xi​​\=(xi1​​,xi2​​,xi3​​,xi4​​,xi5​​,xi6​​,xi7​​)

Aqui (`x_i_1, x_i_2, x_i_3, x_i_4, x_i_5, x_i_6, x_i_7`) são os valores de sete parâmetros com impacto na população de pinguins.

```
print(penguins_df.drop('Population', axis = 1)) # imprimindo características sem a variável objetivo
```

```
Temp  Sea_temp  Ecology  Seals_pop  Seagulls_pop  People_dens  \\
Year                                                                  
2000    17        10        7       2028            36         2522   
2001    14        12        6       2005            37         2713   
2002    13        11        8       2011            37         2608   
2003    15        11        7       2093            37         2679   
2004    14        10        5       2002            34         2625   
2005    14        12        6       2098            35         2849   
2006    13        14        6       2098            38         2794   
2007    15        12        5       1968            36         2867   
2008    14        13        6       1999            35         2987   
2009    16        13        6       1976            37         2924   
2010    16        16        5       2078            38         2951   
2011    15        14        4       2023            37         2835   
2012    13        13        4       2041            36         2941   
2013    15        14        3       2021            37         2977   
2014    16        14        3       2073            38         3046   
2015    17        15        5       2006            40         3095   
2016    13        16        3       2041            39         3025   
2017    16        17        4       2029            38         2991   
2018    17        17        3       1982            39         2970   
2019    17        17        2       2049            37         3032   

      Eco_protection  
Year                  
2000               3  
2001               3  
2002               5  
2003               4  
2004               5  
2005               4  
2006               4  
2007               5  
2008               4  
2009               4  
2010               4  
2011               5  
2012               5  
2013               6  
2014               7  
2015               6  
2016               6  
2017               7  
2018               6  
2019               7
```

Os dados para 20 anos e sete parâmetros podem ser apresentados como uma tabela ou **matriz**. As linhas vão conter observações ou **objetos**. As colunas irão conter características. Vamos definir uma **matriz objeto-característico** como X:

X\=(x11⋯x17⋮⋱⋮x201⋯x207)X = \\begin{pmatrix} x\_{1\_1} & \\cdots & {x\_{1\_7}}\\\\ \\vdots & \\ddots & \\vdots \\\\ x\_{20\_1} & \\cdots & {x\_{20\_7}} \\end{pmatrix}X\=​x11​​⋮x201​​​⋯⋱⋯​x17​​⋮x207​​​​

Aqui está como fica o conjunto de dados dinâmicos:

```
print(penguins_df)
```

```
Population  Temp  Sea_temp  Ecology  Seals_pop  Seagulls_pop  \\
Year                                                                 
2000         200    17        10        7       2028            36   
2001         187    14        12        6       2005            37   
2002         151    13        11        8       2011            37   
2003         121    15        11        7       2093            37   
2004         119    14        10        5       2002            34   
2005          95    14        12        6       2098            35   
2006          80    13        14        6       2098            38   
2007          84    15        12        5       1968            36   
2008          60    14        13        6       1999            35   
2009          52    16        13        6       1976            37   
2010          51    16        16        5       2078            38   
2011          55    15        14        4       2023            37   
2012          50    13        13        4       2041            36   
2013          48    15        14        3       2021            37   
2014          47    16        14        3       2073            38   
2015          45    17        15        5       2006            40   
2016          39    13        16        3       2041            39   
2017          43    16        17        4       2029            38   
2018          40    17        17        3       1982            39   
2019          42    17        17        2       2049            37   

      People_dens  Eco_protection  
Year                               
2000         2522               3  
2001         2713               3  
2002         2608               5  
2003         2679               4  
2004         2625               5  
2005         2849               4  
2006         2794               4  
2007         2867               5  
2008         2987               4  
2009         2924               4  
2010         2951               4  
2011         2835               5  
2012         2941               5  
2013         2977               6  
2014         3046               7  
2015         3095               6  
2016         3025               6  
2017         2991               7  
2018         2970               6  
2019         3032               7 
```

Sua tarefa é usar esses dados para criar uma **função** que (relativamente) **reflita fielmente a inter-relação** dos fenômenos naturais e identifique a relação entre os valores do recurso e a variável de destino (chamaremos de ŷ: esta é a previsão feita usando a função):

f(x⃗)\=f(x1,x2,...,x7)\=y^f(\\vec{x}) = f(x\_1, x\_2, ..., x\_7) = \\hat{y} f(x)\=f(x1​,x2​,...,x7​)\=y^​

Se você aplicar essa função aos valores característicos de um objeto em particular, você vai obter a **previsão da variável objetivo para essa observação**. Deve ser perto do tamanho da população observada no passado:

f(xi⃗)\=f(xi1,xi2,...,xi7)\=yi^f(\\vec{x\_i}) = f(x\_{i1}, x\_{i2}, ..., x\_{i7}) = \\hat{y\_i} f(xi​​)\=f(xi1​,xi2​,...,xi7​)\=yi​^​

yi^∼yi\\hat{y\_i} \\sim y\_iyi​^​∼yi​

Quando sua fórmula gera uma **previsão** ou **estimativa** do valor de uma variável objetivo que está próxima do valor histórico na maioria dos casos, você basicamente encontrou uma relação oculta entre os valores. No exemplo do pinguim, é a relação entre o tamanho da população e as condições ambientais e de clima.

Como deduzimos essa fórmula? Nós devemos assumir que a "lei de população dos pinguins" é assim:

y^\=w0+w1∗x1+w2∗x2+w3∗x3+w4∗x4+w5∗x5+w6∗x6+w7∗x7\\hat{y} = w\_0 + w\_1 \* x\_1 + w\_2 \* x\_2 + w\_3\*x\_3 + w\_4 \* x\_4 + w\_5 \* x\_5 + w\_6 \*x\_6 + w\_7 \*x\_7 y^​\=w0​+w1​∗x1​+w2​∗x2​+w3​∗x3​+w4​∗x4​+w5​∗x5​+w6​∗x6​+w7​∗x7​

De acordo com essa fórmula, a previsão do número de pinguins é a soma do coeficiente "null" (o número de pinguins quando todas as características são iguais a 0) e o valor de características multiplicada pelos coeficientes.

Vamos assumir que adivinhamos corretamente a fórmula. Mas como escolhemos os valores `w_0, w_1, w_2, w_3, w_4, w_5, w_6, w_7` (também chamados de **parâmetros de função**, ou **pesos**) como que a lei que deduzimos funciona para todas as observações?

Claro, você pode tentar selecionar os valores do parâmetro por si só, mas será muito mais fácil de delegar a busca por inter-relações a uma máquina (um algoritmo ou programa). Deixe-a construir a relação matemática ideal enquanto você relaxa na praia!

Nós passamos de pinguins para uma bruta mas metafórica definição de **aprendizado de máquina**. É o que você diz quando uma máquina procura por inter-relações entre valores baseados em vários objetos.

À medida que o algoritmo processa grandes conjuntos de dados, ele **aprende** com eles e constrói um **modelo de mundo** que reflete relações escondidas entre processos ou objetos.

Além disso, você precisa conseguir dizer o quão bom é um modelo, ou seja, o quão precisa sua previsão é em relação à realidade. Nós vamos discutir isso com mais detalhes na nossa lição sobre **métricas**.

O modelo aprende com um número de observações, assim como as pessoas. Quanto mais observações existirem, melhor. Imagine que você tem uma lista de ingredientes de panquecas, mas você não tem a receita real. É pouco provável que você encontre a proporção certa de leite, farinha, manteiga e ovos na primeira tentativa. Provavelmente levará algo como cinco ou dez tentativas (observações).

A mesma coisa é verdade com algoritmos. Se você tem sete características e apenas três observações, será difícil construir modelos de qualidade.

Pergunta

Escolha a opção correta.

Os valores de peso podem mudar para diferentes observações, enquanto os valores das características permanecem os mesmos.

Quando nós treinamos modelos de aprendizado de máquina, nós selecionamos o conjunto apropriado de pesos para cada observação. Os valores das características são diferentes.

Os valores de características para diferentes observações podem diferir, mas os pesos devem ser os mesmos para todas as observações.

Correto. Isso porque usamos a característica **matriz** e um peso **vetor**.

Nós selecionamos a previsão da variável objetivo mais próxima de todas as observações.

Você conseguiu!

Pergunta

O que estamos tentando "adivinhar" quando treinamos uma máquina?

Uma métrica objetivo

O valor de uma variável objetivo usando dados históricos

A função mostrando a relação entre a variável objetivo e as características

Exatamente. Nós estamos tentando encontrar uma função que funcionaria bem na maioria dos casos.

Os valores dos vetores característicos e a variável objetivo usando dados futuros

Você conseguiu!

Pergunta

No exemplo do pinguim, quem/o que está "sendo treinado" no processo do aprendizado de máquina?

Você

Pinguins

Modelo

Sim, o modelo é treinado.

A função

Você conseguiu!

Pergunta

Para que serve um modelo que irá te dizer o valor de uma variável objetivo em um ano baseado em características que irão se tornar conhecidos em um ano? Ecologistas também podem apenas gastar aquele ano contando os pinguins um a um, sem qualquer modelagem.

Ele pode ajudar a descobrir o impacto exato que uma característica particular tem no tamanho da população.

Eles podem ajudar a prever a população de pinguins sob vários cenários e dados valores de várias características.

Isso pode ajudar você a fazer previsões com anos de avanço baseadas nas estruturas dos dados, histórico dos dados da população de pinguins, e outras características.

Todas as opções acima.

Sim. Até mesmo um modelo simples pode nos ajudar a entender inter-relações básicas. Além do mais, nós podemos transformar o conjunto de dados mudando ligeiramente o vetor variável objetivo para o próximo ano e construir um segundo modelo. Ele irá usar as características do ano atual para prever a variável objetivo no ano seguinte.

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-26-28-756Z.md
### Última modificação: 2025-05-28 20:26:29

# Aprendizado Supervisionado - TripleTen

Capítulo 2/6

Tarefas de Negócios Envolvendo Aprendizado de Máquina

# Aprendizado Supervisionado

Vamos mapear os branches da máquina de aprendizado passo a passo para ter certeza de como solucionar várias tarefas.

A tarefa dos pinguins das lições anteriores é um exemplo de **aprendizado supervisionado**.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.2.4PT.png)

Os modelos nesses casos dão um número enorme de observações como entrada, onde cada observação tem valores de características `(X`) e o valor variável objetivo `(y)`, ou **legenda**. Portanto, você pode ouvir alguém dizer, "**Os** **dados de entrada são legendas**."A tarefa do modelo é descobrir a relação entre `X` e y e aprender a prever y para novos objetos que só tem o vetor de valores de características `(X)`. Nesse caso é você que está treinando o modelo, já que você determina o que pode ser considerado características (objetos) e o que deve ser considerado como variável objetivo, ou **respostas corretas**.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.2.4.2PT.png)

Os modelos de aprendizado supervisionado são decompostos em modelos de **classificação** e **regressão**.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.2.4.3PT.png)

**Classificação é usada quando precisamos obter o nome da classe** de cada novo objeto como resposta do modelo. Por exemplo, essa resposta pode ser baseada nas características do objeto:

-   tamanho da folha
-   cor
-   altura do caule
-   presença ou ausência de flores

Você quer identificar a classe a que uma planta caseira pertence: "ficus" ou "orquídea."Nessas tarefas, a resposta, ou legendas, levam seus valores de um conjunto limitado de classes.

Lá talvez tenha qualquer número de classes (N), mas deve ser finito e maior do que 1. Quando N=2, a classificação será binária. Se N>2, é uma **classificação multi classe**.

Com **regressão**, a resposta será uma **variável contínua**. Nos negócios existem várias de tarefas de regressão: por exemplo, prever lucros ou receitas de saídas de novos pontos de venda, ou estimativa sobre preços de imóveis no mercado imobiliário. Se você criar uma regressão linear no Excel e calcular os coeficientes vinculando os valores das variáveis ao da variável objetivo, você realizará uma tarefa de aprendizado de máquina.

Pergunta

Você está fazendo uma tarefa que envolve prever o índice de satisfação do cliente. Qual tipo de tarefa de aprendizado supervisionado é essa?

Regressão

Classificação

Isso pode ser resolvido de ambas maneiras

Isso é correto, mas essa pode não ter sido a reposta mais óbvia. Tudo depende de como você define o índice. Você pode criar uma escala fixa de "notas de satisfação" ( por exemplo, números inteiros de 1 a 10). Primeiro, você precisa decidir como definir a variável objetivo.

Muito bem!

Pergunta

Porque você acha que as tarefas que consideramos são chamadas "aprendizado supervisionado"?

Porque o modelo recebe a resposta para cada conjunto de características como entrada

De maneira que podemos dizer que quando você constrói esse modelo, você é que ensina, já que você mostra "a maneira correta de fazer as coisas."

Porque nós estamos supervisionando você e te dizendo sobre aprendizado de máquina

Muito bem!

Pergunta

Uma regressão linear no Excel é um exemplo de aprendizado de máquina?

Sim

Certo. AM (aprendizado de máquina) antes de AM era legal.

Não

Você conseguiu!

Pergunta

Você quer prever as receitas de ganho (em $) de uma propaganda da campanha baseada nas suas características (o mês é seu início, o tipo de produto, orçamento, etc.). Que tipo de tarefa é essa?

Regressão

Já que está especificado que precisamos fazer a previsão "em $" (e não dizer, em termos de lucratividade vs. não lucratividade) nós estamos lidando com uma tarefa de regressão.

Classificação

Trabalho maravilhoso!

Pergunta

Você está olhando para a ocorrência de certas palavras e usando aquela informação para prever a qual dos quatro gêneros um livro pertence. Que tipo de tarefa é essa?

Regressão

Classificação multi classe

Isso mesmo!

Classificação binária

Muito bem!

Pergunta

Prevendo se um time de futebol específico vai ganhar ou perder um jogo é um exemplo de:

Regressão

Classificação multi classe

Classificação binária

Já que temos apenas duas opções, essa é uma classificação binária.

Excelente!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-26-30-109Z.md
### Última modificação: 2025-05-28 20:26:30

# Aprendizado não supervisionado - TripleTen

Capítulo 2/6

Tarefas de Negócios Envolvendo Aprendizado de Máquina

# Aprendizado não supervisionado

Você estudou tarefas em que a "verdade fundamental" é conhecida, mas você pode dar um modelo dados sem fornecer uma variável objetivo.

Você pode passar um grande número de observações para um algoritmo sem uma resposta (`y`) e treiná-lo para construir inter-relações entres os objetos em si. O modelo é capaz de identificar quais objetos se parecem um com o outro e quais não. Você pode dizer olhando para duas maçãs que elas têm algumas características iguais, mas uma laranja é diferente. Esse processo é chamado **aprendizado não supervisionado**. Esse é o segundo maior tipo de aprendizado de máquina.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.2.5PT.png)

Aprendizado não supervisionado pode ser usado quando você precisa identificar tipos comuns de clientes (em termos de, digamos, passatempos, receita financeira, ou características democráticas). Você deve saber exatamente quais desses grupos são, mas você tem certeza que eles existem e que esse conhecimento pode ser usado para otimizar o serviço de clientes.

Dividindo objetos em grupos chamados **agrupamentos**. A principal diferença entre agrupamento e classificação é que não existem classes dadas ou "respostas corretas."

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.2.5.2PT.png)

Tenha cuidado para não confundir agrupamento com classificação. Uma tarefa de classificação deve ser identificar um gênero musical de uma determinada lista (jazz, pós punk, música clássica) através da análise de uma faixa de áudio. Agrupamento pode ser usado para modelagem do assunto dos artigos e identificar os que são relacionados. Com agrupamento, nós não damos classes; é o algoritmo que as forma.

Aprendizado não supervisionado é aplicado também em **redução de dimensionalidade das matrizes de características.**

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.2.5.3PT.png)

Porque nós precisamos disso? Quanto mais características, melhor, certo? Nem sempre, ao que parece.

Vamos dizer que você precisa prever preço de novas habitações. A característica inicial é:

-   distância até a estação de metrô mais próxima
-   área total
-   área da cozinha
-   distância até o centro da cidade
-   ano da construção
-   número do andar do apartamento
-   distância do parque mais próximo
-   qualidade geral
-   preço

Para deixar as coisas simples, você pode reduzir essas características para localização, qualidade do apartamento, e preço. Isso vai facilitar decidir se o apartamento vale ser comprado ou não.

Outra razão para redução da dimensionalidade é que algoritmos não funcionam bem quando existem muitas características e não observações suficientes.

O último tipo principal de AM é **reforço de aprendizado**. Aqui o modelo aprende passo a passo e altera levemente seu algoritmo operacional em cada etapa, usando dicas do ambiente externo. Reforço de aprendizado é uma área popular e de tendência de aprendizado de máquina, mas vamos focar nos dois primeiros tipos.

Pergunta

Qual das seguintes opções é verdadeira para um algoritmo de aprendizado não supervisionado?

O algoritmo recebe características e variáveis objetivo como entrada para que ele possa treinar.

Aprendizado não supervisionado torna possível encontrar relações dentro dos dados.

Isso mesmo!

Para definir a inter-relação entre características e valor objetivo

Fantástico!

Pergunta

Você quer dividir usuários em 10 agrupamentos que ainda não estão definidos antecipadamente. Qual tarefa você está tentando resolver?

Agrupamento

Nada surpreendente, certo? Nós dividimos as observações em grupos, e não existem rótulos. Esse é o agrupamento clássico.

Classificação

Não podemos dizer

Excelente!

Pergunta

Você quer ordenar seus e-mails usando aprendizado de máquina. Qual tipo de tarefa é essa?

Aprendizado Supervisionado

Aprendizado não supervisionado

Tanto aprendizado supervisionado como não supervisionado vão servir

Aqui também, não é preto no branco! Se você for capaz de passar um tempo (ou bastante tempo, em alguns casos) rotulando e-mails com categorias como "desconto", "e-mail de Recursos Humanos", "passagem de avião", e "spam", isso tornará a tarefa em aprendizado supervisionado. Mas se você que economizar tempo e fornecer o modelo com fartura de textos em vez de obter agrupamentos em troca, isso é aprendizado não supervisionado.

Fantástico!

Pergunta

O banco de dados contém informações sobre usuários e outras atividades. Você também sabe quando clientes param de usar seu aplicativo. Com esses dados você quer treinar um modelo para que ele preveja a probabilidade de que cada cliente saia no próximo mês. `1` — o usuário vai sair; `0` — o usuário vai ficar. Que tipo de tarefa é essa?

Redução de dimensionalidade

Classificação binária

Correto. A previsão deve ser apresentada como uma de duas classes: `0` ("não saiu") ou `1` ("saiu"). Prevendo o fluxo de saída (assim como resgate de empréstimo) é um exemplo comum de uma classificação binária nos negócios.

Agrupamento

Regressão

Fantástico!

Pergunta

O que é necessário para qualquer tarefa de aprendizado de máquina?

Rótulos

Características

Certo. Nós precisamos pelo menos de alguns dados, mesmo se nós não tivermos os rótulos.Esse é o minimum minimorum (minímo absoluto).

Uma rede neural

Fantástico!

Pergunta

Qual é a maior dificuldade em aprendizado não supervisionado?

Interpretação é problemática

Não há conexão com relacionamentos reais no mundo

Avaliando a qualidade de um modelo é mais difícil

Isso mesmo! Com agrupamento, é difícil julgar a qualidade de um modelo, já que não há respostas corretas claramente definidas com as quais o resultado possa ser comparado. No entanto, ainda existem métricas especiais para tarefas de aprendizado não supervisionado. Normalmente eles são baseados em relações de distância entre intra grupo e intergrupo.

Muito bem!

Pergunta

Qual dessas tarefas pode ser resolvida usando aprendizado de máquina?

Determine o preço de uma nova propriedade no Airbnb

Previsão de vendas de uma nova loja

Identifique grupos comuns de usuários baseados em dados sobre eles

Procurar por atividades anormais em um site

Crie imagens em estilos específicos

Traduzir textos

Todas as opções acima

Isso mesmo! O objetivo desse quiz é demonstrar o como as ferramentas do aprendizado de máquina são amplamente aplicáveis. Atualmente, parece mais difícil encontrar uma tarefa que o AM não tenha sido usado para resolver.

Você conseguiu!

Pergunta

A qual branch do aprendizado de máquina a robótica pertence?

Aprendizado Supervisionado

Reforço de aprendizado

Isso mesmo! Na robótica, o algoritmo obtém "encorajamento" do ambiente se ele der as respostas corretas.

Aprendizado não supervisionado

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-26-31-753Z.md
### Última modificação: 2025-05-28 20:26:32

# Treinando um Modelo em Python: sklearn - TripleTen

Teoria

# Treinando um Modelo em Python: sklearn

Você aprendeu que um **modelo** é um sistema de inter-relações entre características e variáveis alvo ou entre observações que refletem a realidade para um alto nível de acurácia.

Modelos são treinados usando **observações** existentes e construídos com vários métodos, ou **algoritmos**. Nas próximas lições, vamos estudar alguns modelos populares de aprendizado supervisionado e não supervisionado em detalhes.

Normalmente "algoritmo" se refere a uma abordagem abstrata a um modelo de treinamento. Quando lhe aplicamos linguagens de computador, isso é chamado de **implementação de algoritmo**.

Nesta seção você vai estudar exemplos de implementação de algoritmos em Python, a linguagem com as bibliotecas mais convenientes para tarefas de aprendizado de máquina. Por enquanto só precisamos de dois deles:

-   pandas — para análise e pré-processamento
-   scikit-learn (sklearn) — para a implementação dos algoritmos do aprendizado de máquina

Vamos treinar nosso primeiro modelo em Python com modelos da biblioteca **scikit-learn**, que é geralmente importada como **sklearn** ("scientific kit for learning" - kit científico para aprendizado)

A biblioteca sklearn tem várias ferramentas para trabalhar com dados e modelos, por isso elas são agrupadas em subseções. O módulo **árvore** contém a árvore de decisão.

Em sklearn, cada modelo tem uma estrutura de dados correspondente. **DecisionTreeClassifier** tem uma estrutura de dados projetada para classificação com a árvore de decisão. Você vai estudar árvores de decisão depois. Vamos importar a estrutura da biblioteca:

```
from sklearn.tree import DecisionTreeClassifier
```

Depois vamos criar um objeto que tem essa estrutura de dados.

```
model = DecisionTreeClassifier()
```

A variável `model` irá armazenar nosso modelo. Verdade, ele ainda não pode fazer predições. Para treiná-lo nós precisamos iniciar o algoritmo de treino.

Os modelos recebem um conjunto de valores de características, `X`, e uma variável objetivo, `y`, como entrada. Normalmente seu DataFrame contém a coluna com valores objetivo e depois outras colunas. Por exemplo, vamos dizer que você tenha a tabela `data`. Sua coluna com a variável objetivo é chamada `'target'`. Para definir uma matriz de objeto-característica `X` e o vetor da variável objetivo `y`, vamos chamar o método **drop()** da biblioteca pandas:

```
y = data['target']
X = data.drop(['target'], axis = 1)
```

O método recebe uma lista com os nomes das colunas a serem deletadas. O parâmetro `axis=1` indica que é uma coluna que queremos remover.

Agora nós podemos construir uma inter-relação e usá-la para predizer `y` do novo `X`.

Para começar a treinar, chame o método **fit()** e passe a ele os dados como parâmetro:

```
model.fit(X, y)
```

Para treinar o modelo, você passa a matriz por ele com as características (`X`) e o vetor com os valores da variável objetivo (`y`).

Quando um modelo recebe treinamento, ele encontra uma fórmula capaz de fazer predições que são próximas do estado real das coisas. Como? Você verá logo.

Para fazer predições para um conjunto de dados, você só precisa chamar o método **predict()**:

```
predictions = model.predict(X)
```

Agora treine seu primeiro modelo de aprendizado de máquina!

Treinando um Modelo em Python: sklearn

Tarefa3 / 3

1.

O arquivo `/datasets/dataset_facebook_cosmetics_us.csv` contém dados sobre atividade do usuário em contas verificadas do Facebook de empresas de cosméticos. "Activity" se refere ao número total de interações em uma página: likes, compartilhamentos, e comentários. Esses dados estão armazenados na coluna `'Total interactions'`. A tabela também contém as próprias características das contas do Facebook. Sua tarefa é treinar um modelo para prever a atividade do usuário nas páginas oficiais das empresas.

Leia os dados e salve-os na variável `fb`. Imprima o tamanho do conjunto de dados original e as primeiras cinco linhas.

2.

Os valores na coluna `'Total Interactions'` pode levar qualquer número inteiro não negativo. Você irá descobrir também que essa é um **tarefa de regressão**.

Agora vamos preparar os dados para que eles possam ser usados como entrada do modelo. Crie o DataFrame `X` para armazenar apenas características. Passe os valores objetivo de `'Total Interactions'` para a variável `y`.

3.

Antes de você treinar seu modelo, você precisa especificar o algoritmo a que ele se refere. Você vai estudar algoritmos em detalhes um pouco depois. Nós já indicamos os algoritmos para o modelo de regressão: `RandomForestRegressor`.

Treine o modelo usando os dados que você tem. Depois use o modelo para fazer previsões para o mesmo conjunto de objetos.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

import pandas as pd

from sklearn.ensemble import RandomForestRegressor

import matplotlib.pyplot as plt

import seaborn as sns

  

\# leia os dados sobre as contas e as atividades das empresas.

fb \= pd.read\_csv('/datasets/dataset\_facebook\_cosmetics\_us.csv', sep\=';')

  

\# divida os dados em características (a matriz X) e a variável objetivo (y)

X \= fb.drop('Total Interactions', axis\=1)

y \= fb\['Total Interactions'\]

  

\# definindo o algoritmo para o modelo

model \= RandomForestRegressor()

model.fit(X, y)

  

predictions \= model.predict(X)

  

\# treinando o modelo

\# escreva seu código aqui

  

  

  

\# construindo o gráfico de produção fato

sns.scatterplot(x\=y, y\=predictions, s\=15, alpha\=0.6)

plt.title('The Forecast-Fact graph')

plt.ylabel('Forecast')

plt.xlabel('Fact')

plt.show()

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-26-33-069Z.md
### Última modificação: 2025-05-28 20:26:33

# Treinamento, Validação, e Teste de Dados - TripleTen

Capítulo 2/6

Tarefas de Negócios Envolvendo Aprendizado de Máquina

# Treinamento, Validação, e Teste de Dados

Você treinou o seu primeiro modelo. Mas como sabemos o quão bem ele funciona?

Vamos dizer que a tarefa é aprender a prever se um usuário vai ficar leal no próximo mês ou se sairá para um concorrente. "Will-they-won't-they" (eles-irão-ou-não): essa é obviamente uma tarefa de classificação binária.

Você deseja treinar um modelo para que ele preveja a rotatividade para usuários atuais e futuros. Antes de passar os dados de entrada reais do modelo, você precisa ter certeza de que funciona bem. Você pode treinar o modelo (usando **dados de treinamento**) e comparar suas previsões com os valores de destino reais a partir do mês seguinte. Os dados fornecidos ao modelo durante o treinamento para ajustá-lo são chamados de **dados de validação**. Por fim, testamos o modelo em dados de **teste** ou **de retenção**. A segunda fase desse processo, envolvendo dados de validação e teste, é frequentemente chamada de **fase de validação**. (A terminologia pode ser confusa, nós sabemos!)

**Neste curso, a abordagem será levemente simplificada: nós vamos apenas executar treinamento e validação, não testes.** Portanto, vamos dividir os dados em duas partes.

Digamos que você tem dados sobra 150.000 clientes. Para cada um você tem o valor de 20 características (gênero, idade, frequência de uso, etc.). Você também sabe se o usuário saiu após do mês que as características foram gravadas.

Vamos pegar as 150.000 observações e as dividir em duas partes diferentes, de 100.000 e 50.000. Nós vamos passar a primeira parte para o modelo e treinar o algoritmo nelas. Depois vamos usar o modelo para predizer as respostas na segunda parte dos dados e comparar os resultados com os reais valores objetivo. Isso vai nos permitir refinar o modelo.

As 100,000 observações usadas para treinar o modelo são usadas para treinar dados. As 50,000 usadas para testar o encaixe final são dados de validação.

Pergunta

Como você chama a porção dos dados cujos algoritmos aprenderam as respostas corretas?

Dados de validação

Teste de dados

Treinamento de dados

Isso mesmo!

Trabalho maravilhoso!

Pergunta

Como você chama os dados que são colocados de lado e usados para fornecer uma estimativa final do desempenho do modelo?

Dados de validação

Teste de dados

Isso mesmo!

Treinamento de dados

Você conseguiu!

Pergunta

Quais dados são usados para ver se o algoritmo identifica corretamente as relações entre a variável objetivo e as características durante o treinamento?

Treinamento de dados

Teste de dados

Dados de validação

Totalmente certo!

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-26-34-389Z.md
### Última modificação: 2025-05-28 20:26:34

# Subajuste e Superajuste - TripleTen

Capítulo 2/6

Tarefas de Negócios Envolvendo Aprendizado de Máquina

# Subajuste e Superajuste

Quando treinamos um modelo e "ajustamos" ponderações usando dados de treinamento, nós checamos o quão perto a resposta com as ponderações escolhidas está das medidas reais.

Portanto, nós **estimamos o erro do treinamento** mesmo durante a etapa de preparação.

É quase impossível escolher as ponderações apropriadas para um modelo linear ou qualquer outro algoritmo para que ele dê a resposta correta para cada observação. O modelo estará errado em algumas observações. Isso é chamado de **treinamento de erro**. Nosso objetivo é minimizar isso. Se não fizermos isso, e o algoritmo que acabarmos escolhendo for errado para metade de todas as observações, isso significa que o modelo está com **subajuste**. O erro que ocorre nesses casos é chamado de **bias** (tendência). Uma função tendenciosa falha em levar em conta todas as relações dentro de um conjunto de dados. As razões mais comuns para tendenciamento são:

-   O número de amostras ou características são muito pequenas
-   A função está muito simples
-   Uma abordagem defeituosa ao selecionar opções para a relação objetivo

Esse modelo irá produzir resultados fracos tanto com dados de treinamento e de teste.

Preferencialmente, um modelo (função, algoritmo) não só comete erros raramente quando está sendo treinado, mas também trabalha bem com novos dados que ele não "enxergou" quando estávamos "ajustando" as ponderações e procurando pela melhor relação possível. Em outras palavras, o modelo deve ter uma alta **capacidade de generalização**. Depois, usar aprendizado de máquina será realmente útil.

Pense novamente sobre a tarefa do pinguim e imagine que em vez de encontrar uma função, nós só memorizamos cenários específicos:

-   Se `feature_1 = 7`, `feature_2 = 10`, e `feature_3 = 12`, a resposta é 0
-   Se `feature_1 = 9`, `feature_10 = 3`, `feature_3 = 7`, a resposta é 1
-   ...e assim por diante.

Isso não é a busca por padrões, é apenas memorização. Seu modelo parece perfeito... mas apenas até novos valores de características serem dados para novos objetos. Quando o modelo mostrar resultados bem piores nos dados de validação do que apresentou durante o treinamento, isso é chamado de **superajuste**. Esses erros são chamados de **erros de variação**. Isso implica que o modelo está se esforçando demais para ajustar os dados e não ignora o ruído: quando foi treinado, levou em consideração informações excessivas além das relações reais dentro dos dados.

Vamos pensar sobre superajuste e subajuste no contexto da preparação para um exame físico.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.2.8PT.png)

Superajuste: para preparar para um teste sobre eletrodinâmica, você treinou com tarefas e respostas, mas ignorou leis e fórmulas.

Subajuste: você focou em leis e fórmulas, mas confundiu o numerador e denominador na lei de Ohm.

Pergunta

O que um modelo deve ter?

Superajuste

Capacidade de resolução

Capacidade de generalização

Certo. Um modelo deve ser capaz de generalizar o resultado de dados antigos para aplicá-los aos novos dados.

Subajuste

Muito bem!

Pergunta

Que erro pode ser estimado na fase do treinamento?

Variância

Tendências

De fato, durante o treinamento nós podemos estimar o erro de tendência, que representa a diferença entre valores reais e previsões feitas usando os pesos atuais.

Fantástico!

Pergunta

Quais dessas situações é um exemplo de superajuste?

O modelo realiza bem tanto treinamento quanto validação dos dados, mas os resultados são um pouco piores para os últimos.

O modelo não realiza bem treinamentos, mas faz um ótimo trabalho com validação de dados.

O modelo retorna erros mínimos para dados treinados, mas trabalha mal durante a validação.

Alguns algoritmos permitem quase 100% de precisão na etapa do treinamento, mas isso não significa que o modelo é bom. Se isso demonstra resultados com validação de dados, nós vamos ter superajuste.

O modelo realiza igualmente mal com os dados do treinamento e validação dos dados.

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-26-35-708Z.md
### Última modificação: 2025-05-28 20:26:36

# Dividir e Validar - TripleTen

Teoria

# Dividir e Validar

Os termos "validação de dados" e "teste de dados" às vezes são usados intercambiavelmente, e não existem consensos perfeitos a respeito das suas definições.

O seu chefe pode perguntar a você: "Como o modelo trabalhará com dados de teste? Sua qualidade melhorou?"Aqui, no entanto, eles parecem estar se referindo à validação de dados, que são usados para melhorar o modelo. E nós encontramos o mesmo tipo de coisa nas bibliotecas Python: a função que divide dados é chamada `train_test_split()`, mesmo que nós possamos usá-los para conseguir a validação dos dados. Não entre em pânico; em cada caso, considere o contexto e você será capaz de descobrir ao que ele é indicado.

Não importa como você chama as parcelas dos dados, as perguntas básicas continuam as mesmas:

-   **Em qual proporção nós dividimos os dados?** Normalmente a proporção é 80/20 ou 70/30. A regra geral é que dados de treinamento devem ser maiores que dados de validação, mas depois devem ser grandes o suficiente para justificar deixar o teste "de castigo."
-   **Como os dividimos?** Você vai aprender as duas maneiras de dividir os dados: **por tempo** ou **aleatoriamente**. O primeiro método funciona bem quando observações anteriores impactam nas mais recentes. O segundo é útil se a estrutura de tempo não é tão importante no treinamento dos dados.

Uma maneira é chamar a função `train_test_split()` do módulo `train_test_split()`da biblioteca sklearn. Esta é a sintaxe:

```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

Aqui os dados de entrada para a função é a matriz de características `X`, o vetor da variável objetivo `y`, e o parâmetro `test_size`, que controla o quanto dos dados serão divididos.

A função retorna duas matrizes de característica e dois vetores de variáveis objetivo obtidos ao dividir os dados originais de acordo com a proporção definida em `test_size`.

Por definição, `train_test_split()` divide aleatoriamente os dados na proporção que você definiu. Se você rodar o código várias vezes, você vai obter diferentes matrizes e vetores.

Para fazer o seu trabalho nesse curso comparável com o de outros estudantes, dê ao parâmetro `random_state` o valor zero quando dividir os dados:

```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

O parâmetro `random_state` está presente em outras funções também, e é levemente responsável pela "aleatoriedade." Por exemplo, ele pode ser usado quando definir o algoritmo do modelo:

```
model = RandomForestRegressor(random_state=0)
```

## Avaliando a qualidade de um modelo

Depois que você dividir os dados da maneira correta e nas proporções certas, treine-os com dados de treino e verifique sua performance com dados de validação.

Por enquanto vamos focar em uma das métricas aplicável para modelos de regressão: o coeficiente de determinação, ou **R ao quadrado**. Assim é como chamamos a parte da variância da variável objetivo que o modelo explica. Essa métrica leva valores entre 0 e 1e representa com quanta precisão os modelos de previsão representam as variáveis objetivo. Se as previsões são perfeitas, o valor de R quadrado será 1; se elas forem horríveis, o valor será 0. A fórmula do coeficiente de determinação da amostra vai ser assim:

R2\=1−∑i\=1n(yi−y^i)2∑i\=1n(yi−y‾)2R^2 = 1 - \\frac{\\sum\_{i=1}^{n}{(y\_i-{\\hat {y}}\_{i})^{2}}}{\\sum\_{i=1}^{n}{(y\_i-{{\\overline {y}})^{2}}}}R2\=1−∑i\=1n​(yi​−y​)2∑i\=1n​(yi​−y^​i​)2​

Pode parecer assustador, mas, na verdade, é simples.

-   O numerador é a soma de todos os valores de erro ao quadrado. ∑i\=1n\\sum\_{i=1}^{n}∑i\=1n​ significa,"A soma de todos os valores é definida no seu conjunto do primeiro ao último", enquanto(yi−y^i)2{(y\_i-{\\hat {y}}\_{i})^{2}}(yi​−y^​i​)2significa, "Pegue a diferença entre cada valor atual e o valor correspondente previsto, depois eleve-o ao quadrado." A soma será pequena se as predições forem precisas.
    
-   O denominador contém a soma das diferenças entre cada valor atual e a média (y‾{{\\overline {y}}}y​).
    

A fração pode ser pensada assim:

o erro quadrado do seu modeloerro quadrado relativo aˋ meˊdia\\frac{\\text{o erro quadrado do seu modelo}}{\\text{erro quadrado relativo à média}}erro quadrado relativo aˋ meˊdiao erro quadrado do seu modelo​

Isso é, ele diz a você qual proporção da variância observada não é coberta pelo modelo (quando o modelo é bom, os valores são pequenos). Nós o subtraímos de 1 para ver qual percentual da variância **é** realmente explicada.

Se o seu modelo é altamente qualificado, ele supera fortemente a média, então a fração é perto de 0 e R quadrado é perto de 1.

(Se você não tem certeza que entendeu, esse [Khan Academy video](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/assessing-fit-least-squares-regression/v/r-squared-or-coefficient-of-determination) _(os materiais estão em inglês)_ pode ajudar).

Vamos discutir a qualidade do modelo em mais detalhes em lições futuras. Por enquanto, vamos olhar como conseguir R quadrado em Python.

### R ao quadrado em Python

A sintaxe de R quadrado (`r2_score`) é simples: aprove o real vetor da variável objetivo e a preveja um como parâmetros:

```
from sklearn.metrics import r2_score

r2_score(y_test, y_pred)
```

A função retorna um valor: o coeficiente de determinação.

Dividir e Validar

Tarefa2 / 2

1.

Vamos continuar a atividade de previsão nas páginas oficiais do Facebook das companhias de cosméticos. Divida os dados dentro de `train` e `test` com uma taxa de 80% a 20%. Indique `random_state=0` no parâmetro da função `train_test_split`.

2.

Agora estamos prontos treinamento e validação. Treine o modelo usando apenas dados treinados, faça a previsão para validação dos dados, e imprima o valor da métrica `r2_score`. Especifique `random_state=0` quando definir o algoritmo para o modelo.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

import pandas as pd

from sklearn.model\_selection import train\_test\_split

from sklearn.ensemble import RandomForestRegressor

from sklearn.metrics import r2\_score

  

\# leia os dados contendo os atributos das contas e atividades das empresas

fb \= pd.read\_csv('/datasets/dataset\_facebook\_cosmetics\_us.csv', sep\=';')

  

\# divida os dados em características (a matriz X) e a variável objetivo (y)

X \= fb.drop('Total Interactions', axis\=1)

y \= fb\['Total Interactions'\]

  

\# divida os dados sobre treinos e testes

X\_train, X\_test, y\_train, y\_test \= train\_test\_split(

X, y, test\_size\=0.2, random\_state\=0

)

  

\# defina o algoritmo do modelo

model \= RandomForestRegressor(random\_state\=0)

  

\# treine o modelo

model.fit(X\_train, y\_train)

  

\# use o modelo treinado para fazer previsões

predictions \= model.predict(X\_test)

  

\# estime R quadrado usando dados de teste e imprima o resultado

r2 \= r2\_score(y\_test, predictions)

print('Valor de R quadrado:', r2)

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-26-37-054Z.md
### Última modificação: 2025-05-28 20:26:37

# O Pipeline do Aprendizado de Máquina - TripleTen

Teoria

# O Pipeline do Aprendizado de Máquina

"Aprendizado de máquina" se tornou o termo da moda, mas o conceito básico é bastante simples. Nós temos um conjunto de observações para cada valor de característica (vetor de característica `X`), e às vezes o valor da variável objetivo `y`, é dado. Nós dividimos esse conjunto em partes: a primeira parte será usada para treinar o modelo, o segundo para teste e refinar o conjunto, o terceiro para fazer previsões. Isso é bastante simples, mas na verdade treinar um modelo é um pouco mais complicado.

Aqui estão as etapas básicas do aprendizado de máquina: ![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.2.10PT.png)

A sequência pode diferir dependendo da tarefa (aprendizado supervisionado e não supervisionado, regressão ou classificação) e algoritmo. Porém, existem alguns princípios gerais que valem a pena prestar atenção..

## Definindo a tarefa

Nesse ponto, você determina o destino do modelo. Você translada uma tarefa de negócios para matemática, ferramentas analíticas e aprendizado de máquina. Certifique-se de que entendeu completamente o problema que os negócios estão enfrentando, já que isso determina a escolha do modelo, algoritmo e métricas.

## EDA

Antes de aprovar a entrada de dados para o seu modelo para conseguir previsões, você precisa realizar uma **EDA (análise exploratória de dados)**. Nessa etapa você estuda distribuição de certas características e variáveis objetivo, identifica correlações entre valores e estuda o conjunto de dados de perto. Às vezes é vantajoso fazer histogramas de características. Use a função **histplot()** da biblioteca seaborn:

```
import seaborn as sns

sns.histplot(df['feature_1'], kde=True)
```

A análise exploratória de dados permite que você formule hipóteses iniciais sobre a qualidade dos dados e a presença de anomalias. Nesse ponto já é possível dizer quais características serão cruciais para o modelo, e ainda, quais podem ser ignoradas. EDA pode inspirar você a criar novas características baseada nas que já existem. Essa análise é importante tanto em termos de negócios quanto para a qualidade do modelo atual. Você vai mergulhar fundo nos dados, descobrir o sentido por trás dos números e fazer hipóteses válidas.

## Pré-processamento de Dados

Se você tiver apenas duas horas para construir a linha de base de um modelo, você provavelmente pode pular o pré-processamento dos dados. Mas na realidade, os dados são frequentemente **pré-processados**, o que significa:

-   Nós nos livramos dos valores ausentes.
-   Algumas características são transformadas (por exemplo, características categóricas são convertidas em quantitativas).
-   Os dados são normalizados ou padronizados (nós vamos estudar esse processo no próximo capítulo).
-   Novas características são criadas com base nas que já existem. Esse processo é chamado também de **engenharia de característica**, e às vezes pode melhorar a drasticamente a qualidade do seu modelo.

## Escolhendo uma estratégia de validação

Na última lição você dividiu os dados em treinamento e conjuntos de validação e aprendeu porque é importante estimar métricas com observações de retenção. Nessa etapa você deve escolher como vai criar o conjunto de validação baseado nos tipos de dados e na tarefa. Vamos conversar mais sobre isso no terceiro capítulo.

Importante também nessa etapa garantir que a distribuição de valores nos dados de treino seja próxima da distribuição com a qual o modelo vai ter que lidar. De outro modo, seu trabalho será em vão.

## Escolhendo um algoritmo

Você tem uma caixa de ferramentas de algoritmos, que tem prós e contras, que você pode usar dependendo do tipo de tarefa (aprendizado supervisionado e não supervisionado) e a tarefa em si. Alguns algoritmos são mais precisos, mas difíceis de interpretar, outros são mais rápidos, mas fracos. Aqui estão os critérios básicos para escolher um algoritmo:

-   Acurácia
-   Velocidade
-   Interpretabilidade
-   Características dos algoritmos individuais: eles trabalham de maneira diferente para cada um das características

Mesmo o algoritmo mais simples tem um número de parâmetros que pode ser ajustado. Às vezes eles podem impactar a qualidade e a velocidade do treinamento do seu modelo. Normalmente você escolhe parâmetros nas iterações: você treina um modelo que tem certos parâmetros, estima a métrica, vê que os resultados são pobres e muda os parâmetros, e faz o procedimento todo de novo. Isso pode ir até o infinito, mas nós vamos te ensinar a parar na hora certa.

No próximo capítulo você vai estudar algoritmos com mais detalhes.

## Escolhendo métricas

Antes de você seguir no treinamento do seu algoritmo, determine a forma que você avalia a performance dele.

Existe um conjunto de métricas para cada tipo de tarefa (classificação, regressão, agrupamento).Mas é importante não simplesmente executar os resultados através desse conjunto, mas entender qual métrica reflete melhor a essência de um processo de negócios.

No segundo capítulo, você vai aprender quais métricas existem e como elas são diferentes. Talvez você até crie sua própria métrica para, um dia, persuadir seus colegas de equipe de que um modelo é muito útil.

Nesta etapa, vale a pena descobrir quais métodos a sua empresa já usa para cada tarefa. Dessa forma você será capaz de comparar a eficácia do aprendizado de máquina com linha de base já estabelecida.

## Treinando e prevendo

Você completou todas as etapas. A seguir vem um algoritmo com o tradicional `ajuste-previsão`. Na fase de ajuste você o passou pelo conjunto de treinamento para que ele possa identificar claramente relações entre características.

Depois partimos para as previsões. Você tem dados de retenção para cada característica e resposta que você conhece. Nessa etapa você pega as características por si só, dá a elas o modelo treinado como entrada, e salva os valores previstos.

## Avaliando a qualidade dos resultados e escolhendo o melhor modelo

Nessa etapa você olha para as diferenças entre os valores previstos e reais para os objetos do conjunto de validação. Frequentemente analistas avaliam vários algoritmos e escolhem aquele com as melhores métricas.

## Analisando a importância das características

Você escolheu o algoritmo que é mais efetivo em comparação com a linha de base e outros algoritmos, mas você não está pronto para iniciar o modelo ainda. Primeiro você precisa confirmar mais uma vez que o modelo reflete os padrões certos e inter-relações dentro dos dados. Como? Por exemplo, **analisando a importância das características**. Isso deixa você avaliar não apenas avaliar as predições em si, mas também as razões que o modelo as fez. Assim, colegas de equipe de outros departamentos poderão confiar no seu modelo e começar a usá-lo.

## O que vem a seguir?

Você encontrou os dados, os transformou, os dividiu em conjuntos de treinamento e validação, e escolheu vários algoritmos em potencial. Depois chegou a fase `ajuste-previsão`. Você escolheu o melhor modelo, interpretou a importância das características, e gradualmente ficou convencido que isso funciona. Excelente! Agora podemos introduzir o modelo no fluxo de trabalho atual e colocá-lo em uso de verdade?

Não é tão simples: depois de passar por todo o pipeline uma vez, você provavelmente precisa retornar para as etapas anteriores, fazer mudanças, e ver quais são os efeitos. Isso é totalmente OK. E não se esqueça de que existem situações onde o aprendizado de máquina simplesmente não se encaixa, não importa o quanto você, os dados, ou o modelo sejam bons.

O Pipeline do Aprendizado de Máquina

Tarefa2 / 2

1.

Vamos continuar trabalhando com contas do Facebook das empresas de cosméticos. Faça histogramas da variável objetivo usando dados de treino e teste variável (estritamente falando, validação). Use o método `histplot()` da biblioteca seaborn com o parâmetro `kde=` definido como `True`. Os gráficos são diferentes?

2.

Faça uma matriz de correlação para características numéricas no conjunto de dados. Visualize-a usando o método `heatmap()`. O que você pode dizer sobre a correlação entre a variável objetivo e outras características? Existe alguma característica que correlaciona a variável objetivo com um valor absoluto de mais do que 0,9? Isso vai nos ajudar a entender o impacto deles na variável objetivo mesmo sem construir um modelo.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

  

\# leia os dados sobre as contas e as atividades das empresas.

fb \= pd.read\_csv('/datasets/dataset\_facebook\_cosmetics\_us.csv', sep\=';')

  

\# matriz de correlação

corr\_m \= fb.corr()

  

\# faça um mapa de calor

plt.figure(figsize\=(15, 15))

sns.heatmap(corr\_m, square\=True, annot\=True)

plt.show()

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-26-38-371Z.md
### Última modificação: 2025-05-28 20:26:38

# Por que o Aprendizado de Maquina Não é Universal? - TripleTen

Capítulo 2/6

Tarefas de Negócios Envolvendo Aprendizado de Máquina

# Por que o Aprendizado de Maquina Não é Universal?

Você sabe mais ou menos o que é aprendizado de máquina. Agora é hora de aprender uma coisa tão importante quanto: quando ele deve ser usado.

Antes de aprovar os dados para um modelo de aprendizado de máquina, responda essas perguntas chave:

-   A amostra é grande o suficiente?
-   Os dados são de alta qualidade?
-   O seu modelo é capaz de fazer previsões realistas?

## Amostras

Não existe resposta definitiva sobre qual amostra pode ser considerada grande o suficiente; isso depende da situação. Especialistas são frequentemente guiados por **regras empíricas**, por exemplo, aquelas derivadas da experiência, quando é decidido o tamanho da amostra. Eles levam em conta o número de características, a variedade de valores das variáveis objetivo, e os detalhes dos algoritmos em si,

De acordo com a primeira regra, o número mínimo requerido de observações em uma amostra é linearmente relacionado ao **número de características.** Em outras palavras, o tamanho mínimo de uma amostra pode ser encontrado com a fórmula `s = k * n`, em que `n` é o número de características para cada observação e `k` é a constante, cuja experiência mostra que normalmente é igual a 10. Se existem 20 características por usuário, então você vai precisar de pelo menos 200 observações para identificar relações. Mas se você tem 150 características, mesmo mil usuários não serão suficientes.

Outra regra empírica é boa para tarefas de agrupamento e tem como ponto de partida **o número de agrupamentos objetivo**. Quanto mais agrupamentos você tiver, mais difícil é diferenciá-los com base nas características disponíveis. Se você calcular o número mínimo requerido de observações usando a regra anterior e aumentar o número de classes alvo por fator de `n`, você terá que aumentar o valor resultante no mesmo grau.

Imagine que você é analista em uma startup. O serviço está coletando dados dos usuários das redes sociais. Você tem carregamentos de características de clientes à sua disposição, de dados demográficos à atividade deles no seu site. O gestor de produtos pede a você que use essa informação valiosa a segmentos dos usuários. O problema é que a empresa entrou no mercado recentemente e tem apenas 400 clientes.

Nós podemos dividir 400 usuários em 10 agrupamentos quando temos 234 características? É mais provável que você encare o **curso da dimensionalidade**. O número de observações não será grande o bastante para fazer um agrupamento para todo o conjunto de características.

A última regra tem como ponto de partida a família a que o algoritmo pertence. Por exemplo, redes neurais não trabalham bem com pequenos conjuntos de dados. Eles precisam receber centenas de milhares de observações.

Em resumo: se o número de clientes ou observações não são medidos em milhares, você não precisa mesmo de aprendizado de máquina. Caso contrário, julgue cada caso por seus próprios méritos!

## A qualidade dos dados

Imagine que você tem um milhão de observações. O sonho de um modelo! Mas a quantidade é a coisa mais importante? E se seus dados não contiverem realmente a quantidade de informações que você espera que tenha para identificar relações?

No aprendizado de máquina, existe uma regra chamada GIGO: garbage in, garbage out (RERS: resto entra, resto sai). Se a entrada do modelo é de baixa qualidade, você terá resultados ruins mesmo que escolha o algoritmo correto. Você deve encontrar os seguintes problemas nos dados:

-   ruído
-   valores ausentes
-   erros e valores atípicos
-   mudanças na distribuição com o passar do tempo

Tente descobrir se seus dados têm esse tipo de problema na etapa AED. Você pode resolver alguns deles com meios de processamento.

Mas talvez tenham casos quando você não pode afetar a qualidade dos dados. Por exemplo, se por uma determinada característica a parcela de valores ausentes for maior que 50%, e as observações não têm correlação de tempo, (por exemplo, você não será capaz de "esticar" o valor dos períodos anteriores), o mais provável é que você tenha que se livrar da variável.

**Variabilidade nos dados** é outro problema que você pode encontrar. As distribuições de características em treinamento, validação, e teste de dados devem ser similares, ou então o modelo será inútil.

Imagine que você treinou um modelo que faz predições de um certo defeito de fabricação usando dados de um sensor de máquina. Ele funcionou por um mês, mas de repente começou a produzir resultados não esperados. O que aconteceu? A máquina foi atualizada ou substituída, causando mudanças na distribuição dos valores. Sua única escolha é esperar até que existam novos dados suficientes para retreinar o modelo.

## Modelos de baixa qualidade

Isso também acontece. Pode parecer que você tem um monte de dados de alta qualidade, mas a métrica pode dizer a você que você não pode prever a variável objetivo escolhida usando os dados que você tem. Por quê?Porque suas características não estão relacionadas à sua variável objetivo, então você não pode prever o valor.

Dados sobre o número de visitantes a redes de restaurantes dos EUA não irão te ajudar a prever a população de pinguins africanos, não importa o quanto esses dados sejam precisos. Você provavelmente vai conseguir um erro relativo perto de 100% e um valor R quadrado perto de 0.

Ou talvez os dados serão muito ruidosos e voláteis. É quase impossível encontrar um "bom sinal" em tal conjunto. Em casos como esse, aprendizado de máquina não servirá; você vai precisar usar outros métodos de modelagem matemática.

Pergunta

Você está fazendo longboards customizados com um desenho único. Para qual das opções seguintes o aprendizado de máquina não é apropriado?

Prever a probabilidade de um pedido repetido (já pedido ou não)

Correto. Aprendizado de máquina normalmente funciona em big data. Se você não for um androide, fabricar uma única prancha leva bastante tempo e você provavelmente não tem centenas de milhares de clientes, então não existem dados suficientes para um modelo. Uma abordagem individual e conversar com seus clientes lhe trará muito mais informações sobre se eles vão ou não voltar..

Agrupar os usuários baseados em algumas características

Desenvolvendo projetos com uma rede neural

Prever a probabilidade de cliques para propagandas de redes sociais

Excelente!

Pergunta

Qual das opções seguintes não é uma indicação de baixa qualidade de dados?

A alta proporção de valores ausentes (mais do que 50%) de todas as características

Dados ruidosos

A presença de características categóricas

Se os dados são categóricos, isso não significa que sejam de baixa qualidade. Essas características podem ser úteis.

Muitos erros nos dados

Excelente!

Pergunta

Você tem dados sobre a história de vendas da empresa em 24 meses, sem qualquer outra característica: 24 observações no total. Você julga que vale a pena aplicar aprendizado de máquina?

Opções:

Provavelmente

Provavelmente não

Isso mesmo! Quando você tem apenas séries temporais com a variável objetivo e 24 pontos, aplicar aprendizado de máquina não é apropriado; existem também algumas observações e características (nenhuma, para ser preciso).

Seu entendimento sobre o material é impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-26-39-941Z.md
### Última modificação: 2025-05-28 20:26:40

# Conclusão - TripleTen

Capítulo 2/6

Tarefas de Negócios Envolvendo Aprendizado de Máquina

# Conclusão

Você aprendeu o que é aprendizado de máquina, quais tarefas ele pode te ajudar resolver, e as etapas básicas que ele envolve.

Você treinou seu primeiro modelo, embora não tenhamos especificado exatamente como. No próximo capítulo você vai se familiarizar com vários algoritmos de treinamento de modelos, aprender porque e com eles funcionam, e ver a quais tipos de tarefas eles se aplicam.

Se você quer ler mais sobre scikit-learn, aqui está um link útil:

[https://scikit-learn.org/stable/](https://scikit-learn.org/stable/) _(os materiais estão em inglês)_.

---

Faça download do [sumário do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/PT_Sumrio_do_Captulo_Tarefas_de_Negcios_Envolvendo_Aprendizado_de_Mquina.pdf?etag=7abbc8f4d6679c8f80432b27580db4b4) e [folha de conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/PT_Folha_de_Concluses_Tarefas_de_Negcios_Envolvendo_Aprendizado_de_Mquina.pdf?etag=24cb6ad7c73f9839cd8fa32459d12147) da Base de Conhecimento para que você possa consultá-los quando necessário.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-36-461Z.md
### Última modificação: 2025-05-28 20:27:36

# Introdução - TripleTen

Capítulo 3/6

Algoritmos de Aprendizado de Máquina

# Introdução

Você aprendeu o que é aprendizado de máquina e até construiu seu primeiro modelo.

Neste capítulo, você vai ver mais de perto vários algoritmos e aprenderá como eles funcionam e como usá-los, na prática. Até agora você lidou com um modelo. Aqui você trabalhará com seis, cada um será do tipo diferente, para tarefas diferentes, e treinará todos eles. Você verá o que cada modelo é capaz, quais erros comete e qual é o tamanho deles. Você também preparará conjuntos de características para seus modelos. É uma arte.

### O que você irá aprender:

-   Como estimar o valor da função de erro
-   Como escalar características por meio de padronização e normalização
-   Lutar com a multicolinearidade das características
-   Calcular métricas de qualidade do modelo
-   Resolver tarefas de aprendizagem supervisionadas e não supervisionadas

### Quanto tempo irá demorar:

_16 lições, aproximadamente de 15 a 20 minutos_

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-37-824Z.md
### Última modificação: 2025-05-28 20:27:38

# Funções de regressão linear e erro - TripleTen

Teoria

# Funções de regressão linear e erro

Vamos começar nossa viagem ao mundo dos algoritmos de aprendizado de máquina. Nossa primeira parada é a regressão!

A regressão é um tipo de aprendizado supervisionado. Aqui o modelo deve identificar uma relação entre as características dos objetos e o valor da variável alvo. Com a regressão, a variável de destino é um número, em vez de uma classe.

Na tarefa do pinguim, formulamos nosso objetivo ao treinar o modelo: criar uma fórmula que nos permita fazer previsões para qualquer objeto de acordo com suas características. As previsões precisam ser bastante precisas, ou seja, próximas das previsões reais em dados históricos. Se as leis da natureza e dos negócios não mudarem, a fórmula funcionará para os dados futuros, cujas respostas são desconhecidas.

Vamos lembrar o que foi dado na tarefa:

`n` — o número de observações no conjunto de dados

`k` — o número de características

y⃗ – o vetor (lista) de valores de variaˊveis de objetivo para nossos dados\\vec{y} \\text{ -- o vetor (lista) de valores de variáveis de objetivo para nossos dados}y​ – o vetor (lista) de valores de variaˊveis de objetivo para nossos dados

`X` — uma matriz objeto-característica (`n x k`) na qual as linhas são observações e as colunas são características

Dentro da matriz, cada observação tem um vetor correspondente ou conjunto de valores de características

x⃗\=(x1,x2,...,xk)\\vec{x} = (x\_1, x\_2, ..., x\_k)x\=(x1​,x2​,...,xk​)

A matriz X tem a seguinte aparência:

X\=(x11⋯x1k⋮⋱⋮xn1⋯xnk)X = \\begin{pmatrix} x\_{1\_1} & \\cdots & {x\_{1\_k}}\\\\ \\vdots & \\ddots & \\vdots \\\\ x\_{n\_1} & \\cdots & {x\_{n\_k}} \\end{pmatrix}X\=​x11​​⋮xn1​​​⋯⋱⋯​x1k​​⋮xnk​​​​

Estamos procurando uma fórmula única que faça uma previsão (estimativa) para qualquer objeto usando os valores de características `k`:

f(x⃗)\=f(x1,x2,...,xk)\=y^f(\\vec{x}) = f(x\_1, x\_2, ..., x\_k) = \\hat{y}f(x)\=f(x1​,x2​,...,xk​)\=y^​

Se encontrarmos essa fórmula e a aplicarmos a todos os objetos do conjunto de dados, obteremos um vetor de previsão (conjunto) para todos os objetos:

y^⃗\=(y1^,y2^,...,yn^) – um vetor de previsa˜o (lista)\\vec{\\hat{y}} = (\\hat{y\_1}, \\hat{y\_2}, ..., \\hat{y\_n}) \\text{ -- um vetor de previsão (lista)}y^​​\=(y1​^​,y2​^​,...,yn​^​) – um vetor de previsa˜o (lista)

Onde:

yi^ – uma previsa˜o para uma certa observac¸a˜o i\\hat{y\_i} \\text{ -- uma previsão para uma certa observação i}yi​^​ – uma previsa˜o para uma certa observac¸​a˜o i

O vetor resultante pode ser comparado com o vetor original de valores de variáveis de objetivo para descobrir o quão boa é a fórmula. Isso pode ser feito, por exemplo, calculando o erro médio para todas as observações.

Primeiro, pensaremos em como a fórmula pode ficar, depois ajustaremos seus parâmetros.

A maneira mais simples é assumir que a variável de objetivo é descrita com uma função linear de características:

y(x)\=w0+w1∗x1+w2∗x2+...+wk∗xky(x) = w\_0 + w\_1 \* x\_1 + w\_2 \* x\_2 + ... + w\_k \* x\_ky(x)\=w0​+w1​∗x1​+w2​∗x2​+...+wk​∗xk​

Aqui:

(w0,w1,w2,...,wk) – coeficientes numeˊricos(w\_0, w\_1, w\_2,..., w\_k) \\text{ -- coeficientes numéricos}(w0​,w1​,w2​,...,wk​) – coeficientes numeˊricos

Esses coeficientes são chamados de **ponderações**: eles determinam a contribuição de cada característica para a soma que define a nossa previsão. Portanto, precisamos encontrar valores de ponderações que permitirão que a função produza resultados muito próximos dos valores reais.

Vamos supor que temos apenas uma característica para cada observação. Vamos fazer um gráfico em que cada observação tenha um ponto, com a característica ao longo do eixo X e o valor da variável de objetivo ao longo do eixo Y:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-20T171619.940_1592662721.png)

Precisamos encontrar uma função `y = w_0 + w * x` que produza um gráfico de linha, e essa linha deve descrever bem as nossas observações. Algumas linhas caracterizam um conjunto, ou **nuvem de dados**, melhor do que outras. Por exemplo, você pode traçar um gráfico para a função `y = 3 + 1 * x` ou `y = 2 + 2 * x`:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_-_2020-06-20T171852.538_1592662750.png)

A linha verde descreve as observações melhor do que a vermelha.

Mas como podemos descrever mais formalmente até que ponto uma função reflete a real inter-relação entre características?

A maneira mais fácil é estimar **como estamos errados** em nossas previsões em comparação com o valor real da variável de objetivo.

Vamos apresentar a **função de erro** `Q(w)`. Ela vai depender dos pesos que escolhemos.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.2PT.png)

O que deveríamos considerar um erro? A diferença entre os valores previstos e reais, certo? Na verdade, isso não está certo. A diferença entre a expectativa e a realidade pode ser negativa, mas o erro não. Se a previsão for perfeita, o erro é 0. Se um valor de previsão for menor ou maior que o valor real, o erro será positivo.

Para garantir que o erro seja um valor positivo, o elevamos ao quadrado:

(yi−yi^)2(y\_i - \\hat{y\_i})^2(yi​−yi​^​)2

Com certos objetos o modelo errará mais, com outros, menos.

Q(w)\=∑i\=1n(yi−yi^)2nQ(w) = \\frac{\\sum\_{i = 1}^{n}(y\_i - \\hat{y\_i})^2}{n}Q(w)\=n∑i\=1n​(yi​−yi​^​)2​

Agora temos que encontrar pesos (`w`) que minimizem o valor da função de erro.

Vamos ver como calculamos o erro de uma função em Python. Temos quatro observações com valores reais de variáveis de objetivo para uma tarefa de regressão:

y1\=5.5,y2\=6.4,y3\=1.2,y4\=3.3y\_1 = 5.5, \\newline y\_2 = 6.4, \\newline y\_3 = 1.2, \\newline y\_4 = 3.3y1​\=5.5,y2​\=6.4,y3​\=1.2,y4​\=3.3

Temos apenas uma característica, e seus valores são:

x1\=34,x2\=38,x3\=14,x4\=20x\_1 = 34, \\newline x\_2 = 38, \\newline x\_3 = 14, \\newline x\_4 = 20 x1​\=34,x2​\=38,x3​\=14,x4​\=20

Vamos supor que a função seja:

y(x)\=−2+0.3∗xy(x) = -2 + 0.3\*xy(x)\=−2+0.3∗x

Como calculamos o erro? Vamos criar os vetores `x` e `y` correspondentes:

```
x = [34,38,14,20]
y = [5.5, 6.4, 1.2, 3.3]
```

Para criar imediatamente um vetor de previsões feitas pela função, desenvolveremos um gerador de listas aplicando a função a cada um dos elementos da lista `x`:

```
y_pred = [(-2 + 0.3*i) for i in x]
print(y_pred)
```

```
[8.2, 9.4, 2.2, 4.0]
```

Só precisamos calcular a diferença entre ele e o valor real:

```
print(sum((np.array(y) - np.array(y_pred)) ** 2 )/ len(y))
```

```
4.444999999999999
```

O erro quadrático médio parece ser aceitável, dada a escala da variável alvo. Mas obteremos algumas informações interessantes se compararmos os erros de diferentes modelos.

Funções de regressão linear e erro

Tarefa2 / 2

1.

Estime o valor de uma função de erro para um subconjunto do nosso exemplo. Vamos supor que você esteja construindo um modelo para prever uma variável com os seguintes valores:

`y = [28.1, 18.7, 1.0, 10.2, 11.6, 19.9, 24.4, 18.1, 18.5, 25.0, 21.8, 13.4, 18.0, 11.1, 21.1]`.

Você tem só uma característica e seus valores são:

`x = [12, 9, 4, 5, 5, 7, 7, 6, 7, 11, 9, 8, 10, 4, 11]`.

Você precisa estimar quão bem as funções `y = 2 + 2*x` e `y = 3 + 1*x` descrevem seus dados.

Faça previsões com as duas equações e imprima-as.

2.

-   Crie uma função para calcular o erro quadrático médio (a fórmula está no final da teoria desta lição).
    
-   Calcule os valores da função de erro para as duas previsões.
    
-   Imprima-as. Qual modelo é melhor?
    

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

import numpy as np

  

\# Crie os vetores x e y

x \= \[12, 9, 4, 5, 5, 7, 7, 6, 7, 11, 9, 8, 10, 4, 11\]

y \= \[28.1, 18.7, 1.0, 10.2, 11.6, 19.9, 24.4, 18.1, 18.5, 25.0, 21.8, 13.4, 18.0, 11.1, 21.1\]

  

y\_1 \= \[2 + 2\*i for i in x\] \# faça previsão com a primeira função

y\_2 \= \[3 + 1\*i for i in x\] \# faça previsão com a segunda função

  

def error\_function(y\_real, y\_pred):

q \= sum((np.array(y\_real) \- np.array(y\_pred)) \*\* 2 )/ len(y\_real)

return q

  

q\_1 \= error\_function(y, y\_1)

q\_2 \= error\_function(y, y\_2)

  

  

print('O erro do primeiro modelo:', q\_1)

print('O erro do segundo modelo:', q\_2)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-39-159Z.md
### Última modificação: 2025-05-28 20:27:39

# Gradiente Descendente - TripleTen

Capítulo 3/6

Algoritmos de Aprendizado de Máquina

# Gradiente Descendente

O erro que o modelo comete com um determinado conjunto de ponderações é definido pela fórmula:

Q(w)\=∑i\=1n(yi−yi^)2nQ(w) = \\frac{\\sum\_{i = 1}^{n}(y\_i - \\hat{y\_i})^2}{n}Q(w)\=n∑i\=1n​(yi​−yi​^​)2​

onde `y(x) = w₀ + w₁ * x₁ + w₂ * x₂ + ... + w_k * x_k`

Sua tarefa é encontrar a combinação de pesos que minimizará esse valor. Vamos voltar ao caso em que temos só uma característica e dois coeficientes para os quais precisamos escolher valores: `y = w₀ + w₁ * x`. Podemos calcular os valores da função de erro para uma variedade de valores `w₀` e `w₁`. Então o gráfico da função de erro ficará assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1580056806_1592662968.png)

Este é um **gráfico de três superfícies**. Vamos supor que você escolheu os valores dos dois coeficientes `w_0_a` e `w_1_a` e obteve o valor da função `Q_a`. Na imagem, isso corresponde ao ponto A. Precisamos encontrar o ponto no qual o valor da função será a menor. Vamos marcá-la como B. O ponto B corresponde ao conjunto de pesos para o qual o erro quadrático médio das observações é o menor. Se pensarmos no exemplo da lição anterior, podemos dizer que o ponto A corresponde à linha vermelha e o ponto B à verde.

Encontrar um mínimo local é a essência da otimização. Em teoria, você pode encontrá-lo com cálculos, mas isso leva muito tempo quando você tem muitas observações e características. No aprendizado de máquina, aplica-se um método especial de otimização: **gradiente descendente**. Escolhemos um ponto inicial (no nosso caso, ponto A) com valores iniciais de ponderação. Ele é deslocado passo a passo em direção ao mínimo da função de erro.

As questões cruciais neste método são:

-   Em que direção devemos começar?
-   Qual deve ser o tamanho desse primeiro passo?

Onde exatamente precisamos pisar? Faria sentido mover-se na direção onde a descida ao mínimo é a mais íngreme. Isso significa que precisamos nos mover na direção que fornece a maior diminuição no valor da função para um passo de uma determinada largura. Se estamos lidando com duas dimensões, o deslocamento é ao longo da tangente do ponto atual para baixo. Se fixarmos o valor do coeficiente zero `w_0`, então a fatia de superfície que descreve a dependência do erro `Q` na ponderação `w_1` dado uma característica que parecerá uma curva parabólica:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/13.3.3PT_1661437703.png)

Movemos do ponto inicial para baixo ao longo da tangente - se você estudou cálculo, saberá que esta é a derivada da função. Nesse caso, estamos nos movendo ao longo da derivada da função de erro em relação às ponderações (o eixo X):

∂Q∂w1\\frac{\\partial Q}{\\partial w\_1}∂w1​∂Q​

Se temos várias dimensões, a tangente à superfície é o vetor das derivadas para cada peso:

∇Q(w)\=(∂Q∂w0,...,∂Q∂wk)\\nabla Q (w) = (\\frac{\\partial Q}{\\partial w\_0}, ..., \\frac{\\partial Q}{\\partial w\_k})∇Q(w)\=(∂w0​∂Q​,...,∂wk​∂Q​)

É um vetor chamado **gradiente.** Ele pode ser representado como um conjunto de equações, cada uma das quais é a derivada da função de erro em relação a uma das variáveis, enquanto trata o restante delas como fixas (tais equações são chamadas derivadas parciais). Para cada ponto em nosso espaço multidimensional, podemos calcular todas essas derivadas. O vetor resultante será a direção mais "subida" em um determinado espaço. Multiplicado por -1, nos dá a direção "descendo" que precisamos.

Nós descobrimos a direção. Agora temos que descobrir como nos mover rapidamente: quão largos devem ser os passos para chegar ao "chão". A **taxa de aprendizado** é responsável precisamente por isso. O chamamos de `α`. Então podemos formular a correção de peso em cada etapa da seguinte forma:

w⃗t+1\=w⃗t−α∇Q(w⃗t)\\vec {w}\_{t+1} = \\vec{w}\_t - \\alpha\\nabla Q (\\vec{w}\_t)wt+1​\=wt​−α∇Q(wt​)

Para que o passo não seja muito longo, ou termos que pular sobre o mínimo. Mas ele não deve minúsculo também, ou o treinamento vai demorar muito. Normalmente, um **hiperparâmetro** é escolhido nos experimentos. Esse termo é destinado para distingui-lo dos parâmetros `w` da função. O valor inicial do hiperparâmetro é relativamente pequeno, por exemplo 0.001 ou 0.003. Aqui o passo (isto é o hiperparâmetro) é a **taxa de aprendizado**.

Na prática, o gradiente descendente básico ou "vanilla" é raramente usado, principalmente por causa das seguintes desvantagens:

-   Se a superfície da função tem muitos "furos", dependendo da seleção inicial de ponderação, nós devemos terminar em um **mínimo local**, ao invés de **mínimo global** das funções. Então vamos ter que executar pelo processo várias vezes, mudando as suposições iniciais.
-   Calcular o gradiente para todas as amostras leva muito tempo. Para acelerar a convergência e tentar prevenir superajuste, **gradiente aleatório (estocástico)** decrescente é aplicado. Aqui o gradiente é calculado para subconjuntos, ou **mini-lotes**, em vez de um conjunto inteiro.
-   A superfície da função de erro deve ser complexa. Nesses casos, nós aplicamos não apenas direções, mas outros derivados de uma função de erro.

Não é um material fácil, então aqui está um link útil que pode ajudar você a entender: [https://builtin.com/data-science/gradient-descent](https://builtin.com/data-science/gradient-descent) _(os materiais estão em inglês)_

Pergunta

O que é minimizado no processo de gradiente descendente para uma regressão linear?

A soma total do coeficiente angular

Os valores da função de erro para todas as observações

Isso mesmo! A função de erro pode parecer diferente daquela desta lição (embora esse tipo seja comum), mas isso não afeta o objetivo principal da descida do gradiente, que é encontrar o mínimo da função.

A soma média dos coeficientes angulares ao quadrado

O erro quadrático médio para todas as observações

Fantástico!

Pergunta

O que mudou em cada etapa da descida do gradiente vanilla descrita nesta lição?

A matriz de valores de característica X

Coeficientes angulares

Isso mesmo! A cada passo do gradiente descendente ajustamos levemente nossa função alterando o vetor de coeficientes (pesos) das características no modelo linear.

A função de erro

A taxa de aprendizagem

Você conseguiu!

Pergunta

O que fazemos com as observações quando realizamos a descida do gradiente de baunilha?

O gradiente é calculado para lotes (subconjuntos)

O gradiente é encontrado para apenas uma observação

O gradiente é calculado para todas as observações

Certo. Com a descida do gradiente baunilha, todo o conjunto de trens é usado para calcular o antigradiente.

Fantástico!

Pergunta

Quando calculamos o gradiente, diferenciamos (tiramos a derivada de) a função de erro em relação a quê?

Características

Pesos

Sim esta correto. Para entender como alterar os pesos, veremos como o valor da função de erro muda com pequenas alterações nos pesos (essa é a essência da diferenciação).

O valor real da variável de objetivo

Tempo

Trabalho maravilhoso!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-41-867Z.md
### Última modificação: 2025-05-28 20:27:42

# Pré-processamento: Características de Dimensionamento - TripleTen

Teoria

# Pré-processamento: Características de Dimensionamento

Quando você realizar uma regressão linear, tenha duas coisas em mente:

-   Características devem ser padronizadas
-   A regressão linear é propensa ao superajuste e é instável quando características são correlacionadas mutuamente

Nesta lição, consideraremos o primeiro problema. Vamos estudá-lo tentando prever a receita de um novo ponto de venda. Usaremos apenas três características para simplificar as coisas:

-   Distância do centro da cidade em quilômetros (uma variável numérica dentro do intervalo `[0.1;10]`)
-   Uma característica binária: se a saída é o **flagship** ou não (`0` — não é o flagship, `1` — flagship)
-   O tráfego médio de pedestres da rua (pessoas/dia) (uma variável numérica dentro do intervalo `[100;3000]`)

Os intervalos dos valores dessas características diferem significativamente. Se você começar selecionando coeficientes, verá que ponderações para diferentes características devem ser consideradas para a escala dessas características. Características de escala menor devem ter ponderações maiores, e outras devem ter ponderações menores. Ao mesmo tempo, os valores dos coeficientes devem tornar possível estimar a contribuição de cada característica para a soma total ponderada.

Para comparar as ponderações, nós devemos unificar a escala de várias características. Isso pode ser feito através da **normalização**, que envolve converter os valores de características ao intervalo de 0 a 1. Porém, escalabilidade é normalmente levada pela **padronização**, o que significa que nós fazemos a distribuição ter uma média de 0 e um desvio padrão de 1. (Lembrou? Essa é a **distribuição padrão normal**.) A razão para isso é que somente quando temos essa distribuição faz sentido minimizar o erro quadrado médio.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_sigma_1580057475_1592665162.jpg)

A padronização transforma o valor da característica para que ele fique assim: para cada observação, o valor inicial da característica é diminuída pela média, e a diferença resultante é dividida pelo desvio padrão. Na prática, você não tem necessariamente que saber a fórmula pela qual as características iniciais recebem valores desses intervalos. Entretanto, para resolver tarefas com métodos de regressão linear (por exemplo, isso é verdade para agrupamento também, que vamos discutir depois) os dados devem ser padronizados.

O módulo `pré-processamento` em `sklearn` as classes prontas `MinMaxScaler()` e `StandardScaler()` para normalização e padronização dos dados, respectivamente. Eles parecem modelos: primeiro eles precisam ser treinados, ou mostrar quais valores uma característica leva em dados de treino. Somente aí eles poderão ser aplicados a novas observações. Normalização e padronização têm a mesma sintaxe:

```
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler() # criando um objeto de classe escalonador
scaler.fit(X) # treinando um padronizador
X_sc = scaler.transform(X) # transformando o conjunto de dados
```

Em linha com o pipeline do aprendizado de máquina padronizado, você dividiu os dados em conjuntos de treino e validação. Para padronizar características, você tem que saber a sua média e desvio padrão para observações, mas você não pode prever valores de características. Você deve levar em conta esse fato quando fizer ao validar.

Já que o modelo vê apenas a média e desvio padrão enquanto está treinando nos dados de treinamento, a padronização é feita seguindo os seguintes passos:

-   Calcule a média e desvio padrão nos dados de treinamento (por treinamento `StandardScaler()`) e transforme os dados.
-   Aplique o treinado `StandardScaler()` para dados de validação (levando a média dos dados de treino e desvio padrão em conta).
-   Treine o modelo nos dados de treino padronizados.
-   Faça uma previsão para dados padronizados no conjunto de validação.

Vamos voltar ao nosso exemplo envolvendo a previsão da receita de pontos de venda futuros e ver como a padronização afeta a característica. Temos dados de 50 pontos de venda. Vamos construir histogramas de distribuição inicial das três características:

```
for feature in df.columns:
  sns.distplot(df[feature], bins = 10)
  plt.show()
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.4PT.png)

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.4.2PT.png)

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.4.3PT.png)

Vamos padronizar essas três características e mais uma vez olhar para os histogramas:

```
scaler = StandardScaler() # criando um objeto de classe escalonador (padronizador)
scaler.fit(df[['Distância do centro da cidade, km.',
               'Flagship',
               'Tráfego médio de pessoas/dia']]) # treinando um padronizador
X_sc = scaler.transform(df[['Distância do centro da cidade, km.',
               'Flagship',
               'Tráfego médio de pessoas/dia']]) # transformando o conjunto de dados
df_new = pd.DataFrame(data = X_sc, columns = [['Distância do centro da cidade, km. (stand.)', 'Flagship (stand.)', 'Tráfego médio de pedestres, pessoas/dia (stand.)']])
for feature in df_new.columns:
    sns.distplot(df_new[feature ], bins = 10)
    plt.title(feature)
    plt.show()
```

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.4.4PT.png)

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.4.5PT.png)

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.4.6PT.png)

A primeira e a terceira característica agora se parecem mais com um gráfico de distribuição padrão, enquanto a segunda característica manteve sua forma, mas agora assume dois novos valores binários.

Pré-processamento: Características de Dimensionamento

Tarefa

Voltemos ao Facebook. Sua tarefa é prever a atividade total nas contas oficiais das empresas com base nos parâmetros dessas páginas. Você treinará um modelo linear na próxima lição. Por enquanto,

-   Imprima a lista de características do conjunto de dados.
-   Divida a amostra em conjuntos de treinamento e validação usando `train_test_split()`.
-   Calcule a média e o desvio padrão para a característica `'Page total likes'` nos dados do trem.
-   Padronizar os conjuntos.
-   Imprima a média e o desvio padrão da característica `'Page total likes'` para os conjuntos de treinamento e teste. O que você vê? Como você pode contabilizá-lo?

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

import pandas as pd

import numpy as np

from sklearn.model\_selection import train\_test\_split

from sklearn.preprocessing import StandardScaler

  

\# leia os dados contendo os atributos das contas e atividades das empresas

fb \= pd.read\_csv('/datasets/dataset\_facebook\_cosmetics\_us.csv', sep \= ';')

  

\# divida os dados em características (a matriz X) e a variável objetivo (y)

X \= fb.drop('Total Interactions', axis \= 1)

y \= fb\['Total Interactions'\]

  

\# imprima os nomes das características do conjunto de dados

print(X.columns)

  

\# divida os dados sobre treinos e testes

X\_train, X\_test, y\_train, y\_test \= train\_test\_split(X, y, test\_size\=0.2)

  

\# imprima a média e o desvio padrão da característica 'Page total likes'

print('Média para trem', np.mean(X\_train\['Page total likes'\]))

print('Padrão para trem', np.std(X\_train\['Page total likes'\]))

  

  

\# padronize os dados usando o método StandardScaler()

scaler \= StandardScaler()

scaler.fit(X\_train)

X\_train\_st \= scaler.transform(X\_train)

X\_test\_st \= scaler.transform(X\_test)

  

print('Média para trem padronizado', np.mean(X\_train\_st\[:,0\]))

print('Padrão para trem padronizado', np.std(X\_train\_st\[:,0\]))

print('Média para teste padronizado', np.mean(X\_test\_st\[:,0\]))

print('Padrão para teste padronizado', np.std(X\_test\_st\[:,0\]))

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-43-221Z.md
### Última modificação: 2025-05-28 20:27:43

# Regularização - TripleTen

Capítulo 3/6

Algoritmos de Aprendizado de Máquina

# Regularização

Modelos lineares simples são fáceis de criar e interpretar, mas pode haver problemas com eles. Isso pode ser devido a **multicolinearidade**, por exemplo.

Multicolinearidade ocorre quando nós temos um grupo de características que são relacionadas linearmente (ou seja, independente ou tendo fortes correlações). Talvez duas características que correspondem a mesma distância tenham escapado para dentro do conjunto de dados. Uma está em metros, e a outra está em quilômetros. Ou as características podem estar fortemente relacionadas, como a temperatura da água e a temperatura do ar. Se a correlação do coeficiente for muito alta (maior do que 0.8, geralmente), você pode ter problemas com a regressão linear: **ponderações não representativas e um modelo superajustado para essas características**. Mas por quê?

Aqui está um exemplo ilustrativo. Nós temos duas características idênticas, o último caso de alta correlação mútua:

x1\=x2x\_1 = x\_2x1​\=x2​

E agora olhe para essas equações:

y\=w0+w1∗x1+w2∗x2y\=w0+(w1+w2)∗x1+0∗x2y\=w0+1000000∗(w1+w2)∗x1−999999∗(w1+w2)∗x2y = w\_0 + w\_1\*x\_1 + w\_2\*x\_2 \\newline y = w\_0 + (w\_1+w\_2)\*x\_1 + 0\*x\_2 \\newline y = w\_0 + 1000000\*(w\_1+w\_2)\*x\_1 - 999999\*(w\_1+w\_2)\*x\_2y\=w0​+w1​∗x1​+w2​∗x2​y\=w0​+(w1​+w2​)∗x1​+0∗x2​y\=w0​+1000000∗(w1​+w2​)∗x1​−999999∗(w1​+w2​)∗x2​

Elas têm o mesmo resultado! Os erros das previsões desses modelos com respeito aos valores reais, serão os mesmos também. O algoritmo não pode decidir qual desses três vetores é melhor. Talvez haja um número infinito de vetores equivalentes. Se o algoritmo os utiliza, ele vai falhar em dar uma estimativa adequada do impacto de uma característica individual tem sobre a variável objetivo.

Isso dificulta o treinamento de algoritmos lineares. Eles começam a mover o vetor de pesos, minimizando o erro e aumentando repetidamente o peso de uma das características idênticas. O peso da outra característica cresce menos. O algoritmo produzirá um vetor de pesos onde alguns pesos têm valores absolutos anormalmente altos e sinais diferentes. Interpretar esses pesos é inútil. Eles não estão relacionados à contribuição real de uma característica. Além disso, a multidirecionalidade das características correlacionadas não significa que eles tenham o impacto oposto - afinal são idênticas!

Como lidamos com isso? Mantendo aquelas características cujas correlações estão abaixo do limiar (0.8). Por exemplo, nós podemos calcular a matriz de correlação usando o método `corr()` e sempre que encontrarmos um par de características fortemente correlacionadas, nós removemos uma delas manualmente:

```
cm = df.corr() # calculando a matriz de correlação
```

```
         Feature_1    Feature_2    Feature_3    Feature_4
Feature_1    1.000000    0.938980    0.505419    -0.348746
Feature_2    0.938980    1.000000    0.547476    -0.377874
Feature_3    0.505419    0.547476    1.000000    -0.288137
Feature_4    -0.348746    -0.377874    -0.288137    1.000000
```

Como podemos ver, `Feature_1` e `Feature_2` têm uma forte correlação. Para se livrar da multicolinearidade, removeremos um deles do DataFrame:

```
df.drop('Feature_2', axis = 1, inplace = True) # removendo uma das duas características correlacionadas
```

Se você não quer características correlacionadas à mão, você pode realizar uma **regularização**. Em linhas gerais, isso se refere a qualquer limitação adicional no modelo, ou para uma ação que vai diminuir a complexidade e minimizar o impacto do superajuste. No caso de modelos lineares, regularização envolve um limite sobre ponderações.

Nós podemos definir a condição diretamente como se segue:

∑i\=1mwi2<b\\sum\_{i=1}^{m}{w\_i^2} < b∑i\=1m​wi2​<b

Uma limitação similar para as duas características e b=9 vai ser assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1580058279_1592665476.png)

Outra maneira de limitar a escala de ponderações é limitar seus valores absolutos:

∑i\=1m∣wi∣<a\\sum\_{i=1}^{m}{|w\_i|} < a∑i\=1m​∣wi​∣<a

Uma limitação parecida para a duas características e para a=3 vai ser assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1_1580058282_1592665525.png)

Essas limitações são frequentemente definidas diretamente através da função do erro. Dessa forma tentamos minimizar o erro com as menores ponderações de características (em termos de soma de valores absolutos ou soma de quadrados). Em outras palavras, nós proibimos o modelo de escolher altas ponderações de características para que ele não se calibre baseado em características multicolineares. Nós podemos criar limitações de duas maneiras.

**Regularização L1**, ou **Regressão Lasso** associadas a essa fórmula:

QLasso\=1n∗(∑i\=1n(yi−yi^)2+λ∗∣∣w∣∣1)Q\_{Lasso} = \\frac{1}{n}\*(\\sum\_{i=1}^{n}{(y\_i - \\hat{y\_i})^2 + \\lambda \* ||w||\_1})QLasso​\=n1​∗(∑i\=1n​(yi​−yi​^​)2+λ∗∣∣w∣∣1​)

onde QLasso eˊ a func¸a˜o de erro, yi eˊ o valor real da variaˊvel de destino para o objeto i, yi^ eˊ a previsa˜o para o objeto i,λ eˊ o coeficiente de regularizac¸a˜o e∣∣w∣∣1 eˊ a soma total dos valores absolutos dos pesos\\text{onde } Q\_{Lasso} \\text{ é a função de erro, } \\newline y\_i \\text{ é o valor real da variável de destino para o objeto i, } \\newline \\hat{y\_i} \\text{ é a previsão para o objeto i,} \\newline \\lambda \\text{ é o coeficiente de regularização e} \\newline ||w||\_1 \\text{ é a soma total dos valores absolutos dos pesos} onde QLasso​ eˊ a func¸​a˜o de erro, yi​ eˊ o valor real da variaˊvel de destino para o objeto i, yi​^​ eˊ a previsa˜o para o objeto i,λ eˊ o coeficiente de regularizac¸​a˜o e∣∣w∣∣1​ eˊ a soma total dos valores absolutos dos pesos

(Isto é, quando temos que minimizar o erro escolhendo ponderações ideais, nós também tentamos minimizar a **soma dos valores absolutos** dessas ponderações.)

**Regularização L2**, ou **Regressão Ridge**:

QRidge\=1n∗(∑i\=1n(yi−yi^)2+λ∗∣∣w∣∣22)Q\_{Ridge} = \\frac{1}{n}\*(\\sum\_{i=1}^{n}{(y\_i - \\hat{y\_i})^2 + \\lambda \* ||w||\_2^2})QRidge​\=n1​∗(∑i\=1n​(yi​−yi​^​)2+λ∗∣∣w∣∣22​)

onde QRidge eˊ a func¸a˜o de erro, yi eˊ o valor real da variaˊvel de destino para o objeto i, yi^ eˊ a previsa˜o para o objeto i,λeˊ o coeficiente de regularizac¸a˜o e∣∣w∣∣22 eˊ a soma total dos pesos quadrados\\text{onde } Q\_{Ridge} \\text{ é a função de erro, } \\newline y\_i \\text{ é o valor real da variável de destino para o objeto i, } \\newline \\hat{y\_i} \\text{ é a previsão para o objeto i,} \\newline \\lambda \\text{é o coeficiente de regularização e} \\newline ||w||\_2^2 \\text{ é a soma total dos pesos quadrados} onde QRidge​ eˊ a func¸​a˜o de erro, yi​ eˊ o valor real da variaˊvel de destino para o objeto i, yi​^​ eˊ a previsa˜o para o objeto i,λeˊ o coeficiente de regularizac¸​a˜o e∣∣w∣∣22​ eˊ a soma total dos pesos quadrados

(Isso é, enquanto minimizamos o erro escolhendo ponderações ideais, nós também tentamos minimizar a **soma dos quadrados** dessas ponderações.)

`sklearn` tem muitos algoritmos de regressão linear que já incluem regularizações:

-   **Regressão Lasso** usa regulação L1. Esse algoritmo zera todas as ponderações de características fortemente correlacionadas, exceto uma. Assim, esse algoritmo tem um mecanismo embutido para a **seleção de características**. Ele é usado quando precisamos diminuir a dimensionalidade e nos livrarmos de características duplicadas.
-   **Regressão Ridge** usa regularização L2. Nesse caso, as ponderações entre as características correlacionadas obtém uma distribuição quase igualitária.

Pergunta

O que é regularização?

Converter os valores das características para que correspondam à distribuição normal padrão

Limitando os valores das características

Limitando os pesos das características

Correto. A regularização coloca limites no tamanho das características enquanto treinamos um modelo para que eles não disparem devido à multicolinearidade.

Você conseguiu!

Pergunta

Você treinou um modelo de regressão linear com dois algoritmos envolvendo regularização e obteve as funções ideais com os seguintes vetores de peso: `w_a = [10.3, 5.7, 1.8, 1.8, 4.7, 5.9]` e `w_b = [10.3, 0, 1.8, 0, 4.7, 5.9]` (o primeiro coeficiente é o zero). Adivinhe quais algoritmos foram usados para obter `w_a` e `w_b`, respectivamente.

Lasso (regularização L1) e Ridge (regularização L2)

Ridge (regularização L2) e Lasso (regularização L1)

Certo. O algoritmo Lasso zera os coeficientes de algumas características correlacionadas, enquanto Ridge as distribui quase uniformemente.

Ridge (regularização L1) e Lasso (regularização L2)

Regressão linear padrão regular (LinearRegression) e Ridge (regularização L2)

Você conseguiu!

Pergunta

O valor absoluto máximo na matriz de correlações de características pareadas é 0.163. Precisamos de regularização?

Não

Correto. Poderíamos aplicar a regularização, mas não podemos dizer que é necessário.

Sim, a regularização é boa para qualquer modelo linear

Sim, mas a regularização L2 será melhor, para não zerar os coeficientes das características relevantes

Sim, o modelo sobreajustará caso contrário

Fantástico!

Pergunta

Qual dos algoritmos pode ser usado para seleção de recursos?

L2

Regressão linear padrão sem regularização

L1

Isso mesmo! Como a regressão Lasso zera os coeficientes de algumas das características dependentes, ela é frequentemente usada para seleção de características. Assim, podemos nos livrar de características supérfluas que não transmitem nenhuma informação adicional.

As características são selecionadas com outros algoritmos, a regressão linear não tem nada a ver com isso

Fantástico!

Pergunta

Em qual grupo as características provavelmente NÃO são multicolineares?

"A distância até a estação de metrô mais próxima, km", "A distância até a estação de metrô mais próxima, m", "A distância até a estação de metrô mais próxima, milhas"

"Área total do apartamento, pés quadrados", "Área total da cozinha, pés quadrados", "Área total do apartamento fora da cozinha, pés quadrados"

"A proporção de portadores de ingressos que são homens", "A parcela de portadores de ingressos que são mulheres"

"O número de homens que compraram ingressos para shows", "O número de mulheres que compraram ingressos para shows"

Esta opção difere da anterior. Se não soubermos o número total de pessoas que compraram ingressos para shows, não podemos dizer o número exato de mulheres que compraram, mesmo que saibamos o número de homens. A menos que adicionemos um terceiro recurso, esses recursos não podem ser considerados multicolineares.

Fantástico!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-44-541Z.md
### Última modificação: 2025-05-28 20:27:45

# Implementação de Modelos Lineares - TripleTen

Teoria

# Implementação de Modelos Lineares

O `sklearn` tem o módulo `linear_model` contendo algoritmos de regressão linear.

-   `linear_model.LinearRegression()`: o modelo para regressão linear padrão. Antes de aplicá-lo, você precisa normalizar os dados e filtrar características à mão para evitar multicolinearidade.
-   `linear_model.Lasso()`: o modelo de regressão linear com regularização de ponderação L1 embutida (limitando a soma totl de valores absolutos das ponderações).
    
    Isso também requer dados normalizados, mas aqui a regularização embutida faz toda a filtragem para você. Você não precisa se livrar das características fortemente correlacionadas por si só.
    
-   `linear_model.Ridge()`: um modelo de regressão linear com regularização de ponderação L2 embutida (limitando a soma total das ponderações quadradas).
    
    Aqui também você tem dados normalizados, como as escalas das características devem ser unificadas. Com regressão Ridge, assim como a Lasso, você não precisa se livrar das características correlacionadas. A regressão Ridge sozinha não vai filtrar características para você, já que ela não zera os valores das características duplicadas. A função de regularização é projetada para distribuir ponderações mesmo entre características similares.
    

Você pode estudar a sintaxe de cada uma das funções no documento oficial:

[https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear\_model](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) _(os materiais estão em inglês)_

Com tarefas de regressão linear, nós treinamos um modelo e fazemos predições usando os métodos padrão `fit()` e `predict()`. Por exemplo, para a regressão Ridge, o código é assim:

```
from sklearn.linear_model import Ridge

model = Ridge() # criando um modelo da classe de regressão Ridge
model.fit(X_train, y_train)
y_pred = model.predict(X_val)
```

Algoritmos de regressão linear diferem no sentido que eles permitem que interpretemos ponderações para cada característica. Depois da padronização, os valores de coeficiente das características refletem o impacto de cada uma delas na predição final. Quanto mais alto o valor absoluto do coeficiente, maior o impacto que ele tem. Tendo treinado o modelo, você será capaz de emitir diretamente todos os coeficientes, que correspondem à função final ideal. As ponderações das características são impressas pelas médias do método do modelo `.coef_`:

```
print(model.coef_)
```

...e o valor do coeficiente zero, ou intercepto, é impresso com o método `.intercept_`.

```
print(model.intercept_)
```

Implementação de Modelos Lineares

Tarefa2 / 2

1.

Vamos voltar à nossa tarefa do Facebook. Aplique um algoritmo linear a ele. Declare seu modelo como um elemento de classe Lasso (um modelo linear com a regularização L1) de `sklearn.linear_model` . Treine o modelo e faça uma previsão para os dados de validação.

2.

Aproveite uma característica importante dos modelos lineares e dê uma olhada nos coeficientes que correspondem à função ideal. Crie um DataFrame com as características e imprima-o, classificando-o pelos valores absolutos dos coeficientes em ordem decrescente. Coloque as características na coluna `feature` e os valores dos coeficientes na coluna `coeff`. Os valores absolutos dos coeficientes devem ficar na coluna `coeff_abs`.

Quais características chegaram ao topo?

Veja todas as características do DataFrame. Quais características dos coeficientes agora são zero como resultado do processamento Lasso?

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

import pandas as pd

from sklearn.model\_selection import train\_test\_split

from sklearn.linear\_model import Lasso

from sklearn.preprocessing import StandardScaler

  

\# leia os dados sobre as contas e as atividades das empresas

fb \= pd.read\_csv('/datasets/dataset\_facebook\_cosmetics\_us.csv', sep \= ';')

  

\# divida os dados em características (a matriz X) e a variável objetivo (y)

X \= fb.drop('Total Interactions', axis \= 1)

y \= fb\['Total Interactions'\]

  

\# divida os dados sobre treinos e testes

X\_train, X\_test, y\_train, y\_test \= train\_test\_split(X, y, test\_size\=0.2, random\_state\=0)

  

\# padronize os dados usando o método StandardScaler

scaler \= StandardScaler()

scaler.fit(X\_train)

X\_train\_st \= scaler.transform(X\_train)

X\_test\_st \= scaler.transform(X\_test)

  

\# defina o algoritmo do modelo

model \= Lasso()

  

\# treine seu modelo

model.fit(X\_train\_st, y\_train)

  

\# use o modelo treinado para fazer uma previsão

predictions \= model.predict(X\_test\_st)

  

\# crie um DataFrame com características e seus pesos

features \= pd.DataFrame({'feature': X\_train.columns, 'coeff': model.coef\_})

features\['coeff\_abs'\] \= \[abs(i) for i in features\['coeff'\]\]

  

\# imprima o DataFrame com características ordenadas pelos valores absolutos dos coeficientes

print(features.sort\_values('coeff\_abs', ascending \= False))

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-46-414Z.md
### Última modificação: 2025-05-28 20:27:46

# Métricas de Regressão - TripleTen

Teoria

# Métricas de Regressão

Para os negócios, a parte mais importante do processo de desenvolvimento de um modelo por meio do aprendizado de máquina é avaliar sua qualidade. Não importa o quão sofisticado seja seu algoritmo, se ele não tiver métricas aceitáveis, você não poderá usá-lo para tarefas reais.

Outro fator significativo é quão bem as métricas matemáticas estão de acordo com as métricas da empresa. Ao treinar um modelo de aprendizado de máquina, você precisa entender o processo de negócios exato que ele precisa refletir e os resultados que deseja alcançar. O processo que você está modelando determina quais métricas você deve considerar em primeiro lugar e o que pode ser estudado posteriormente.

As métricas também são estudadas em dados de validação para verificar se o modelo funciona bem em dados que não foram vistos no treinamento. Isso é o que acontece em situações da vida real: você aplica o modelo quando as respostas são conhecidas.

Todas as fórmulas para calcular métricas dadas aqui são para validação dos dados.

## Métricas de Regressão

O objetivo final da tarefa de regressão é chegar o mais próximo possível ao valor real.

Deixe:

yi ser o valor real da variaˊvel objetivo para a observac¸a˜o i,yi^ ser a previsa˜o para observac¸a˜o i, en ser o nuˊmero de observac¸o˜es no conjunto de validac¸a˜o.y\_i \\text{ ser o valor real da variável objetivo para a observação i,} \\newline \\hat{y\_i} \\text{ ser a previsão para observação i, e} \\newline n \\text{ ser o número de observações no conjunto de validação.}yi​ ser o valor real da variaˊvel objetivo para a observac¸​a˜o i,yi​^​ ser a previsa˜o para observac¸​a˜o i, en ser o nuˊmero de observac¸​o˜es no conjunto de validac¸​a˜o.

Vamos olhar para a métrica mais popular nos negócios:

-   Erro absoluto médio, **EAM**
-   Erro quadrado médio, **EQM**, e nessa linha, **REQM**
-   Coeficiente de determinação, ou R-quadrado (**R2**)
-   Erro percentual absoluto médio, **EPAM**

### Erro absoluto médio (EAM)

O nome dessa métrica revela como é calculada. Primeiro nós estimamos estender nossas previsões desviadas dos valores reais da variável objetivo (usando valores absolutos). Depois encontramos o valor absoluto do erro médio para todas as observações no conjunto de validação. Nós pegamos os valores absolutos da diferença, então não importa se o desvio é positivo ou negativo:

EAM\=1n∗∑i\=1n∣yi−yi^∣EAM = \\frac{1}{n} \* \\sum\_{i=1}^{n}{|y\_i - \\hat{y\_i}}|EAM\=n1​∗∑i\=1n​∣yi​−yi​^​∣

### Erro quadrático médio (EQM) e raiz do erro quadrático médio (REQM)

Nós podemos também fazer a métrica refletir a sensitividade do modelo para valores atípicos. Se o modelo tem erros maiores, nós temos que fazer isso amplificar esse erro ainda mais. Depois nós precisamos encontrar o erro quadrático médio:

EQM\=1n∗∑i\=1n(yi−yi^)2EQM = \\frac{1}{n} \* \\sum\_{i=1}^{n}{(y\_i - \\hat{y\_i}})^2EQM\=n1​∗∑i\=1n​(yi​−yi​^​)2

... ou sua raiz:

REQM\=1n∗∑i\=1n(yi−yi^)2REQM = \\sqrt{\\frac{1}{n} \* \\sum\_{i=1}^{n}{(y\_i - \\hat{y\_i}})^2}REQM\=n1​∗∑i\=1n​(yi​−yi​^​)2​

### Coeficiente de determinação, ou R-quadrado (R2)

Você já está familiarizado com essa métrica. Ela mostra a parcela da variação da variável objetivo que o modelo considera. A fórmula do coeficiente de determinação é:

R2\=1−∑i\=1n(yi−yi^)2∑i\=1n(yˉ−yi)2R^2 = 1 - \\frac{\\sum\_{i=1}^{n}{(y\_i - \\hat{y\_i}})^2}{\\sum\_{i=1}^{n}{(\\bar{y} - y\_i})^2}R2\=1−∑i\=1n​(yˉ​−yi​)2∑i\=1n​(yi​−yi​^​)2​

onde:

yˉ\=1n∗∑i\=1nyi\\bar{y} = \\frac{1}{n} \* \\sum\_{i=1}^{n}{y\_i}yˉ​\=n1​∗∑i\=1n​yi​

O coeficiente de determinação leva valores de 0 a 1. Quanto mais próximo a 1, mais próxima sua previsão está de combinar com valores de variáveis objetivo reais.

### Erro percentual absoluto médio (EPAM)

Conhecendo o erro como porcentagem pode ser mais que o conhecendo como um valor absoluto (tal como EAM, EQM, e REQM). Vamos dizer `MAE=42` — é muito ou pouco? Se a média do seu valor é 56, o desvio de 42 sugere que o seu modelo não funciona. Mas se o intervalo da variável objetivo está entre 3000 e 4000, 42 é um resultado ideal (quase suspeitosamente bom). Erro percentual absoluto médio é encontrado com a fórmula:

EPAM\=100%∗1n∗∑i\=1n∣yi−yi^∣∣yi∣EPAM = 100\\% \* \\frac{1}{n} \* \\sum\_{i=1}^{n}{\\frac{|y\_i - \\hat{y\_i}|}{|y\_i|}}EPAM\=100%∗n1​∗∑i\=1n​∣yi​∣∣yi​−yi​^​∣​

EPAM é intuitivamente claro. Um erro percentual absoluto médio de 2% nos deixa otimistas.

Mas se for 95% você deve retrabalhar o seu modelo, procurar por um erro nos seus cálculos, ou mesmo encontrar dados novos. Mas ainda esse sentimento de incerteza não desaparece: em alguns casos acurácia de 80% é o suficiente, mas em outros mesmo 95% será pouco. Você aprenderá mais sobre a conformidade entre métricas e metas de negócios no terceiro capítulo desta seção.

## Métricas e a função de erro

Quando procuramos pela função ideal, todos os algoritmos das bibliotecas padrão do Python minimizam a função de prejuízo conforme selecionam as ponderações. Essa função frequentemente coincide com a métrica que você está estimando no conjunto de validação. Por exemplo, muitos algoritmos consideram EQM como função de prejuízo por definição. Mas às vezes a função padrão e métrica pretendida não coincidem. Depois o algoritmo vai ajustar "ajustar" para coisas que você não precisa.

Em quase todas as implementações de algoritmos, você pode especificar a função para ser otimizado corretamente na etapa de treinamento. Se você quer comparar a função de prejuízo com a métrica que pretende para estimar com os dados de validação, leia a documentação.

## Calculando métricas no sklearn

Desenvolvedores do `sklearn` criaram o módulo `metrics`, que te permite criar várias métricas:

-   EAM — `metrics.mean_absolute_error`
-   EQM — `metrics.mean_squared_error`
-   R2 — `metrics.r2_score`

As métricas REQM e EPAM não são parte desse módulo, mas você mesmo(a) pode escrever as para eles, se necessário.

Vamos estudar a sintaxe para trabalhar com métodos de métrica olhando para essa função `metrics.mean_absolute_error`:

```
from sklearn.metrics import mean_absolute_error

print('EAM:', mean_absolute_error(y_true, y_pred))
```

Nós passamos o vetor de valores das variáveis objetivos verdadeiros (`y_true`) e o vetor de valores preditos (`y_pred`) como parâmetros. As métrica `metrics.mean_squared_error` e `metrics.r2_score` são calculadas de maneira análoga.

Métricas de Regressão

Tarefa

Você preparou os dados, treinou seu modelo linear no conjunto de trens e fez previsões para as observações do conjunto de validação. Agora estime EAM, EQM e R2 para os dados de validação.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

import pandas as pd

from sklearn.model\_selection import train\_test\_split

from sklearn.linear\_model import Lasso

from sklearn.preprocessing import StandardScaler

from sklearn.metrics import mean\_absolute\_error, mean\_squared\_error, r2\_score

  

\# leia os dados sobre as contas e as atividades das empresas

fb \= pd.read\_csv('/datasets/dataset\_facebook\_cosmetics\_us.csv', sep \= ';')

  

\# divida os dados em características (a matriz X) e a variável objetivo (y)

X \= fb.drop('Total Interactions', axis \= 1)

y \= fb\['Total Interactions'\]

  

\# divida os dados sobre treinos e testes

X\_train, X\_test, y\_train, y\_test \= train\_test\_split(X, y, test\_size\=0.2, random\_state\=0)

  

\# padronize os dados usando o método StandardScaler

scaler \= StandardScaler()

scaler.fit(X\_train)

X\_train\_st \= scaler.transform(X\_train)

X\_test\_st \= scaler.transform(X\_test)

  

\# defina o algoritmo do modelo

model \= Lasso()

  

\# treine seu modelo

model.fit(X\_train\_st, y\_train)

  

\# use o modelo treinado para fazer uma previsão

predictions \= model.predict(X\_test\_st)

  

\# imprima o valor médio da variável de destino no conjunto de teste

print('Médio: {:.2f}'.format(y\_test.mean()))

  

\# imprima as principais métricas

print('EAM: {:.2f}'.format(mean\_absolute\_error(y\_test, predictions)))

print('EQM: {:.2f}'.format(mean\_squared\_error(y\_test, predictions)))

print('R2: {:.2f}'.format(r2\_score(y\_test, predictions)))

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-47-726Z.md
### Última modificação: 2025-05-28 20:27:48

# Regressão logística - TripleTen

Teoria

# Regressão logística

Esse é um algoritmo para resolver tarefas de classificação binária. Assim: "regressão" para "classificação". Algumas pessoas acham essa combinação de termos confusa, mas vamos explicar.

Lembre-se que classificação binária é um exemplo de classificação onde há apenas duas classes: `"0"` ou `"1"`. A variável objetivo aqui é chamada de **variável binária.**

Modelos treinados com algoritmos de classificação binária não apenas predizem o valor final de uma classe de um determinado objeto ou cliente, mas também estimam a probabilidade do evento em questão. Por exemplo:

-   a probabilidade de um cliente pagar um empréstimo
-   a probabilidade de um prejuízo coberto
-   a probabilidade de um cliente deixar de usar seu produto (churn)
-   a probabilidade de um paciente ter uma determinada doença (diagnóstico)
-   a probabilidade de que uma transação tenha sido uma fraude

Um algoritmo comum para resolver tais tarefas é **regressão lógica**. É implementado como a classe `LogisticRegression()` dentro do módulo `linear_model` de `sklearn`, ao longo do algoritmo de regressão linear:

```
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
```

Vamos olhar para a teoria atrás desse algoritmo. Sua função deve retornar a probabilidade de um certo evento acontecer. Vamos assumir que ela encontrou com a seguinte fórmula:

f(z)\=P{y\=1∣x}f(z) = \\displaystyle \\mathbb {P} \\{y=1\\mid x\\}f(z)\=P{y\=1∣x}

Essa equação significa que a probabilidade que _y_ seja igual a 1, dado que as características levam os valores _x_, é determinado pela função _f_ aplicada a variável _z_.

Aqui _z_ é uma função linear dos valores das características (o vetor _x_), como na regressão linear:

z(x)\=w0+w1∗x1+w2∗x2+...+wn∗xnz(x) = w\_0 + w\_1\*x\_1 + w\_2 \* x\_2 + ... + w\_n\*x\_nz(x)\=w0​+w1​∗x1​+w2​∗x2​+...+wn​∗xn​

Na regressão logística, nós assumimos que _f_ é uma **função logística.** Ela será da seguinte maneira:

f(z)\=11+e−z(x)f(z) = \\frac{1}{1+ e ^{-z(x)}}f(z)\=1+e−z(x)1​

O gráfico da função é assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/AD_1_1661430850.png)

O gráfico da função logística é chamado de **curva sigmoide**. O gráfico mostra claramente que a função tem valores de 0 a 1. A forma e tolerância do intervalo sigmoide é muito bom para representar a probabilidade de ocorrência de alguns eventos. Essa linha "se curva" para valores infinitamente ideais e infinitamente pequenos _z_. Se a soma ponderada tem valores altos, para o qual a probabilidade do evento ocorrer é, digamos, 0.95, além de aumentar nessas características, não fará essa alta probabilidade ser significativamente maior. Conforme as características aumentam, ela se aproxima de 1. Para pequenas probabilidades, ela se aproxima de 0.

Depois um algoritmo (por exemplo, para gradiente descendente) seleciona os coeficientes _w_ para a função _z(x)_, para que _f(z)_ possa aceitar novamente respostas que estão mais perto dos valores reais da variável objetivo.

De fato, quando estamos trabalhando em uma tarefa de classificação, nós transformamos regressão linear para encontrar a probabilidade de que um evento entre a classe 1. Isso porque logística assim como regressões lineares (e suas variações Lasso e Ridge) pertencem a modelos lineares. Ele pode ser encontrado no mesmo módulo `linear_model` da biblioteca sklearn.

Para fazer a predição para uma classe depois de treinar esse modelo você vai precisar do método `predict()`:

```
y_pred = model.predict(X_test)
```

E para obter a probabilidade de que um objeto insira a primeira ou segunda classes, use o método `predict_proba()`:

```
y_probas = model.predict_proba(X_test)
```

`y_probas` é um vetor bidimensional em que cada objeto do conjunto de validação tem dois valores correspondentes: a probabilidade (ou melhor, confiança) de pertencer à classe 0 ("o evento não vai ocorrer") e a probabilidade de pertencer à classe 1 ("o evento vai ocorrer"). A soma desses dois valores é 1 para cada um dos objetos.

Regressão logística

Tarefa2 / 2

1.

Preveja a estabilidade do desempenho da rede elétrica. Você recebeu o arquivo:

`"/datasets/Electrical_Grid_Stability_us.csv"`.

As características não são específicas: você não pode dizer o que eles significam pelos nomes. A variável de destino `'stability'` representa a estabilidade da grade.

`"1"` significa que a grade é estável e `"0"` significa que não é.

Antes de construir um modelo:

-   Use a função `read_csv()` para ler o arquivo csv (pegue `';'` como separador de dados, escreva-o no parâmetro da função `'sep'` e armazene os dados como o DataFrame `electrical_grid`.
-   Criar um DataFrame correspondente a uma matriz objeto-característica e um vetor de variável alvo (tipo Série).
-   Divida os dados em conjuntos de treinamento e validação com uma proporção de 80% a 20%.

2.

-   Introduza a variável de `model` da classe `LogisticRegression()` de `sklearn.linear_model`
-   Treine seu modelo no conjunto de trem
-   Use seu modelo para fazer uma previsão binária para os objetos do conjunto de validação e armazene-o em `predictions`
-   **Estime a probabilidade de que um objeto** do conjunto de validação **pertença à segunda classe** (classe `"1"` das duas classes `"0"` e `"1"`) e armazene-o em `probabilities`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

import pandas as pd

  

from sklearn.model\_selection import train\_test\_split

  

from sklearn.linear\_model import LogisticRegression

  

\# leia os dados nos parâmetros da grade e sua estabilidade do arquivo csv

electrical\_grid \= pd.read\_csv('/datasets/Electrical\_Grid\_Stability\_us.csv', sep \= ';')

print('Tamanho do conjunto de dados:', electrical\_grid.shape)

electrical\_grid.head()

  

\# divida os dados em características (a matriz X) e a variável objetivo (y)

X \= electrical\_grid.drop('stability', axis \= 1)

y \= electrical\_grid\['stability'\]

  

\# divida os dados em conjuntos de treino e de teste

X\_train, X\_test, y\_train, y\_test \= train\_test\_split(X, y, test\_size\=0.2, random\_state\=0)

  

\# defina o algoritmo do modelo

model \= LogisticRegression()

  

\# treine seu modelo

model.fit(X\_train, y\_train)

  

\# use o modelo treinado para fazer previsões

predictions \= model.predict(X\_test)

probabilities \= model.predict\_proba(X\_test)\[:,1\]

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-49-051Z.md
### Última modificação: 2025-05-28 20:27:49

# Métricas de Classificação: Trabalhando com Legendas - TripleTen

Teoria

# Métricas de Classificação: Trabalhando com Legendas

Na lição anterior, você treinou um modelo de classificação binária. Agora é o momento perfeito para aprender a avaliar a qualidade de tais modelos. A classificação, assim como a regressão, faz parte do aprendizado supervisionado. Lembre-se, isso significa que você tem as respostas corretas para avaliar o modelo no conjunto de validação e pode comparar os valores previstos com os valores reais.

## Métricas de classificação com base na classe prevista

Vamos começar com métricas que levam em conta apenas o valor final predito (0 ou 1):

-   **Matriz de Confusão**
-   **Acurácia**
-   **Precisão** e **sensibilidade**
-   **F1\_score**

### **Matriz de confusão**

As possíveis classes de valores são 0 e 1, e o seu modelo pode emitir a predição final na forma de um desses valores. Dessa forma, a predição para cada projeto se insere em um dos quatro grupos:

-   Predição do modelo = 1, valor real = 1. Essas predições são chamadas de **Positivo Verdadeiro**, ou **PV**.
-   Predição do modelo = 1, valor real = 0. Essas predições são chamadas **Positivo Falso**, ou **PF**.
-   Predição do modelo = 0, valor real = 1. Essas predições são chamadas de **Negativo Falso**, ou **NF**.
-   Predição do modelo = 0, valor real = 0. Essas predições são chamadas de **Negativo Verdadeiro,** ou **NV**.

Se um modelo é bom, a maioria das suas predições se inserem em grupos PV e NV.

A matriz de confusão mostra o número de observações para cada grupo. Se parece com isso:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.9PT.png)

A matriz de confusão é calculada no módulo `metrics` da sklearn. Você pode definir as variáveis NV, PF, NF e PV, também:

```
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_true,y_pred)
NV, PF, NF, PV = cm.ravel() # "equilibrando" a matriz para obter os valores pretendidos
```

A função `confusion_matrix()` retorna uma matriz quadrada igual ao número de classes. O método `.ravel()` nos permite transformar a matriz em uma tupla dinâmica (unidimensional) `(NV, PF, NF, PV)`. Como podemos ver no código acima, podemos atribuir simultaneamente os valores tuplas a variáveis individuais.

Você pode dizer a partir da matriz de confusão quais erros o seu algoritmo tende fazer. Talvez ele tenha preconceito em relação à classe positiva (otimismo excessivo, muitas PFs), ou ele pode ser um pessimista e amplie o grupo NF para ser o lado seguro.

Os termos que declaramos para a matriz de confusão são verdadeiros para a métrica remanescente da previsão de classe.

### Acurácia

Acuraˊcia\=PV+TNn\=TP+NVPV+NV+PF+NFAcurácia = \\frac {PV+TN}{n} = \\frac {TP+NV}{PV+NV+PF+NF}Acuraˊcia\=nPV+TN​\=PV+NV+PF+NFTP+NV​

Essa a parte das predições precisas entre todas as predições. Quanto mais perto que estivermos de 100% de acurácia, melhor.

Essa métrica pode ser calculada com a função `accuracy_score` no módulo `metrics`. A função leva valores de classe atuais e preditos para os dados de validação como entrada:

```
acc = accuracy_score(y_true, y_pred)
```

Acurácia apenas funciona quando **as classes estão equilibradas**, ou seja, quando objetos são distribuídos quase uniformemente entre as classes, aproximadamente 50/50. Aqui tem o porquê: Digamos que as classes não estejam balanceadas: apenas 1% dos objetos estão na classe `1` e o restante está em `0`. Se for esse o caso, você não precisará de um algoritmo; você pode apenas manter a lógica "sempre prever `0` ." Legal, né? Isso dará 99% de precisão. Não há realmente necessidade de aprendizado de máquina aqui.

Mas estamos hiperbolizando, é claro. Em casos como esse, você deveria usar outras métricas para avaliar seu modelo.

### Precisão e sensibilidade

Para avaliar o modelo sem olhar para o equilíbrio das classes, você pode calcular a métrica:

Precisa~o\=PVPV+PFPrecisão = \\frac {PV}{PV+PF}Precisa~o\=PV+PFPV​

Sensibilidade\=PVPV+NFSensibilidade = \\frac {PV}{PV+NF}Sensibilidade\=PV+NFPV​

Precisão nos diz qual parte das predições na classe `1` são verdade. Em outras palavras, nós olhamos para **a parcela de respostas corretas apenas na classe objetivo**. Nos negócios, essa métrica é necessária apenas quando cada **alerta** de modelo (por exemplo quando um objeto é colocado na classe `1`) custa recursos. E você obviamente não quer que o modelo alerte por qualquer razão. Por exemplo, se você decidir dar empréstimo ou não, não pagar um reembolso é menos desejável do que não dar empréstimo.

A segunda métrica visa minimizar os riscos opostos. A sensibilidade demonstra **o número de objetos de classes reais `1` que você é capaz de descobrir** com seu modelo. Essa métrica é útil no diagnóstico de doenças: é melhor pedir a um paciente que repita o exame do que perder uma doença real.

Cada métrica leva valores de 0 a 1. Quanto mais perto de 1, melhor. Porém, quando você está ajustando os parâmetros do modelo (normalmente o limite da probabilidade além do qual colocamos um objeto dentro da classe `1`), optimizando uma métrica pode frequentemente levar a deterioração da outra. Por essa razão, você deve manter o equilíbrio entre essas duas métricas conforme você ajusta o modelo. Seu produto final deve ser baseado nos objetos do seu trabalho.

A precisão e métrica de sensibilidade são geradas pelas funçoes `precision_score` e `recall_score` no módulo `metrics`. Sua sintaxe é similar a de outras métricas:

```
precision = precision_score (y_true, y_pred)
recall = recall_score (y_true, y_pred)
```

Ambas as funções retornam valores numéricos de 0 a 1.

### Valor F1

Já que a precisão e sensibilidade são destinadas a evitar riscos opostos, nós precisamos de uma métrica de harmonização que leva em conta o equilíbrio entre as métricas. Essa métrica é **valor F1**:

F1\=2∗precisa~o∗sensibilidadeprecisa~o+sensibilidadeF1 = \\frac {2\*precisão \* sensibilidade}{precisão + sensibilidade}F1\=precisa~o+sensibilidade2∗precisa~o∗sensibilidade​

Em `sklearn.metrics` o valor F1 é encontrado através do método `f1_score`:

```
f1= f1_score(y_true, y_pred)
```

A função também retorna um valor entre 0 e 1. Quanto mais perto de 1, melhor.

Métricas de Classificação: Trabalhando com Legendas

Tarefa

Calcule as métricas que foram estudadas e imprima-as. Tente interpretá-los do ponto de vista do negócio.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

import pandas as pd

  

from sklearn.model\_selection import train\_test\_split

from sklearn.linear\_model import LogisticRegression

from sklearn.preprocessing import StandardScaler

from sklearn.metrics import accuracy\_score, precision\_score, recall\_score, f1\_score

  

\# leia os dados nos parâmetros da grade e sua estabilidade do arquivo csv

electrical\_grid \= pd.read\_csv('/datasets/Electrical\_Grid\_Stability\_us.csv', sep \= ';')

print('Tamanho do conjunto de dados:', electrical\_grid.shape)

electrical\_grid.head()

  

\# veja a proporção entre as classes do conjunto de dados

print('A proporção entre as classes:\\n', electrical\_grid\['stability'\].value\_counts())

  

\# divida os dados em características (a matriz X) e a variável objetivo (y)

X \= electrical\_grid.drop('stability', axis \= 1)

y \= electrical\_grid\['stability'\]

  

\# divida os dados em conjuntos de treino e de teste

X\_train, X\_test, y\_train, y\_test \= train\_test\_split(X, y, test\_size\=0.2, random\_state\=0)

  

\# defina o algoritmo do modelo

model \= LogisticRegression()

  

\# treine seu modelo

model.fit(X\_train, y\_train)

  

\# use o modelo treinado para fazer previsões

probabilities \= model.predict\_proba(X\_test)\[:,1\]

  

\# previsão binária

predictions \= model.predict(X\_test)

  

\# imprima as métricas estudadas para a previsão resultante

print('Acurácia: {:.2f}'.format(accuracy\_score(y\_test, predictions)))

print('Precisão: {:.2f}'.format(precision\_score(y\_test, predictions)))

print('Sensibilidade: {:.2f}'.format(recall\_score(y\_test, predictions)))

print('F1: {:.2f}'.format(f1\_score(y\_test, predictions)))

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-51-339Z.md
### Última modificação: 2025-05-28 20:27:51

# Métricas de Classificação: Estudando Probabilidades - TripleTen

Teoria

# Métricas de Classificação: Estudando Probabilidades

Até agora, você estudou métricas com base nas legendas de previsão finais `0` ou `1`.

Mas além da resposta para cada observação (`0` ou `1`), também podemos calcular a probabilidade de uma observação pertencer a uma das classes (`0` ou `1`). Por exemplo, um modelo estudando fotos de pessoas pode identificar se homens ou mulheres são retratados. Ou pode dizer: "A probabilidade de que esta seja uma foto de um homem é de 90%".

Isso porque usamos o valor **roc\_auc**, ou a **área sob a curva característica operacional do receptor**, para avaliar a qualidade do classificador (um modelo de classificação). **AUC-ROC** apoia a Área Sob a Curva Característica Operacional do Receptor.

Essa métrica segue a seguinte lógica: "Se ordenarmos objetos pela probabilidade predita pelo modelo, vamos ver uma clara distinção entre as classes reais desses objetos?" Em outras palavras, os objetos da classe `0` serão concentrados em uma extremidade e os objetos da classe `1` na outra? Se as probabilidades ordenam mal os nossos objetos, outras métricas não vão melhorar, não importa qual limiar você selecione. Mas quando os objetos são bem ordenados, nós podemos concluir que o modelo funciona adequadamente e, em seguida, vai definir o limiar.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.10PT.png)

Essa característica do AUC-ROC faz essa métrica ser uma grande ferramenta, mesmo para trabalhar com classes desbalanceadas.

Vamos dar uma olhada mais de perto em como isso funciona. Vamos declarar os termos **Taxa de Positivos Verdadeiros** (TPV corresponde a sensibilidade) e **Taxa de Positivos Falsos** (TPF é para esses objetos que o algoritmo incorretamente colocou dentro da classe `1` ao invés da `0`):

TPV\=TPTP+FNTPV = \\frac {TP}{TP+FN}TPV\=TP+FNTP​

TPF\=FPFP+TNTPF = \\frac {FP}{FP+TN}TPF\=FP+TNFP​

Vamos ordenar os objetos em ordem crescente por probabilidade predita e iterativamente seleciona o limiar para colocação dentro das classes. Vamos parar em cada observação para calcular TPV e TPF para os limiares dados. Assim é como os objetos em um modelo perfeito devem ser:

Mas na vida real, nem os dados nem os modelos são perfeitos. Nunca vamos ser capazes de alcançar ordenamento perfeito pela probabilidade. Então na maioria dos casos a curva ROC é da seguinte forma:

![](https://practicum-content.s3.us-west-1.amazonaws.com:443/resources/13.3.10.2PT_1669966793.png)

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.10.3PT.png)

Quanto mais alto a linha se curvar na direção do canto superior esquerdo, ou seja, quanto mais próxima estiver de 1, melhor. O mais perto que ela esteja da linha `TPV=TPF` (do canto inferior esquerdo ao superior direito, pior. Isso corresponde ao padrão do modelo "aleatório", quando você advinha a classe.

A métrica roc\_auc pode ser calculada no módulo de `metrics` e é chamada de `roc_auc_score`:

```
roc_auc = roc_auc_score(y_true, probabilities[:,1])
```

A entrada dessa função é o vetor de respostas verdadeiras `y_true` e o vetor da probabilidade de atribuir a classe `1`. Isso pode ser feito tomando apenas os segundos números (`probabilities[:,1]`) do vetor de probabilidades pareadas das classes `0` ou `1`. A função gera um único valor numérico entre 0 (mais frequentemente 0.5) e 1.

Métricas de Classificação: Estudando Probabilidades

Tarefa

Calcule e imprima a pontuação `roc_auc` com base na previsão de probabilidade.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

import pandas as pd

  

from sklearn.model\_selection import train\_test\_split

from sklearn.linear\_model import LogisticRegression

  

from sklearn.metrics import roc\_auc\_score

  

\# leia os dados nos parâmetros da grade e sua estabilidade do arquivo csv

electrical\_grid \= pd.read\_csv('/datasets/Electrical\_Grid\_Stability\_us.csv', sep \= ';')

print('Tamanho do conjunto de dados:', electrical\_grid.shape)

electrical\_grid.head()

  

\# veja a proporção entre as classes do conjunto de dados

electrical\_grid\['stability'\].value\_counts()

  

\# divida os dados em características (a matriz X) e a variável objetivo (y)

X \= electrical\_grid.drop('stability', axis \= 1)

y \= electrical\_grid\['stability'\]

  

\# divida os dados em conjuntos de treino e de teste

X\_train, X\_test, y\_train, y\_test \= train\_test\_split(X, y, test\_size\=0.2, random\_state\=0)

  

\# defina o algoritmo do modelo

model \= LogisticRegression()

  

\# treine seu modelo

model.fit(X\_train, y\_train)

  

\# use o modelo treinado para fazer previsões

probabilities \= model.predict\_proba(X\_test)\[:,1\]

  

\# imprima roc\_auc\_score

print('ROC\_AUC: {:.2f}'.format(roc\_auc\_score(y\_test, probabilities)))

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-52-652Z.md
### Última modificação: 2025-05-28 20:27:52

# Limiares e Balanço de Classe - TripleTen

Teoria

# Limiares e Balanço de Classe

Conforme ele seleciona as melhores ponderações possíveis para a função _z(x)_ por meio da função sigmoide _f(z),_ o algoritmo de regressão logística calcula a probabilidade de que evento vai ocorrer. Depois ele calcula a resposta final, `0` ou `1`, baseado nessa probabilidade. Isso é feito pela comparação com um certo **limiar**. Na maioria dos casos o limiar é 0.5. Mas por quê?

Na verdade, para muitos algoritmos, o método `predict_proba()` gera não a probabilidade de pertencer a uma classe mas o grau de confiança do modelo que o objeto pertence à classe. Se a certeza do modelo no pertencimento do objeto à classe `1` for maior que 0.5, então a resposta é `1`; caso contrário será `0`. Vamos ver quais casos a probabilidade e "confiança" podem ser considerados a mesma coisa, e onde eles se diferem.

Imagine que metade dos objetos do conjunto de treino pertença à classe `0`, e a outra metade a `1`. Esses conjuntos são chamados de **equilibrados**.

Sem algoritmos e conhecimento do novo objeto, você não pode dizer com certeza a qual classe eles pertencem. É 50/50, tanto `0` ou `1`, já que a probabilidade básica de que um objeto pertença a qualquer classe é 0.5. Essa probabilidade pode ser considerada o grau de confiança de que um objeto pertença ou a `1` ou `0`. Se você pode dizer que a probabilidade de pertencer à classe `1` é maior que 0.5 baseado em características que contêm informações adicionais sobre o objeto, então a confiança de que o objeto pertença à classe `1` é maior que para `0`. Isso explica o limite: nós atribuímos `1` para todos os objetos para os quais `predict_proba()` produz um resultado maior do que 0.5.

Enquanto aprende o básico, você vai completar tarefas de classificação binária simplificada que não são sempre realistas. Nessas tarefas objetos são mais ou menos uniformemente distribuídos entre as classes. (Imagine uma sacola que contém um número igual de bolas pretas e brancas.) Na vida real, classes são raramente equilibradas, então `predict_proba()` dá a você a probabilidade que isso é corrigido com referência ao equilíbrio entre classes.

Por exemplo, se você deseja prever o índice de cancelamento, geralmente não pode dizer que metade dos clientes da empresa sai todo mês. E para detecção de fraude, a classe objetivo ("a transação foi fraudulenta") raramente é maior que alguns por cento.

São classes não balanceadas que revelam problemas com métricas baseadas em respostas binárias e impossibilitando interpretar o resultado de `predict_proba()` como a probabilidade de pertencer a uma classe.

Mas e se o número de bolas brancas for quatro vezes maior? A probabilidade de você retirar uma bola preta é obviamente de 20%, enquanto para uma bola branca é de 80%. A probabilidade de um objeto pertencer à primeira classe (uma bola branca) é 0.8 (sem qualquer análise de características ou algoritmos). Este caso demonstra que a precisão de 0.8 pode ser alcançada mesmo sem modelos ou qualquer informação adicional. Para atribuir um objeto a uma classe mais rara, precisamos de um valor maior que 0.8.

Porém, muitos algoritmos normalizam probabilidades e aplicam outros métodos matemáticos de maneira que `predict_proba()` reflita a confiança do modelo na resposta final binária. Como antes, você pode configurar o limiar para o nível de confiança quando definir classes binárias. Fazendo dessa maneira, você vai corrigir a métrica que você considera a mais importante.

Pense nas métricas da tarefa sobre previsões de estabilidade da rede elétrica. A sensibilidade foi de cerca de 0.7. Isso implica que o modelo identificou apenas 70% do total de redes estáveis. Talvez esses resultados devam ser usados para aumentar a receita da empresa: cada rede estável significa outro contrato que pode ser prorrogado, por exemplo. Então, se perdermos uma rede estável, estamos perdendo receita. A sensibilidade é crucial e deve ser melhorada.

Se cortarmos a certeza pelo limiar de 0.5, `predict_proba()` produzirá automaticamente um resultado no qual os objetos da classe objetivo (`1`) pertencem a `0`. Se usarmos um valor menor, como 0.4, e assim determinarmos "manualmente" as classes com esse limiar personalizado, obteremos melhores resultados.

Tente você!

Limiares e Balanço de Classe

Tarefa

Calcule o vetor das previsões finais, considerando o limiar de 0.4, e compará-lo com as métricas obtidas a partir do cálculo automatizado de classes usando o método `predict()`.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

import pandas as pd

  

from sklearn.model\_selection import train\_test\_split

from sklearn.linear\_model import LogisticRegression

from sklearn.preprocessing import StandardScaler

from sklearn.metrics import accuracy\_score, precision\_score, recall\_score, f1\_score

  

\# leia os dados nos parâmetros da grade e sua estabilidade do arquivo csv

electrical\_grid \= pd.read\_csv('/datasets/Electrical\_Grid\_Stability\_us.csv', sep \= ';')

print('Tamanho do conjunto de dados:', electrical\_grid.shape)

electrical\_grid.head()

  

\# veja a proporção entre as classes do conjunto de dados

print('A proporção entre as classes:\\n', electrical\_grid\['stability'\].value\_counts())

  

\# divida os dados em características (a matriz X) e a variável objetivo (y)

X \= electrical\_grid.drop('stability', axis \= 1)

y \= electrical\_grid\['stability'\]

  

\# divida os dados em conjuntos de treino e de teste

X\_train, X\_test, y\_train, y\_test \= train\_test\_split(X, y, test\_size\=0.2, random\_state\=0)

  

\# defina o algoritmo do modelo

model \= LogisticRegression()

  

\# treine seu modelo

model.fit(X\_train, y\_train)

  

\# use o modelo treinado para fazer previsões

probabilities \= model.predict\_proba(X\_test)\[:,1\]

  

\# previsão binária

predictions \= model.predict(X\_test)

  

\# imprima todas as métricas estudadas para a previsão resultante

print('Métricas para a previsão automática feita com a previsão')

print('Acurácia: {:.2f}'.format(accuracy\_score(y\_test, predictions)))

print('Precisão: {:.2f}'.format(precision\_score(y\_test, predictions)))

print('Sensibilidade: {:.2f}'.format(recall\_score(y\_test, predictions)))

print('F1: {:.2f}\\n'.format(f1\_score(y\_test, predictions)))

  

\# defina o limiar

threshold \= 0.4

  

\# calcule a previsão com base nas probabilidades e na razão entre as classes

custom\_predictions \= \[0 if i<threshold else 1 for i in probabilities\]

  

\# imprima todas as métricas para a previsão com um novo limiar

print('Métricas para a previsão com limiar personalizado')

print('Acurácia para personalizado: {:.2f}'.format(accuracy\_score(y\_test, custom\_predictions)))

print('Precisão para personalizado: {:.2f}'.format(precision\_score(y\_test, custom\_predictions)))

print('Sensibilidade para personalizado: {:.2f}'.format(recall\_score(y\_test, custom\_predictions)))

print('F1 para personalizado: {:.2f}'.format(f1\_score(y\_test, custom\_predictions)))

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-53-964Z.md
### Última modificação: 2025-05-28 20:27:54

# Árvores de decisão - TripleTen

Teoria

# Árvores de decisão

É hora de estudar um algoritmo totalmente novo para tarefas de classificação e regressão. Conheça a **árvore de decisão**! Considerando que anteriormente nós assumimos que nós estávamos procurando por uma resposta, o valor da variável objetivo (uma função linear), árvores de decisão usam **cenários** para selecionar uma resposta.

Digamos que alguém está decidindo se fica na cidade no Natal ou se vai embora. A classificação é binária: 0 ou 1. Aqui estão os fatores que precisamos levar em consideração:

-   Com quem eles querem comemorar (família ou amigos)
-   Se sua família está hospedada na cidade
-   Se seus amigos estão hospedados na cidade
-   Se eles estão prontos para gastar uma quantia significativa (sim ou não)
-   O quanto eles gostam de viajar (numa escala de 0 a 10, onde 0 é caseiro e 10 é viajante inquieto)

O cenário de seu processo de pensamento pode ser ilustrado assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.12PT.png)

O algoritmo usa a sequência de características para dividir o conjunto de tal forma que os grupos no final da árvore (em suas folhas) sejam tão distintos quanto possível com base nos dados à nossa disposição. Por exemplo, se perguntarmos sobre a disponibilidade de gastar dinheiro no final de um ramo, veremos que 95% das observações "não" que chegaram ao final do ramo permaneceram na cidade.

Vantagens das árvores de decisão:

-   Elas são fáceis de interpretar. Em negócios, isso normalmente é o fator-chave. Algoritmos mais complexos (a serem discutidos mais tarde) são como "caixas pretas," porque os processos de tomar decisões são opacos.
-   Eles trabalham tanto com classificações e regressões. No último caso, as folhas da árvore irão ter números em vez de valores de classe (como em nosso exemplo com o Natal).
-   Elas são muito rápidos.
-   Elas não exigem pré-processamento de dados considerável: as árvores não são sensíveis à escala dos recursos e não são vulneráveis à multicolinearidade.

Desvantagens:

-   Elas são fortemente propensas ao superajuste. Lembre-se, isso significa que o modelo se adapta seus parâmetros ou a estrutura das ramificações aos dados de treino. Ele vai trabalhar muito bem no conjunto de treino, mas vai falhar quando são dados novos dados. Às vezes, quando existem muitas características, você deve construir uma ramificação com apenas um objeto, ou apenas alguns. Essencialmente, a árvore vai adaptar aos dados de treino e tentar colocar objetos de apenas uma classe em cada folha.

Para evitar isso, nós **reduzimos** as árvores. Reduzir envolve cortar uma ramificação em um certo ponto para prevenir a árvore de superajuste. Pequenas amostras são especialmente reduzidas para superajustar.

Lembre-se da tarefa sobre a atividade do usuário nas contas do Facebook das empresas? Uma regressão linear regular produziu melhores resultados que o outro algoritmo. Este último foi, na verdade, baseado em árvores de decisão.

o sklearn possui um módulo `tree` que inclui os algoritmos `DecisionTreeClassifier` e `DecisionTreeRegressor`:

```
tree_model = DecisionTreeClassifier()
tree_model.fit(X_train, y_train)
y_pred = tree_model.predict(X_test)
```

Em alguns casos você precisa visualizar a árvore de decisão resultante. Você pode usar a função **plot\_tree()** do mesmo módulo:

```
plt.figure(figsize = (20,15)) # defina o tamanho da figura para obter uma imagem maior
plot_tree(tree_model, filled=True, feature_names = X_train.columns, class_names = ['não culpado', 'culpado'])
plt.show()
```

O resultado será mais ou menos assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved_Untitled_1580061505_1592668692.png)

Essa é a árvore do modelo `tree_model =DecisionTreeClassifier(min_samples_leaf=500)`, que resolve tarefas de classificação binária (`"culpado"`/`"não culpado"`). A amostra contém por volta de 10 000 objetos. Cada nó tem a condição para futuras construções de ramificações, assim como o resultado mais provável na amostra correspondente para esse nó. Tenha cuidado: se a amostra é grande defina o limite para o número mínimo de objetos na folha, por exemplo `min_samples_leaf=500`. Se você ignora isso, a árvore vai terminar com muitas ramificações para visualização útil.

Árvores de decisão

Tarefa

Realize a classificação binária usando uma árvore de decisão. Com base nas métricas que você estudou, compare os resultados com os obtidos usando a regressão logística anteriormente.

Especifique `random_state=0` ao definir o algoritmo para seu novo modelo.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

import pandas as pd

  

from sklearn.model\_selection import train\_test\_split

from sklearn.linear\_model import LogisticRegression

from sklearn.tree import DecisionTreeClassifier

  

  

from sklearn.metrics import accuracy\_score, precision\_score, recall\_score, f1\_score

from sklearn.metrics import roc\_auc\_score

  

\# leia os dados nos parâmetros da grade e sua estabilidade do arquivo csv

electrical\_grid \= pd.read\_csv('/datasets/Electrical\_Grid\_Stability\_us.csv', sep \= ';')

print('Tamanho do conjunto de dados:', electrical\_grid.shape)

electrical\_grid.head()

  

\# veja a proporção entre as classes do conjunto de dados

electrical\_grid\['stability'\].value\_counts()

  

\# divida os dados em características (a matriz X) e a variável objetivo (y)

X \= electrical\_grid.drop('stability', axis \= 1)

y \= electrical\_grid\['stability'\]

  

\# divida os dados sobre treinos e testes

X\_train, X\_test, y\_train, y\_test \= train\_test\_split(X, y, test\_size\=0.2, random\_state\=0)

  

\# defina o algoritmo do modelo

model \= LogisticRegression(random\_state\=0)

  

\# treine seu modelo

model.fit(X\_train, y\_train)

  

\# use o modelo treinado para fazer previsões

predictions \= model.predict(X\_test)

probabilities \= model.predict\_proba(X\_test)\[:,1\]

  

\# imprima todas as métricas estudadas

print('Métricas de regressão logística')

print('Acurácia: {:.2f}'.format(accuracy\_score(y\_test, predictions)))

print('Precisão: {:.2f}'.format(precision\_score(y\_test, predictions)))

print('Sensibilidade: {:.2f}'.format(recall\_score(y\_test, predictions)))

print('F1: {:.2f}'.format(f1\_score(y\_test, predictions)))

print('ROC\_AUC: {:.2f}\\n'.format(roc\_auc\_score(y\_test, probabilities)))

  

\# defina o algoritmo do novo modelo com base na árvore de decisão

tree\_model \= DecisionTreeClassifier(random\_state\=0)

  

\# treine seu modelo

tree\_model.fit(X\_train, y\_train)

  

\# use o modelo treinado para fazer previsões

tree\_predictions \= tree\_model.predict(X\_test)

tree\_probabilities \= tree\_model.predict\_proba(X\_test)\[:,1\]

  

\# imprima todas as métricas

print('Métricas da árvore de decisão')

print('Acurácia: {:.2f}'.format(accuracy\_score(y\_test, tree\_predictions)))

print('Precisão: {:.2f}'.format(precision\_score(y\_test, tree\_predictions)))

print('Sensibilidade: {:.2f}'.format(recall\_score(y\_test, tree\_predictions)))

print('F1: {:.2f}'.format(f1\_score(y\_test, tree\_predictions)))

print('ROC\_AUC: {:.2f}'.format(roc\_auc\_score(y\_test, tree\_probabilities)))

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-57-696Z.md
### Última modificação: 2025-05-28 20:27:58

# Conjuntos de Arvores de Decisão: Floresta Aleatória e Gradient Boosting - TripleTen

Teoria

# Conjuntos de Arvores de Decisão: Floresta Aleatória e Gradient Boosting

**Conjuntos** são modelos poderosos de aprendizado de máquina. Eles são poderosos e ajudam a descobrir relações complexas dentro dos dados. Mas o custo desse poder diminuição de interpretabilidade.

Esses algoritmos às vezes são chamados de **caixas pretas**, já que é difícil dizer com uma característica particular impactou a predição de um objeto. Cabe a você decidir o que você dispõe a sacrificar quando está resolvendo tarefas de negócios.

## Mais árvores são melhores do que uma

A ideia básica de um conjunto é simples. Vamos dizer que você resolveu um problema de classificação binária com uma árvore de decisão baseada no modelo `m_1`. Você tem o valor `roc_auc_score_1 = 0.8`. No entanto, todos os modelos são reduzidos para superajustar, então um algoritmo que trabalha bem para observações e características do conjunto de treino pode cometer grandes erros com os dados de validação. Nós podemos criar outro modelo, `m_2`, que vai compensar os erros do primeiro modelo. Por si só, ele pode render um valor relativamente baixo `roc_auc` (digamos, `roc_auc_score_2=0.79`). Depois você pode mudar as condições mais uma vez e construir outro modelo, `m_3`, e assim por diante até `m_n`. Cada um dos modelos `n` será diferente, sensitivo a fatores diferentes e transmitindo informações unívocas sobre as observações. Você pode tentar dar conta de todos eles ao mesmo tempo, e fazer um modelo consolidado, `M`, que vai produzir melhores resultados que cada modelo pego separadamente (por exemplo `roc_auc_score = 0.91`). Esse modelo é dito ser **forte** ("um classificador forte"), enquanto os modelos nos quais ele é baseado são **fracos** ("classificadores fracos").

Existem dois tipos básicos de conjuntos de modelos: **floresta aleatória** e **gradient boosting**. Com esses, árvores de decisão são normalmente tomados como modelos de predição fracos. A seleção de árvores não é aleatória. Diferente de modelos lineares, por exemplo, árvores de decisão permitem você mude os subconjuntos de observações e características, tão bem como outros parâmetros, para cada classificação fraca separada. Por exemplo, você pode limitar a profundidade das árvores ou o número mínimo de objetos para cada nó. Assim, a árvore provém muito mais modelos para resolver uma dada tarefa.

Tirar a média funciona melhor quando modelos diferem e compensam outros erros. Mas se tirar a média de modelos similares cujas falhas são similares, você não terá esse efeito.

## Florestas Aleatórias

O primeiro tipo de conjunto é chamado de **floresta aleatória**. A palavra "floresta" fala por si: um algoritmo consolidado consiste em uma infinidade de árvores. Mas por que "aleatória"? Sabemos que um algoritmo de árvore pode dividir amostras de forma diferente com base em características e criar diferentes cenários que levam a respostas finais nas folhas da árvore. Uma maneira extra de obter árvores diferentes é passar a elas subconjuntos aleatórios do conjunto de trens original como dados de entrada. Você pode adicionar ainda mais variação tomando diferentes conjuntos de `n` características fixas para cada árvore.

É assim que o algoritmo de floresta aleatória realmente funciona. Ela gera muitas árvores mutuamente independentes de maneiras ligeiramente diferentes (tratando de diferentes subconjuntos ou características) e alcança uma decisão final com base nas suas próprias repostas. O algoritmo da floresta aleatória, **tira a média das respostas** de todas as árvore (em regressão) ou **usa** a **votação** (na classificação) para selecionar a resposta que a maioria das árvores determina como verdadeira.

Ao falar sobre os conjuntos de modelos, podemos usar o termo **bagging** (de "agregação de bootstrap"). Isso envolve tirar a média de modelos treinados e, diferentes subconjuntos. A abordagem em si, que consiste em estimar várias características ou predições usando uma multiplicidade de diferentes subconjuntos obtidos do conjunto inicial, é chamado de **bootstraping**. Como você deveria saber, essa palavra implica fazer algo impossível sem ajuda externa – como o Barão Munchausen, que se salvou de um afogamento puxando o próprio cabelo. Bootstrapping é uma abordagem comum em aprendizado de máquina e estatística.

Quando trabalhar com florestas aleatórias, lembre-se se que as árvores são treinadas em paralelo, ou independentemente uma da outra. O treinamento de cada árvore não depende do resultado de outros. Isso representa uma diferença conceitual entre floresta aleatória e gradient boosting.

## Gradient boosting

O conceito subjacente é praticamente o mesmo: nós coletamos muitos modelos simples (árvores) de modo a compensar por outros erros e aleatoriedade. Somente nesse caso, os erros são compensados através do **treinamento**, em vez de pela média.

Em gradient boosting, árvores são treinadas **consecutivamente**: cada árvore é construída com referência aos resultados do anterior. Você já conhece os princípios do gradiente descendente: nós tentamos minimizar o erro em cada passo conforme nos movemos na direção que queremos. A ideia por trás do gradient boosting é muito parecida. Nós temos um modelo em uma dada etapa, e ela faz a predição com algum erro. Depois nós construímos outro modelo para predizer o erro do primeiro modelo em vez do primeiro valor original e ajustamos a predição final nessa base. Então um terceiro modelo prediz o erro do segundo modelo, e assim por diante. Finalmente, nós usamos a sequência inteira para fazer a predição para uma observação específica. Nós dizemos que o primeiro modelo produziu esse resultado, mas nós adicionamos a ele a correção do segundo modelo, depois, do terceiro, e assim por diante, até o último modelo. Assim, com cada etapa (cada novo modelo), nós melhoramos nossas predições anteriores. "Boosting" se refere a esse processo de melhora, e "gradient" boosting significa que a melhora toma lugar passo a passo.

## Usando conjuntos

No sklearn, florestas aleatórias são implementadas no módulo `ensemble`. Os algoritmos mais populares dos módulos são `RandomForestClassifier()` e `RandomForestRegressor()`. Nós precisamos que eles resolvam problemas de classificação e regressão, respectivamente.

Dê uma olhada à sintaxe por declaração, treinamento, e predições em problemas de regressão:

```
from sklearn.ensemble import RandomForestRegressor

# definindo novos algoritmos do modelo baseados no algoritmo da floresta aleatória
rf_model = RandomForestRegressor(n_estimators = 100)
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_val)
```

Quando declaramos nosso modelo, nós definimos `n_estimators`, o número de árvores na nossa floresta. Para os parâmetros remanescentes de árvores (sua profundidade `max_depth`, o tamanho do subconjunto da característica `max_features`, e o número mínimo de objetos por nó `min_samples_leaf`), nós deixamos os valores padrão inalterados. Você pode brincar com esses valores e ver como eles afetam o resultado.

Gradient boosting é implementado no módulo `ensemble` do sklearn. Ele tem algoritmos `GradientBoostingClassifier()` e `GradientBoostingRegressor()` para resolver problemas de classificação e regressão, respectivamente.

Dê uma olhada a sua sintaxe em um problema de regressão:

```
from sklearn.ensemble import GradientBoostingRegressor

# definir o algoritmo dos novos modelos baseado no algoritmo gradient boosting
gb_model = GradientBoostingRegressor(n_estimators = 100)
gb_model.fit(X_train, y_train)
y_pred = gb_model.predict(X_val)
```

Você também deve ter ouvido sobre `xgboost`. Esse é um algoritmo de outra biblioteca, e é usada frequentemente em competições de aprendizado de máquina. Você não vai precisar disso nesse curso, mas você pode ler mais sobre isso aqui _(os materiais estão em inglês)_:

[https://xgboost.readthedocs.io/en/latest/](https://xgboost.readthedocs.io/en/latest/)

[https://www.datacamp.com/community/tutorials/xgboost-in-python](https://www.datacamp.com/community/tutorials/xgboost-in-python)

Conjuntos de Arvores de Decisão: Floresta Aleatória e Gradient Boosting

Tarefa

Aplicar outros dois métodos para realizar a classificação binária para prever a estabilidade da rede elétrica. Use as métricas que você conhece para comparar os quatro modelos:

-   linear (regressão logística)
-   árvore de decisão
-   sklearn floresta aleatória com parâmetros `n_estimators = 100`, `random_state = 0`
-   sklearn gradient boosting com parâmetros `n_estimators = 100`, `random_state = 0`

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

79

80

81

82

83

84

85

86

87

88

import pandas as pd

  

from sklearn.model\_selection import train\_test\_split

from sklearn.linear\_model import LogisticRegression

from sklearn.tree import DecisionTreeClassifier

from sklearn.preprocessing import StandardScaler

  

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

  

from sklearn.metrics import accuracy\_score, precision\_score, recall\_score, f1\_score

from sklearn.metrics import roc\_auc\_score

  

\# defina a função que produzirá suas métricas

def print\_all\_metrics(y\_true, y\_pred, y\_proba, title \= 'Métricas de classificação'):

print(title)

print('\\tAcurácia: {:.2f}'.format(accuracy\_score(y\_true, y\_pred)))

print('\\tPrecisão: {:.2f}'.format(precision\_score(y\_true, y\_pred)))

print('\\tSensibilidade: {:.2f}'.format(recall\_score(y\_true, y\_pred)))

print('\\tF1: {:.2f}'.format(f1\_score(y\_true, y\_pred)))

print('\\tROC\_AUC: {:.2f}'.format(roc\_auc\_score(y\_true, y\_proba)))

  

\# leia os dados nos parâmetros da grade e sua estabilidade do arquivo csv

electrical\_grid \= pd.read\_csv('/datasets/Electrical\_Grid\_Stability\_us.csv', sep \= ';')

print('Tamanho do conjunto de dados:', electrical\_grid.shape)

electrical\_grid.head()

  

\# veja a proporção entre as classes do conjunto de dados

electrical\_grid\['stability'\].value\_counts()

  

\# divida os dados em características (a matriz X) e a variável objetivo (y)

X \= electrical\_grid.drop('stability', axis \= 1)

y \= electrical\_grid\['stability'\]

  

\# divida os dados sobre treinos e testes

X\_train, X\_test, y\_train, y\_test \= train\_test\_split(X, y, test\_size\=0.2, random\_state\=0)

  

\# treine StandardScaler no conjunto de trem

scaler \= StandardScaler()

scaler.fit(X\_train)

  

\# transforme os conjuntos de trem e teste

X\_train\_st \= scaler.transform(X\_train)

X\_test\_st \= scaler.transform(X\_test)

  

\# defina o algoritmo para o modelo de regressão logística

lr\_model \= LogisticRegression(random\_state\=0)

\# treine o modelo

lr\_model.fit(X\_train\_st, y\_train)

\# use o modelo treinado para fazer previsões

lr\_predictions \= lr\_model.predict(X\_test\_st)

lr\_probabilities \= lr\_model.predict\_proba(X\_test\_st)\[:,1\]

\# imprima todas as métricas

print\_all\_metrics(y\_test, lr\_predictions, lr\_probabilities , title\='Métricas para regressão logística:')

  

  

  

\# defina o algoritmo para o novo modelo de árvore de decisão

tree\_model \= DecisionTreeClassifier(random\_state\=0)

\# treine o modelo de árvore de decisão

tree\_model.fit(X\_train\_st, y\_train)

\# use o modelo treinado para fazer previsões

tree\_predictions \= tree\_model.predict(X\_test\_st)

tree\_probabilities \= tree\_model.predict\_proba(X\_test\_st)\[:,1\]

\# imprima todas as métricas

print\_all\_metrics(y\_test, tree\_predictions, tree\_probabilities, title\='Métricas para árvore de decisão:')

  

  

  

\# define o algoritmo para o novo modelo de floresta aleatória

rf\_model \= RandomForestClassifier(n\_estimators \= 100, random\_state \= 0)

\# treine o modelo de floresta aleatória

rf\_model.fit(X\_train\_st, y\_train)

\# use o modelo treinado para fazer previsões

rf\_predictions \= rf\_model.predict(X\_test\_st)

rf\_probabilities \= rf\_model.predict\_proba(X\_test\_st)\[:,1\]

\# imprima todas as métricas

print\_all\_metrics(y\_test, rf\_predictions, rf\_probabilities, title\='Métricas para floresta aleatória:')

  

  

\# defina o algoritmo para o novo modelo de gradient boosting

gb\_model \= GradientBoostingClassifier(n\_estimators \= 100, random\_state \= 0)

\# treine o modelo de gradient boosting

gb\_model .fit(X\_train\_st, y\_train)

\# use o modelo treinado para fazer previsões

gb\_predictions \= gb\_model .predict(X\_test\_st)

gb\_probabilities \= gb\_model .predict\_proba(X\_test\_st)\[:,1\]

\# imprima todas as métricas

print\_all\_metrics(y\_test, gb\_predictions, gb\_probabilities, title='Métricas para aumento de gradiente:')

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-27-58-992Z.md
### Última modificação: 2025-05-28 20:27:59

# Algoritmos de Aprendizado Não Supervisionado: Agrupamento - TripleTen

Capítulo 3/6

Algoritmos de Aprendizado de Máquina

# Algoritmos de Aprendizado Não Supervisionado: Agrupamento

Agora você está familiarizado com o aprendizado supervisionado. Mas nos negócios, saber como aplicar o aprendizado não supervisionado quando você não tem as respostas necessárias para treinar seu modelo não é menos importante.

De forma geral, aprendizado não supervisionado é sobre encontrar similaridade entre objetos. Uma vez que o modelo o detectou, você pode agrupar os objetos em si, ou seja, resolver um **problema de agrupamento**. Você pode encontrar similaridades entre eles, também. Esse será um **problema de redução de dimensionalidade**.

Vamos dar uma olhada mais próxima ao agrupamento. É muito comum em negócios. Por exemplo, agrupamento é usado:

-   **Para segmentação de clientes**

Imagine que você está trabalhando para um serviço de plano de saúde privado. Os clientes não são homogêneos; há jovens empresários e engenheiros de software interessados em serviços médicos menos comuns, mas também há alguns clientes mais velhos que não estão muito satisfeitos com os planos que suas empresas oferecem e querem somente produtos tradicionais de alta qualidade. Você suspeita que existem outros tipos de clientes também, mas se for verdade, são menos óbvios.O agrupamento pode ajudá-lo a separá-los e oferecer a cada segmento o produto e serviço que eles desejam.

-   **Para segmentação de produtos**

Você começou a trabalhar como analista para uma rede de cafés. Sua tarefa é desenvolver uma estratégia de distribuição. Você acredita que pode identificar vários tipos de cafés com base em sua distância do centro da cidade e pontos turísticos populares e seu tamanho. Para cada tipo, você pode definir várias metas ou KPIs, decorar os interiores e criar anúncios externos. O agrupamento ajudará você a identificar os tipos.

-   **Para modelação de tópicos**

Digamos que você esteja trabalhando para um serviço que ajuda os clientes a selecionar as belas visualizações de dados. Você coletou um grande número de exemplos e gostaria de organizá-los de maneira amigável para usuários. O agrupamento ajudará você a analisar códigos de visualização e criar tópicos dependendo dos métodos, funções e rótulos de gráfico usados. Isso permitirá que os visitantes naveguem em sua enciclopédia de visualizações mais rapidamente.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-28-00-338Z.md
### Última modificação: 2025-05-28 20:28:00

# O Que Tudo Isso Tem a Ver com Distância? - TripleTen

Capítulo 3/6

Algoritmos de Aprendizado de Máquina

# O Que Tudo Isso Tem a Ver com Distância?

Digamos que você esteja trabalhando em uma agência de viagens. Você tem uma enorme base de destinos que seus clientes podem escolher. Cada país tem seus próprios encantos: natureza, museus, belas praias...

Sua tarefa é dividir os destinos em segmentos para uma publicidade mais personalizada.

Vamos ler as primeiras 10 linhas de uma tabela que contém classificações da natureza, cultura, gastronomia e praias de países que os turistas gostam de visitar:

```
import pandas as pd

countries = pd.read_csv('countries.csv')
print(countries.head(10))
```

```
Counry  The level of narural sights  \\
0         France                                          5   
1          Italy                                          5   
2          Spain                                          7   
3            USA                                          6   
4           China                                         4   
5          Turkey                                         4   
6         Germany                                         3   
7        Thailand                                         7   
8   Great Britain                                         3   
9          Russia                                         7   

              Cultural heritage     Level of cultural activity  \\
0                            92                           4425   
1                            98                           4496   
2                            96                           4354   
3                            24                           4283   
4                            78                           3218   
5                            84                           2366   
6                            60                           3786   
7                            82                           3076   
8                            76                           4212   
9                            72                           3147   

                        Local cuisine                           Beaches
0                                  10                                 6  
1                                  10                                10  
2                                   6                                 4  
3                                   3                                 4  
4                                   5                                 2  
5                                   5                                 7  
6                                   3                                 1  
7                                   7                                10  
8                                   2                                 1  
9                                   6                                 1
```

## Distância

Como podemos medir a semelhança entre os países? Objetos similares compartilham características similares. Em outras palavras, a "distância entre eles é pequena. Nós apenas substituímos o conceito de "similaridade" pelo de "distância," que é mais fácil de trabalhar em um nível intuitivo.

O primeiro passo em um problema de agrupamento é determinar a distância entre os objetos, ou seja, descrever sua proximidade em números. Depois você pode agrupar os países na base dessa proximidade. Vamos definir a função para a distância entre objetos então:

ρ(x1,x2)\\rho(x\_1, x\_2)ρ(x1​,x2​)

Deixe cada país ter apenas duas características:

-   a beleza de sua natureza (classificações dos especialistas em uma escala de 0 a 10)
-   o número de instituições e eventos culturais (o número de museus, teatros, exposições; de 300 a 3000)

Como podemos definir a distância entre dois objetos baseados em dados de duas características? Nós podemos fazer um gráfico com as classificações naturais ao longo do eixo X e o número de instituições culturais ao longo do eixo Y. Vamos terminar com algo assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.15PT.png)

Mas será uma ideia melhor unificar as escalas das características primeiro. Aqui os métodos **normalização** e **padronização**que você já conhece serão úteis. Lembre-se de que a normalização vai atribuir valores entre 0 e 1 para as caraterísticas. A padronização vai transformar os valores de forma que eles formem uma distribuição padrão normal: a média será 0, e o desvio padrão será 1. Vamos padronizar os dados:

```
from sklearn.preprocessing import StandardScaler()

scaler = StandardScaler() # criando um objeto de classe escalonador (normalizador)
x_sc = scaler.fit_transform(countries.drop(columns = ['Country'])) # treinando o normalizador e transformando o conjunto de dados
```

O resultado será tabelas transformadas de características para os dados de trem e validação. São o que usaremos para treinamento e previsão.

Nossa tabela de origem transformada ficará assim:

```
print(x_sc[:10])
```

```
[[-0.45315151  1.23571694  1.23067191  2.05016406  0.67162042]
 [-0.45315151  1.43475859  1.289553    2.05016406  2.03618254]
 [ 0.29048174  1.36841138  1.17179081  0.46931466 -0.01066064]
 [-0.08133489 -1.02008848  1.11290971 -0.71632238 -0.01066064]
 [-0.82496814  0.77128641  0.22969329  0.07410232 -0.6929417 ]
 [-0.82496814  0.97032807 -0.47687986  0.07410232  1.01276095]
 [-1.19678477  0.17416145  0.70074205 -0.71632238 -1.03408223]
 [ 0.29048174  0.90398085  0.1119311   0.86452701  2.03618254]
 [-1.19678477  0.70493919  1.05402862 -1.11153473 -1.03408223]
 [ 0.29048174  0.57224476  0.17081219  0.46931466 -1.03408223]]
```

Nós unificamos a escala das características. Agora vamos exibir todos os países como pontos em um gráfico: as classificações de natureza padronizadas estarão no eixo X, enquanto as classificações culturais com a mesma escala estarão no eixo Y:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.15.3PT.png)

Como nós podemos encontrar a distância se sabemos os valores das características de quaisquer dois objetos? Aplique a fórmula padrão de distância (às vezes chamada de distância Euclideana):

ρ(x1,x2)\=∑i\=1n(x1i−x2i)2\\rho(x\_1, x\_2) = \\sqrt{\\sum\_{i=1}^{n}{(x\_{1\_i} - x\_{2\_i})^2}}ρ(x1​,x2​)\=∑i\=1n​(x1i​​−x2i​​)2​

Aqui `n` é o número de dimensões (países). Depois, se nós tivermos duas características, a distância entre países será definida como se segue:

ρ(x1,x2)\=(x11−x21)2+(x12−x22)2\\rho(x\_1, x\_2) = \\sqrt{{(x\_{1\_1} - x\_{2\_1})^2 + (x\_{1\_2} - x\_{2\_2})^2 }}ρ(x1​,x2​)\=(x11​​−x21​​)2+(x12​​−x22​​)2​

Aqui _x₁_ é a classificação padronizada da natureza, enquanto _x₂_ é a classificação cultural padronizada. Tomemos a Alemanha e a Islândia, por exemplo. A natureza da Islândia tem uma classificação de `1.4`, e a da Alemanha tem uma classificação de `-1.2`. A cultura islandesa foi classificada com `-2.2` (sugerindo que a Islândia é um país pouco tranquilo), enquanto a Alemanha obteve `0.7`. Então a distância entre eles será:

ρ(Iceland,Germany)\=(1.4−(−1.2))2+((−2.2)−0.7)2\=3.89\\rho(Iceland, Germany) = \\sqrt{{(1.4 - (-1.2))^2 + ((-2.2) - 0.7)^2 }} = 3.89ρ(Iceland,Germany)\=(1.4−(−1.2))2+((−2.2)−0.7)2​\=3.89

Você encontrou a distância entre a Alemanha e a Islândia, em termos de turismo. Ou a hipotenusa, em termos da matemática que você aprendeu na escola.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.15.2PT.png)

Há uma diferença significativa entre a Alemanha e a Islândia. Você provavelmente terá que fazer publicidade desses dois países para diferentes segmentos turísticos. Você provavelmente vai querer oferecer a quem esteve na Alemanha uma viagem aos Países Baixos. É mais parecido com a Alemanha do que com a Islândia, a julgar por suas características.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-28-02-209Z.md
### Última modificação: 2025-05-28 20:28:02

# K-Means e Agrupamento Aglomerativo Hierárquico - TripleTen

Teoria

# K-Means e Agrupamento Aglomerativo Hierárquico

Agora você conhece o conceito de distância, que é fundamental para o agrupamento, e pode encontrá-lo entre quaisquer dois pontos no gráfico. É a base para todos os algoritmos de agrupamento.

Agora é hora de olhar para os mais populares:

-   **K-means**
-   **Agrupamento aglomerativo hierárquico**

O primeiro método é rápido e claro, ainda que sensitivo para o número de agrupamento que você define. O segundo é mais complicado, mas ele não requer a definição do número de agrupamentos antecipadamente e permite que você visualize a relação entre objetos. Vamos olhar para eles um de cada vez.

### K-means

K-means agrupa objetos passo a passo. O algoritmo é baseado na presunção de que o número de agrupamentos (grupos) já é conhecido. Essa é uma suposição bastante forte, e encontrar o número ideal de agrupamentos geralmente vale a pena resolver tarefas separadas.

Aqui está o algoritmo para K-means:

1.  Nós temos agrupamentos K. O algoritmo leva o centro deles um por um e coloca cada objeto no agrupamento cujo centro é mais perto.
2.  O centro são corrigidos (eles são movidos) até que a distância média entre os objetos dentro de cada agrupamento e seu centro sejam minimizados.
3.  Quando a distância dos objetos ao centro parar de diminuir ou diminuir apenas de maneira insignificante, o algoritmo para e fixa o desvio, reconhecendo-o como ideal.

Agora que sabemos como o algoritmo funciona, vamos executá-lo em Python.

O algoritmo chamado `KMeans` é implementado no módulo `sklearn.cluster`. Veja como fica sua sintaxe:

```
from sklearn.cluster import KMeans

# a padronização obrigatória dos dados antes de passá-los pelo algoritmo
sc = StandardScaler()
X_sc = sc.fit_transform(X)

km = KMeans(n_clusters = 5) # definindo o número de agrupamentos como 5
labels = km.fit_predict(X_sc) # aplicando o algoritmo os dados e formando um vetor de agrupamento
```

A variável `labels` armazena o índice dos grupos sugeridos pelo algoritmo. O algoritmo atribui aleatoriamente um número para cada grupo, então não é necessário tentar identificar a lógica: grupo `2` não é necessariamente mais perto do grupo `3` do que é do grupo `1`. A coisa mais importante aqui é que objetos com o mesmo índice se referem ao mesmo agrupamento.

### **Agrupamento aglomerativo hierárquico**

O ponto crucial do agrupamento hierárquico é simples. Quando definimos a função de distância, nós podemos calcular a matriz das distâncias entre todos os objetos. Suas células vão conter a distância enter pares de objetos. Além do mais, ele levará em conta todas as características dos objetos, não apenas duas delas. Por exemplo:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.16PT.png)

Podemos usar essa matriz para unir agrupamentos próximos de maneira consistente. É impossível estimar a proximidade de países separados ou detectar rapidamente os países mais perto, ou mais longe apenas olhando. Mas a distância entre os objetos e o agrupamento aglomerativo hierárquico em si, pode ser visualizado com gráficos especiais chamados **dendrogramas**:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.16.2PT.png)

A distância entre os grupos continua pelo eixo Y e os objetos pelo eixo X. Cada link horizontal corresponde à distância entre os objetos que não tem nome. Se nos movermos de baixo para cima do gráfico, primeiro vamos ver que isso une objetos separados. Isso é seguido por links que unem objetos com grupos, ou grupos com grupos. Se você não parar esse processo intencionalmente, ele vai acabar quando um agrupamento gigante for deixado de fora.

Assim é como o **agrupamento aglomerativo hierárquico** realmente funciona. Agora vamos dar uma olhada mais de perto e ver como funciona com os dados do nosso país.

Podemos ver que o Canadá tem a menor distância para a Áustria (eles são os países mais semelhantes em termos das características que escolhemos). Vamos uni-los dentro de um agrupamento. Podemos então definir seu centro e recalcular a matriz de distâncias, levando em consideração a distância de outros objetos até o novo centro composto.

Então podemos unir outros pares de países que estão situados próximos um do outro, por exemplo. França e Itália, Tailândia e Grécia. Cada vez, obteremos um novo agrupamento, seu centro composto e matrizes de distância recalculadas.

Em algum momento, os agrupamentos mais próximos serão os agrupamentos "emparelhados" "Canadá-Áustria" e "EUA-Austrália". Vamos colocá-los em um grupo composto de quatro países.

Esse método de agrupamento é realmente intuitivo. A cada etapa,o algoritmo aumenta os agrupamentos adicionando os vizinhos. É por isso que é chamado de aglomerativo: ele transforma agrupamentos em aglomerações. (É assim que também chamamos um grupo de áreas residenciais.) É importante definir o momento quando precisamos parar o processo de combinação.

Podemos usar dendrogramas para estimar visualmente o número necessário de agrupamentos. Podemos também ter uma noção da distância em que vamos parar de unir objetos. Aqui está um exemplo de como podemos selecionar limiares de diferentes distâncias para um grande número de objetos:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.16.3PT.png)

O valor K é o número de linhas atravessado por linhas dash.

Vamos construir um dendrograma em Python. Primeiro vamos importar classes de modelo de agrupamento `linkage()` e `dendrogram()` formam o módulo de agrupamento `hierarchy`:

```
from scipy.cluster.hierarchy import dendrogram, linkage
```

Depois vamos padronizar os dados e aprovar a tabela resultante como parâmetro para a função `linkage()`. Para fazer nosso gráfico mais representativo, vamos passar `'ward'` para o parâmetro `method`:

```
# padronização obrigatória de dados antes de passá-la para o algoritmo
sc = StandardScaler()
X_sc = sc.fit_transform(X)

linked = linkage(x_sc, method = 'ward')
```

A variável `linked` armazena a tabela com objetos vinculados. Ele pode ser visualizado como um dendrograma:

```
plt.figure(figsize=(15, 10))  
dendrogram(linked, orientation='top')
plt.title('Agrupamento Hierárquico para GYM')
plt.show()
```

Vamos obter os seguintes gráficos:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.16.4PT.png)

O número ideal de agrupamentos sugerido (5), corresponde a cinco cores diferentes no gráfico.

A complexidade do agrupamento aglomerativo tem a ver com os cálculos que a máquina faz para traçar um dendrograma, e não com o algoritmo como tal. Calcular pares de distância pode levar bastante tempo, então quando resolver problemas de agrupamentos, você pode desenhar um dendrograma em um subconjunto aleatório, estimando o número ideal de agrupamentos, e iniciar o algoritmo K-means mais rápido.

K-Means e Agrupamento Aglomerativo Hierárquico

Tarefa

Você tem um conjunto de dados com as avaliações feitas por usuários do TripAdvisor. Ele contém as classificações médias que os viajantes deram aos resorts na Ásia.

Sua tarefa é agrupar essas taxas. Isso significa que você precisa:

-   ler os dados
-   padronizar as características
-   definir um modelo baseado no algoritmo k-means com 3 agrupamentos
-   prever os grupos de revisão
-   obter os valores médios de características para os agrupamentos resultantes usando o método `groupby()`
-   construir os agrupamentos resultantes nos gráficos de combinação emparelhados para os seguintes pares:
    -   `Average user feedback on juice bars` e `Average user feedback on religious institutions`
    -   `Average user feedback on juice bars` e `Average user feedback on restaurants`

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

import pandas as pd

  

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans

import seaborn as sns

import matplotlib.pyplot as plt

  

  

\# defina a função para renderizar gráficos de características emparelhadas para os agrupamentos

def show\_clusters\_on\_plot(df, x\_name, y\_name, cluster\_name):

plt.figure(figsize \= (10, 10))

sns.scatterplot(x\=df\[x\_name\], y\=df\[y\_name\],

hue \= df\[cluster\_name\], palette \= 'Paired'

)

plt.title('{} vs {}'.format(x\_name, y\_name))

plt.show()

  

\# leia os dados

travel \= pd.read\_csv('/datasets/tripadvisor\_review\_case\_us.csv')

print(travel.shape)

  

\# padronize os dados

sc \= StandardScaler()

x\_sc \= sc.fit\_transform(travel)

  

\# defina o modelo k\_means com 3 agrupamentos

km \= KMeans(n\_clusters\=3)

  

  

\# preveja os agrupamentos para observações (o algoritmo atribui a eles um número de 0 a 4)

labels \= km.fit\_predict(x\_sc)

  

\# armazene rótulos de agrupamento no campo do nosso conjunto de dados

travel\['cluster\_km'\] \= labels

  

\# obtenha as estatísticas dos valores médios de características por agrupamento

travel.groupby(\['cluster\_km'\]).mean()

  

\# renderize o gráfico para as características "bar de suco" e "religião" emparelhadas

show\_clusters\_on\_plot(travel, 'Average user feedback on juice bars',

'Average user feedback on religious institutions', 'cluster\_km')

  

\# renderize o gráfico para as características emparelhadas de "bar de sucos" e "restaurantes"

show\_clusters\_on\_plot(travel, 'Average user feedback on juice bars',

'Average user feedback on restaurants', 'cluster\_km')

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-28-03-543Z.md
### Última modificação: 2025-05-28 20:28:03

# Métricas para Problemas de Aprendizado Não Supervisionado - TripleTen

Teoria

# Métricas para Problemas de Aprendizado Não Supervisionado

Um fato engraçado: existe um **[teorema da impossibilidade de agrupamento](https://www.cs.cornell.edu/home/kleinber/nips15.pdf)** _(os materiais estão em inglês)_ formulado por Jon Kleinberg, segundo o qual não existe um algoritmo ótimo para agrupamento. Isso significa que já que não há legendagem, não há maneira correta de avaliar a performance de um algoritmo. Todas as abordagens para determinar se os objetos foram bem divididos são baseadas em opiniões de experts ou avaliações investigativas (tentativa e erro). Contudo, nos negócios, tarefas de agrupamento são muito comuns, e existe a necessidade de ter uma forma de avaliá-las.

De fato, tudo era simples quando estávamos com métricas de problemas para problemas com dados legendados. Nós tivemos predições, tivemos valores reais de variáveis objetivo, e tivemos especialistas em aprendizado de máquina para nos mostrar as várias formas de como essas duas se diferem.

Mas podemos dizer que nosso modelo é o correto quando não temos as respostas corretas? Como nós poderíamos avaliar a qualidade dos algoritmos que agrupam objetos com base na similaridade?

A resposta está na maneira que configuramos a tarefa. Se nós pegamos a distância entre objetos como a base da "similaridade", nós podemos calcular as métricas em termos de distância, o que pode ajudar a avaliar o quão bem nosso modelo divide os objetos.

## Métricas para problemas de agrupamento

Você dividiu clientes em dois grupos usando K-means e agrupamento hierárquico. Como podemos determinar qual algoritmo fez um trabalho melhor? Assuma que você tem apenas duas características e o agrupamento final é assim:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.3.17PT.png)

Você vai olhar para como a "distinguibilidade" das classes é: Existe alguma diferença visual entre os objetos de diferentes agrupamentos nessas imagens?

Para atingir uma solução mais objetiva, vamos precisar de conceitos de **distância** **intracluster** e **intercluster**. Não é surpreendente que a distância intracluster é a distância entre objetos dentro de um agrupamento, enquanto distância intercluster é a distância entre objetos de diferentes agrupamentos. Quanto maior diferença entre distâncias intracluster e intercluster, melhor vai ser o agrupamento.

Existe uma métrica popular chamada **pontos de silhueta**. Existem outras métricas por aí, e elas são similares.

Os pontos de silhueta mostra até que ponto um objeto de um agrupamento é similar ao seu agrupamento, e não a outro. Aqui está a fórmula:

Silhueta\=1n∗∑ck∈C∑xi∈ckb(xi,ck)−a(xi,ck)max\[a(xi,ck),b(xi,ck)\]Silhueta = \\frac {1} {n} \* \\sum\_{c\_k\\in{C}}{\\sum\_{x\_i\\in{c\_k}}}{\\frac{b(x\_i,c\_k) - a(x\_i, c\_k)}{max\[a(x\_i, c\_k), b(x\_i, c\_k)\]}}Silhueta\=n1​∗∑ck​∈C​∑xi​∈ck​​max\[a(xi​,ck​),b(xi​,ck​)\]b(xi​,ck​)−a(xi​,ck​)​

onde `n` no número de observações,

`c_k` é um agrupamento específico,

`x_i` é um objeto específico de `c_k`,

`a(x_i, c_k)` é a distância média de `x_i` para os outros objetos no agrupamento (a medida da **densidade do agrupamento)**, e

`b(x_i, c_k)` é a distância média de `x_i` de objetos de outro agrupamento (a medida de **separabilidade** de um agrupamento).

A sintaxe desse método é a seguinte:

```
from sklearn.metrics import silhouette_score

silhouette_score(x_sc, labels)
```

Os dados de entrada são a matriz de característica normalizada ou padronizada e a lista de legendas preditas pelo nosso algoritmo de agrupamento.

Os pontos de silhueta levam valores de -1 a 1. Quanto mais perto de 1, melhor o agrupamento.

Métricas para Problemas de Aprendizado Não Supervisionado

Tarefa

Calcule a pontuação da silhueta para o agrupamento de avaliações no TripAdvisor.

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

import pandas as pd

  

from sklearn.preprocessing import StandardScaler

from sklearn.cluster import KMeans

from sklearn.metrics import silhouette\_score

  

  

\# leia os dados

travel \= pd.read\_csv('/datasets/tripadvisor\_review\_case\_us.csv')

print(travel.shape)

  

\# padronize os dados

sc \= StandardScaler()

x\_sc \= sc.fit\_transform(travel)

  

\# defina o modelo k\_means com 3 agrupamentos

km \= KMeans(n\_clusters \= 3)

\# preveja os agrupamentos para observações (o algoritmo atribui a eles um número de 0 a 2)

labels \= km.fit\_predict(x\_sc)

  

\# armazene rótulos de agrupamento no campo do nosso conjunto de dados

travel\['cluster\_km'\] \= labels

  

\# calcule a pontuação de silhueta para o agrupamento

print('Silhouette\_score: {:.2f}'.format(silhouette\_score(x\_sc, labels)))

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-28-04-838Z.md
### Última modificação: 2025-05-28 20:28:05

# Conclusão - TripleTen

Capítulo 3/6

Algoritmos de Aprendizado de Máquina

# Conclusão

Esse capítulo foi bem intenso.

Você se familiarizou com seis algoritmos populares. Você aprendeu como eles funcionam, viu o que eles produzem e descobriu seus pontos fracos e fortes. Você tem seis ferramentas vitais de ciência de dados. Agora é hora de aplicá-los para resolver uma tarefa real de negócios. Para isso, no próximo capítulo, você passará por todo o pipeline de desenvolvimento de um modelo de aprendizado de máquina. Você aprenderá como está estruturado e dominará os métodos que discutimos. A parte mais interessante virá no final: você fará uma previsão e explicará por que ela é boa. Veja por que os analistas estudam o aprendizado de máquina!

Links para ajudá-lo a dominar o material e aprofundar sua compreensão _(os materiais estão em inglês)_:

-   Informação geral sobre os algoritmos de ML:
    -   [https://www.ibm.com/topics/machine-learning-algorithms](https://www.ibm.com/topics/machine-learning-algorithms)
    -   [https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/lecture-videos/lecture-11-introduction-to-machine-learning/](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/lecture-videos/lecture-11-introduction-to-machine-learning/)
-   Regressão linear e gradiente descendente
    -   [https://machinelearningmastery.com/linear-regression-for-machine-learning/](https://machinelearningmastery.com/linear-regression-for-machine-learning/)
    -   [https://www.youtube.com/watch?v=qlLChbHhbg4&feature=youtu.be&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&t=50m16s](https://www.youtube.com/watch?v=qlLChbHhbg4&feature=youtu.be&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&t=50m16s)
-   Regressão logística
    -   [https://ml-cheatsheet.readthedocs.io/en/latest/logistic\_regression.html](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html)
-   Não analisamos algoritmos como KNN neste curso. Você pode ler mais sobre isso aqui:
    -   [https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/](https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/)
-   Árvores de decisão
    -   [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html)
    -   [https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb](https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb)
-   Conjuntos/floresta aleatória:
    -   [https://machinelearningmastery.com/random-forest-ensemble-in-python/](https://machinelearningmastery.com/random-forest-ensemble-in-python/)
-   Сonjuntos/gradient boosting:
    -   [https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)
-   O gradient boosting é geralmente realizado com CatBoost e LightGBM. Você pode ler mais sobre isso aqui:
    -   CatBoost: [https://catboost.ai/docs/concepts/about.html](https://catboost.ai/docs/concepts/about.html)
    -   LightGBM: [https://github.com/Microsoft/LightGBM](https://github.com/Microsoft/LightGBM)
-   Agrupamento:
    -   [https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/video-lectures/lecture-35-finding-clusters-in-graphs-second-project-handwriting/](https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/video-lectures/lecture-35-finding-clusters-in-graphs-second-project-handwriting/)
-   Reduzir a dimensionalidade:
    -   [https://www.geeksforgeeks.org/dimensionality-reduction/](https://www.geeksforgeeks.org/dimensionality-reduction/)

---

Faça download do [sumário do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/Resumo_do_Captulo_Algoritmos_do_Aprendizado.pdf?etag=36137ad408cd0aaf843d2a67c18e719e) e [folha de conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/PT_Folha_de_Concluses_Algoritmos_do_Aprendizado_de_Mquina.pdf?etag=f93e27e45df2553eba9b4e880df5f236) da Base de Conhecimento para que você possa consultá-los quando necessário.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-29-138Z.md
### Última modificação: 2025-05-28 20:29:29

# Introdução - TripleTen

Capítulo 4/6

Resolvendo Tarefas Relacionadas a Aprendizado de Máquina

# Introdução

Neste capítulo você vai passar por todo o pipeline, desde declarar a tarefa, até treinar um modelo e avaliar suas predições.

Você viu os algoritmos mais populares para resolver problemas do aprendizado de máquina e aprendeu quais métricas são usadas para avaliar e comparar diferentes modelos. Agora de hora de praticar e resolver casos reais envolvendo otimizar um serviço de compartilhamento de carros.

### O que você irá aprender:

-   Como selecionar o melhor modelo entre vários baseados nas suas métricas, depois de avaliar a tarefa e o volume de dados e selecionar a melhor maneira para processá-los
-   Como treinar um modelo
-   Como determinar se um modelo é bom e como ele pode melhorar o processo de negócios da empresa

Isso é, de fato, o pipeline completo para modelos preditivos.

Conforme você avança nisso, você vai solidificar o que aprendeu e seguirá todas as etapas que um analista trabalhando seguiria. Algumas lições vão te lembrar de fazer uma check-list: o que olhar para checar, o que comparar em cada etapa do desenvolvimento do protótipo.

Vamos estudar a etapa final em detalhes.

### Quanto tempo irá demorar:

9 lições de aproximadamente 10 a 15 minutos cada

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-30-420Z.md
### Última modificação: 2025-05-28 20:29:31

# Instruções da Tarefa - TripleTen

Capítulo 4/6

Resolvendo Tarefas Relacionadas a Aprendizado de Máquina

# Instruções da Tarefa

A etapa mais importante no desenvolvimento me um modelo de aprendizado de máquina é definir a tarefa adequadamente. Primeiro elabore-a em termos de negócios, depois translade-a para a linguagem matemática e do aprendizado de máquina.

# Formulando uma tarefa de negócios

As questões principais a serem respondidas são:

-   **O que estamos prevendo, e qual tarefa de negócios ela resolve?**
-   **Quais dados nós temos?**
-   **Quem vai usar nosso modelo e como (como ele é integrado no processo do negócio)?**
-   **Qual resultado você espera alcançar?**
    -   Como o problema foi resolvido até agora? Qual método foi mais bem-sucedido?
    -   Que efeito comercial o seu modelo poderia ter?
    -   Você tem recursos suficientes (tempo, pessoal, recursos computacionais e financeiros)?
    -   Qual métrica você vai usar para avaliar o desempenho do seu modelo?
    -   Existe alguma referência bem sucedida (exemplos existentes no mercado) do uso do aprendizado de máquina para tarefas parecidas?

# Definindo a tarefa e transladando-a para a linguagem do aprendizado de máquina

Se você for capaz de dar respostas claras para todas essas perguntas e confirmar que você realmente precisa usar o aprendizado de máquina, depois translade a instrução de negócios para linguagem de AM. Isso implica:

-   Identificar o tipo de tarefa (aprendizado supervisionado/não supervisionado, classificação/ registro/agrupamento)
-   Características:
    -   Quais dados selecionar como características?
    -   Qual é o tamanho da amostra?
    -   Qual é o tempo do período?
    -   O quão bons são os dados?
    -   O dado tem uma estrutura de tempo?
-   A métrica que você vai usar para otimizar e avalie o seu modelo
-   Se o modelo tem limitações:
    -   Velocidade
    -   Interpretabilidade do resultado
    -   Acurácia
    -   Tempo necessário para o desenvolvimento
-   O algoritmo que você vai usar (informados por todos os pontos anteriores)

Você vai considerar um caso da vida real e seguir os mesmos passos que um profissional de dados faria, desde definir as tarefas de negócios, até analisar os resultados dos modelos.

Você é um analista de um serviço bem-sucedido de compartilhamento de carros. Existem várias oportunidades empolgantes para aplicar o aprendizado de máquina lá. Em adição à avaliação de riscos associada com usuários e agrupamento de usuários ou prevendo receitas e rotatividade, existe uma outra tarefa importante: estimar o custo de combustível para diferentes modelos de carros. Os lucros dependem não apenas da receita dos clientes, mas também do custo para fornecer o serviço. Você pode aumentar lucros reduzindo esse último.

Infelizmente, as especificações fornecidas pelos fabricantes nem sempre são verdadeiras. Você quer coletar seus próprios dados e estimar o custo efetividade de vários modelos de carros. Um gestor diria que você está otimizando a estratégia de compras da empresa; em outras palavras, você vai identificar os carros beberrões de combustível que a empresa comprou para si mesma.

Pergunta

Qual das opções seguintes descreve a tarefa em questão?

Aumentando a receita

Aumentando o lucro

Exatamente. Cortando despesas. É sempre importante lembrar porque você está trabalhando duro com dados.

Calculando o consumo de combustível

Agrupamentos de usuários

Fantástico!

Pergunta

Quais valores você está predizendo?

O total de consumo de combustível por mês de cada carro

O custo efetividade de cada modelo na sua frota de carros

Consumo de combustível para cada 100 km para cada carro

Isso mesmo! Predizendo esse valor, e encontrando o número médio de quilômetros percorridos por mês, vai te ajudar a estimar o total de custos de combustível para a frota inteira.

Consumo de cada carro por hora

Excelente!

Pergunta

Qual dos processos abaixo descreve o que você está modelando?

A popularidade de cada modelo de carro

O potencial de conversão dentro da energia cinética, levando o projeto do carro e parâmetros em conta

Isso é bem perto do que você precisa. Em teoria você pode deduzir um consumo de combustível universal, baseado nas leis da física e todos os possíveis parâmetros de projeto para carros. Mas o aprendizado de máquina irá simplificar o processo.

Consumo de combustível por 100 km, expresso em litros

A frequência com a qual clientes usam carros de diferentes modelos, expresso em km/mês

Muito bem!

Pergunta

Quais dados serão bons para o seu modelo?

Contas de mídia social dos usuários

Telemetria minuto a minuto, dados sobre movimentos e parâmetros de carros

Dados sobre consumo de combustível e distância por mês

Dados sobre consumo e distância ao lado dos parâmetros de cada carro

Dados como esses fornecem a você uma variável objetivo (as medidas do período de tempo de consumo e distância não são importantes, mas eles devem corresponder entre si) e características.

Trabalho maravilhoso!

Pergunta

Vamos passar para formalização matemática. Com qual tipo de problema de aprendizado de máquina o nosso caso está mais provavelmente relacionado?

Aprendizado supervisionado, regressão

Correto. Nós temos as respostas, e eles irão provavelmente ser números ao invés de classes.

Aprendizado supervisionado, classificação binária

Aprendizado não supervisionado, agrupamento

Reforço de aprendizado

Excelente!

Pergunta

Os seus dados têm uma estrutura de tempo (o valor predito pode ser representado como séries temporais)?

Yes

Não

Não podemos dizer

De fato, você não pode dar uma resposta exata até você ver os dados em si. A resposta mais provável é não, e para simplificar você não precisa vincular os dados (modelo do carro, consumo, características) ao tempo.

Fantástico!

Pergunta

Qual métrica encaixa na sua tarefa?

Precisão e sensibilidade

Acurácia

R2, MAPE, RMSE

Correto. Essas são métricas clássicas de regressão.

Pontos de silhueta

Fantástico!

Pergunta

Qual algoritmo (ou sua implementação) irá encaixar na sua tarefa?

Regressão linear

Certo. Esse algoritmo simples é bom para seu primeiro modelo de regressão. Você pode também tentar floresta aleatória e gradient boosting.

Regressão logística

RandomForestClassifier

K-Means

Muito bem!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-31-737Z.md
### Última modificação: 2025-05-28 20:29:32

# AED: Analisando a Qualidade das Características - TripleTen

Teoria

# AED: Analisando a Qualidade das Características

Na primeira etapa, você definiu a tarefa e os dados que você vai usar, depois, crucialmente, obtenha os dados em si (talvez como um arquivo CSV); na próxima etapa, você vai estudar isso. Você pode vir imediatamente com um número de hipóteses sobre o processo que você está modelando, ou sobre suas características mais importantes.

No primeiro capítulo, nós descrevemos a essência da **análise exploratória de dados** que você terá que realizar.

Os objetivos desse tipo de exploração são:

1.  Avaliar a qualidade dos dados e o volume de processamento requerido
2.  Examinar as distribuições e correlações e anomalias diretas, se eles estão presentes
3.  Formular hipóteses iniciais relativas a características ou variáveis objetivo

Você deve seguir isso em ordem.

Aqui estão algumas questões úteis que você precisará responder quando avaliar a qualidade dos dados:

-   O quão grande é o conjunto de dados?
-   Quais características ele inclui? Se você está anonimizado, entrando anonimamente (o que acontece mais frequentemente em competições especializadas), é estudar melhor o que cada característica significa para se aprofundar no processo dos negócios.
-   Quais tipos de características você tem? Normalmente elas são **numéricas** ou **categóricas**. Nós trabalhamos com características do primeiro tipo nos capítulos anteriores. Essa são características cujo valor são expressos como números reais: por exemplo, temperatura expressa em graus, o número de visitas de um site, etc. Características categóricas podem tirar seus valores de um conjunto definido anteriormente. Por exemplo, a característica "City" pode ter seu valor dos conjuntos de dados "Kansas City," "Washington," "Tampa," etc. A maioria dos algoritmos exige que características categóricas sejam convertidas em numéricas, então quando realizar uma avaliação você pode ver imediatamente quantas características você terá que processar. Com uma característica categórica, é importante estimar o número de características que ela pode ter. Isso determina o quão grande o conjunto de dados será depois que você transformar as características.
-   A variável objetivo tem estrutura de tempo? (Nós precisamos predizer algo para uma série temporal?) Isso determina quais métodos para separação de dados entre de treino e conjuntos de validação que você será capaz de aplicar em etapas futuras e quais características derivativas (baseadas nas originais) você poderá usar.
-   Quantos valores ausentes existem? Esse é um ponto importante. Às vezes você vê rapidamente que algumas características ajudam bastante, mas infelizmente, apenas 1% das observações as incluem, então elas não podem ser usadas em um modelo. Nessa etapa, você pode decidir como processar os valores ausentes: removê-los do conjunto de dados, ou preenchê-los com informação do "passado" ou "futuro."

AED: Analisando a Qualidade das Características

Tarefa2 / 2

1.

Dados sobre consumo de combustível e características dos carros são coletados no arquivo CSV `/datasets/auto_cons_us.csv`. O nome da variável objetivo é "Consumo de combustível."

Leia os dados para ter uma impressão geral deles:

-   Salve os dados no DataFrame `cars`
-   Imprima o tamanho do DataFrame. Quantas observações e características você tem?
-   Imprima as primeiras 5 linhas

2.

Estude as informações sumárias no DataFrame usando o método `info()`. Encontre:

-   Se existem campo categóricos
-   Se alguma característica contém valores ausentes

99

1

2

3

4

5

6

7

8

9

10

11

12

13

import pandas as pd

  

\# leia os dados sobre carros e seu consumo de combustível do arquivo CSV

cars \= pd.read\_csv('/datasets/auto\_cons\_us.csv')

  

\# imprima seu tamanho e as primeiras 5 linhas

print(cars.shape)

print(cars.head())

  

cars.info()\# escreva seu código aqui

  

  

  

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-32-975Z.md
### Última modificação: 2025-05-28 20:29:33

# AED: Formulando Hipóteses - TripleTen

Teoria

# AED: Formulando Hipóteses

Uma vez que você tenha alguma noção das suas características, você pode dar uma olhada mais de perto para elas.

-   Estude a distribuição de características numéricas. Faça histogramas para elas. Isso permite que você veja se existe algum valor atípico e examine como as características se comportam. Você quer que elas sejam mais ou menos normais.
-   Se seus dados tem uma dimensão de tempo, faça gráficos de distribuição de valores ao longo do tempo. Isso é especialmente importante quando você está trabalhando com processos industriais. Isso vai deixar você ver se o comportamento das características mudou ao longo do tempo. Nós dissemos anteriormente que isso só faz sentido se usar um modelo quando você tem certeza que o comportamento das características não mudou muito. Caso contrário, os modelos não terão muita utilidade.
-   Use a distribuição de características e variáveis objetivo para determinar se valores ausentes estão presentes. Eles se destacam em histogramas e outros gráficos.
-   Calcule a matriz de correlação e construa um mapa de calor baseado nela.
    
    Você pode calcular a matriz de correlação com apenas uma linha do código usando o método do DataFrame `corr()`:
    
    ```
      cm = df.corr()  
      
    ```
    
    Agora a variável `cm` armazena a matriz de correlação. Para apresentar isso visualmente, use `heatmap()` da biblioteca seaborn:
    
    ```
      sns.heatmap(cm, annot = True, square=True)
      
    ```
    
    Nessa etapa você já pode dar uma olhada nas:
    
    -   Características que têm uma forte correlação com a variável alvo.
    -   Características que se correlacionam fortemente. Lembre-se que correlações mútuas são indesejáveis com modelos lineares, então preste bastante atenção nisso, se você escolher esses modelos.
    -   Como gráficos emparelhados são (podem ser úteis quando são juntados ao passo anterior, já que a correlação reflete apenas relações lineares, enquanto que gráficos emparelhados podem mostrar outras inter-relações):
        
        -   característica-característica
        -   característica-variável objetivo
        
        Gráficos emparelhados podem ser renderizados com o método da biblioteca `scatterplot()`:
        
        ```
          sns.scatterplot(df['Feature 1'], df['Feature 2'])
          
        ```
        

Uma vez que você passar um tempo meditando sobre seus gráficos, você será capaz de formular hipóteses preliminares:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/resources/moved___meditaciya_1580065558_1592820655.jpg)

-   Quais características devem ser as mais valiosas para o modelo, baseado em correlações?
-   Qual outra característica é útil pode ser criada com base nas existentes?
-   Em geral, o quão útil pode ser um modelo com dados dessa qualidade? Se muitas características são ruidosas com grande variância, você precisa saber disso desde o começo e não esperar que o AM faça milagres.

AED: Formulando Hipóteses

Tarefa

Vamos continuar a explorar os dados da tarefa anterior:

-   Calcule a matriz de correlação e a construa com base em um mapa de calor anotado
-   Use `scatterplot()` para construir correlações entre cada característica e a variável objetiv

Use esses gráficos para responder as seguintes questões:

-   Existe algum valor atípico claro entre as características?
-   Existem características dentro de uma correlação mútua?
-   Existem quaisquer características com uma forte correlação com a variável objetivo "Consumo de combustível"?

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

  

\# leia os dados na descrição do carro e seu consumo de combustível do arquivo csv

cars \= pd.read\_csv('/datasets/auto\_cons\_us.csv')

  

\# imprima seu tamanho e as primeiras 5 linhas

print(cars.shape)

print(cars.head())

  

\# estude as informações sumárias no conjunto de dados

print(cars.info())

  

\# construa e renderize uma matriz de correlação

cm \= cars.corr() \# calcule uma matriz de correlação

fig, ax \= plt.subplots()

  

\# faça um mapa de calor anotado para a matriz de correlação

sns.heatmap(cm, annot\=True)

ax.set\_ylim(7, 0) \# corrija as "irregularidades" dos campos do mapa de calor da última versão da biblioteca

plt.show()

  

\# faça diagramas da variável característica-objetivo de distribuição emparelhada para cada característica

for col in cars.drop('Fuel consumption', axis\=1).columns:

sns.scatterplot(x\=cars\[col\], y\=cars\['Fuel consumption'\])

plt.show()

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-34-338Z.md
### Última modificação: 2025-05-28 20:29:34

# Pré-processamento de dados - TripleTen

Teoria

# Pré-processamento de dados

Uma vez que você olhar para os dados e perceber as dificuldades que eles apresentam (valores atípicos, valores ausentes, características categóricas), você vai saber qual pré-processamento é necessário para aplicar o algoritmo que você escolheu para treinar o seu modelo.

Pré-processamento de dados geralmente consiste das seguintes etapas:

1.  Processar valores ausentes
2.  Processar valores atípicos
3.  Converter variáveis categóricas
4.  Padronizar dados (para modelos lineares ou modelos sensíveis para distância, tal como agrupamento)

Às vezes a seleção de características consideradas parte do pré-processamento, mas isso também pode ser feito nas etapas posteriores no desenvolvimento de um modelo.

Vamos olhar para cada etapa de cada vez.

## 1\. Processando valores ausentes

Existem várias maneiras de lidar com valores ausentes:

-   Simplesmente remover as observações com valores ausentes. Isso é efetivo quando existem vários dados e não muitos valores ausentes. Nesses casos, você pode sacrificar algumas poucas observações que têm valores ausentes. Geralmente, no entanto, essa é uma abordagem radical e não deve ser sua escolha.
-   Substituindo-as por valores próximos do passado. Você já conhece o método `fillna()` da parte do curso sobre pré-processamento de dados. Se seus dados estão relacionados com o tempo e um dos parâmetros aparece menos frequentemente do que outros, use `fillna()` com o parâmetro `'method'='ffill'`. Nós temos um motivo para concluir que valores ausentes devem ser preenchidos com valores do passado.
-   Substituindo-os por valores médios. Esse método, diferentemente desse acima, é típico de características que não tem uma estrutura de tempo ou nada para se "agarrar." Em outras palavras, se o valor de uma característica está ausente em uma observação, você pode assumir que não é a média (se não tem muito impacto nas predições) ou na mediana.
-   Substituindo as observações por valores nulos. Esse é outro método radical. Seja muito cuidadoso, especialmente se você vai trabalhar com algoritmos lineares: valores nulos podem afetar a relação linear inteira. Se os valores nulos são relacionados com a variável objetivo, esse tipo de processamento funciona bem para árvores, já que, ao separar o conjunto, elas podem explicar o fato de alguma característica não estar preenchida.
-   Em alguns casos (particularmente com valores categóricos) nós podemos substituir valores ausentes com o indicador "não especificado." Às vezes o fato de a pessoa não especificar sua cidade nativa em seu perfil é significante, e é melhor não substituir a lacuna com a resposta mais frequente.

A abordagem comum para valores é assim: tente processá-los com cuidado e certifique-se que os valores preenchidos não resultam em anomalias quando você está treinando o modelo. Se eles resultarem, você provavelmente terá que se livrar de características que têm muitos valores ausentes.

## 2\. Processando valore atípicos

Essa etapa é similar à anterior de várias maneiras.A maioria dos algoritmos é resistente a valores atípicos, mas alguns não são. Por exemplo, em modelos lineares, valores atípicos podem puxar a relação para um lado e para o outro. Olhe como apenas um ponto pode mudar a relação proposta pelo modelo da versão 1 para a versão 2:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.4.5PT.png)

O procedimento padrão para trabalhar com valores atípicos é assim:

-   Defina o limiar além de cada observação que é considerada um valor atípico (por exemplo, acima de 5 ou abaixo da porcentagem de 95).
-   Remova essas observações da amostra (bom para amostras grandes com um pequeno número de valores atípicos) ou os substitua pela média, ou valor máximo/mínimo para essa característica.

## 3\. Convertendo variáveis categóricas

Esses são os dois métodos mais comuns para transformar variáveis categóricas:

1.  Convertendo-as a valores numéricos sem qualquer transformação significativa. Existem duas abordagens conceituais diferentes:
    
    -   Substituindo categorias por valores numéricos (**codificação de legenda**)
    
    Por exemplo, convertendo os valores da **string** "Moscou," "Berlim" e "Paris" a **númericos** 0, 1 e 2. Aqui nós precisamos da classe `LabelEncoder()` do módulo sklearn.preprocessing:
    
    ```
         from sklearn.preprocessing import LabelEncoder
    
         print(df['City'].head())
    
         encoder =  LabelEncoder() # criando a variável da classe LabelEncoder
         df['City'] = encoder.fit_transform(df['City']) # usando a codificação para transformar strings em números
    
         print(df['City'].head())
         
    ```
    
    ```
         0             Moscow
         1             Berlin
         2              Paris
         3             Berlin
         4              Paris
         Name: City, dtype: object
         0    1
         1    2
         2    0
         3    2
         4    0
         Name: City, dtype: int64
         
    ```
    
    A característica "Cidade" é agora um valor numérico em vez de categórico. Mas tem uma desvantagem significativa: ele cria relações do tipo "maior"/"menor" para as categorias. Por exemplo, para o modelo, Moscou será "menor" do que Berlim, já que de legenda é 1, enquanto o de Berlim é 2. Isso não é tão bom para modelos baseados em inter-relações lineares (por exemplo, regressão linear), então outras abordagens tendem a ser usadas.
    
    -   Transformando um campo categórico em um conjunto de binárias (**codificação quente**)
        
        Aqui, em vez do campo "Cidade" nós teremos os campos "Moscou," "Berlim," "Paris," e outras, e essas novas características irão receber os valores 0 e 1. Para fazer isso acontecer, nós vamos precisar da função `pandas.get_dummies()`. Ela leva o DataFrame todo como input, identifica variáveis, as transforma em novas características (com nomes convencionais) chamada **variáveis fictícias**, e retorna um DataFrame atualizado::
        
        ```
          print(df.head())
          df = pd.get_dummies(df)
          print(df.head())
          
        ```
        
        ```
                      City         Population
          0           Moscow       11.9
          1           Berlin        3.8
          2            Paris        2.1
          3           Berlin        3.8
          4            Paris        2.1
             Population    City_Paris   City_Moscow            City_Berlin
          0       11.9             0             1                      0
          1        3.8             0             0                      1
          2        2.1             1             0                      0
          3        3.8             0             0                      1
          4        2.1             1             0                      0
          
        ```
        
        Para fazer essa transformação, você ainda pode usar a classe OneHotEncoder do módulo sklearn.preprocessing, mas não é conveniente.
        
        Substituições binárias de características categóricas são quase sempre uma boa ideia. Mas se tiverem muitas delas ou se uma das características tiver muitos valores únicos (ou as duas coisas), transformação fará a matriz (DataFrame) muito grande. Isso cria problemas para quase todos os modelos de aprendizado de máquina, mesmo ignorando que grandes matrizes significam computações lentas. Então aqui nós podemos aplicar codificação de legenda ou simplesmente remover características particulares.
        
2.  Criando novas características baseadas nas existentes. Por exemplo, se nós tivermos uma característica que consiste nos comentários do texto, nós podemos usá-la para gerar novas características como comprimento da mensagem, repetição de palavras, tom negativo ou positivo, etc.
    

## 4\. Padronizando dados

No segundo capítulo você aprendeu que muitos algoritmos de AM trabalham melhor em dados padronizados, nas quais os valores de características correspondem a distribuição normal padrão.

A padronização é totalmente obrigatória em duas áreas do AM:

-   Regressão linear
-   Agrupamentos e métodos baseados em distância mútua entre objetos

Padronizar uma certa característica envolve as seguintes etapas:

-   Calcular a característica média na amostra e decrescer cada observação por seu valor (para centralizar os valores em torno de 0)
-   Dividir cada valor resultante pelo desvio padrão (para escalar a dispersão para um valor de 1)

Assim, no pipeline do modelo de desenvolvimento, a padronização fica assim:

-   Divida os dados em conjuntos de treino e validação.
    
    ```
      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
      
    ```
    
    Nesse exemplo nós temos a clássica divisão 80/20.
    
-   Treine e aplique a "padronização" no conjunto de treino.
    
    ```
      scaler = StandardScaler()
      X_train_st = scaler.fit_transform(X_train)
      
    ```
    
    Aqui o padronizador relembra a média e desvio padrão do seu conjunto de treino e, levando esse conhecimento em conta, aplique a padronização a ele.
    
-   Aplique padronização aos dados de validação.
    
    ```
      X_test_st = scaler.transform(X_test)
      
    ```
    

Aqui você padroniza os dados de teste e a matriz (não o DataFrame) com os valores de características transformados na variável `X_test_st`. Tenha cuidado: você está transformando valores no conjunto de validação baseada no valor médio e desvio padrão encontrado no outro conjunto, os dados de treino.

Isso pode não parecer lógico, e depois de tamanha transformação, a distribuição dos dados de validação alterados pode não combinar com a distribuição normal.

Existem duas razões para isso. Primeiro, quando você testa um modelo na vida real, você não conhece a distribuição futura dos valores de características. Segundo, se as distribuições do conjunto de dados (e assim o desvio padrão e a média) forem muito diferentes do que estava no conjunto de treino, seu modelo não vai trabalhar muito bem.

Pré-processamento de dados

Tarefa3 / 3

1.

Durante a fase da AED, você aprendeu que a variável "Potência do motor" tem seis valores vazios. Isso não é muito comparado com o total da amostra (≈1.5%), então você pode remover essas observações usando o método `dropna()`. Depois imprima o tamanho do DataFrame atualizado.

2.

Análise exploratória de dados também te permite saber que você tem uma característica categórica, "Origin".

-   Escolha o método para manusear a variável categórica que se adequa à tarefa.
-   Aplique esse método.
-   Imprima o tamanho do DataFrame atualizado e as primeiras cinco linhas.

3.

Para ser capaz de usar regressão linear para treinar o seu modelo, você precisa padronizar as características.

-   Armazene as características na variável `X` e a variável objetivo em `y`
-   Use a função `train_test_split()` para dividir os dados entre conjuntos de treino e "teste" (validação) com uma proporção de 80 para 20
-   Treine `StandardScaler()` e aplique-a à matriz de treino de características usando o método `fit_transform()`. Depois imprima os primeiros cinco elementos da matriz com valores padronizados. Perceba: o método `head()` não serve, pois `fit_transform()` retorna um vetor em vez de um DataFrame
-   Padronize o conjunto de validação

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

import pandas as pd

import seaborn as sns

from sklearn.preprocessing import StandardScaler

from sklearn.model\_selection import train\_test\_split

import matplotlib.pyplot as plt

  

  

#read the data on the car description and their fuel consumption from the csv file

cars \= pd.read\_csv('/datasets/auto\_cons\_us.csv')

  

#print its size and the first 5 rows

print(cars.shape)

print(cars.head())

  

#remove the rows with missing values and print the new size of the DataFrame

cars.dropna(inplace \= True)

print(cars.shape)

  

#store the DataFrame with transformed features and print the new size and first 5 rows

cars \= pd.get\_dummies(cars)

print(cars.shape)

print(cars.head())

  

#divide the data into features (the X matrix) and a target variable (y)

X \= cars.drop(columns \= \['Fuel consumption'\])

y \= cars\['Fuel consumption'\]

  

#divide the data into train and test

X\_train, X\_test, y\_train, y\_test \= train\_test\_split(X, y, test\_size\=0.2)

  

  

#create a StandardScaler object and apply it to train set

scaler \= StandardScaler()

X\_train\_st \= scaler.fit\_transform(X\_train) #train the scaler and transform the matrix for train set

print(X\_train\_st\[:5\])

  

#apply standardization to the feature matrix for test set

X\_test\_st \= scaler.transform(X\_test)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-35-657Z.md
### Última modificação: 2025-05-28 20:29:36

# Aleatoriedade e Divisão de Tempo - TripleTen

Capítulo 4/6

Resolvendo Tarefas Relacionadas a Aprendizado de Máquina

# Aleatoriedade e Divisão de Tempo

Com o pré-processamento pronto, você pode treinar o modelo. Seu objetivo não é apenas construir um modelo, mas escolher o melhor modelo entre vários. Isso pode ser feito comparando as métricas que você obtiver de diferentes modelos para validação dos dados. Aqui nós devemos responder a seguinte pergunta:

1.  Como você vai separar os dados para avaliar o desempenho do modelo com a validação de dados?
2.  Quais modelos você vai escolher?
3.  Quais métricas você vai usar para julgar modelos?

Nesta lição, você vai aprender a responder à primeira questão.

# Decidindo como dividir os dados

Existem duas abordagens para a divisão:

-   Aleatoriamente
-   Baseada no tempo

Você pode usar divisão aleatória quando você não precisa predizer séries temporais. Em outras palavras, você ignora o impacto que as observações vizinhas têm uma sobre a outra.

Imagine que você precisa prever a bilheteria anual de filmes que serão lançados nos Estados Unidos no próximo mês. Você tem dados dizendo o orçamento de cada filme, gênero, produtor, atores principais, e média de ganhos por filmes que eles estrelam, mais a média da avaliação de filmes por diretor. Cada predição para uma observação (filme) é absolutamente independente de outras predições. É claro, se dois filmes sobre missões em Júpiter forem lançados ao mesmo tempo, isso pode afetar os ganhos de ambos (especialmente e um for lançado vários dias depois do outro). Mas esse não é o fator principal. Suas observações provavelmente são independentes de tempo, então você consegue aplicar divisão aleatória e testar o modelo de dados históricos. Nesse caso, o padrão `train_test_split()` é usado.

Adote uma abordagem baseada em tempo quando você estiver predizendo o valor da variável objetivo para observações consecutivas. Vamos dizer que você precise predizer a receita de locação de um determinado filme. Aqui sua observação é a receita da locação em um certo mês.

Nesse tipo de tarefas nós predizemos o valor da variável objetivo "N meses no futuro" para cada observação: "um mês no futuro" (como em nossa tarefa), "dois meses no futuro," etc. A característica será os valores atuais da variável mais outras características baseadas em tempo, incluindo aquelas relacionadas diretamente à variável objetivo:

-   Valores do passado (lags). Por exemplo, os valores de características e variáveis objetivo 1, 3, 5, 10, 30, ou qualquer outro número de meses atrás (ou minutos, dependendo da granularidade dos dados — os intervalos de tempo).
-   O valor de funções agregadas (soma, média, desvio padrão, mediana) para características e variáveis objetivo sobre janelas de tempo deslizantes. Por exemplo, a soma de valores dos últimos três meses, a soma de valores dos últimos cinco meses, o valor médio dos últimos três meses, ou o valor médio dos últimos cinco meses.

Cada observação (ponto no tempo) está vinculado à informações do passado, então aqui a divisão aleatória não vai servir. Veja o que acontece se nós a usarmos par tentar predizer os ganhos de bilheteria:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.4.6PT.png)

Nós escolhemos o ponto vermelho para validação e os verdes para treinamento. A granularidade é um mês. A variável objetivo (ganhos de bilheteria) deve ser predito com um mês de antecipação. O ponto verde circulado (no mês t) está seis meses à frente do ponto de validação. Mas assuma que as características incluem o valor da característica e a **variável objetivo** de atrasos de cinco meses. Você consegue adivinhar onde o problema está? Como o modelo está sendo treinado, ele recebeu o valor da variável objetivo no ponto da validação (ele espiou a resposta). Quando o modelo foi treinado no mês _t_ (previsão de ganhos para o mês _t_+1), uma das características era o atraso da variável objetivo: o valor do total de ganhos do filme cinco meses atrás. Então nós o prevemos a partir do ponto vermelho (seis meses atrás). O algoritmo pode simplesmente "lembrar" desse valor do passado. Mas nós precisamos realizar a validação para reproduzir o desempenho do modelo em condições reais. Na verdade, um algoritmo treinado nos dados APENAS do passado (sem qualquer possibilidade de considerar atrasos "do futuro") pode ser aplicado APENAS a dados futuros (isso é, o último ponto na imagem). Você pode validar o modelo usando dados que ele viu enquanto era treinado. Isso vai resultar em métricas superestimadas (com diferença positiva).

Assim é como as coisas devem ser:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.4.6.3PT.png)

Quando você está treinando usando dados de séries temporais e usando validação para modelar um verdadeiro conjunto de redenção do "futuro" para um algoritmo "do passado", você deve usar um **tempo** (ou uma**separação** **baseado**em**\-**tempo\*\*). Isso implica fazer o conjunto de validação de observações do final do período histórico disponível, em vez de através dos pontos no tempo aleatoriamente selecionados. Para esse tipo de divisão a proporção de treino/val é 80/20 ou 70/30, como na separação aleatória.

E mais uma vez, abaixo está como ele NÃO deve ser:

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.4.6.2PT.png)

Para tal divisão, nós podemos dizer que o modelo foi treinado nos dados "do futuro" com respeito ao ponto de validação. Na vida real isso é impossível. Devido à estrutura de tempo dos dados, com uma divisão tão pobre, o modelo vê pontos de validação intermediários. Mas o objetivo da avaliação do modelo é olhar para o seu desempenho em condições da vida real, onde somos incapazes de usar pontos do futuro em conta durante o treinamento.

Pergunta

Você precisa predizer mensalmente a rotatividade para todos os membros para dois anos a partir de agora. Qual tipo de divisão você irá aplicar?

Divisão aleatória

Divisão de tempo

Isso mesmo! Essa é uma típica tarefa de previsão de séries temporais, então nós usamos divisão de tempo. Esse tipo de coisa aparece com frequência em finanças.

Seu entendimento sobre o material é impressionante!

Pergunta

Você precisa predizer a rotatividade de uma cliente em particular um mês a partir de agora. O conjunto de treino contém observações feitas no curso de um ano. Para cada observação, você tem um conjunto de características pelo mês _n_ e a rotatividade atual para o mês seguinte. Qual tipo de divisão você irá aplicar?

Divisão aleatória

Isso mesmo! Mesmo que quizzes com duas opções não sejam empolgantes, tente se lembrar desses casos.

Divisão de tempo

Você conseguiu!

Pergunta

Você precisa predizer se pílulas lançadas por uma empresa farmacêutica contém uma certa substância útil. Para características, você selecionou leituras minuto a minuto de medidores e valores minuto a minuto das variáveis objetivo. Qual método você vai aplicar?

Divisão aleatória

Divisão de tempo

Isso mesmo. Nesses casos industriais é melhor usar divisão de tempo, como nossas observações consistem em leituras regulares dos instrumentos e aparelhos. Eles têm uma estrutura de tempo bem definida.

Fantástico!

Pergunta

Qual tempo de divisão você usaria para predizer o consumo de combustível no cenário descrito em lições anteriores?

Divisão aleatória

Isso mesmo! Seus dados não têm uma estrutura de tempo, então separação aleatória com `train_test_split()` é bastante razoável.

Divisão de tempo

Muito bem!

Pergunta

Você tem que predizer a probabilidade de um acidente na Seção 2 na fábrica de leite Dairyland. Para qual tipo de variável objetivo e de divisão os recursos a seguir apontam? chamber\_2\_pressure; chamber\_2\_temperature; chamber\_2\_pressure\_5\_min\_lag; chamber\_2\_temperature\_5\_min\_lag; chamber\_2\_pressure\_30\_min\_std; chamber\_2\_temperature\_30\_min\_std

Observações independentes no tempo, divisão aleatória

Séries temporais, divisão de tempo

Isso mesmo! A validação do modelo do aprendizado de máquina para a fabricação de manteiga para Dairyland está em boas mãos.

Trabalho maravilhoso!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-36-776Z.md
### Última modificação: 2025-05-28 20:29:37

# Selecionando Métricas - TripleTen

Capítulo 4/6

Resolvendo Tarefas Relacionadas a Aprendizado de Máquina

# Selecionando Métricas

Para avaliar seus modelos e selecionar o melhor, você precisa definir a métrica que descreve a essência da sua tarefa.

Existem duas coisas importantes que ainda não discutimos:

1.  Tendo selecionado métrica prioritária, você pode sintonizar o processo de treinamento para isso otimizar aquela métrica. Por exemplo, se você está trabalhando em um problema de regressão e usando `RandomForestRegressor()`, você pode especificar a métrica diretamente quando otimizar o treinamento do algoritmo:
    
    ```
     model = RandomForestRegressor(criterion='mae')
     
    ```
    
    Isso vai fazer seu algoritmo ser menos sensível para casos em que o modelo comete grandes erros para observações específicas; ou seja, você otimiza o treinamento minimizando os erros do valor absoluto. Mas se você define o modelo dessa forma:
    
    ```
     model = RandomForestRegressor(criterion='mse')
     
    ```
    
    ... depois o modelo vai receber "multas" maiores por erros, já que aqui nós elevamos ao quadrado a diferença entre os valores reais e preditos, ao invés de simplesmente pegar os valores absolutos.
    
    Nesse caso, essas são os únicos dois valores possíveis do parâmetro `criterion`. Outros algoritmos têm mais opções. Às vezes, ao contrário daqui, sua métrica objetivo não estará entre aqueles que podem ser ajustados, para que você tenha que encontrar uma métrica próxima da sua. Imagine você revolvendo uma classificação binária do problema. Você selecionou precisão como a métrica básica, e essa escolha é baseada nos objetivos dos negócios. Os algoritmos são treinados pelo gradient boosting com `XGBClassifier()` da biblioteca XGBoost. Esse algoritmo tem o parâmetro `eval_metric` que permite que você ajuste o treinamento do modelo para otimizar a métrica escolhida. No entanto, precisão não uma opção válida. Então nesse caso, nós podemos selecionar `auc`, que irá visar melhorar de maneira geral seu classificador, e então ajustar a precisão usando o limite.
    
    Por isso é importante ler a descrição do algoritmo na documentação e descobrir como otimizá-lo.
    
2.  Para algumas tarefas de negócios, você terá que criar sua própria métrica baseada nas existentes para evoluir o modelo. Um exemplo simples e comum é a "quota de respostas falsas".Na verdade, essa é a proporção do PF (respostas positivas falsas) ao tamanho da sua amostra. Se cada resposta custar algo a você, e depois você querer que seu modelo te dê o menor número possível de respostas falsas. Como `precision_score`, essa métrica caracteriza a precisão das respostas, e é calculada usando aquela contagem:
    
    ```
     false_positive_rate = 1 - precision_score(y_true, y_pred)
     
    ```
    

Pergunta

Você está predizendo a possibilidade que um paciente tenha uma perigosa e rara doença usando classificação binária (0 - saudável, 1 - doente). Uma condição importante: exames adicionais e em profundidade não representam riscos aos pacientes. Qual métrica matemática você deve escolher para que ela corresponda à sua tarefa?

Precisão

Recordar

Isso mesmo! Não é um grande problema se o modelo predizer que um paciente está doente e os exames de acompanhamento mostrar que eles estão certos. Perder um caso real é muito pior.

EQM

Acurácia

Você conseguiu!

Pergunta

Você está predizendo receitas de clientes em vez de definir o direcionamento das prioridades. Quando a sua empresa espera receitas altas de um cliente, ela gasta mais recursos de marketing nelas já que esses investimentos são mais efetivos. Qual parâmetro você vai selecionar para `RandomForestRegressor()` nesse caso?

`criterion = 'mae'`

`criterion = 'mse'`

Certo. É importante que o seu modelo trabalhe bem tanto para a média de clientes e para os extremos da sua amostra — tanto os mais promissores como a maioria dos clientes deficitários.

`criterion = 'r2'`

`criterion = 'f1_score'`

Seu entendimento sobre o material é impressionante!

Pergunta

Qual métrica encaixa melhor com os objetivos dos negócios da empresa de caronas, levando em conta a AED que nós fizemos na tarefa de consumo de combustível nas lições anteriores?

EAM

EAM

R2

Todas essas métricas servem

Isso mesmo! Esse ponto de vista dos negócios é mais interessante comparar EAM, EQM (se não existem valores atípicos, eles são equialentes), e MAPE, como eles dão uma ideia dos valores reais (litros/100km, que pode ser convertido para dólares por litro, ou galão). Mas R2 pode ser útil para comparar modelos também, tudo o mais sendo igual.

Você conseguiu!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-38-079Z.md
### Última modificação: 2025-05-28 20:29:38

# Selecionando o Modelo de Aprendizado de Máquina - TripleTen

Capítulo 4/6

Resolvendo Tarefas Relacionadas a Aprendizado de Máquina

# Selecionando o Modelo de Aprendizado de Máquina

Até agora nós olhamos de perto para os tipos de problemas mais comuns de AM:

-   Aprendizado Supervisionado:
    -   Classificação (classificação binária em particular)
    -   Regressão
-   Aprendizado não supervisionado:
    -   Agrupamento

Além disso, prendemos os algoritmos mais populares para abordar a maioria dos problemas, assim como implementações populares deles em várias bibliotecas. Aqui está a tabela correspondente "tipo de problema - algoritmos possíveis - implementações." Ela tem informações novas para você:

Tipo de problema

Algoritmos possíveis

Implementações

Aprendizado supervisionado: classificação binária

Regressão logística

sklearn.linear\_model.LogisticRegression()

Árvore de decisão

sklearn.tree.DecisionTreeClassifier()

Floresta aleatória

sklearn.ensemble.RandomForestClassifier()

Gradient boosting

sklearn.ensemble.GradientBoostingClassifier(), xgboost.XGBClassifier

Aprendizado supervisionado: regressão

Regressão linear

sklearn.linear\_model.LinearRegression()

Regressão linear (regularização L1)

sklearn.linear\_model.Lasso()

Regressão linear (regularização L2)

sklearn.linear\_model.Ridge()

Árvore de decisão

sklearn.tree.DecisionTreeRegressor()

Floresta aleatória

sklearn.ensemble.RandomForestRegressor()

Gradient boosting

sklearn.ensemble.GradientBoostingRegressor(), xgboost.XGBRegressor()

Aprendizado não supervisionado: agrupamento

K-Means

sklearn.cluster.KMeans()

Agrupamento acumulativo

sklearn.cluster.AgglomerativeClustering()

DBSCAN

sklearn.cluster.DBSCAN()

DBSCAN e gradient boosting estão além do escopo desse curso. Se você quer aplicá-los no seu trabalho, nós recomendamos que você estude a documentação por conta própria.

Quando aparecer com um problema, você tem uma ampla variedade de algoritmos populares para escolher. O seu objetivo é selecionar o mais apropriado.

Pergunta

Quais algoritmos você pode usar para resolver a tarefa sobre predição de consumo de combustíveis?

XGBClassifier(), LogisticRegression(), RandomForestClassifier(), DecisionTreeClassifier()

AgglomerativeClustering(), KMeans()

RandomForestRegressor(), LinearRegression(), Lasso(), Ridge(), DecisionTreeRegressor(), GradientBoostingRegressor(), XGBRegressor()

Você pode selecionar desses sete modelos (até mais, na verdade) para resolver sua tarefa!

LogisticRegression(), LinearRegression(), DecisionTreeClassifier(), GradientBoostingRegressor(), KMeans()

Fantástico!

Pergunta

Você está predizendo para uma rede de lojas de cosméticos (as mesmas para as quais você analisou a atividade dos usuários no Facebook). Quais algoritmos você pode aplicar?

XGBClassifier()

AgglomerativeClustering()

LogisticRegression()

Ridge()

Certo. Isso funciona para tarefas de regressão e ajuda a minimizar os efeitos negativos da multicolinearidade entre as características.

Seu entendimento sobre o material é impressionante!

Pergunta

Você vende cosméticos novos e clássicos online. Para cada cliente, você tem dados como "tempo desde a assinatura," "gênero indicado na assinatura," "idade indicada na assinatura," "número de quadrinhos de ficção científica foram comprados," "número de quadrinhos com tema militar foram comprados," etc. Você gostaria de identificar grupos típicos de clientes para que você possa oferecer a eles recomendações direcionadas e gerenciar a efetividade do orçamento de propagandas. Qual algoritmo você vai escolher?

XGBRegressor()

KMeans()

De fato, esse é um problema clássico de segmentação de cliente, então agrupamento k-means é uma boa escolha.

DecisionTreeClassifier()

RandomForestClassifier()

Fantástico!

Pergunta

Você trabalha na gestão de um curso em uma escola sem fins lucrativos que vem coletando e analisando dados sobre estudantes. Você vai usar as características "número de classes atendidas," "média das notas nas tarefas," e outras para aprender se seus estudantes vão conseguir o total de créditos para o semestre em questão. Qual algoritmo é apropriado?

GradientBoostingClassifier()

Esse vai funcionar bem, assim como `RandomForestClassifier()` e `LogisticRegression()` de sklearn. Você ainda não trabalhou com essa implementação de gradient boosting, mas você definitivamente vai no futuro.

KMeans()

DBSCAN()

Lasso()

Trabalho maravilhoso!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-39-385Z.md
### Última modificação: 2025-05-28 20:29:40

# Treinando Modelos e Selecionando o Melhor - TripleTen

Teoria

# Treinando Modelos e Selecionando o Melhor

Agora que você completou todas as etapas preliminares, você está importante para a mais importante: treinar modelos, fazer predições, avaliar métricas e selecionar o melhor modelo.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.4.9PT.png)

Treinando Modelos e Selecionando o Melhor

Tarefa2 / 2

1.

Você pode comparar modelos e escolher o melhor baseado na divisão dentro dos dados de treino/validação, as opções disponíveis com respeito aos algoritmos, e métricas relevantes para a tarefa em questão.

-   Faça uma lista de modelos para os algoritmos:
    -   Lasso()
    -   Ridge()
    -   DecisionTreeRegressor()
    -   RandomForestRegressor()
    -   GradientBoostingRegressor()
-   Escreva uma função que irá treinar cada modelo no conjunto de treino, faça uma predição para os dados de teste e imprima as métricas EQM, EAM, R2 e MAPE. Para relembrar as fórmulas verifique a lição "Métricas de Regressão" do capítulo "Algoritmos de Aprendizado de Máquina".

2.

Os modelos podem ser executados em ciclos, como outros tipos de objetos. Tente você:

-   Faça um ciclo para imprimir as métricas para todos os algoritmos
-   Estude os resultados. Qual algoritmo tem as melhores?

99

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

import pandas as pd

import seaborn as sns

from sklearn.preprocessing import StandardScaler

from sklearn.model\_selection import train\_test\_split

from sklearn.linear\_model import Lasso, Ridge

from sklearn.tree import DecisionTreeRegressor

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

import matplotlib.pyplot as plt

from sklearn.metrics import mean\_absolute\_error, mean\_squared\_error, r2\_score

  

  

\# leia os dados sobre descrição de carro e seu consumo de combustível do arquivo csv, imprima seu tamanho e as primeiras 5 linhas

cars \= pd.read\_csv('/datasets/auto\_cons\_us.csv')

  

\# remova as linhas com valores ausentes do conjunto

cars.dropna(inplace \= True)

  

\# armazene o DataFrame com as características transformadas

cars \= pd.get\_dummies(cars)

  

\# divida os dados em características (a matriz X) e a variável objetivo (y)

X \= cars.drop(columns \= \['Fuel consumption'\])

y \= cars\['Fuel consumption'\]

  

\# divida os dados sobre treinos e testes

X\_train, X\_test, y\_train, y\_test \= train\_test\_split(X, y, test\_size\=0.2)

  

  

\# crie um objeto StandardScaler e aplique-o no conjunto de treino

scaler \= StandardScaler()

X\_train\_st \= scaler.fit\_transform(X\_train) \# treine o escalonador e transforme a matriz para o conjunto de treino

  

\# aplique a padronização à matriz de característica para testar o conjunto

X\_test\_st \= scaler.transform(X\_test)

  

\# defina a lista de modelos

models \= \[Lasso(), Ridge(), DecisionTreeRegressor(), RandomForestRegressor(), GradientBoostingRegressor()\]

  

\# a função que calcula MAPE

def mape(y\_true, y\_pred):

y\_error \= y\_true \- y\_pred

y\_error\_abs \= \[abs(i) for i in y\_error\]

perc\_error\_abs \= y\_error\_abs / y\_true

return (perc\_error\_abs.sum() / len(y\_true))

  

\# a função que leva o modelo e dados como entrada e saída de métricas

def make\_prediction(m, X\_train, y\_train, X\_test, y\_test):

model \= m

model.fit(X\_train, y\_train)

y\_pred \= model.predict(X\_test)

print('MAE:{:.2f} MSE:{:.2f} MAPE:{:.2f} R2:{:.2f} '.format(mean\_absolute\_error(y\_test, y\_pred),

mean\_squared\_error(y\_test, y\_pred),

mape(y\_test, y\_pred),

r2\_score(y\_test, y\_pred)))

  

\# escreva um ciclo que produza métricas de acordo com a lista de modelos

for i in models:

print(i)

make\_prediction(m\=i,X\_train \= X\_train\_st, y\_train\= y\_train,

X\_test\=X\_test\_st, y\_test \= y\_test)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-41-643Z.md
### Última modificação: 2025-05-28 20:29:42

# A Importância das Características - TripleTen

Teoria

# A Importância das Características

Você selecionou o melhor modelo. Ele funciona e ainda faz predições. Mas para que elas sejam significativas, aqueles que pretendem usar essas predições precisam confiar nelas. Mais importante, você mesmo(a) deve confiar nelas. Consequentemente, você deve entender porque seu modelo funciona.

Aqui o foco muda para a **interpretabilidade** do modelo.

Normalmente, os analitas preparam muitos dados e os mandam para a caixa preta de um algoritmo, que emitem a predição. Mas nós precisamos de pelo menos um entendimento básico não apenas do "que" o modelo disse, mas também "como" e "porque" ele fez dessa maneira. Quais características ele considerou importante quando computou uma predição em particular? Isso é crucial para que o fundamento do processo seja modelado e para avaliar a performance do modelo de forma geral. Assim, nós precisamos prestar atenção especial para análise da **importância da característica**.

As coisas são mais simples com modelos lineares. Aqui o treinamento envolve selecionar os coeficientes apropriados para cada característica. Desde que essas características foram padronizadas, o valor do coeficiente para cada uma delas te diz a sua importância. Vamos dizer que enquanto está sendo treinado, o modelo seleciona ótimos coeficientes para a equação:

y\=132+37∗x1+105∗x2+0.3∗x3−79∗x4y = 132 + 37\*x\_1+105\*x\_2+0.3\*x\_3-79\*x\_4y\=132+37∗x1​+105∗x2​+0.3∗x3​−79∗x4​

O que acontece se mudarmos o valor da terceira característica, x₃, de 0 para 1? O resultado não vai mudar consideravelmente: mudou apenas 0.3, o que não é muito. Isso significa que a terceira característica não tem um impacto forte na predição do modelo, e sua importância não é grande. Mas a segunda característica é diferente: mudando mesmo que seja pouco (por exemplo: de 0 para 1) vai afetar significativamente o resultado, então essa característica é muito importante para o modelo. O coeficiente da quarta característica é -79. Já que é negativa, se nós aumentarmos o valor da característica, o valor predito vai diminuir, e por uma quantia justa, já que o valor absoluto do coeficiente é alto. A quarta característica é importante também.

Os coeficientes da regressão linear são ordenados no atributo `.coef_` do modelo treinado:

```
feature_weights = model.coef_
```

Aqui nós armazenamos os coeficientes de todas as características na variável `feature_weights`.

Para imprimir o coeficiente nulo (o valor da predição quando o valores de todas as características for 0), use o atributo `.intercept_`:

```
weight_0 = model.intercept_
```

Ótimo. Nós descobrimos coisas para os modelos lineares: nós precisamos olhar para os valores absolutos dos coeficientes. Mas e essas árvores? Se existe uma árvore, nós geralmente usamos o parâmetro que predomina mais forte nas predições em nós subsequentes. A importância da característica é definida por um ranking e vez de coeficientes.

Uma nuance importante: quanto mais próxima uma característica está da base da árvore (o topo, na imagem abaixo), maior será sua importância e impacto em resultados preditos.

A imagem mostra uma árvore de decisão simples para decidir se vai passar o natal na cidade, ou não. É baseado em duas características: se a família dessa pessoa e os amigos vão ficar na cidade e o quanto a pessoa gosta de viajar (em uma escala de 0 a 10). Podemos ver que a família e os amigos determinam a decisão em grande medida As exceções são casos raros quando a pessoa realmente odeia ou ama muito viajar.

![image](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/13.4.10PT.png)

A importância das características para árvores (tanto para os algoritmos de regressão e classificação) é armazenado no atributo `.feature_importances_` do modelo treinado.

```
importances = model.feature_importances_
```

Determinar a importância não é a tarefa mais óbvia mesmo quando nós temos uma única árvore. Quando nós temos exemplos de modelos (gradient boosting e floresta aleatória), é um campo inteiro de estudo, para o qual algoritmos especiais são desenvolvidos. Mas na vida real você provavelmente não terá de ir tão fundo em aspectos teóricos da importância da computação.

As implementações da floresta aleatória e gradient boosting em sklearn também armazenam o valor das características no atributo `.feature_importances_`.

A Importância das Características

Tarefa

-   Treine o melhor modelo novamente e faça uma predição.
-   Crie um DataFrame contendo o valor da característica e imprima as características em ordem decrescente por importância.
-   Qual característica acabou sendo a mais importante para o modelo?

999

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

67

68

import pandas as pd

import seaborn as sns

from sklearn.preprocessing import StandardScaler

from sklearn.model\_selection import train\_test\_split

from sklearn.linear\_model import Lasso, Ridge

from sklearn.tree import DecisionTreeRegressor

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

import matplotlib.pyplot as plt

from sklearn.metrics import mean\_absolute\_error, mean\_squared\_error, r2\_score

  

  

\# leia os dados sobre descrição de carro e seu consumo de combustível do arquivo csv, imprima seu tamanho e as primeiras 5 linhas

cars \= pd.read\_csv('/datasets/auto\_cons\_us.csv')

  

  

\# remova as linhas com valores ausentes do conjunto

cars.dropna(inplace \= True)

  

\# armazene o DataFrame com as características transformadas

cars \= pd.get\_dummies(cars)

  

\# divida os dados em características (a matriz X) e a variável objetivo (y)

X \= cars.drop(columns \= \['Fuel consumption'\])

y \= cars\['Fuel consumption'\]

  

\# divida os dados sobre treinos e testes

X\_train, X\_test, y\_train, y\_test \= train\_test\_split(X, y, test\_size\=0.2)

  

  

\# crie um objeto StandardScaler e aplique-o no conjunto de treino

scaler \= StandardScaler()

X\_train\_st \= scaler.fit\_transform(X\_train) \# treine o escalonador e transforme a matriz para o conjunto de treino

  

\# aplique a padronização à matriz de característica para o conjunto de teste

X\_test\_st \= scaler.transform(X\_test)

  

  

\# declare a lista de modelos

models \= \[Lasso(), Ridge(), DecisionTreeRegressor(), RandomForestRegressor(), GradientBoostingRegressor()\]

  

  

\# a função que calcula MAPE

def mape(y\_true, y\_pred):

y\_error \= y\_true \- y\_pred

y\_error\_abs \= \[abs(i) for i in y\_error\]

perc\_error\_abs \= y\_error\_abs / y\_true

return (perc\_error\_abs.sum() / len(y\_true))

  

\# a função que leva o modelo e dados como métricas de entrada e saída

def make\_prediction(m, X\_train, y\_train, X\_test, y\_test):

model \= m

model.fit(X\_train, y\_train)

y\_pred \= model.predict(X\_test)

print('MAE:{:.2f} MSE:{:.2f} MAPE:{:.2f} R2:{:.2f} '.format(mean\_absolute\_error(y\_test, y\_pred),

mean\_squared\_error(y\_test, y\_pred),

mape(y\_test, y\_pred),

r2\_score(y\_test, y\_pred)))

  

\# escreva um ciclo que produza métrica para cada modelo

for i in models:

print(i)

make\_prediction(m\=i,X\_train \= X\_train\_st, y\_train\= y\_train,

X\_test\=X\_test\_st, y\_test \= y\_test)

  

\# treine o modelo final

final\_model \= GradientBoostingRegressor()

final\_model.fit(X\_train, y\_train)

Resultado

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-42-924Z.md
### Última modificação: 2025-05-28 20:29:43

# Conclusão - TripleTen

Capítulo 4/6

Resolvendo Tarefas Relacionadas a Aprendizado de Máquina

# Conclusão

Parabéns! Você realizou toda a jornada de um analista de dados que está trabalhando com um problema de predição e está aplicando ferramentas de aprendizado de máquina.

Você irá encontrar o conteúdo teórico deste capítulo e muito mais em nosso site de Base de Conhecimento.

[Folha de Conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/PT_Folha_de_Concluses_Resolvendo_Tarefas_Relacionadas_ao_Aprendizado_de_Mquina.pdf?etag=26891a1812adda6b4cd8b4f194c43392)

[Resumo do Capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/PT_Resumo_do_Captulo_Resolvendo_Tarefas_Relacionadas_com_Aprendizado_de_Mquina.pdf?etag=e32a63d4ec4302bbfce63703dd79adcc)

---

Tudo o que é deixado à sua frente agora é o seu projeto final! Antes que você prossiga, reserve um momento para se parabenizar por chegar tão longe. Você realmente fez algo impressionante!

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-44-286Z.md
### Última modificação: 2025-05-28 20:29:44

# Sprint 13 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 5/6

Projeto do Curso

# Sprint 13 - Projeto

Parabéns! Você completou a seção sobre fazer predições. É hora de aplicar o conhecimento e as habilidades que você adquiriu em um projeto: um estudo de caso analítico da vida real que você concluirá por conta própria.

Quando você terminar o projeto, envie seu trabalho para o revisor do projeto para avaliação. Ele te dará feedback dentro de 48 horas. Use o feedback para fazer alterações e, em seguida, envie a nova versão de volta ao revisor do projeto.

Você poderá receber mais feedback sobre a nova versão. Isso é completamente normal. Não é incomum passar por vários ciclos de feedback e revisão.

Seu projeto será considerado concluído assim que o revisor do projeto o aprovar.

# Conjuntos de dados

A rede de academias Model Fitness está desenvolvendo uma estratégia de interação com o cliente baseados em dados analíticos.

Um dos problemas mais comuns que academias e outros serviços enfrentam é a rotatividade de clientes. Como você sabe se um cliente não está mais com você? Você pode calcular a rotatividade baseado nas pessoas que se livram das suas contas ou não renovam seus contratos. No entanto, às vezes não é óbvio que um cliente saiu: eles podem sair de fininho.

Indicadores de rotatividade varia de área para área. Se um usuário compra de uma loja online raramente mas regularmente, você não pode dizer que é um fujão. Mas se por duas semanas eles não abriam o canal que é atualizado diariamente, essa é uma razão para se preocupar: seu seguidor pode ter ficado aborrecido e deixado você.

Para uma academia, faz sentido dizer que os clientes saíram se eles não aparecem há um mês. É claro, é possível que eles estejam em Cancun e virão tornar a frequentar quando retornarem, mas esse não é um caso comum. Normalmente, se um cliente ingressa, vem algumas vezes, depois desaparece, ele provavelmente não voltará.

Em vez de lutar com a rotatividade, Model Fitness digitalizou uma parte dos perfis dos clientes. Sua tarefa é analisá-los e vir com uma estratégia de retenção de clientes.

Você deve:

-   Aprender a predizer a probabilidade de rotatividade (para o mês seguinte) para cada cliente
-   Elabore retratos de usuários típicos: selecione os grupos mais marcantes e descreva suas principais características
-   Analise os fatores que mais impactam a rotatividade
-   Tire conclusões básicas e desenvolva recomendações sobre como melhorar o serviço de clientes:
    -   Identifique grupos alvo
    -   Sugira medidas para diminuir a rotatividade
    -   Descreva qualquer outro padrão que você vir com respeito às interações com clientes

## Instruções para completar o projeto

### Passo 1. Baixar os dados

Model Fitness forneceu a você dados CSV contendo dados sobre rotatividade em um determinado mês e informações sobre o mês anterior. O conjunto de dados inclui os seguintes campos:

-   `'Churn'` — a rotatividade do mês em questão
-   Campos de dados atuais:
    -   Dados do mês anterior
        -   `'gender'`
        -   `'Near_Location'` — se o cliente morar ou trabalhar na vizinhança onde a academia está localizada
        -   `'Partner'` — se o usuário for um funcionário de uma companhia parceira (a academia tem empresas parceiras cujos funcionários conseguem descontos; nesses casos, a academia armazena informações sobre clientes de são funcionários)
        -   `Promo_friends` — se o cliente originalmente se inscreveu através de uma oferta "traga um amigo" eles normalmente usam o código de promoção do amigo quando pagam pela primeira filiação)
        -   `'Phone'` — se o usuário fornece o seu número de telefone
        -   `'age'` (idade)
        -   `'Lifetime'` — o tempo (em meses) desde a primeira vez que o cliente veio à academia
-   Dados do log de frequência e compras e dados sobre status de filiação atual
    -   `'Contract_period'` — 1 mês, 3 meses, 6 meses, ou um ano
    -   `'Month_to_end_contract'` — os meses remanescentes até que o contrato expira
    -   `'Group_visits'` — se o cliente participa de sessões em grupo
    -   `'Avg_class_frequency_total'` — frequência média de idas por semana por toda a vida do cliente
    -   `'Avg_class_frequency_current_month'` — frequência média de visitas por semana durante o mês corrente
    -   `'Avg_additional_charges_total'` — a quantidade total de dinheiro gasto em outros serviços da academia: café, artigos esportivos, cosméticos, massagem, etc.

Caminho do arquivo: `/datasets/gym_churn_us.csv`. [Baixe o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/gym_churn_us.csv).

### Passo 2. Realize análise exploratória dos dados (AED)

-   Olhe para o conjunto de dados: ele contém alguma característica ausente? Estude a média de valores e desvio padrão (use o método `describe()`).
-   Observe a média dos valores médios das características em dois grupos: para aqueles que ficaram (use o método `groupby()`).
-   Faça histogramas de barra e distribuições de características para aqueles que saíram (rotatividade) e aqueles que ficaram.
-   Construa a matriz de correlação e a exiba.

### Passo 3. Construa um modelo para predizer a rotatividade de clientes

Construa um modelo de classificação binária para clientes onde a variável objetivo é a saída de usuários do próximo mês.

-   Divida os dados de treinamento e validação em dois conjuntos usando a função `train_test_split()`.
-   Treine o modelo no conjunto com dois métodos:
    -   regressão logística
    -   floresta aleatória
-   Avalie acurácia, precisão e sensibilidade para ambos os modelos usando dados de validação. Use-os para comparar os modelos. Qual modelo rendeu melhores resultados?

Lembre-se de indicar o parâmetro `random_state` quando dividir os dados e definir o algoritmo.

### Passo 4. Crie agrupamentos de clientes

Defina ao lado colunas com dados sobre rotatividade e identifique agrupamentos do objeto (cliente):

-   Padronize os dados.
-   Use a função `linkage()` para construir a matriz das distâncias baseada na matriz de características padronizada e construa um dendrograma. Perceba: renderizar o dendrograma pode demorar um tempo! Use o gráfico resultante para estimar o número de agrupamentos que você pode destacar.
-   Treine o modelo de agrupamento com o algoritmo K-means e preveja agrupamentos de clientes. (Deixe que o número de agrupamentos seja `n=5`,para que seja fácil comparar seus resultados com os de outros estudantes. No entanto, na vida real, ninguém vai te dar essas dicas, para que você tenha que decidir baseados nos gráficos das etapas anteriores.)
-   Olhe para os valores médios das características para agrupamentos. Nada chama a sua atenção?
-   Faça distribuições de características para os agrupamentos. Você notou alguma coisa?
-   Calcule a taxa de rotatividade para cada agrupamento (use o método `groupby()`). Eles diferem em termos de taxa de rotatividade? Quais agrupamentos são propensos a sair, e quais são leais?

### Passo 5. Chegue a conclusões e faça recomendações básicas sobre trabalhar com clientes

Tirar conclusões e formular recomendações sobre a estratégia de interação e retenção de clientes.

Você não precisa entrar em detalhes. Três ou quatro princípios essenciais e exemplos da sua implementação na forma de passos de marketing específicos vai funcionar.

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-83f329d2-acdf-4429-894f-4415e4f1259c.containerhub.tripleten-services.com/doc/tree/notebook.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-53-176Z.md
### Última modificação: 2025-05-28 20:29:53

# Sprint 13 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 5/6

Projeto do Curso

# Sprint 13 - Projeto

Parabéns! Você completou a seção sobre fazer predições. É hora de aplicar o conhecimento e as habilidades que você adquiriu em um projeto: um estudo de caso analítico da vida real que você concluirá por conta própria.

Quando você terminar o projeto, envie seu trabalho para o revisor do projeto para avaliação. Ele te dará feedback dentro de 48 horas. Use o feedback para fazer alterações e, em seguida, envie a nova versão de volta ao revisor do projeto.

Você poderá receber mais feedback sobre a nova versão. Isso é completamente normal. Não é incomum passar por vários ciclos de feedback e revisão.

Seu projeto será considerado concluído assim que o revisor do projeto o aprovar.

# Conjuntos de dados

A rede de academias Model Fitness está desenvolvendo uma estratégia de interação com o cliente baseados em dados analíticos.

Um dos problemas mais comuns que academias e outros serviços enfrentam é a rotatividade de clientes. Como você sabe se um cliente não está mais com você? Você pode calcular a rotatividade baseado nas pessoas que se livram das suas contas ou não renovam seus contratos. No entanto, às vezes não é óbvio que um cliente saiu: eles podem sair de fininho.

Indicadores de rotatividade varia de área para área. Se um usuário compra de uma loja online raramente mas regularmente, você não pode dizer que é um fujão. Mas se por duas semanas eles não abriam o canal que é atualizado diariamente, essa é uma razão para se preocupar: seu seguidor pode ter ficado aborrecido e deixado você.

Para uma academia, faz sentido dizer que os clientes saíram se eles não aparecem há um mês. É claro, é possível que eles estejam em Cancun e virão tornar a frequentar quando retornarem, mas esse não é um caso comum. Normalmente, se um cliente ingressa, vem algumas vezes, depois desaparece, ele provavelmente não voltará.

Em vez de lutar com a rotatividade, Model Fitness digitalizou uma parte dos perfis dos clientes. Sua tarefa é analisá-los e vir com uma estratégia de retenção de clientes.

Você deve:

-   Aprender a predizer a probabilidade de rotatividade (para o mês seguinte) para cada cliente
-   Elabore retratos de usuários típicos: selecione os grupos mais marcantes e descreva suas principais características
-   Analise os fatores que mais impactam a rotatividade
-   Tire conclusões básicas e desenvolva recomendações sobre como melhorar o serviço de clientes:
    -   Identifique grupos alvo
    -   Sugira medidas para diminuir a rotatividade
    -   Descreva qualquer outro padrão que você vir com respeito às interações com clientes

## Instruções para completar o projeto

### Passo 1. Baixar os dados

Model Fitness forneceu a você dados CSV contendo dados sobre rotatividade em um determinado mês e informações sobre o mês anterior. O conjunto de dados inclui os seguintes campos:

-   `'Churn'` — a rotatividade do mês em questão
-   Campos de dados atuais:
    -   Dados do mês anterior
        -   `'gender'`
        -   `'Near_Location'` — se o cliente morar ou trabalhar na vizinhança onde a academia está localizada
        -   `'Partner'` — se o usuário for um funcionário de uma companhia parceira (a academia tem empresas parceiras cujos funcionários conseguem descontos; nesses casos, a academia armazena informações sobre clientes de são funcionários)
        -   `Promo_friends` — se o cliente originalmente se inscreveu através de uma oferta "traga um amigo" eles normalmente usam o código de promoção do amigo quando pagam pela primeira filiação)
        -   `'Phone'` — se o usuário fornece o seu número de telefone
        -   `'age'` (idade)
        -   `'Lifetime'` — o tempo (em meses) desde a primeira vez que o cliente veio à academia
-   Dados do log de frequência e compras e dados sobre status de filiação atual
    -   `'Contract_period'` — 1 mês, 3 meses, 6 meses, ou um ano
    -   `'Month_to_end_contract'` — os meses remanescentes até que o contrato expira
    -   `'Group_visits'` — se o cliente participa de sessões em grupo
    -   `'Avg_class_frequency_total'` — frequência média de idas por semana por toda a vida do cliente
    -   `'Avg_class_frequency_current_month'` — frequência média de visitas por semana durante o mês corrente
    -   `'Avg_additional_charges_total'` — a quantidade total de dinheiro gasto em outros serviços da academia: café, artigos esportivos, cosméticos, massagem, etc.

Caminho do arquivo: `/datasets/gym_churn_us.csv`. [Baixe o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/gym_churn_us.csv).

### Passo 2. Realize análise exploratória dos dados (AED)

-   Olhe para o conjunto de dados: ele contém alguma característica ausente? Estude a média de valores e desvio padrão (use o método `describe()`).
-   Observe a média dos valores médios das características em dois grupos: para aqueles que ficaram (use o método `groupby()`).
-   Faça histogramas de barra e distribuições de características para aqueles que saíram (rotatividade) e aqueles que ficaram.
-   Construa a matriz de correlação e a exiba.

### Passo 3. Construa um modelo para predizer a rotatividade de clientes

Construa um modelo de classificação binária para clientes onde a variável objetivo é a saída de usuários do próximo mês.

-   Divida os dados de treinamento e validação em dois conjuntos usando a função `train_test_split()`.
-   Treine o modelo no conjunto com dois métodos:
    -   regressão logística
    -   floresta aleatória
-   Avalie acurácia, precisão e sensibilidade para ambos os modelos usando dados de validação. Use-os para comparar os modelos. Qual modelo rendeu melhores resultados?

Lembre-se de indicar o parâmetro `random_state` quando dividir os dados e definir o algoritmo.

### Passo 4. Crie agrupamentos de clientes

Defina ao lado colunas com dados sobre rotatividade e identifique agrupamentos do objeto (cliente):

-   Padronize os dados.
-   Use a função `linkage()` para construir a matriz das distâncias baseada na matriz de características padronizada e construa um dendrograma. Perceba: renderizar o dendrograma pode demorar um tempo! Use o gráfico resultante para estimar o número de agrupamentos que você pode destacar.
-   Treine o modelo de agrupamento com o algoritmo K-means e preveja agrupamentos de clientes. (Deixe que o número de agrupamentos seja `n=5`,para que seja fácil comparar seus resultados com os de outros estudantes. No entanto, na vida real, ninguém vai te dar essas dicas, para que você tenha que decidir baseados nos gráficos das etapas anteriores.)
-   Olhe para os valores médios das características para agrupamentos. Nada chama a sua atenção?
-   Faça distribuições de características para os agrupamentos. Você notou alguma coisa?
-   Calcule a taxa de rotatividade para cada agrupamento (use o método `groupby()`). Eles diferem em termos de taxa de rotatividade? Quais agrupamentos são propensos a sair, e quais são leais?

### Passo 5. Chegue a conclusões e faça recomendações básicas sobre trabalhar com clientes

Tirar conclusões e formular recomendações sobre a estratégia de interação e retenção de clientes.

Você não precisa entrar em detalhes. Três ou quatro princípios essenciais e exemplos da sua implementação na forma de passos de marketing específicos vai funcionar.

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-83f329d2-acdf-4429-894f-4415e4f1259c.containerhub.tripleten-services.com/doc/tree/notebook.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-55-645Z.md
### Última modificação: 2025-05-28 20:29:55

# Sprint 13 - Projeto - TripleTen

DescriçãoEnvio

Capítulo 5/6

Projeto do Curso

# Sprint 13 - Projeto

Parabéns! Você completou a seção sobre fazer predições. É hora de aplicar o conhecimento e as habilidades que você adquiriu em um projeto: um estudo de caso analítico da vida real que você concluirá por conta própria.

Quando você terminar o projeto, envie seu trabalho para o revisor do projeto para avaliação. Ele te dará feedback dentro de 48 horas. Use o feedback para fazer alterações e, em seguida, envie a nova versão de volta ao revisor do projeto.

Você poderá receber mais feedback sobre a nova versão. Isso é completamente normal. Não é incomum passar por vários ciclos de feedback e revisão.

Seu projeto será considerado concluído assim que o revisor do projeto o aprovar.

# Conjuntos de dados

A rede de academias Model Fitness está desenvolvendo uma estratégia de interação com o cliente baseados em dados analíticos.

Um dos problemas mais comuns que academias e outros serviços enfrentam é a rotatividade de clientes. Como você sabe se um cliente não está mais com você? Você pode calcular a rotatividade baseado nas pessoas que se livram das suas contas ou não renovam seus contratos. No entanto, às vezes não é óbvio que um cliente saiu: eles podem sair de fininho.

Indicadores de rotatividade varia de área para área. Se um usuário compra de uma loja online raramente mas regularmente, você não pode dizer que é um fujão. Mas se por duas semanas eles não abriam o canal que é atualizado diariamente, essa é uma razão para se preocupar: seu seguidor pode ter ficado aborrecido e deixado você.

Para uma academia, faz sentido dizer que os clientes saíram se eles não aparecem há um mês. É claro, é possível que eles estejam em Cancun e virão tornar a frequentar quando retornarem, mas esse não é um caso comum. Normalmente, se um cliente ingressa, vem algumas vezes, depois desaparece, ele provavelmente não voltará.

Em vez de lutar com a rotatividade, Model Fitness digitalizou uma parte dos perfis dos clientes. Sua tarefa é analisá-los e vir com uma estratégia de retenção de clientes.

Você deve:

-   Aprender a predizer a probabilidade de rotatividade (para o mês seguinte) para cada cliente
-   Elabore retratos de usuários típicos: selecione os grupos mais marcantes e descreva suas principais características
-   Analise os fatores que mais impactam a rotatividade
-   Tire conclusões básicas e desenvolva recomendações sobre como melhorar o serviço de clientes:
    -   Identifique grupos alvo
    -   Sugira medidas para diminuir a rotatividade
    -   Descreva qualquer outro padrão que você vir com respeito às interações com clientes

## Instruções para completar o projeto

### Passo 1. Baixar os dados

Model Fitness forneceu a você dados CSV contendo dados sobre rotatividade em um determinado mês e informações sobre o mês anterior. O conjunto de dados inclui os seguintes campos:

-   `'Churn'` — a rotatividade do mês em questão
-   Campos de dados atuais:
    -   Dados do mês anterior
        -   `'gender'`
        -   `'Near_Location'` — se o cliente morar ou trabalhar na vizinhança onde a academia está localizada
        -   `'Partner'` — se o usuário for um funcionário de uma companhia parceira (a academia tem empresas parceiras cujos funcionários conseguem descontos; nesses casos, a academia armazena informações sobre clientes de são funcionários)
        -   `Promo_friends` — se o cliente originalmente se inscreveu através de uma oferta "traga um amigo" eles normalmente usam o código de promoção do amigo quando pagam pela primeira filiação)
        -   `'Phone'` — se o usuário fornece o seu número de telefone
        -   `'age'` (idade)
        -   `'Lifetime'` — o tempo (em meses) desde a primeira vez que o cliente veio à academia
-   Dados do log de frequência e compras e dados sobre status de filiação atual
    -   `'Contract_period'` — 1 mês, 3 meses, 6 meses, ou um ano
    -   `'Month_to_end_contract'` — os meses remanescentes até que o contrato expira
    -   `'Group_visits'` — se o cliente participa de sessões em grupo
    -   `'Avg_class_frequency_total'` — frequência média de idas por semana por toda a vida do cliente
    -   `'Avg_class_frequency_current_month'` — frequência média de visitas por semana durante o mês corrente
    -   `'Avg_additional_charges_total'` — a quantidade total de dinheiro gasto em outros serviços da academia: café, artigos esportivos, cosméticos, massagem, etc.

Caminho do arquivo: `/datasets/gym_churn_us.csv`. [Baixe o conjunto de dados](https://practicum-content.s3.us-west-1.amazonaws.com/datasets/gym_churn_us.csv).

### Passo 2. Realize análise exploratória dos dados (AED)

-   Olhe para o conjunto de dados: ele contém alguma característica ausente? Estude a média de valores e desvio padrão (use o método `describe()`).
-   Observe a média dos valores médios das características em dois grupos: para aqueles que ficaram (use o método `groupby()`).
-   Faça histogramas de barra e distribuições de características para aqueles que saíram (rotatividade) e aqueles que ficaram.
-   Construa a matriz de correlação e a exiba.

### Passo 3. Construa um modelo para predizer a rotatividade de clientes

Construa um modelo de classificação binária para clientes onde a variável objetivo é a saída de usuários do próximo mês.

-   Divida os dados de treinamento e validação em dois conjuntos usando a função `train_test_split()`.
-   Treine o modelo no conjunto com dois métodos:
    -   regressão logística
    -   floresta aleatória
-   Avalie acurácia, precisão e sensibilidade para ambos os modelos usando dados de validação. Use-os para comparar os modelos. Qual modelo rendeu melhores resultados?

Lembre-se de indicar o parâmetro `random_state` quando dividir os dados e definir o algoritmo.

### Passo 4. Crie agrupamentos de clientes

Defina ao lado colunas com dados sobre rotatividade e identifique agrupamentos do objeto (cliente):

-   Padronize os dados.
-   Use a função `linkage()` para construir a matriz das distâncias baseada na matriz de características padronizada e construa um dendrograma. Perceba: renderizar o dendrograma pode demorar um tempo! Use o gráfico resultante para estimar o número de agrupamentos que você pode destacar.
-   Treine o modelo de agrupamento com o algoritmo K-means e preveja agrupamentos de clientes. (Deixe que o número de agrupamentos seja `n=5`,para que seja fácil comparar seus resultados com os de outros estudantes. No entanto, na vida real, ninguém vai te dar essas dicas, para que você tenha que decidir baseados nos gráficos das etapas anteriores.)
-   Olhe para os valores médios das características para agrupamentos. Nada chama a sua atenção?
-   Faça distribuições de características para os agrupamentos. Você notou alguma coisa?
-   Calcule a taxa de rotatividade para cada agrupamento (use o método `groupby()`). Eles diferem em termos de taxa de rotatividade? Quais agrupamentos são propensos a sair, e quais são leais?

### Passo 5. Chegue a conclusões e faça recomendações básicas sobre trabalhar com clientes

Tirar conclusões e formular recomendações sobre a estratégia de interação e retenção de clientes.

Você não precisa entrar em detalhes. Três ou quatro princípios essenciais e exemplos da sua implementação na forma de passos de marketing específicos vai funcionar.

Enviar projeto

Revisar progresso

Parabéns! Você passou na revisão e pode continuar avançando.

Como foi a avaliação?

<iframe class="project-workspace__iframe" src="https://cnt-83f329d2-acdf-4429-894f-4415e4f1259c.containerhub.tripleten-services.com/doc/tree/notebook.ipynb" allow="clipboard-read; clipboard-write"></iframe>

Você não pode fazer alterações depois que for aprovado.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-29-59-101Z.md
### Última modificação: 2025-05-28 20:29:59

# Feedback do Sprint 13 - TripleTen

Capítulo 6/6

Conclusão

# Compartilhe sua opinião

**Olá!**

Você acabou de concluir um sprint e enviar seu projeto. Parabéns por alcançar este marco! 🚀

Agora é o momento perfeito para dar seu feedback sobre o sprint. Sua opinião vai nos ajudar a aprimorar ainda mais o processo de aprendizagem.

Este questionário curto vai levar apenas 2 minutos. Todas as respostas vão ser confidenciais: não vamos compartilhá-las de forma que identifique você.

Agradecemos por nos ajudar a evoluir com você!

![](https://practicum-content.s3.us-west-1.amazonaws.com/common/ok.svg)

Muito obrigado pelas suas respostas!

Já as encaminhamos para os departamentos correspondentes.

7 — 7

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-30-00-465Z.md
### Última modificação: 2025-05-28 20:30:00

# Conclusão - TripleTen

Capítulo 6/6

Conclusão

# Conclusão

Parabéns! Você terminou o curso sobre Previsões e Predições. Essa é uma baita conquista — essa parte não é fácil!

O conhecimento que você adquiriu vai ajudar você a conversar com colegas de ciência de dados que estão no corredor.

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-30-01-750Z.md
### Última modificação: 2025-05-28 20:30:02

# Guru de Decisões de Negócios - TripleTen

Capítulo 6/6

Conclusão

# Guru de Decisões de Negócios

Agora você é um verdadeiro Guru de Decisões de Negócios! Agora você pode dar conselhos a empresas de todos os tipos para que elas tomem decisões melhores e ajudar qualquer negócio a crescer e melhorar, tomando decisões com base nos dados que você descobriu! Isso é incrível, não é?

### Compartilhe sua nova habilidade

É hora de mostrar sua nova habilidade no LinkedIn. Você pode mostrar suas habilidades a potenciais empregadores mostrando seus projetos de portfólio, e também obter feedback de amigos e colegas no GitHub.

Fique à vontade para copiar a medalha e o texto do bloco abaixo para o seu post (claro, você pode personalizar o texto como quiser)!

Certifique-se de marcar [@TripleTenBrasil](https://www.linkedin.com/school/tripleten-brasil/) para que possamos compartilhar a sua conquista.

Jonathas Martins da Rocha

Ei! Agora sou um Guru de Decisões de Negócios e posso aconselhar qualquer pessoa a tomar decisões baseadas em dados e ajudar empresas a desenvolver seus negócios com sucesso #TripleTen #TripleTenBrasil

![](https://practicum-content.s3.amazonaws.com/resources/Dashboards_Whiz_PT_1_1687160293.png)

Copiar textoSalvar imagem[Criar um post no LinkdIn](https://www.linkedin.com/feed/)

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-30-04-031Z.md
### Última modificação: 2025-05-28 20:30:04

# Questionário sobre situação empregatícia - TripleTen

Capítulo 6/6

Conclusão

# Questionário sobre situação empregatícia

![](https://practicum-content.s3.us-west-1.amazonaws.com/common/ok.svg)

7 — 7

## Embedded Content

### Caminho: C:\Users\jonat\Dropbox\Obsidian\TripleTen\webpage-2025-05-28T23-30-05-308Z.md
### Última modificação: 2025-05-28 20:30:06

# Conclusão - TripleTen

Capítulo 3/6

Algoritmos de Aprendizado de Máquina

# Conclusão

Esse capítulo foi bem intenso.

Você se familiarizou com seis algoritmos populares. Você aprendeu como eles funcionam, viu o que eles produzem e descobriu seus pontos fracos e fortes. Você tem seis ferramentas vitais de ciência de dados. Agora é hora de aplicá-los para resolver uma tarefa real de negócios. Para isso, no próximo capítulo, você passará por todo o pipeline de desenvolvimento de um modelo de aprendizado de máquina. Você aprenderá como está estruturado e dominará os métodos que discutimos. A parte mais interessante virá no final: você fará uma previsão e explicará por que ela é boa. Veja por que os analistas estudam o aprendizado de máquina!

Links para ajudá-lo a dominar o material e aprofundar sua compreensão _(os materiais estão em inglês)_:

-   Informação geral sobre os algoritmos de ML:
    -   [https://www.ibm.com/topics/machine-learning-algorithms](https://www.ibm.com/topics/machine-learning-algorithms)
    -   [https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/lecture-videos/lecture-11-introduction-to-machine-learning/](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/lecture-videos/lecture-11-introduction-to-machine-learning/)
-   Regressão linear e gradiente descendente
    -   [https://machinelearningmastery.com/linear-regression-for-machine-learning/](https://machinelearningmastery.com/linear-regression-for-machine-learning/)
    -   [https://www.youtube.com/watch?v=qlLChbHhbg4&feature=youtu.be&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&t=50m16s](https://www.youtube.com/watch?v=qlLChbHhbg4&feature=youtu.be&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&t=50m16s)
-   Regressão logística
    -   [https://ml-cheatsheet.readthedocs.io/en/latest/logistic\_regression.html](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html)
-   Não analisamos algoritmos como KNN neste curso. Você pode ler mais sobre isso aqui:
    -   [https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/](https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/)
-   Árvores de decisão
    -   [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html)
    -   [https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb](https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb)
-   Conjuntos/floresta aleatória:
    -   [https://machinelearningmastery.com/random-forest-ensemble-in-python/](https://machinelearningmastery.com/random-forest-ensemble-in-python/)
-   Сonjuntos/gradient boosting:
    -   [https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)
-   O gradient boosting é geralmente realizado com CatBoost e LightGBM. Você pode ler mais sobre isso aqui:
    -   CatBoost: [https://catboost.ai/docs/concepts/about.html](https://catboost.ai/docs/concepts/about.html)
    -   LightGBM: [https://github.com/Microsoft/LightGBM](https://github.com/Microsoft/LightGBM)
-   Agrupamento:
    -   [https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/video-lectures/lecture-35-finding-clusters-in-graphs-second-project-handwriting/](https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/video-lectures/lecture-35-finding-clusters-in-graphs-second-project-handwriting/)
-   Reduzir a dimensionalidade:
    -   [https://www.geeksforgeeks.org/dimensionality-reduction/](https://www.geeksforgeeks.org/dimensionality-reduction/)

---

Faça download do [sumário do capítulo](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/Resumo_do_Captulo_Algoritmos_do_Aprendizado.pdf?etag=36137ad408cd0aaf843d2a67c18e719e) e [folha de conclusões](https://practicum-content.s3.us-west-1.amazonaws.com/new-markets/DA_sprint_12/prt/PT_Folha_de_Concluses_Algoritmos_do_Aprendizado_de_Mquina.pdf?etag=f93e27e45df2553eba9b4e880df5f236) da Base de Conhecimento para que você possa consultá-los quando necessário.

## Embedded Content